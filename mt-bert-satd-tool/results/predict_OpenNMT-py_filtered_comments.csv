Commit Message,predict
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '6.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
Build the translator (along with the model),0
Put messages sizes in antichronological order,0
Caluculate antichronological history sizes,0
Prune the history from the beginning,0
Put back indices in chronological order.,0
Build the translator (along with the model),0
We need to build the Llama tokenizer to count tokens and prune the history.,0
The hypotheses are lists of one element but we still need to take the first one.,0
#####,0
UI #,0
#####,0
What are the 3 best french cities ?,0
Which one is better if I like outdoor activities ?,0
Which one is better if I like cultural outings?,0
What are the best neighborhoods in these 5 cities?,0
!/usr/bin/env python3,0
Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens,0
flake8: noqa,0
-*- coding: utf-8 -*-,0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: sentencepiece_model.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(module_scope),0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
!/usr/bin/env python,0
if shard == 0:,0
"vocab_size = onmt_safetensor[""generator.weight""].size(0)",0
"vocab[11] = ""</s>""  # Falcon only",0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
from onmt.utils.misc import use_gpu,0
"Add in default model arguments, possibly added since training.",0
this patch is no longer needed included in converter,0
"if hasattr(model_opt, 'rnn_size'):",0
model_opt.hidden_size = model_opt.rnn_size,0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
Falcon stores QKV in one single tensor but it is not simply piled up Q+K+V,0
it is heads interleaved to we need to slice first,0
also it uses the HF rotary so we need to permute Q and K interleave,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python3,0
coding: utf-8,0
"In order to use this tool, please install comet first",0
https://github.com/Unbabel/COMET,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate source and reference sentences nbest times,0
for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
same for source.,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \,0
--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best comet score,0
when choosing a reference-less model no nbest-ref is required,0
for nbest in nbests:,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python mbr_bleu.py --nbest-hyp target.5.tl \,0
--nbest-order 5 --output target.mbr.tl,0
It will compare all hyp with eachother and output the max bleu,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
Now we can pipe the full file through the model using the Iterator,0
reminder a batch includes .src .tgt .indices and it is sorted,0
print(batch),0
Compute and retrieve the loss for EACH sentence,0
Now we need to rearrange the batch of ppl,0
in the original order with indices,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate reference sentences nbest times for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \,0
--nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best bleu wrt reference,0
and output the max bleu,0
!/usr/bin/env python,0
!/usr/bin/env python,0
with the two module = imp.load_source() below,0
we ghost the old torchtext.data.field and depercated,0
onmt.inputters.text_dataset,0
however this require some functions / classes to be,0
monkey patched for loading the old field/vocab objects.,0
"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
!/usr/bin/env python,0
redpajama stores QKV in one single tensor but it is not simply piled up Q+K+V,0
it is heads interleaved to we need to slice first,0
also it uses the HF rotary so we need to permute Q and K interleave,0
Avoid functionality on inference,0
weights are in the .pt file,0
weights are not in the .pt checkpoint but stored in the safetensors file,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
Remove old vocabulary associated embeddings,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
If new training initialize the model params,0
If update_vocab init also but checkpoint will overwrite old weights,0
ONLY for legacy fusedam with amp pytorch requires NOT to half the model,0
Update model embeddings with those from the checkpoint,0
after initialization,0
after this checkpoint contains no embeddings,0
when using LoRa or updating the vocab (no more embeddings in ckpt),0
=> strict=False when loading state_dict,0
weights are in the .pt file,0
weights are not in the .pt checkpoint but stored in the safetensors file,0
!/usr/bin/env python,0
"maybe prepare pretrained embeddings, if any",0
if transform + options set in 'valid' we need to copy in main,0
transform / options for scoring considered as inference,0
Load checkpoint if we resume from a previous training.,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
Override checkpoint's freezing settings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
GPU,0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
Freeze Encoder and/or Decoder,0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
LoRa,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options related to source and target features,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Adding options related to Transforms,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Create a thread to listen for errors in the child processes.,0
Build translator,0
Build vocab,0
Build transform pipe,0
Basic attributes.,0
Set model in training mode.,0
Let's clean the GPUs before training loop,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
raw_srcs = [],0
raw_refs = [],0
F-prop through the model.,0
Compute loss.,0
Compute validation metrics (at batch.dataset level),0
Compute stats,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
The loss of the prompt will be set to zero.,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
src lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Auto import python files in this directory,0
batch x len x dim,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
Padding mask is now (batch x 1 x slen x slen),0
1 to be expanded to number of heads in MHA,0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
Help functions for Rotary Embeddings,0
https://arxiv.org/pdf/2104.09864.pdf,0
too convoluted to make maxseqlen a parameter.,0
we suppose src_seq_len at training and max_length at inference,0
are both < 2048 tokens.,0
"rope is now matrix [maxseqlen, dim/2]",0
Help functions for max_relative positions,0
https://arxiv.org/abs/1803.02155,0
Shift values to be >= 0,0
"now relative_position is in the range [0, inf)",0
half of the buckets are for exact increments in positions,0
The other half of the buckets are for logarithmically bigger bins in positions,0
up to max_distance,0
Help functions to split model dim per head,0
https://arxiv.org/pdf/1803.02155.pdf,0
in the paper they suggest either two embeds,0
relative_key / relative_value or only,0
relative_key. We implemented the same embed,0
for both.,0
"1) Project key, value, and query.",0
as a reminder at training layer_cache[0] remains False,0
Retrieve keys and values from the KV cache (decoding mode only).,0
Resize rotary embeddings.,0
Resize rotary embeddings.,0
We take a margin of 32 tokens as the kv_cache,0
is incremented by 32 tokens every 32 tokens.,0
Increase the cached key pad mask by concatenation.,0
For decoding only.,0
Retrieve keys and values from linear layers (training mode).,0
Resize rotary embeddings.,0
expand key on heads dimension when it's less than query heads (multi-query variant),0
expand value on heads dimension when it's less than query heads (multi-query variant),0
"2) When standard pos. enc. or rotary, use flash attention",0
Ultimately flashv2 will be part of pytorch https://github.com/pytorch/pytorch/pull/105602,0
"In the meantime: if vanilla tranformer or Rotary embeddings (not rel_pos, not alibi)",0
then use flash2 if seq len > 256 otherwise use xtransformer from pt2 uptream,0
Apply flash2 attention.,0
Apply scaled dot product attention.,0
batch x num_heads x query_len x key_len,0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
not 100% necessary but expand to nb of heads,0
now mask and scores have the same shape,0
3) Apply attention dropout and compute context vectors.,0
We use the same embeddings for key and value,0
--------------------------------------------------------------------------,0
copied and adapted https://github.com/microsoft/LoRA/,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License (MIT).,0
Support bnb quantization of nderlying layers,0
--------------------------------------------------------------------------,0
Optional dropout,0
Mark the weight as unmerged,0
LoRA implemented in a dense layer,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
we do not super().reset_parameters() save lot of time and useless when no grad.,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
cannot merge/unmerge quantized weigts with unquantized lora_X,0
Check if QLoraLinear has a custom __init__ method,0
Invoke the __init__ method of QLoraLinear,0
LoRA implemented in a dense layer,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
for some reason list comprehension is slower in this scenario,0
"for silu, see: https://arxiv.org/pdf/2002.05202.pdf",0
-*- coding: utf-8 -*-,0
class AverageAttention(torch.jit.ScriptModule):,0
@torch.jit.script,0
Code taken from bitsandbytes but modified with arg device to accept skipt_init,0
from torch.nn.utils => makes model building way faster.,0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
We exclude tokenization for contractions in,0
order to avoid inconsistencies with pyonmtok's tokenization.,0
"(e.g. ""I ca n't"" with spacy, ""I can ' t"" with pyonmttok)",0
Use Spacy's stopwords to get rid of junk entries,0
Perform tokenization with spacy for consistency.,0
We ensure that the target lemma is present in the lemmatized,0
"target string, that the match is an exact match (there is",0
whitespace before or after the term),0
and we perform some bound checking.,0
Map the lemmatized string match index to,0
the lemmatized list index,0
We need to know if the term is multiword,0
Join multiword target lemmas with a unique separator so,0
we can treat them as single word and not change the indices.,0
Construct the final source from the lemmatized list,0
that contains the terms. We compare the tokens in the,0
term-augmented lemma list with the tokens in the original,0
"lemma list. If the lemma is the same, then we replace with",0
the token from the original tokenized source list. If they,0
"are not the same, it means the lemma has been augemented",0
"with a term, so we inject this in the final list.",0
Restore the spaces in multi-word terms,0
Skip half examples to improve performance. This means we set,0
"a hard limit for the `term_corpus_ratio` to 0.5, which is actually",0
quite high. TODO: We can add this (skipping examples) as an option,1
Filter out very short or very long sentences,0
from the TM for better performance,0
We split the `batch` and perform fuzzy matching,0
in smaller chunks of 10.000 examples in order to,0
reduce memory usage.,0
Perfomance is not affected.,0
Probably redundant but let's be safe,0
in case some examples are already fuzzied,0
(e.g. from another pipeline or workflow),0
We don't want exact matches,0
Apply a basic filtering to leave out very short or very long,0
sentences and speed up things a bit during fuzzy matching,0
Do nothing,0
We set the start number of tags to a random number from 1,0
to 12 + the number of subsequent tags that,0
will be added. We also apply weights to this choice so tags,0
"are more probable to start from 1, then from 2, etc.",0
This way we cover most scenarios met in real usage and,0
the system will learn to handle a fairly large number of,0
numbered tags (but not an excessively large number),0
Make sure we only search for exact matches (we don't want,0
to match part of words) and perform some bound checking,0
Create all possible tag forms. We inject a special,0
unicode char (∥) as a placeholder for whitespace in order,0
to keep the indices unaltered. This char is replaced with,0
spaces before we return the augmented examples.,0
Make a weighted choice between paired tags or single tags.,0
"We usually encounter, and thus here we favor, paired tags",0
with a ratio 1/3.,0
Check if the tags include the,0
"mandatory ""#"" number placeholder""",0
We split the user-defined tags in the # placeholder,0
in order to number them,0
Skip half examples to speed up the transform. This sets,0
"a hard limit of 0.5 to the `tags_corpus_ratio`, which is",0
excessive and should be avoided anyway.,0
normalize dict src/tgt for each dataset,0
"print(""src empty"")",0
"print(""too many same char in src"")",0
"print(""too many same word in src"")",0
"print(""avg token min"", len(src_str) / len(ex['src']))",0
"print(""avg token max"", len(src_str) / len(ex['src']))",0
"print(""text does not fully belong to wanted script"")",0
"print(""Some text belong to unwanted scripts"")",0
"print(""langid does not match"", _id(src_str))",0
"print(""src = tgt"")",0
"print(""tgt empty"")",0
"print(""src / tgt ratio "", len(src_str) / len(tgt_str))",0
"print(""too many same char in tgt"")",0
"print(""too many same word in tgt"")",0
"print(""avg token min"", len(tgt_str) / len(ex['tgt']))",0
"print(""avg token max"", len(tgt_str) / len(ex['tgt']))",0
"print(""text does not fully belong to wanted script"")",0
"print(""Some text belong to unwanted scripts"")",0
"print(""langid does not match"", _id(tgt_str))",0
"doc break we add it, restart new doc",0
case 1st ex is already longer,0
adding cur ex is too long we add cur doc,0
and reset doc to cur ex,0
we start the new doc with cur ex,0
we cumulate cur ex to cur doc,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
This method embeds a custom logic to correctly handle certain placeholders,0
in case the tokenizer doesn't preserve them.,0
Locate the end-of-sentence placeholders.,0
Tokenize each sentence separately.,0
Locate the mask-before placeholders,0
(to zero-out the prompt loss during LM finetuning).,0
Tokenize each chunk separately and insert the padding token.,0
between each sequence of tokens.,0
Re-insert the eos token.,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
Load vocabulary file if provided and set threshold,0
Load Subword Model,0
"ugly patch to make sure ""\n\n"" is split in two items",0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
prefix src/tgt for each dataset,0
prefix as general option for inference,0
suffix src/tgt for each dataset,0
suffix as general option for inference,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Most code taken from: https://github.com/alvations/sacremoses,0
Which in turn is based on the Moses punctuation normalizer.,0
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/,0
tokenizer/normalize-punctuation.perl,0
don't fix period at end of sentence,0
Regex substitutions from replace-unicode-punctuation.perl,0
https://github.com/moses-smt/mosesdecoder/blob/master/,0
scripts/tokenizer/replace-unicode-punctuation.perl,0
Adds the penn substitutions after extra_whitespace regexes.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
Actual normalization.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
normalize dict src/tgt for each dataset,0
One source feature expected but none given and no default provided,0
Provided default does not match required features,0
Data not properly annotated.,0
In this case we do not use the default as it might be an error,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
make sure the scalars are in the event accumulator tags,0
required arguments,0
transforms that require vocab will not create if not provide vocab,0
1. Init first transform in the pipe,0
2. Init second transform in the pipe,0
3. Sequential combine them into a transform pipe,0
4. apply transform pipe for example,0
"5. example after the pipe exceed the length limit, thus filtered",0
6. Transform statistics registed (here for filtertoolong),0
"7. after report, statistics become empty as a fresh start",0
filter_transform.warm_up(),0
test BPE-dropout:,0
1. disable bpe dropout for not training example,0
2. enable bpe dropout for training example,0
3. (NOTE) disable dropout won't take effect if already seen,0
this is caused by the cache mechanism in bpe:,0
return cached subword if the original token is seen when no dropout,0
test SP regularization:,0
1. enable regularization for training example,0
2. disable regularization for not training example,0
Test mask location,0
Test mask location,0
Test mask location,0
Not apply token drop for not training example,0
apply token drop for training example,0
Not apply token mask for not training example,0
apply token mask for training example,0
require vocabs to warm_up,0
Not apply token mask for not training example,0
apply token mask for training example,0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
random_ratio of inserted tokens are chosen in vocab,0
others are MASK_TOK,0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
all token are considered as an individual word,0
1. tokens are dropped when replace_length is 0,0
"print(f""token delete: {masked} / {tokens}"")",0
2. tokens are replaced by MASK when replace_length is 1,0
"print(f""token mask: {masked} / {tokens}"")",0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
"1. replace_length 0: ""words"" are dropped",0
"print(f""word delete: {masked} / {tokens}"")",0
"self.assertEqual(len(masked), n_words - n_masked)",0
"2. replace_length 1: ""words"" are replaced with a single MASK",0
"print(f""whole word single mask: {masked} / {tokens}"")",0
len(masked) depend on number of tokens in select word,0
"3. replace_length -1: all tokens in ""words"" are replaced with MASK",0
"print(f""whole word multi mask: {masked} / {tokens}"")",0
number of mask_tok depend on number of tokens in selected word,0
number of MASK_TOK can be greater than n_masked,0
"insert_ratio=0.5,",0
"random_ratio=0.3,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
n_words = sum(token_starts),0
n_masked = math.ceil(n_words * bart_noise.mask_ratio),0
"print(f""Text Span Infilling: {infillied} / {tokens}"")",0
"print(n_words, n_masked)",0
Build the translator (along with the model),0
Required arguments,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
we finish 3 hyps per example in this step,0
new beam 1 is old beam 3,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('hidden_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
"attns[""coverage""] is actually c^(t+1) of See et al(2017)",0
1-index shifted,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Auto import python files in this directory,0
src_len is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate src_len as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
"Add triangular future_mask and pad_mask, result mask in (B, T, T).",0
Patch for scaled dot product attention.,0
"Only mask padding, result mask in (B, 1, T).",0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x s or t len),0
1 = heads to be expanded in MHA,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
first value set to True triggered by the beginning of decoding,0
layer_cache becomes active in the MultiHeadedAttention fwd,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
Masking is necessary when sequence length is greater than one,0
"The decoding has not started yet,",0
we compute the scores on the source tokens in one shot.,0
mask now are (batch x 1 x tlen x tlen),0
1 = heads to be expanded in MHA,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
decoding mode.,0
Initialize KV and key_pad_mask cache.,0
training mode.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"print(filled, sz)",0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
"propagate exception to parent process, keeping original traceback",0
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use apex.amp,0
In this case use the old FusedAdam with,0
FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
should be: self._optimizer.zero_grad(set_to_none),0
but apex.amp is not up-to-date:,0
https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
placing this here make it easier to call logger.info,0
"from anywhere, just 'from onmt.utils.logging import logger'",0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
Create a mask with zeros at prompt positions and ones at answer postions.,0
Apply the mask on the target side.,0
Put the padding token index at the prompt positions.,0
take into account here the tgt_shift_index (0 / 1 = LM/NMT),0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
in the case criterion reduction is None then we need,0
to sum the loss of each sentence in the batch,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check weight,0
Check features,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
It comes from training,0
TODO: needs to be added as inference opt,1
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
"this param init is overridden by model_builder, useless then.",0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
Helper functions,0
Keeps track of the original words/subwords,0
('prior_tokenization' option),0
In case there is a final case_markup when new_spacer is on,0
########## #,0
Translator #,0
########## #,0
"Set ""default"" translation options on empty cfgfile",0
Build translator from options,0
################### #,0
Validation iterator #,0
################### #,0
Reinstantiate the validation iterator,0
transforms_cls = get_transforms_cls(model_opt._all_transform),0
Retrieve raw references and sources,0
########### #,0
Predictions #,0
########### #,0
####### #,0
Outputs #,0
####### #,0
Flatten predictions,0
Save results,0
-*- coding: utf-8 -*-,0
this one is needed for Random Shuffler of batches,0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
we need to check the model path + any tokenizer path,0
patch to log stdout spawned processes of dataloader,0
bucket_size = batch_size,0
For TRAIN we shuffle batches within the bucket,0
otherwise sequential,0
for specific case of rnn_packed need to be sorted,0
within the batch,0
single thread - create batch directly on GPU if device is gpu,0
multithread faster to create batch on CPU in each thread and then move it to gpu,0
Move tensor_batch from cpu to device,0
Check if all tokens have features or none at all,0
Make features part of src like,0
"{'src': {'src': ..., 'feats': [...., ....]}}",0
careful below it will return a bucket sorted by corpora,0
but we sort by length later and shuffle batches,0
at this point an example looks like:,0
"{'src': {'src': ..., 'feats': [....]},",0
"'tgt': {'tgt': ...},",0
"'src_original': ['tok1', ...'tokn'],",0
"'tgt_original': ['tok1', ...'tokm'],",0
'cid': corpus id,0
'cid_line_number' : cid line number,0
"'align': ...,",0
},0
Need to add features in last dimensions,0
Keep it consistent with dynamic data,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
-*- coding: utf-8 -*-,0
this is hack: if the special separator ｟newline｠is returned because of the,1
"""docify"" transform.get_specials we don't add it if the corresponding newline code",0
is already included in the sentencepiece or BPE-with-gpt2-pretok.,0
Reached end of file,0
'src_original' and 'tgt_original' store the,0
original line before tokenization. These,0
fields are used later on in the feature,0
transforms.,0
empty example: skip,0
ugly patch because in_feat and out_feat are reversed in WQLinear_GEMM,1
bitsandbytes quantize weights when .cuda() is called,0
for huge models we need to save Ram,0
so we load the weights  module by module and transfer them to GPU for quantization,0
bitsandbytes quantize weights when .cuda() is called,0
for huge models we need to save Ram,0
so we load the weights  module by module and transfer them to GPU for quantization,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
This preserves backward-compat for models using customed layernorm,0
Force add_ffnbias to True if bias found in model w_1 keys,0
fix v2 compatibility,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
BPE training,0
SentencePiece training,0
!/usr/bin/env python,0
!/usr/bin/env python,0
Set sharing strategy manually instead of default based on the OS.,0
torch.multiprocessing.set_sharing_strategy('file_system'),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
magic indices,0
result caching,0
Here we set the decoder to start with self.start (BOS or EOS),0
not 100% necessary to define those,0
self.is_finished = torch.zeros(,0
"[self.batch_size, self.parallel_paths], dtype=torch.bool",0
),0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
those two should be the same except feat dim,0
"batch['src'][perm[j], :, :])",0
trans.src,0
we rebuild a small batch made of the sub-segments,0
in the long segment.,0
new sub-batch ready to be translated,0
we re-insert the sub-batch in the initial translations,0
For seq2seq when we need to force doc to spit the same number of sents,0
In the case of length_penalty = none we report the total logprobs,0
divided by the number of sentence to get an approximation of the,0
per sentence logprob. We also return the corresponding ppl,0
"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs",0
are normalized per token we report the per line per token logprob,0
"and the corresponding ""per word perplexity""",0
Turn any copied words into UNKs.,0
"Decoder forward, takes [batch, tgt_len, nfeats] as input",0
"and [batch, src_len, hidden] as enc_out",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
at this point scores is batch first (dim=0),0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``",0
Quick fix. Transformers return None as enc_states.,0
enc_states are only used later on to init decoder's state,0
"but are never used in Transformer decoder, so we can skip",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
it should be done in a better way,0
here dec_in is batch first,0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
Reorder states.,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
beg_time = time(),0
Reorder states.,0
select indexes in model state/cache,0
if step == 0:,0
"print(""step0 time: "", time() - beg_time)",0
beam parameters,0
beam state,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
after this we get topk_ids between 0 and beam_size*vocab_size,0
topk_ids // vocab_size => indice in beam,0
topk_ids % vocab_size => true vocab indice,0
using lists instead of tensors for topk_scores and is_finished make things faster,0
Store finished hypotheses for this example in the batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
early stop when top beam is finished,0
Penalize beams that finished.,0
this is required to pursue finished beams in non finished batches,0
"If all sentences are translated, no need to go further.",0
reset the selection for the next step,0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction to reordered alive sequence,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
These comp lists are costy but less than for loops,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
Auto import python files in this directory,0
contractions,0
number separators,0
punctuation,0
double brackets,0
miscellaneous,0
Clean and Concat the dataset,0
"joiner = tokenizer._tokenize(""\n"")",0
tokens += tokenizer._tokenize([x]),0
Tokenize the dataset.,0
Build the translator (along with the model.,0
Score the dataset.,0
zero out the context tokens,0
"def custom_stopping_criteria(input_ids, score, **kwargs):",0
"stop_ids = [29871, 13, 13] # \n\n",0
return input_ids[-len(stop_ids)],0
Build the translator (along with the model),0
get prompt and make sure it fits,0
"def custom_stopping_criteria(input_ids, score, **kwargs):",0
"stop_ids = [29871, 13, 13] # \n\n",0
return input_ids[-len(stop_ids)],0
Build the translator (along with the model),0
get prompt and make sure it fits,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '6.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
Build the translator (along with the model),0
Put messages sizes in antichronological order,0
Caluculate antichronological history sizes,0
Prune the history from the beginning,0
Put back indices in chronological order.,0
Build the translator (along with the model),0
We need to build the Llama tokenizer to count tokens and prune the history.,0
The hypotheses are lists of one element but we still need to take the first one.,0
#####,0
UI #,0
#####,0
What are the 3 best french cities ?,0
Which one is better if I like outdoor activities ?,0
Which one is better if I like cultural outings?,0
What are the best neighborhoods in these 5 cities?,0
!/usr/bin/env python3,0
Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens,0
flake8: noqa,0
-*- coding: utf-8 -*-,0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: sentencepiece_model.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(module_scope),0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
!/usr/bin/env python,0
if shard == 0:,0
"vocab_size = onmt_safetensor[""generator.weight""].size(0)",0
"vocab[11] = ""</s>""  # Falcon only",0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
from onmt.utils.misc import use_gpu,0
"Add in default model arguments, possibly added since training.",0
this patch is no longer needed included in converter,0
"if hasattr(model_opt, 'rnn_size'):",0
model_opt.hidden_size = model_opt.rnn_size,0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
Falcon stores QKV in one single tensor but it is not simply piled up Q+K+V,0
it is heads interleaved to we need to slice first,0
also it uses the HF rotary so we need to permute Q and K interleave,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python3,0
coding: utf-8,0
"In order to use this tool, please install comet first",0
https://github.com/Unbabel/COMET,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate source and reference sentences nbest times,0
for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
same for source.,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \,0
--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best comet score,0
when choosing a reference-less model no nbest-ref is required,0
for nbest in nbests:,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python mbr_bleu.py --nbest-hyp target.5.tl \,0
--nbest-order 5 --output target.mbr.tl,0
It will compare all hyp with eachother and output the max bleu,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
Now we can pipe the full file through the model using the Iterator,0
reminder a batch includes .src .tgt .indices and it is sorted,0
print(batch),0
Compute and retrieve the loss for EACH sentence,0
Now we need to rearrange the batch of ppl,0
in the original order with indices,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate reference sentences nbest times for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \,0
--nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best bleu wrt reference,0
and output the max bleu,0
!/usr/bin/env python,0
!/usr/bin/env python,0
with the two module = imp.load_source() below,0
we ghost the old torchtext.data.field and depercated,0
onmt.inputters.text_dataset,0
however this require some functions / classes to be,0
monkey patched for loading the old field/vocab objects.,0
"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
!/usr/bin/env python,0
redpajama stores QKV in one single tensor but it is not simply piled up Q+K+V,0
it is heads interleaved to we need to slice first,0
also it uses the HF rotary so we need to permute Q and K interleave,0
Avoid functionality on inference,0
weights are in the .pt file,0
weights are not in the .pt checkpoint but stored in the safetensors file,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
Remove old vocabulary associated embeddings,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
If new training initialize the model params,0
If update_vocab init also but checkpoint will overwrite old weights,0
ONLY for legacy fusedam with amp pytorch requires NOT to half the model,0
Update model embeddings with those from the checkpoint,0
after initialization,0
after this checkpoint contains no embeddings,0
when using LoRa or updating the vocab (no more embeddings in ckpt),0
=> strict=False when loading state_dict,0
weights are in the .pt file,0
weights are not in the .pt checkpoint but stored in the safetensors file,0
!/usr/bin/env python,0
if transform + options set in 'valid' we need to copy in main,0
transform / options for scoring considered as inference,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
Override checkpoint's freezing settings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
GPU,0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
Freeze Encoder and/or Decoder,0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
LoRa,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options related to source and target features,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Adding options related to Transforms,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Create a thread to listen for errors in the child processes.,0
Build translator,0
Build vocab,0
Build transform pipe,0
Basic attributes.,0
Set model in training mode.,0
Let's clean the GPUs before training loop,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
raw_srcs = [],0
raw_refs = [],0
F-prop through the model.,0
Compute loss.,0
Compute validation metrics (at batch.dataset level),0
Compute stats,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
The loss of the prompt will be set to zero.,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
src lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Auto import python files in this directory,0
batch x len x dim,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
Padding mask is now (batch x 1 x slen x slen),0
1 to be expanded to number of heads in MHA,0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
Help functions for Rotary Embeddings,0
https://arxiv.org/pdf/2104.09864.pdf,0
too convoluted to make maxseqlen a parameter.,0
we suppose src_seq_len at training and max_length at inference,0
are both < 2048 tokens.,0
"rope is now matrix [maxseqlen, dim/2]",0
Help functions for max_relative positions,0
https://arxiv.org/abs/1803.02155,0
Shift values to be >= 0,0
"now relative_position is in the range [0, inf)",0
half of the buckets are for exact increments in positions,0
The other half of the buckets are for logarithmically bigger bins in positions,0
up to max_distance,0
Help functions to split model dim per head,0
https://arxiv.org/pdf/1803.02155.pdf,0
in the paper they suggest either two embeds,0
relative_key / relative_value or only,0
relative_key. We implemented the same embed,0
for both.,0
"1) Project key, value, and query.",0
as a reminder at training layer_cache[0] remains False,0
Retrieve keys and values from the KV cache (decoding mode only).,0
Resize rotary embeddings.,0
Resize rotary embeddings.,0
We take a margin of 32 tokens as the kv_cache,0
is incremented by 32 tokens every 32 tokens.,0
Increase the cached key pad mask by concatenation.,0
For decoding only.,0
Retrieve keys and values from linear layers (training mode).,0
Resize rotary embeddings.,0
expand key on heads dimension when it's less than query heads (multi-query variant),0
expand value on heads dimension when it's less than query heads (multi-query variant),0
"2) When standard pos. enc. or rotary, use flash attention",0
Ultimately flashv2 will be part of pytorch https://github.com/pytorch/pytorch/pull/105602,0
"In the meantime: if vanilla tranformer or Rotary embeddings (not rel_pos, not alibi)",0
then use flash2 if seq len > 256 otherwise use xtransformer from pt2 uptream,0
Apply flash2 attention.,0
Apply scaled dot product attention.,0
batch x num_heads x query_len x key_len,0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
not 100% necessary but expand to nb of heads,0
now mask and scores have the same shape,0
3) Apply attention dropout and compute context vectors.,0
We use the same embeddings for key and value,0
--------------------------------------------------------------------------,0
copied and adapted https://github.com/microsoft/LoRA/,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License (MIT).,0
Support bnb quantization of nderlying layers,0
--------------------------------------------------------------------------,0
Optional dropout,0
Mark the weight as unmerged,0
LoRA implemented in a dense layer,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
we do not super().reset_parameters() save lot of time and useless when no grad.,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
cannot merge/unmerge quantized weigts with unquantized lora_X,0
Check if QLoraLinear has a custom __init__ method,0
Invoke the __init__ method of QLoraLinear,0
LoRA implemented in a dense layer,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
for some reason list comprehension is slower in this scenario,0
"for silu, see: https://arxiv.org/pdf/2002.05202.pdf",0
-*- coding: utf-8 -*-,0
class AverageAttention(torch.jit.ScriptModule):,0
@torch.jit.script,0
Code taken from bitsandbytes but modified with arg device to accept skipt_init,0
from torch.nn.utils => makes model building way faster.,0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
We exclude tokenization for contractions in,0
order to avoid inconsistencies with pyonmtok's tokenization.,0
"(e.g. ""I ca n't"" with spacy, ""I can ' t"" with pyonmttok)",0
Use Spacy's stopwords to get rid of junk entries,0
Perform tokenization with spacy for consistency.,0
We ensure that the target lemma is present in the lemmatized,0
"target string, that the match is an exact match (there is",0
whitespace before or after the term),0
and we perform some bound checking.,0
Map the lemmatized string match index to,0
the lemmatized list index,0
We need to know if the term is multiword,0
Join multiword target lemmas with a unique separator so,0
we can treat them as single word and not change the indices.,0
Construct the final source from the lemmatized list,0
that contains the terms. We compare the tokens in the,0
term-augmented lemma list with the tokens in the original,0
"lemma list. If the lemma is the same, then we replace with",0
the token from the original tokenized source list. If they,0
"are not the same, it means the lemma has been augemented",0
"with a term, so we inject this in the final list.",0
Restore the spaces in multi-word terms,0
Skip half examples to improve performance. This means we set,0
"a hard limit for the `term_corpus_ratio` to 0.5, which is actually",0
quite high. TODO: We can add this (skipping examples) as an option,1
Filter out very short or very long sentences,0
from the TM for better performance,0
We split the `batch` and perform fuzzy matching,0
in smaller chunks of 10.000 examples in order to,0
reduce memory usage.,0
Perfomance is not affected.,0
Probably redundant but let's be safe,0
in case some examples are already fuzzied,0
(e.g. from another pipeline or workflow),0
We don't want exact matches,0
Apply a basic filtering to leave out very short or very long,0
sentences and speed up things a bit during fuzzy matching,0
Do nothing,0
We set the start number of tags to a random number from 1,0
to 12 + the number of subsequent tags that,0
will be added. We also apply weights to this choice so tags,0
"are more probable to start from 1, then from 2, etc.",0
This way we cover most scenarios met in real usage and,0
the system will learn to handle a fairly large number of,0
numbered tags (but not an excessively large number),0
Make sure we only search for exact matches (we don't want,0
to match part of words) and perform some bound checking,0
Create all possible tag forms. We inject a special,0
unicode char (∥) as a placeholder for whitespace in order,0
to keep the indices unaltered. This char is replaced with,0
spaces before we return the augmented examples.,0
Make a weighted choice between paired tags or single tags.,0
"We usually encounter, and thus here we favor, paired tags",0
with a ratio 1/3.,0
Check if the tags include the,0
"mandatory ""#"" number placeholder""",0
We split the user-defined tags in the # placeholder,0
in order to number them,0
Skip half examples to speed up the transform. This sets,0
"a hard limit of 0.5 to the `tags_corpus_ratio`, which is",0
excessive and should be avoided anyway.,0
normalize dict src/tgt for each dataset,0
"print(""src empty"")",0
"print(""too many same char in src"")",0
"print(""too many same word in src"")",0
"print(""avg token min"", len(src_str) / len(ex['src']))",0
"print(""avg token max"", len(src_str) / len(ex['src']))",0
"print(""text does not fully belong to wanted script"")",0
"print(""Some text belong to unwanted scripts"")",0
"print(""langid does not match"", _id(src_str))",0
"print(""src = tgt"")",0
"print(""tgt empty"")",0
"print(""src / tgt ratio "", len(src_str) / len(tgt_str))",0
"print(""too many same char in tgt"")",0
"print(""too many same word in tgt"")",0
"print(""avg token min"", len(tgt_str) / len(ex['tgt']))",0
"print(""avg token max"", len(tgt_str) / len(ex['tgt']))",0
"print(""text does not fully belong to wanted script"")",0
"print(""Some text belong to unwanted scripts"")",0
"print(""langid does not match"", _id(tgt_str))",0
"doc break we add it, restart new doc",0
case 1st ex is already longer,0
adding cur ex is too long we add cur doc,0
and reset doc to cur ex,0
we start the new doc with cur ex,0
we cumulate cur ex to cur doc,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
This method embeds a custom logic to correctly handle certain placeholders,0
in case the tokenizer doesn't preserve them.,0
Locate the end-of-sentence placeholders.,0
Tokenize each sentence separately.,0
Locate the mask-before placeholders,0
(to zero-out the prompt loss during LM finetuning).,0
Tokenize each chunk separately and insert the padding token.,0
between each sequence of tokens.,0
Re-insert the eos token.,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
Load vocabulary file if provided and set threshold,0
Load Subword Model,0
"ugly patch to make sure ""\n\n"" is split in two items",0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
prefix src/tgt for each dataset,0
prefix as general option for inference,0
suffix src/tgt for each dataset,0
suffix as general option for inference,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Most code taken from: https://github.com/alvations/sacremoses,0
Which in turn is based on the Moses punctuation normalizer.,0
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/,0
tokenizer/normalize-punctuation.perl,0
don't fix period at end of sentence,0
Regex substitutions from replace-unicode-punctuation.perl,0
https://github.com/moses-smt/mosesdecoder/blob/master/,0
scripts/tokenizer/replace-unicode-punctuation.perl,0
Adds the penn substitutions after extra_whitespace regexes.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
Actual normalization.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
normalize dict src/tgt for each dataset,0
One source feature expected but none given and no default provided,0
Provided default does not match required features,0
Data not properly annotated.,0
In this case we do not use the default as it might be an error,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
make sure the scalars are in the event accumulator tags,0
required arguments,0
transforms that require vocab will not create if not provide vocab,0
1. Init first transform in the pipe,0
2. Init second transform in the pipe,0
3. Sequential combine them into a transform pipe,0
4. apply transform pipe for example,0
"5. example after the pipe exceed the length limit, thus filtered",0
6. Transform statistics registed (here for filtertoolong),0
"7. after report, statistics become empty as a fresh start",0
filter_transform.warm_up(),0
test BPE-dropout:,0
1. disable bpe dropout for not training example,0
2. enable bpe dropout for training example,0
3. (NOTE) disable dropout won't take effect if already seen,0
this is caused by the cache mechanism in bpe:,0
return cached subword if the original token is seen when no dropout,0
test SP regularization:,0
1. enable regularization for training example,0
2. disable regularization for not training example,0
Test mask location,0
Test mask location,0
Test mask location,0
Not apply token drop for not training example,0
apply token drop for training example,0
Not apply token mask for not training example,0
apply token mask for training example,0
require vocabs to warm_up,0
Not apply token mask for not training example,0
apply token mask for training example,0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
random_ratio of inserted tokens are chosen in vocab,0
others are MASK_TOK,0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
all token are considered as an individual word,0
1. tokens are dropped when replace_length is 0,0
"print(f""token delete: {masked} / {tokens}"")",0
2. tokens are replaced by MASK when replace_length is 1,0
"print(f""token mask: {masked} / {tokens}"")",0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
"1. replace_length 0: ""words"" are dropped",0
"print(f""word delete: {masked} / {tokens}"")",0
"self.assertEqual(len(masked), n_words - n_masked)",0
"2. replace_length 1: ""words"" are replaced with a single MASK",0
"print(f""whole word single mask: {masked} / {tokens}"")",0
len(masked) depend on number of tokens in select word,0
"3. replace_length -1: all tokens in ""words"" are replaced with MASK",0
"print(f""whole word multi mask: {masked} / {tokens}"")",0
number of mask_tok depend on number of tokens in selected word,0
number of MASK_TOK can be greater than n_masked,0
"insert_ratio=0.5,",0
"random_ratio=0.3,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
n_words = sum(token_starts),0
n_masked = math.ceil(n_words * bart_noise.mask_ratio),0
"print(f""Text Span Infilling: {infillied} / {tokens}"")",0
"print(n_words, n_masked)",0
Build the translator (along with the model),0
Required arguments,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
we finish 3 hyps per example in this step,0
new beam 1 is old beam 3,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('hidden_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
"attns[""coverage""] is actually c^(t+1) of See et al(2017)",0
1-index shifted,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Auto import python files in this directory,0
src_len is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate src_len as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
"Add triangular future_mask and pad_mask, result mask in (B, T, T).",0
Patch for scaled dot product attention.,0
"Only mask padding, result mask in (B, 1, T).",0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x s or t len),0
1 = heads to be expanded in MHA,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
first value set to True triggered by the beginning of decoding,0
layer_cache becomes active in the MultiHeadedAttention fwd,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
Masking is necessary when sequence length is greater than one,0
"The decoding has not started yet,",0
we compute the scores on the source tokens in one shot.,0
mask now are (batch x 1 x tlen x tlen),0
1 = heads to be expanded in MHA,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
decoding mode.,0
Initialize KV and key_pad_mask cache.,0
training mode.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"print(filled, sz)",0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
"propagate exception to parent process, keeping original traceback",0
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use apex.amp,0
In this case use the old FusedAdam with,0
FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
should be: self._optimizer.zero_grad(set_to_none),0
but apex.amp is not up-to-date:,0
https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
placing this here make it easier to call logger.info,0
"from anywhere, just 'from onmt.utils.logging import logger'",0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
Create a mask with zeros at prompt positions and ones at answer postions.,0
Apply the mask on the target side.,0
Put the padding token index at the prompt positions.,0
take into account here the tgt_shift_index (0 / 1 = LM/NMT),0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
in the case criterion reduction is None then we need,0
to sum the loss of each sentence in the batch,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check weight,0
Check features,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
It comes from training,0
TODO: needs to be added as inference opt,1
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
"this param init is overridden by model_builder, useless then.",0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
Helper functions,0
Keeps track of the original words/subwords,0
('prior_tokenization' option),0
In case there is a final case_markup when new_spacer is on,0
########## #,0
Translator #,0
########## #,0
"Set ""default"" translation options on empty cfgfile",0
Build translator from options,0
################### #,0
Validation iterator #,0
################### #,0
Reinstantiate the validation iterator,0
Retrieve raw references and sources,0
########### #,0
Predictions #,0
########### #,0
####### #,0
Outputs #,0
####### #,0
Flatten predictions,0
Save results,0
-*- coding: utf-8 -*-,0
this one is needed for Random Shuffler of batches,0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
we need to check the model path + any tokenizer path,0
patch to log stdout spawned processes of dataloader,0
bucket_size = batch_size,0
For TRAIN we shuffle batches within the bucket,0
otherwise sequential,0
for specific case of rnn_packed need to be sorted,0
within the batch,0
single thread - create batch directly on GPU if device is gpu,0
multithread faster to create batch on CPU in each thread and then move it to gpu,0
Move tensor_batch from cpu to device,0
Check if all tokens have features or none at all,0
Make features part of src like,0
"{'src': {'src': ..., 'feats': [...., ....]}}",0
careful below it will return a bucket sorted by corpora,0
but we sort by length later and shuffle batches,0
at this point an example looks like:,0
"{'src': {'src': ..., 'feats': [....]},",0
"'tgt': {'tgt': ...},",0
"'src_original': ['tok1', ...'tokn'],",0
"'tgt_original': ['tok1', ...'tokm'],",0
'cid': corpus id,0
'cid_line_number' : cid line number,0
"'align': ...,",0
},0
Need to add features in last dimensions,0
Keep it consistent with dynamic data,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
-*- coding: utf-8 -*-,0
this is hack: if the special separator ｟newline｠is returned because of the,1
"""docify"" transform.get_specials we don't add it if the corresponding newline code",0
is already included in the sentencepiece or BPE-with-gpt2-pretok.,0
Reached end of file,0
'src_original' and 'tgt_original' store the,0
original line before tokenization. These,0
fields are used later on in the feature,0
transforms.,0
empty example: skip,0
ugly patch because in_feat and out_feat are reversed in WQLinear_GEMM,1
bitsandbytes quantize weights when .cuda() is called,0
for huge models we need to save Ram,0
so we load the weights  module by module and transfer them to GPU for quantization,0
bitsandbytes quantize weights when .cuda() is called,0
for huge models we need to save Ram,0
so we load the weights  module by module and transfer them to GPU for quantization,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
This preserves backward-compat for models using customed layernorm,0
Force add_ffnbias to True if bias found in model w_1 keys,0
fix v2 compatibility,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
BPE training,0
SentencePiece training,0
!/usr/bin/env python,0
!/usr/bin/env python,0
Set sharing strategy manually instead of default based on the OS.,0
torch.multiprocessing.set_sharing_strategy('file_system'),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
magic indices,0
result caching,0
Here we set the decoder to start with self.start (BOS or EOS),0
not 100% necessary to define those,0
self.is_finished = torch.zeros(,0
"[self.batch_size, self.parallel_paths], dtype=torch.bool",0
),0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
those two should be the same except feat dim,0
"batch['src'][perm[j], :, :])",0
trans.src,0
we rebuild a small batch made of the sub-segments,0
in the long segment.,0
new sub-batch ready to be translated,0
we re-insert the sub-batch in the initial translations,0
For seq2seq when we need to force doc to spit the same number of sents,0
In the case of length_penalty = none we report the total logprobs,0
divided by the number of sentence to get an approximation of the,0
per sentence logprob. We also return the corresponding ppl,0
"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs",0
are normalized per token we report the per line per token logprob,0
"and the corresponding ""per word perplexity""",0
Turn any copied words into UNKs.,0
"Decoder forward, takes [batch, tgt_len, nfeats] as input",0
"and [batch, src_len, hidden] as enc_out",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
at this point scores is batch first (dim=0),0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``",0
Quick fix. Transformers return None as enc_states.,0
enc_states are only used later on to init decoder's state,0
"but are never used in Transformer decoder, so we can skip",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
it should be done in a better way,0
here dec_in is batch first,0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
Reorder states.,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
beg_time = time(),0
Reorder states.,0
select indexes in model state/cache,0
if step == 0:,0
"print(""step0 time: "", time() - beg_time)",0
beam parameters,0
beam state,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
after this we get topk_ids between 0 and beam_size*vocab_size,0
topk_ids // vocab_size => indice in beam,0
topk_ids % vocab_size => true vocab indice,0
using lists instead of tensors for topk_scores and is_finished make things faster,0
Store finished hypotheses for this example in the batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
early stop when top beam is finished,0
Penalize beams that finished.,0
this is required to pursue finished beams in non finished batches,0
"If all sentences are translated, no need to go further.",0
reset the selection for the next step,0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction to reordered alive sequence,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
These comp lists are costy but less than for loops,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
Auto import python files in this directory,0
contractions,0
number separators,0
punctuation,0
double brackets,0
miscellaneous,0
Clean and Concat the dataset,0
"joiner = tokenizer._tokenize(""\n"")",0
tokens += tokenizer._tokenize([x]),0
Tokenize the dataset.,0
Build the translator (along with the model.,0
Score the dataset.,0
zero out the context tokens,0
"def custom_stopping_criteria(input_ids, score, **kwargs):",0
"stop_ids = [29871, 13, 13] # \n\n",0
return input_ids[-len(stop_ids)],0
Build the translator (along with the model),0
get prompt and make sure it fits,0
"def custom_stopping_criteria(input_ids, score, **kwargs):",0
"stop_ids = [29871, 13, 13] # \n\n",0
return input_ids[-len(stop_ids)],0
Build the translator (along with the model),0
get prompt and make sure it fits,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '6.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
Build the translator (along with the model),0
Put messages sizes in antichronological order,0
Caluculate antichronological history sizes,0
Prune the history from the beginning,0
Put back indices in chronological order.,0
Build the translator (along with the model),0
We need to build the Llama tokenizer to count tokens and prune the history.,0
The hypotheses are lists of one element but we still need to take the first one.,0
#####,0
UI #,0
#####,0
What are the 3 best french cities ?,0
Which one is better if I like outdoor activities ?,0
Which one is better if I like cultural outings?,0
What are the best neighborhoods in these 5 cities?,0
!/usr/bin/env python3,0
Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens,0
flake8: noqa,0
-*- coding: utf-8 -*-,0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: sentencepiece_model.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(module_scope),0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
from onmt.utils.misc import use_gpu,0
"Add in default model arguments, possibly added since training.",0
this patch is no longer needed included in converter,0
"if hasattr(model_opt, 'rnn_size'):",0
model_opt.hidden_size = model_opt.rnn_size,0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
Falcon stores QKV in one single tensor but it is not simply piled up Q+K+V,0
it is heads interleaved to we need to slice first,0
also it uses the HF rotary so we need to permute Q and K interleave,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python3,0
coding: utf-8,0
"In order to use this tool, please install comet first",0
https://github.com/Unbabel/COMET,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate source and reference sentences nbest times,0
for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
same for source.,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \,0
--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best comet score,0
when choosing a reference-less model no nbest-ref is required,0
for nbest in nbests:,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python mbr_bleu.py --nbest-hyp target.5.tl \,0
--nbest-order 5 --output target.mbr.tl,0
It will compare all hyp with eachother and output the max bleu,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
Now we can pipe the full file through the model using the Iterator,0
reminder a batch includes .src .tgt .indices and it is sorted,0
Compute and retrieve the loss for EACH sentence,0
Now we need to rearrange the batch of ppl,0
in the original order with indices,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate reference sentences nbest times for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \,0
--nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best bleu wrt reference,0
and output the max bleu,0
!/usr/bin/env python,0
with the two module = imp.load_source() below,0
we ghost the old torchtext.data.field and depercated,0
onmt.inputters.text_dataset,0
however this require some functions / classes to be,0
monkey patched for loading the old field/vocab objects.,0
"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
!/usr/bin/env python,0
redpajama stores QKV in one single tensor but it is not simply piled up Q+K+V,0
it is heads interleaved to we need to slice first,0
also it uses the HF rotary so we need to permute Q and K interleave,0
Avoid functionality on inference,0
weights are in the .pt file,0
weights are not in the .pt checkpoint but stored in the safetensors file,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
Remove old vocabulary associated embeddings,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
If new training initialize the model params,0
If update_vocab init also but checkpoint will overwrite old weights,0
ONLY for legacy fusedam with amp pytorch requires NOT to half the model,0
Update model embeddings with those from the checkpoint,0
after initialization,0
after this checkpoint contains no embeddings,0
when using LoRa or updating the vocab (no more embeddings in ckpt),0
=> strict=False when loading state_dict,0
weights are in the .pt file,0
weights are not in the .pt checkpoint but stored in the safetensors file,0
!/usr/bin/env python,0
if transform + options set in 'valid' we need to copy in main,0
transform / options for scoring considered as inference,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
Override checkpoint's freezing settings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
GPU,0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
Freeze Encoder and/or Decoder,0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
LoRa,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options related to source and target features,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Adding options related to Transforms,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Create a thread to listen for errors in the child processes.,0
Build translator,0
Build vocab,0
Build transform pipe,0
Basic attributes.,0
Set model in training mode.,0
Let's clean the GPUs before training loop,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
raw_srcs = [],0
raw_refs = [],0
F-prop through the model.,0
Compute loss.,0
Compute validation metrics (at batch.dataset level),0
Compute stats,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
The loss of the prompt will be set to zero.,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
src lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Auto import python files in this directory,0
batch x len x dim,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
Padding mask is now (batch x 1 x slen x slen),0
1 to be expanded to number of heads in MHA,0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
Help functions for Rotary Embeddings,0
https://arxiv.org/pdf/2104.09864.pdf,0
too convoluted to make maxseqlen a parameter.,0
we suppose src_seq_len at training and max_length at inference,0
are both < 2048 tokens.,0
"rope is now matrix [maxseqlen, dim/2]",0
Help functions for max_relative positions,0
https://arxiv.org/abs/1803.02155,0
Shift values to be >= 0,0
"now relative_position is in the range [0, inf)",0
half of the buckets are for exact increments in positions,0
The other half of the buckets are for logarithmically bigger bins in positions,0
up to max_distance,0
Help functions to split model dim per head,0
https://arxiv.org/pdf/1803.02155.pdf,0
in the paper they suggest either two embeds,0
relative_key / relative_value or only,0
relative_key. We implemented the same embed,0
for both.,0
"1) Project key, value, and query.",0
as a reminder at training layer_cache[0] remains False,0
expand key on heads dimension when it's less than query heads (multi-query variant),0
expand value on heads dimension when it's less than query heads (multi-query variant),0
"2) When standard pos. enc. or rotary, use flash attention",0
Ultimately flashv2 will be part of pytorch https://github.com/pytorch/pytorch/pull/105602,0
"In the meantime: if vanilla tranformer or Rotary embeddings (not rel_pos, not alibi)",0
then use flash2 if seq len > 256 otherwise use xtransformer from pt2 uptream,0
batch x num_heads x query_len x key_len,0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
not 100% necessary but expand to nb of heads,0
now mask and scores have the same shape,0
3) Apply attention dropout and compute context vectors.,0
We use the same embeddings for key and value,0
--------------------------------------------------------------------------,0
copied and adapted https://github.com/microsoft/LoRA/,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License (MIT).,0
Support bnb quantization of nderlying layers,0
--------------------------------------------------------------------------,0
Optional dropout,0
Mark the weight as unmerged,0
LoRA implemented in a dense layer,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
we do not super().reset_parameters() save lot of time and useless when no grad.,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
cannot merge/unmerge quantized weigts with unquantized lora_X,0
Check if QLoraLinear has a custom __init__ method,0
Invoke the __init__ method of QLoraLinear,0
LoRA implemented in a dense layer,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
for some reason list comprehension is slower in this scenario,0
"for silu, see: https://arxiv.org/pdf/2002.05202.pdf",0
-*- coding: utf-8 -*-,0
class AverageAttention(torch.jit.ScriptModule):,0
@torch.jit.script,0
Code taken from bitsandbytes but modified with arg device to accept skipt_init,0
from torch.nn.utils => makes model building way faster.,0
"weights are cast automatically as Int8Params, but the bias has to be cast manually",0
reorder weight layout back from ampere/turing to row,0
"we only need to save SCB as extra data, because CB for quantized weights",0
is already stored in weight.data,0
"case 1: .cuda was called, SCB is in self.weight",0
"case 2: self.init_8bit_state was called, SCB is in self.state",0
"buffers not yet initialized, can't call them directly without",0
"weights are cast automatically as Int8Params, but the bias has to be cast manually",0
we converted 8-bit row major to turing/ampere format in the first inference pass,0
we no longer need the row-major weight,0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
We exclude tokenization for contractions in,0
order to avoid inconsistencies with pyonmtok's tokenization.,0
"(e.g. ""I ca n't"" with spacy, ""I can ' t"" with pyonmttok)",0
Use Spacy's stopwords to get rid of junk entries,0
Perform tokenization with spacy for consistency.,0
We ensure that the target lemma is present in the lemmatized,0
"target string, that the match is an exact match (there is",0
whitespace before or after the term),0
and we perform some bound checking.,0
Map the lemmatized string match index to,0
the lemmatized list index,0
We need to know if the term is multiword,0
Join multiword target lemmas with a unique separator so,0
we can treat them as single word and not change the indices.,0
Construct the final source from the lemmatized list,0
that contains the terms. We compare the tokens in the,0
term-augmented lemma list with the tokens in the original,0
"lemma list. If the lemma is the same, then we replace with",0
the token from the original tokenized source list. If they,0
"are not the same, it means the lemma has been augemented",0
"with a term, so we inject this in the final list.",0
Restore the spaces in multi-word terms,0
Skip half examples to improve performance. This means we set,0
"a hard limit for the `term_corpus_ratio` to 0.5, which is actually",0
quite high. TODO: We can add this (skipping examples) as an option,1
Filter out very short or very long sentences,0
from the TM for better performance,0
We split the `batch` and perform fuzzy matching,0
in smaller chunks of 10.000 examples in order to,0
reduce memory usage.,0
Perfomance is not affected.,0
Probably redundant but let's be safe,0
in case some examples are already fuzzied,0
(e.g. from another pipeline or workflow),0
We don't want exact matches,0
Apply a basic filtering to leave out very short or very long,0
sentences and speed up things a bit during fuzzy matching,0
Do nothing,0
We set the start number of tags to a random number from 1,0
to 12 + the number of subsequent tags that,0
will be added. We also apply weights to this choice so tags,0
"are more probable to start from 1, then from 2, etc.",0
This way we cover most scenarios met in real usage and,0
the system will learn to handle a fairly large number of,0
numbered tags (but not an excessively large number),0
Make sure we only search for exact matches (we don't want,0
to match part of words) and perform some bound checking,0
Create all possible tag forms. We inject a special,0
unicode char (∥) as a placeholder for whitespace in order,0
to keep the indices unaltered. This char is replaced with,0
spaces before we return the augmented examples.,0
Make a weighted choice between paired tags or single tags.,0
"We usually encounter, and thus here we favor, paired tags",0
with a ratio 1/3.,0
Check if the tags include the,0
"mandatory ""#"" number placeholder""",0
We split the user-defined tags in the # placeholder,0
in order to number them,0
Skip half examples to speed up the transform. This sets,0
"a hard limit of 0.5 to the `tags_corpus_ratio`, which is",0
excessive and should be avoided anyway.,0
normalize dict src/tgt for each dataset,0
"print(""src empty"")",0
"print(""too many same char in src"")",0
"print(""too many same word in src"")",0
"print(""avg token min"", len(src_str) / len(ex['src']))",0
"print(""avg token max"", len(src_str) / len(ex['src']))",0
"print(""text does not fully belong to wanted script"")",0
"print(""Some text belong to unwanted scripts"")",0
"print(""langid does not match"", _id(src_str))",0
"print(""src = tgt"")",0
"print(""tgt empty"")",0
"print(""src / tgt ratio "", len(src_str) / len(tgt_str))",0
"print(""too many same char in tgt"")",0
"print(""too many same word in tgt"")",0
"print(""avg token min"", len(tgt_str) / len(ex['tgt']))",0
"print(""avg token max"", len(tgt_str) / len(ex['tgt']))",0
"print(""text does not fully belong to wanted script"")",0
"print(""Some text belong to unwanted scripts"")",0
"print(""langid does not match"", _id(tgt_str))",0
"doc break we add it, restart new doc",0
case 1st ex is already longer,0
adding cur ex is too long we add cur doc,0
and reset doc to cur ex,0
we start the new doc with cur ex,0
we cumulate cur ex to cur doc,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
This method embeds a custom logic to correctly handle certain placeholders,0
in case the tokenizer doesn't preserve them.,0
Locate the end-of-sentence placeholders.,0
Tokenize each sentence separately.,0
Locate the mask-before placeholders,0
(to zero-out the prompt loss during LM finetuning).,0
Tokenize each chunk separately and insert the padding token.,0
between each sequence of tokens.,0
Re-insert the eos token.,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
Load vocabulary file if provided and set threshold,0
Load Subword Model,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
prefix src/tgt for each dataset,0
prefix as general option for inference,0
suffix src/tgt for each dataset,0
suffix as general option for inference,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Most code taken from: https://github.com/alvations/sacremoses,0
Which in turn is based on the Moses punctuation normalizer.,0
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/,0
tokenizer/normalize-punctuation.perl,0
don't fix period at end of sentence,0
Regex substitutions from replace-unicode-punctuation.perl,0
https://github.com/moses-smt/mosesdecoder/blob/master/,0
scripts/tokenizer/replace-unicode-punctuation.perl,0
Adds the penn substitutions after extra_whitespace regexes.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
Actual normalization.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
normalize dict src/tgt for each dataset,0
One source feature expected but none given and no default provided,0
Provided default does not match required features,0
Data not properly annotated.,0
In this case we do not use the default as it might be an error,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
make sure the scalars are in the event accumulator tags,0
required arguments,0
transforms that require vocab will not create if not provide vocab,0
1. Init first transform in the pipe,0
2. Init second transform in the pipe,0
3. Sequential combine them into a transform pipe,0
4. apply transform pipe for example,0
"5. example after the pipe exceed the length limit, thus filtered",0
6. Transform statistics registed (here for filtertoolong),0
"7. after report, statistics become empty as a fresh start",0
filter_transform.warm_up(),0
test BPE-dropout:,0
1. disable bpe dropout for not training example,0
2. enable bpe dropout for training example,0
3. (NOTE) disable dropout won't take effect if already seen,0
this is caused by the cache mechanism in bpe:,0
return cached subword if the original token is seen when no dropout,0
test SP regularization:,0
1. enable regularization for training example,0
2. disable regularization for not training example,0
Test mask location,0
Test mask location,0
Test mask location,0
Not apply token drop for not training example,0
apply token drop for training example,0
Not apply token mask for not training example,0
apply token mask for training example,0
require vocabs to warm_up,0
Not apply token mask for not training example,0
apply token mask for training example,0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
random_ratio of inserted tokens are chosen in vocab,0
others are MASK_TOK,0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
all token are considered as an individual word,0
1. tokens are dropped when replace_length is 0,0
"print(f""token delete: {masked} / {tokens}"")",0
2. tokens are replaced by MASK when replace_length is 1,0
"print(f""token mask: {masked} / {tokens}"")",0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
"1. replace_length 0: ""words"" are dropped",0
"print(f""word delete: {masked} / {tokens}"")",0
"self.assertEqual(len(masked), n_words - n_masked)",0
"2. replace_length 1: ""words"" are replaced with a single MASK",0
"print(f""whole word single mask: {masked} / {tokens}"")",0
len(masked) depend on number of tokens in select word,0
"3. replace_length -1: all tokens in ""words"" are replaced with MASK",0
"print(f""whole word multi mask: {masked} / {tokens}"")",0
number of mask_tok depend on number of tokens in selected word,0
number of MASK_TOK can be greater than n_masked,0
"insert_ratio=0.5,",0
"random_ratio=0.3,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
n_words = sum(token_starts),0
n_masked = math.ceil(n_words * bart_noise.mask_ratio),0
"print(f""Text Span Infilling: {infillied} / {tokens}"")",0
"print(n_words, n_masked)",0
Build the translator (along with the model),0
Required arguments,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
we finish 3 hyps per example in this step,0
new beam 1 is old beam 3,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('hidden_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
"attns[""coverage""] is actually c^(t+1) of See et al(2017)",0
1-index shifted,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Auto import python files in this directory,0
src_len is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate src_len as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x s or t len),0
1 = heads to be expanded in MHA,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
first value set to True triggered by the beginning of decoding,0
layer_cache becomes active in the MultiHeadedAttention fwd,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x tlen),0
1 = heads to be expanded in MHA,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"print(filled, sz)",0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
"propagate exception to parent process, keeping original traceback",0
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use apex.amp,0
In this case use the old FusedAdam with,0
FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
should be: self._optimizer.zero_grad(set_to_none),0
but apex.amp is not up-to-date:,0
https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
placing this here make it easier to call logger.info,0
"from anywhere, just 'from onmt.utils.logging import logger'",0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
Create a mask with zeros at prompt positions and ones at answer postions.,0
Apply the mask on the target side.,0
Put the padding token index at the prompt positions.,0
take into account here the tgt_shift_index (0 / 1 = LM/NMT),0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
in the case criterion reduction is None then we need,0
to sum the loss of each sentence in the batch,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check weight,0
Check features,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
It comes from training,0
TODO: needs to be added as inference opt,1
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
"this param init is overridden by model_builder, useless then.",0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
Helper functions,0
Keeps track of the original words/subwords,0
('prior_tokenization' option),0
In case there is a final case_markup when new_spacer is on,0
########## #,0
Translator #,0
########## #,0
Set translation options,0
Build translator from options,0
################### #,0
Validation iterator #,0
################### #,0
Reinstantiate the validation iterator,0
Retrieve raw references and sources,0
########### #,0
Predictions #,0
########### #,0
####### #,0
Outputs #,0
####### #,0
Flatten predictions,0
Save results,0
-*- coding: utf-8 -*-,0
this one is needed for Random Shuffler of batches,0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
we need to check the model path + any tokenizer path,0
patch to log stdout spawned processes of dataloader,0
bucket_size = batch_size,0
For TRAIN we shuffle batches within the bucket,0
otherwise sequential,0
for specific case of rnn_packed need to be sorted,0
within the batch,0
single thread - create batch directly on GPU if device is gpu,0
multithread faster to create batch on CPU in each thread and then move it to gpu,0
Move tensor_batch from cpu to device,0
Check if all tokens have features or none at all,0
Make features part of src like,0
"{'src': {'src': ..., 'feats': [...., ....]}}",0
careful below it will return a bucket sorted by corpora,0
but we sort by length later and shuffle batches,0
at this point an example looks like:,0
"{'src': {'src': ..., 'feats': [....]},",0
"'tgt': {'tgt': ...},",0
"'src_original': ['tok1', ...'tokn'],",0
"'tgt_original': ['tok1', ...'tokm'],",0
'cid': corpus id,0
'cid_line_number' : cid line number,0
"'align': ...,",0
},0
Need to add features in last dimensions,0
Keep it consistent with dynamic data,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
-*- coding: utf-8 -*-,0
this is hack: if the special separator ｟newline｠is returned because of the,1
"""docify"" transform.get_specials we don't add it if the corresponding newline code",0
is already included in the sentencepiece or BPE-with-gpt2-pretok.,0
'src_original' and 'tgt_original' store the,0
original line before tokenization. These,0
fields are used later on in the feature,0
transforms.,0
empty example: skip,0
bitsandbytes quantize weights when .cuda() is called,0
for huge models we need to save Ram,0
so we load the weights  module by module and transfer them to GPU for quantization,0
bitsandbytes quantize weights when .cuda() is called,0
for huge models we need to save Ram,0
so we load the weights  module by module and transfer them to GPU for quantization,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
This preserves backward-compat for models using customed layernorm,0
Force add_ffnbias to True if bias found in model w_1 keys,0
fix v2 compatibility,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
BPE training,0
SentencePiece training,0
!/usr/bin/env python,0
!/usr/bin/env python,0
Set sharing strategy manually instead of default based on the OS.,0
torch.multiprocessing.set_sharing_strategy('file_system'),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
magic indices,0
result caching,0
Here we set the decoder to start with self.start (BOS or EOS),0
not 100% necessary to define those,0
self.is_finished = torch.zeros(,0
"[self.batch_size, self.parallel_paths], dtype=torch.bool",0
),0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
those two should be the same except feat dim,0
"batch['src'][perm[j], :, :])",0
trans.src,0
we rebuild a small batch made of the sub-segments,0
in the long segment.,0
new sub-batch ready to be translated,0
we re-insert the sub-batch in the initial translations,0
For seq2seq when we need to force doc to spit the same number of sents,0
In the case of length_penalty = none we report the total logprobs,0
divided by the number of sentence to get an approximation of the,0
per sentence logprob. We also return the corresponding ppl,0
"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs",0
are normalized per token we report the per line per token logprob,0
"and the corresponding ""per word perplexity""",0
Turn any copied words into UNKs.,0
"Decoder forward, takes [batch, tgt_len, nfeats] as input",0
"and [batch, src_len, hidden] as enc_out",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
at this point scores is batch first (dim=0),0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``",0
Quick fix. Transformers return None as enc_states.,0
enc_states are only used later on to init decoder's state,0
"but are never used in Transformer decoder, so we can skip",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
it should be done in a better way,0
here dec_in is batch first,0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
Reorder states.,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
after this we get topk_ids between 0 and beam_size*vocab_size,0
topk_ids // vocab_size => indice in beam,0
topk_ids % vocab_size => true vocab indice,0
Store finished hypotheses for this example in the batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
early stop when top beam is finished,0
Penalize beams that finished.,0
this is required to pursue finished beams in non finished batches,0
"If all sentences are translated, no need to go further.",0
reset the selection for the next step,0
assert torch.equal(,0
"self.src_len[self.select_indices],",0
"self.src_len.view(_B_old, self.beam_size)[non_finished].view(",0
_B_new * self.beam_size,0
"),",0
),0
Remove finished batches for the next step.,0
here we combine two slections in one,0
self.topk_log_probs = self.topk_log_probs[non_finished],0
"self._batch_index = self._batch_index.index_select(0, non_finished)",0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction to reordered alive sequence,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
These comp lists are costy but less than for loops,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
Auto import python files in this directory,0
"def custom_stopping_criteria(input_ids, score, **kwargs):",0
"stop_ids = [29871, 13, 13] # \n\n",0
return input_ids[-len(stop_ids)],0
Build the translator (along with the model),0
get prompt and make sure it fits,0
"def custom_stopping_criteria(input_ids, score, **kwargs):",0
"stop_ids = [29871, 13, 13] # \n\n",0
return input_ids[-len(stop_ids)],0
Build the translator (along with the model),0
get prompt and make sure it fits,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '6.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
Build the translator (along with the model),0
Put messages sizes in antichronological order,0
Caluculate antichronological history sizes,0
Prune the history from the beginning,0
Put back indices in chronological order.,0
Build the translator (along with the model),0
We need to build the Llama tokenizer to count tokens and prune the history.,0
The hypotheses are lists of one element but we still need to take the first one.,0
#####,0
UI #,0
#####,0
What are the 3 best french cities ?,0
Which one is better if I like outdoor activities ?,0
Which one is better if I like cultural outings?,0
What are the best neighborhoods in these 5 cities?,0
!/usr/bin/env python3,0
Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens,0
flake8: noqa,0
-*- coding: utf-8 -*-,0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: sentencepiece_model.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(module_scope),0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
from onmt.utils.misc import use_gpu,0
"Add in default model arguments, possibly added since training.",0
this patch is no longer needed included in converter,0
"if hasattr(model_opt, 'rnn_size'):",0
model_opt.hidden_size = model_opt.rnn_size,0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
Falcon stores QKV in one single tensor but it is not simply piled up Q+K+V,0
it is heads interleaved to we need to slice first,0
also it uses the HF rotary so we need to permute Q and K interleave,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python3,0
coding: utf-8,0
"In order to use this tool, please install comet first",0
https://github.com/Unbabel/COMET,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate source and reference sentences nbest times,0
for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
same for source.,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \,0
--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best comet score,0
when choosing a reference-less model no nbest-ref is required,0
for nbest in nbests:,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python mbr_bleu.py --nbest-hyp target.5.tl \,0
--nbest-order 5 --output target.mbr.tl,0
It will compare all hyp with eachother and output the max bleu,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
Now we can pipe the full file through the model using the Iterator,0
reminder a batch includes .src .tgt .indices and it is sorted,0
Compute and retrieve the loss for EACH sentence,0
Now we need to rearrange the batch of ppl,0
in the original order with indices,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate reference sentences nbest times for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \,0
--nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best bleu wrt reference,0
and output the max bleu,0
!/usr/bin/env python,0
with the two module = imp.load_source() below,0
we ghost the old torchtext.data.field and depercated,0
onmt.inputters.text_dataset,0
however this require some functions / classes to be,0
monkey patched for loading the old field/vocab objects.,0
"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
!/usr/bin/env python,0
redpajama stores QKV in one single tensor but it is not simply piled up Q+K+V,0
it is heads interleaved to we need to slice first,0
also it uses the HF rotary so we need to permute Q and K interleave,0
Avoid functionality on inference,0
weights are in the .pt file,0
weights are not in the .pt checkpoint but stored in the safetensors file,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
Remove old vocabulary associated embeddings,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
If new training initialize the model params,0
If update_vocab init also but checkpoint will overwrite old weights,0
ONLY for legacy fusedam with amp pytorch requires NOT to half the model,0
Update model embeddings with those from the checkpoint,0
after initialization,0
after this checkpoint contains no embeddings,0
when using LoRa or updating the vocab (no more embeddings in ckpt),0
=> strict=False when loading state_dict,0
weights are in the .pt file,0
weights are not in the .pt checkpoint but stored in the safetensors file,0
!/usr/bin/env python,0
if transform + options set in 'valid' we need to copy in main,0
transform / options for scoring considered as inference,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
Override checkpoint's freezing settings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
GPU,0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
Freeze Encoder and/or Decoder,0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
LoRa,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options related to source and target features,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Adding options related to Transforms,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Create a thread to listen for errors in the child processes.,0
Build translator,0
Build vocab,0
Build transform pipe,0
Basic attributes.,0
Set model in training mode.,0
Let's clean the GPUs before training loop,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
raw_srcs = [],0
raw_refs = [],0
F-prop through the model.,0
Compute loss.,0
Compute validation metrics (at batch.dataset level),0
Compute stats,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
The loss of the prompt will be set to zero.,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
src lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Auto import python files in this directory,0
batch x len x dim,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
mask is now (batch x 1 x slen x slen),0
1 to be expanded to number of heads in MHA,0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
Help functions for Rotary Embeddings,0
https://arxiv.org/pdf/2104.09864.pdf,0
too convoluted to make maxseqlen a parameter.,0
we suppose src_seq_len at training and max_length at inference,0
are both < 2048 tokens.,0
"rope is now matrix [maxseqlen, dim/2]",0
Help functions for max_relative positions,0
https://arxiv.org/abs/1803.02155,0
Shift values to be >= 0,0
"now relative_position is in the range [0, inf)",0
half of the buckets are for exact increments in positions,0
The other half of the buckets are for logarithmically bigger bins in positions,0
up to max_distance,0
Help functions to split model dim per head,0
https://arxiv.org/pdf/1803.02155.pdf,0
in the paper they suggest either two embeds,0
relative_key / relative_value or only,0
relative_key. We implemented the same embed,0
for both.,0
"1) Project key, value, and query.",0
as a reminder at training layer_cache[0] remains False,0
expand key on heads dimension when it's less than query heads (multi-query variant),0
expand value on heads dimension when it's less than query heads (multi-query variant),0
"2) When standard pos. enc. or rotary, use flash attention",0
Ultimately flashv2 will be part of pytorch https://github.com/pytorch/pytorch/pull/105602,0
"In the meantime: if vanilla tranformer or Rotary embeddings (not rel_pos, not alibi)",0
then use flash2 if seq len > 256 otherwise use xtransformer from pt2 uptream,0
batch x num_heads x query_len x key_len,0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
not 100% necessary but expand to nb of heads,0
now mask and scores have the same shape,0
3) Apply attention dropout and compute context vectors.,0
We use the same embeddings for key and value,0
--------------------------------------------------------------------------,0
copied and adapted https://github.com/microsoft/LoRA/,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License (MIT).,0
Support bnb quantization of nderlying layers,0
--------------------------------------------------------------------------,0
Optional dropout,0
Mark the weight as unmerged,0
LoRA implemented in a dense layer,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
we do not super().reset_parameters() save lot of time and useless when no grad.,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
cannot merge/unmerge quantized weigts with unquantized lora_X,0
Check if QLoraLinear has a custom __init__ method,0
Invoke the __init__ method of QLoraLinear,0
LoRA implemented in a dense layer,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
for some reason list comprehension is slower in this scenario,0
"for silu, see: https://arxiv.org/pdf/2002.05202.pdf",0
-*- coding: utf-8 -*-,0
class AverageAttention(torch.jit.ScriptModule):,0
@torch.jit.script,0
Code taken from bitsandbytes but modified with arg device to accept skipt_init,0
from torch.nn.utils => makes model building way faster.,0
"weights are cast automatically as Int8Params, but the bias has to be cast manually",0
reorder weight layout back from ampere/turing to row,0
"we only need to save SCB as extra data, because CB for quantized weights",0
is already stored in weight.data,0
"case 1: .cuda was called, SCB is in self.weight",0
"case 2: self.init_8bit_state was called, SCB is in self.state",0
"buffers not yet initialized, can't call them directly without",0
"weights are cast automatically as Int8Params, but the bias has to be cast manually",0
we converted 8-bit row major to turing/ampere format in the first inference pass,0
we no longer need the row-major weight,0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
We exclude tokenization for contractions in,0
order to avoid inconsistencies with pyonmtok's tokenization.,0
"(e.g. ""I ca n't"" with spacy, ""I can ' t"" with pyonmttok)",0
Use Spacy's stopwords to get rid of junk entries,0
Perform tokenization with spacy for consistency.,0
We ensure that the target lemma is present in the lemmatized,0
"target string, that the match is an exact match (there is",0
whitespace before or after the term),0
and we perform some bound checking.,0
Map the lemmatized string match index to,0
the lemmatized list index,0
We need to know if the term is multiword,0
Join multiword target lemmas with a unique separator so,0
we can treat them as single word and not change the indices.,0
Construct the final source from the lemmatized list,0
that contains the terms. We compare the tokens in the,0
term-augmented lemma list with the tokens in the original,0
"lemma list. If the lemma is the same, then we replace with",0
the token from the original tokenized source list. If they,0
"are not the same, it means the lemma has been augemented",0
"with a term, so we inject this in the final list.",0
Restore the spaces in multi-word terms,0
Skip half examples to improve performance. This means we set,0
"a hard limit for the `term_corpus_ratio` to 0.5, which is actually",0
quite high. TODO: We can add this (skipping examples) as an option,1
Filter out very short or very long sentences,0
from the TM for better performance,0
We split the `batch` and perform fuzzy matching,0
in smaller chunks of 10.000 examples in order to,0
reduce memory usage.,0
Perfomance is not affected.,0
Probably redundant but let's be safe,0
in case some examples are already fuzzied,0
(e.g. from another pipeline or workflow),0
We don't want exact matches,0
Apply a basic filtering to leave out very short or very long,0
sentences and speed up things a bit during fuzzy matching,0
Do nothing,0
We set the start number of tags to a random number from 1,0
to 12 + the number of subsequent tags that,0
will be added. We also apply weights to this choice so tags,0
"are more probable to start from 1, then from 2, etc.",0
This way we cover most scenarios met in real usage and,0
the system will learn to handle a fairly large number of,0
numbered tags (but not an excessively large number),0
Make sure we only search for exact matches (we don't want,0
to match part of words) and perform some bound checking,0
Create all possible tag forms. We inject a special,0
unicode char (∥) as a placeholder for whitespace in order,0
to keep the indices unaltered. This char is replaced with,0
spaces before we return the augmented examples.,0
Make a weighted choice between paired tags or single tags.,0
"We usually encounter, and thus here we favor, paired tags",0
with a ratio 1/3.,0
Check if the tags include the,0
"mandatory ""#"" number placeholder""",0
We split the user-defined tags in the # placeholder,0
in order to number them,0
Skip half examples to speed up the transform. This sets,0
"a hard limit of 0.5 to the `tags_corpus_ratio`, which is",0
excessive and should be avoided anyway.,0
normalize dict src/tgt for each dataset,0
"print(""src empty"")",0
"print(""too many same char in src"")",0
"print(""too many same word in src"")",0
"print(""avg token min"", len(src_str) / len(ex['src']))",0
"print(""avg token max"", len(src_str) / len(ex['src']))",0
"print(""text does not fully belong to wanted script"")",0
"print(""Some text belong to unwanted scripts"")",0
"print(""langid does not match"", _id(src_str))",0
"print(""src = tgt"")",0
"print(""tgt empty"")",0
"print(""src / tgt ratio "", len(src_str) / len(tgt_str))",0
"print(""too many same char in tgt"")",0
"print(""too many same word in tgt"")",0
"print(""avg token min"", len(tgt_str) / len(ex['tgt']))",0
"print(""avg token max"", len(tgt_str) / len(ex['tgt']))",0
"print(""text does not fully belong to wanted script"")",0
"print(""Some text belong to unwanted scripts"")",0
"print(""langid does not match"", _id(tgt_str))",0
"doc break we add it, restart new doc",0
case 1st ex is already longer,0
adding cur ex is too long we add cur doc,0
and reset doc to cur ex,0
we start the new doc with cur ex,0
we cumulate cur ex to cur doc,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
This method embeds a custom logic to correctly handle certain placeholders,0
in case the tokenizer doesn't preserve them.,0
Locate the end-of-sentence placeholders.,0
Tokenize each sentence separately.,0
Locate the mask-before placeholders,0
(to zero-out the prompt loss during LM finetuning).,0
Tokenize each chunk separately and insert the padding token.,0
between each sequence of tokens.,0
Re-insert the eos token.,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
Load vocabulary file if provided and set threshold,0
Load Subword Model,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
prefix src/tgt for each dataset,0
prefix as general option for inference,0
suffix src/tgt for each dataset,0
suffix as general option for inference,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Most code taken from: https://github.com/alvations/sacremoses,0
Which in turn is based on the Moses punctuation normalizer.,0
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/,0
tokenizer/normalize-punctuation.perl,0
don't fix period at end of sentence,0
Regex substitutions from replace-unicode-punctuation.perl,0
https://github.com/moses-smt/mosesdecoder/blob/master/,0
scripts/tokenizer/replace-unicode-punctuation.perl,0
Adds the penn substitutions after extra_whitespace regexes.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
Actual normalization.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
normalize dict src/tgt for each dataset,0
One source feature expected but none given and no default provided,0
Provided default does not match required features,0
Data not properly annotated.,0
In this case we do not use the default as it might be an error,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
make sure the scalars are in the event accumulator tags,0
required arguments,0
transforms that require vocab will not create if not provide vocab,0
1. Init first transform in the pipe,0
2. Init second transform in the pipe,0
3. Sequential combine them into a transform pipe,0
4. apply transform pipe for example,0
"5. example after the pipe exceed the length limit, thus filtered",0
6. Transform statistics registed (here for filtertoolong),0
"7. after report, statistics become empty as a fresh start",0
filter_transform.warm_up(),0
test BPE-dropout:,0
1. disable bpe dropout for not training example,0
2. enable bpe dropout for training example,0
3. (NOTE) disable dropout won't take effect if already seen,0
this is caused by the cache mechanism in bpe:,0
return cached subword if the original token is seen when no dropout,0
test SP regularization:,0
1. enable regularization for training example,0
2. disable regularization for not training example,0
Test mask location,0
Test mask location,0
Test mask location,0
Not apply token drop for not training example,0
apply token drop for training example,0
Not apply token mask for not training example,0
apply token mask for training example,0
require vocabs to warm_up,0
Not apply token mask for not training example,0
apply token mask for training example,0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
random_ratio of inserted tokens are chosen in vocab,0
others are MASK_TOK,0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
all token are considered as an individual word,0
1. tokens are dropped when replace_length is 0,0
"print(f""token delete: {masked} / {tokens}"")",0
2. tokens are replaced by MASK when replace_length is 1,0
"print(f""token mask: {masked} / {tokens}"")",0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
"1. replace_length 0: ""words"" are dropped",0
"print(f""word delete: {masked} / {tokens}"")",0
"self.assertEqual(len(masked), n_words - n_masked)",0
"2. replace_length 1: ""words"" are replaced with a single MASK",0
"print(f""whole word single mask: {masked} / {tokens}"")",0
len(masked) depend on number of tokens in select word,0
"3. replace_length -1: all tokens in ""words"" are replaced with MASK",0
"print(f""whole word multi mask: {masked} / {tokens}"")",0
number of mask_tok depend on number of tokens in selected word,0
number of MASK_TOK can be greater than n_masked,0
"insert_ratio=0.5,",0
"random_ratio=0.3,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
n_words = sum(token_starts),0
n_masked = math.ceil(n_words * bart_noise.mask_ratio),0
"print(f""Text Span Infilling: {infillied} / {tokens}"")",0
"print(n_words, n_masked)",0
Build the translator (along with the model),0
Required arguments,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
we finish 3 hyps per example in this step,0
new beam 1 is old beam 3,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('hidden_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
"attns[""coverage""] is actually c^(t+1) of See et al(2017)",0
1-index shifted,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Auto import python files in this directory,0
src_len is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate src_len as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x s or t len),0
1 = heads to be expanded in MHA,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
first value set to True triggered by the beginning of decoding,0
layer_cache becomes active in the MultiHeadedAttention fwd,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x tlen),0
1 = heads to be expanded in MHA,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"print(filled, sz)",0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
"propagate exception to parent process, keeping original traceback",0
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use apex.amp,0
In this case use the old FusedAdam with,0
FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
should be: self._optimizer.zero_grad(set_to_none),0
but apex.amp is not up-to-date:,0
https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
placing this here make it easier to call logger.info,0
"from anywhere, just 'from onmt.utils.logging import logger'",0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
Create a mask with zeros at prompt positions and ones at answer postions.,0
Apply the mask on the target side.,0
Put the padding token index at the prompt positions.,0
take into account here the tgt_shift_index (0 / 1 = LM/NMT),0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
in the case criterion reduction is None then we need,0
to sum the loss of each sentence in the batch,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check weight,0
Check features,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
It comes from training,0
TODO: needs to be added as inference opt,1
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
"this param init is overridden by model_builder, useless then.",0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
Helper functions,0
Keeps track of the original words/subwords,0
('prior_tokenization' option),0
In case there is a final case_markup when new_spacer is on,0
########## #,0
Translator #,0
########## #,0
Set translation options,0
Build translator from options,0
################### #,0
Validation iterator #,0
################### #,0
Reinstantiate the validation iterator,0
Retrieve raw references and sources,0
########### #,0
Predictions #,0
########### #,0
####### #,0
Outputs #,0
####### #,0
Flatten predictions,0
Save results,0
-*- coding: utf-8 -*-,0
this one is needed for Random Shuffler of batches,0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
we need to check the model path + any tokenizer path,0
patch to log stdout spawned processes of dataloader,0
bucket_size = batch_size,0
For TRAIN we need to group examples by length,0
"for faster performance, but otherwise, sequential.",0
For TRAIN we shuffle batches within the bucket,0
otherwise sequential,0
for specific case of rnn_packed need to be sorted,0
within the batch,0
single thread - create batch directly on GPU if device is gpu,0
multithread faster to create batch on CPU in each thread and then move it to gpu,0
Move tensor_batch from cpu to device,0
Check if all tokens have features or none at all,0
Make features part of src like,0
"{'src': {'src': ..., 'feats': [...., ....]}}",0
at this point an example looks like:,0
"{'src': {'src': ..., 'feats': [....]},",0
"'tgt': {'tgt': ...},",0
"'src_original': ['tok1', ...'tokn'],",0
"'tgt_original': ['tok1', ...'tokm'],",0
'indices' : seq in bucket,0
"'align': ...,",0
},0
Need to add features in last dimensions,0
Keep it consistent with dynamic data,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
-*- coding: utf-8 -*-,0
this is hack: if the special separator ｟newline｠is returned because of the,1
"""docify"" transform.get_specials we don't add it if the corresponding newline code",0
is already included in the sentencepiece or BPE-with-gpt2-pretok.,0
'src_original' and 'tgt_original' store the,0
original line before tokenization. These,0
fields are used later on in the feature,0
transforms.,0
NOTE: moved to dynamic_iterator.py cf process(),0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
bitsandbytes quantize weights when .cuda() is called,0
for huge models we need to save Ram,0
so we load the weights  module by module and transfer them to GPU for quantization,0
bitsandbytes quantize weights when .cuda() is called,0
for huge models we need to save Ram,0
so we load the weights  module by module and transfer them to GPU for quantization,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
This preserves backward-compat for models using customed layernorm,0
Force add_ffnbias to True if bias found in model w_1 keys,0
fix v2 compatibility,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
BPE training,0
SentencePiece training,0
!/usr/bin/env python,0
!/usr/bin/env python,0
Set sharing strategy manually instead of default based on the OS.,0
torch.multiprocessing.set_sharing_strategy('file_system'),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
magic indices,0
result caching,0
Here we set the decoder to start with self.start (BOS or EOS),0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
those two should be the same except feat dim,0
"batch['src'][perm[j], :, :])",0
trans.src,0
we rebuild a small batch made of the sub-segments,0
in the long segment.,0
new sub-batch ready to be translated,0
we re-insert the sub-batch in the initial translations,0
For seq2seq when we need to force doc to spit the same number of sents,0
In the case of length_penalty = none we report the total logprobs,0
divided by the number of sentence to get an approximation of the,0
per sentence logprob. We also return the corresponding ppl,0
"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs",0
are normalized per token we report the per line per token logprob,0
"and the corresponding ""per word perplexity""",0
Turn any copied words into UNKs.,0
"Decoder forward, takes [batch, tgt_len, nfeats] as input",0
"and [batch, src_len, hidden] as enc_out",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
at this point scores is batch first (dim=0),0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``",0
Quick fix. Transformers return None as enc_states.,0
enc_states are only used later on to init decoder's state,0
"but are never used in Transformer decoder, so we can skip",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
it should be done in a better way,0
here dec_in is batch first,0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
"decoder_input = decode_strategy.current_predictions.view(1, -1,",0
1),0
Reorder states.,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
These comp lists are costy but less than for loops,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
Auto import python files in this directory,0
"def custom_stopping_criteria(input_ids, score, **kwargs):",0
"stop_ids = [29871, 13, 13] # \n\n",0
return input_ids[-len(stop_ids)],0
Build the translator (along with the model),0
get prompt and make sure it fits,0
"def custom_stopping_criteria(input_ids, score, **kwargs):",0
"stop_ids = [29871, 13, 13] # \n\n",0
return input_ids[-len(stop_ids)],0
Build the translator (along with the model),0
get prompt and make sure it fits,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '6.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
"inf_type = ""ct2""",0
#####################,0
Inference with CT2 #,0
#####################,0
#####################,0
Inference with -py #,0
#####################,0
"ckpt_path = ""finetuned_llama7B/llama7B-vicuna-onmt_step_4000.pt""",0
we receive a text box content,0
might be good to split also based on full period (later),0
we reformat the transformed batch to be numericalized / tensorified,0
#####,0
UI #,0
#####,0
What are the 3 best french cities ?,0
Which one is better if I like outdoor activities ?,0
Which one is better if I like cultural outings?,0
What are the best neighborhoods in these 5 cities?,0
!/usr/bin/env python3,0
Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens,0
flake8: noqa,0
-*- coding: utf-8 -*-,0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: sentencepiece_model.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(module_scope),0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
from onmt.utils.misc import use_gpu,0
"Add in default model arguments, possibly added since training.",0
this patch is no longer needed included in converter,0
"if hasattr(model_opt, 'rnn_size'):",0
model_opt.hidden_size = model_opt.rnn_size,0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
Falcon stores QKV in one single tensor but it is not simply piled up Q+K+V,0
it is heads interleaved to we need to slice first,0
also it uses the HF rotary so we need to permute Q and K interleave,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python3,0
coding: utf-8,0
"In order to use this tool, please install comet first",0
https://github.com/Unbabel/COMET,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate source and reference sentences nbest times,0
for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
same for source.,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \,0
--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best comet score,0
when choosing a reference-less model no nbest-ref is required,0
for nbest in nbests:,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python mbr_bleu.py --nbest-hyp target.5.tl \,0
--nbest-order 5 --output target.mbr.tl,0
It will compare all hyp with eachother and output the max bleu,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
Now we can pipe the full file through the model using the Iterator,0
reminder a batch includes .src .tgt .indices and it is sorted,0
Compute and retrieve the loss for EACH sentence,0
Now we need to rearrange the batch of ppl,0
in the original order with indices,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate reference sentences nbest times for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \,0
--nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best bleu wrt reference,0
and output the max bleu,0
!/usr/bin/env python,0
with the two module = imp.load_source() below,0
we ghost the old torchtext.data.field and depercated,0
onmt.inputters.text_dataset,0
however this require some functions / classes to be,0
monkey patched for loading the old field/vocab objects.,0
"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
!/usr/bin/env python,0
redpajama stores QKV in one single tensor but it is not simply piled up Q+K+V,0
it is heads interleaved to we need to slice first,0
also it uses the HF rotary so we need to permute Q and K interleave,0
Avoid functionality on inference,0
weights are in the .pt file,0
weights are not in the .pt checkpoint but stored in the safetensors file,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
Remove old vocabulary associated embeddings,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
If new training initialize the model params,0
If update_vocab init also but checkpoint will overwrite old weights,0
ONLY for legacy fusedam with amp pytorch requires NOT to half the model,0
Update model embeddings with those from the checkpoint,0
after initialization,0
after this checkpoint contains no embeddings,0
when using LoRa or updating the vocab (no more embeddings in ckpt),0
=> strict=False when loading state_dict,0
weights are in the .pt file,0
weights are not in the .pt checkpoint but stored in the safetensors file,0
!/usr/bin/env python,0
if transform + options set in 'valid' we need to copy in main,0
transform / options for scoring considered as inference,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
Override checkpoint's freezing settings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
GPU,0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
Freeze Encoder and/or Decoder,0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
LoRa,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options related to source and target features,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Adding options related to Transforms,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Create a thread to listen for errors in the child processes.,0
Basic attributes.,0
Set model in training mode.,0
Let's clean the GPUs before training loop,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
raw_srcs = [],0
raw_refs = [],0
F-prop through the model.,0
Compute loss.,0
Compute validation metrics (at batch.dataset level),0
Compute stats,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
The loss of the prompt will be set to zero.,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
src lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Auto import python files in this directory,0
batch x len x dim,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
mask is now (batch x 1 x slen x slen),0
1 to be expanded to number of heads in MHA,0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
Help functions for Rotary Embeddings,0
https://arxiv.org/pdf/2104.09864.pdf,0
too convoluted to make maxseqlen a parameter.,0
we suppose src_seq_len at training and max_length at inference,0
are both < 2048 tokens.,0
"rope is now matrix [maxseqlen, dim/2]",0
Help functions for max_relative positions,0
https://arxiv.org/abs/1803.02155,0
Shift values to be >= 0,0
"now relative_position is in the range [0, inf)",0
half of the buckets are for exact increments in positions,0
The other half of the buckets are for logarithmically bigger bins in positions,0
up to max_distance,0
Help functions to split model dim per head,0
class MultiHeadedAttention(torch.jit.ScriptModule):,0
https://arxiv.org/pdf/1803.02155.pdf,0
in the paper they suggest either two embeds,0
relative_key / relative_value or only,0
relative_key. We implemented the same embed,0
for both.,0
@torch.jit.script_method,0
"1) Project key, value, and query.",0
as a reminder at training layer_cache[0] remains False,0
expand key on heads dimension when it's less than query heads (multi-query variant),0
expand value on heads dimension when it's less than query heads (multi-query variant),0
"2) When standard pos. enc. or rotary, use flash attention",0
batch x num_heads x query_len x key_len,0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
not 100% necessary but expand to nb of heads,0
now mask and scores have the same shape,0
3) Apply attention dropout and compute context vectors.,0
We use the same embeddings for key and value,0
--------------------------------------------------------------------------,0
copied and adapted https://github.com/microsoft/LoRA/,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License (MIT).,0
Support bnb quantization of nderlying layers,0
--------------------------------------------------------------------------,0
Optional dropout,0
Mark the weight as unmerged,0
LoRA implemented in a dense layer,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
we do not super().reset_parameters() save lot of time and useless when no grad.,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
cannot merge/unmerge quantized weigts with unquantized lora_X,0
Check if QLoraLinear has a custom __init__ method,0
Invoke the __init__ method of QLoraLinear,0
LoRA implemented in a dense layer,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
"for silu, see: https://arxiv.org/pdf/2002.05202.pdf",0
-*- coding: utf-8 -*-,0
class AverageAttention(torch.jit.ScriptModule):,0
@torch.jit.script,0
Code taken from bitsandbytes but modified with arg device to accept skipt_init,0
from torch.nn.utils => makes model building way faster.,0
"weights are cast automatically as Int8Params, but the bias has to be cast manually",0
reorder weight layout back from ampere/turing to row,0
"we only need to save SCB as extra data, because CB for quantized weights",0
is already stored in weight.data,0
"case 1: .cuda was called, SCB is in self.weight",0
"case 2: self.init_8bit_state was called, SCB is in self.state",0
"buffers not yet initialized, can't call them directly without",0
"weights are cast automatically as Int8Params, but the bias has to be cast manually",0
we converted 8-bit row major to turing/ampere format in the first inference pass,0
we no longer need the row-major weight,0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
We exclude tokenization for contractions in,0
order to avoid inconsistencies with pyonmtok's tokenization.,0
"(e.g. ""I ca n't"" with spacy, ""I can ' t"" with pyonmttok)",0
Use Spacy's stopwords to get rid of junk entries,0
Perform tokenization with spacy for consistency.,0
We ensure that the target lemma is present in the lemmatized,0
"target string, that the match is an exact match (there is",0
whitespace before or after the term),0
and we perform some bound checking.,0
Map the lemmatized string match index to,0
the lemmatized list index,0
We need to know if the term is multiword,0
Join multiword target lemmas with a unique separator so,0
we can treat them as single word and not change the indices.,0
Construct the final source from the lemmatized list,0
that contains the terms. We compare the tokens in the,0
term-augmented lemma list with the tokens in the original,0
"lemma list. If the lemma is the same, then we replace with",0
the token from the original tokenized source list. If they,0
"are not the same, it means the lemma has been augemented",0
"with a term, so we inject this in the final list.",0
Restore the spaces in multi-word terms,0
Skip half examples to improve performance. This means we set,0
"a hard limit for the `term_corpus_ratio` to 0.5, which is actually",0
quite high. TODO: We can add this (skipping examples) as an option,1
Filter out very short or very long sentences,0
from the TM for better performance,0
We split the `batch` and perform fuzzy matching,0
in smaller chunks of 10.000 examples in order to,0
reduce memory usage.,0
Perfomance is not affected.,0
Probably redundant but let's be safe,0
in case some examples are already fuzzied,0
(e.g. from another pipeline or workflow),0
We don't want exact matches,0
Apply a basic filtering to leave out very short or very long,0
sentences and speed up things a bit during fuzzy matching,0
Do nothing,0
We set the start number of tags to a random number from 1,0
to 12 + the number of subsequent tags that,0
will be added. We also apply weights to this choice so tags,0
"are more probable to start from 1, then from 2, etc.",0
This way we cover most scenarios met in real usage and,0
the system will learn to handle a fairly large number of,0
numbered tags (but not an excessively large number),0
Make sure we only search for exact matches (we don't want,0
to match part of words) and perform some bound checking,0
Create all possible tag forms. We inject a special,0
unicode char (∥) as a placeholder for whitespace in order,0
to keep the indices unaltered. This char is replaced with,0
spaces before we return the augmented examples.,0
Make a weighted choice between paired tags or single tags.,0
"We usually encounter, and thus here we favor, paired tags",0
with a ratio 1/3.,0
Check if the tags include the,0
"mandatory ""#"" number placeholder""",0
We split the user-defined tags in the # placeholder,0
in order to number them,0
Skip half examples to speed up the transform. This sets,0
"a hard limit of 0.5 to the `tags_corpus_ratio`, which is",0
excessive and should be avoided anyway.,0
normalize dict src/tgt for each dataset,0
"print(""src empty"")",0
"print(""too many same char in src"")",0
"print(""too many same word in src"")",0
"print(""avg token min"", len(src_str) / len(ex['src']))",0
"print(""avg token max"", len(src_str) / len(ex['src']))",0
"print(""text does not fully belong to wanted script"")",0
"print(""Some text belong to unwanted scripts"")",0
"print(""langid does not match"", _id(src_str))",0
"print(""src = tgt"")",0
"print(""tgt empty"")",0
"print(""src / tgt ratio "", len(src_str) / len(tgt_str))",0
"print(""too many same char in tgt"")",0
"print(""too many same word in tgt"")",0
"print(""avg token min"", len(tgt_str) / len(ex['tgt']))",0
"print(""avg token max"", len(tgt_str) / len(ex['tgt']))",0
"print(""text does not fully belong to wanted script"")",0
"print(""Some text belong to unwanted scripts"")",0
"print(""langid does not match"", _id(tgt_str))",0
"doc break we add it, restart new doc",0
case 1st ex is already longer,0
adding cur ex is too long we add cur doc,0
and reset doc to cur ex,0
we start the new doc with cur ex,0
we cumulate cur ex to cur doc,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
This method embeds a custom logic to correctly handle certain placeholders,0
in case the tokenizer doesn't preserve them.,0
Locate the end-of-sentence placeholders.,0
Tokenize each sentence separately.,0
Locate the mask-before placeholders,0
(to zero-out the prompt loss during LM finetuning).,0
Tokenize each chunk separately and insert the padding token.,0
between each sequence of tokens.,0
Re-insert the eos token.,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
Load vocabulary file if provided and set threshold,0
Load Subword Model,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
prefix src/tgt for each dataset,0
prefix as general option for inference,0
suffix src/tgt for each dataset,0
suffix as general option for inference,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Most code taken from: https://github.com/alvations/sacremoses,0
Which in turn is based on the Moses punctuation normalizer.,0
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/,0
tokenizer/normalize-punctuation.perl,0
don't fix period at end of sentence,0
Regex substitutions from replace-unicode-punctuation.perl,0
https://github.com/moses-smt/mosesdecoder/blob/master/,0
scripts/tokenizer/replace-unicode-punctuation.perl,0
Adds the penn substitutions after extra_whitespace regexes.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
Actual normalization.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
normalize dict src/tgt for each dataset,0
One source feature expected but none given and no default provided,0
Provided default does not match required features,0
Data not properly annotated.,0
In this case we do not use the default as it might be an error,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
make sure the scalars are in the event accumulator tags,0
required arguments,0
transforms that require vocab will not create if not provide vocab,0
1. Init first transform in the pipe,0
2. Init second transform in the pipe,0
3. Sequential combine them into a transform pipe,0
4. apply transform pipe for example,0
"5. example after the pipe exceed the length limit, thus filtered",0
6. Transform statistics registed (here for filtertoolong),0
"7. after report, statistics become empty as a fresh start",0
filter_transform.warm_up(),0
test BPE-dropout:,0
1. disable bpe dropout for not training example,0
2. enable bpe dropout for training example,0
3. (NOTE) disable dropout won't take effect if already seen,0
this is caused by the cache mechanism in bpe:,0
return cached subword if the original token is seen when no dropout,0
test SP regularization:,0
1. enable regularization for training example,0
2. disable regularization for not training example,0
Test mask location,0
Test mask location,0
Test mask location,0
Not apply token drop for not training example,0
apply token drop for training example,0
Not apply token mask for not training example,0
apply token mask for training example,0
require vocabs to warm_up,0
Not apply token mask for not training example,0
apply token mask for training example,0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
random_ratio of inserted tokens are chosen in vocab,0
others are MASK_TOK,0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
all token are considered as an individual word,0
1. tokens are dropped when replace_length is 0,0
"print(f""token delete: {masked} / {tokens}"")",0
2. tokens are replaced by MASK when replace_length is 1,0
"print(f""token mask: {masked} / {tokens}"")",0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
"1. replace_length 0: ""words"" are dropped",0
"print(f""word delete: {masked} / {tokens}"")",0
"self.assertEqual(len(masked), n_words - n_masked)",0
"2. replace_length 1: ""words"" are replaced with a single MASK",0
"print(f""whole word single mask: {masked} / {tokens}"")",0
len(masked) depend on number of tokens in select word,0
"3. replace_length -1: all tokens in ""words"" are replaced with MASK",0
"print(f""whole word multi mask: {masked} / {tokens}"")",0
number of mask_tok depend on number of tokens in selected word,0
number of MASK_TOK can be greater than n_masked,0
"insert_ratio=0.5,",0
"random_ratio=0.3,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
n_words = sum(token_starts),0
n_masked = math.ceil(n_words * bart_noise.mask_ratio),0
"print(f""Text Span Infilling: {infillied} / {tokens}"")",0
"print(n_words, n_masked)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
we finish 3 hyps per example in this step,0
new beam 1 is old beam 3,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('hidden_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
"attns[""coverage""] is actually c^(t+1) of See et al(2017)",0
1-index shifted,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Auto import python files in this directory,0
src_len is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate src_len as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
BoolTensor was introduced in pytorch 1.2,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x s or t len),0
1 = heads to be expanded in MHA,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
first value set to True triggered by the beginning of decoding,0
layer_cache becomes active in the MultiHeadedAttention fwd,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x tlen),0
1 = heads to be expanded in MHA,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"print(filled, sz)",0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
"propagate exception to parent process, keeping original traceback",0
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use apex.amp,0
In this case use the old FusedAdam with,0
FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
should be: self._optimizer.zero_grad(set_to_none),0
but apex.amp is not up-to-date:,0
https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
placing this here make it easier to call logger.info,0
"from anywhere, just 'from onmt.utils.logging import logger'",0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
Create a mask with zeros at prompt positions and ones at answer postions.,0
Apply the mask on the target side.,0
Put the padding token index at the prompt positions.,0
take into account here the tgt_shift_index (0 / 1 = LM/NMT),0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
in the case criterion reduction is None then we need,0
to sum the loss of each sentence in the batch,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check weight,0
Check features,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
It comes from training,0
TODO: needs to be added as inference opt,1
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
"this param init is overridden by model_builder, useless then.",0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
Helper functions,0
Keeps track of the original words/subwords,0
('prior_tokenization' option),0
In case there is a final case_markup when new_spacer is on,0
########## #,0
Translator #,0
########## #,0
Set translation options,0
Build translator from options,0
################### #,0
Validation iterator #,0
################### #,0
Reinstantiate the validation iterator,0
Retrieve raw references and sources,0
########### #,0
Predictions #,0
########### #,0
####### #,0
Outputs #,0
####### #,0
Flatten predictions,0
Save results,0
-*- coding: utf-8 -*-,0
this one is needed for Random Shuffler of batches,0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
we need to check the model path + any tokenizer path,0
patch to log stdout spawned processes of dataloader,0
bucket_size = batch_size,0
For TRAIN we need to group examples by length,0
"for faster performance, but otherwise, sequential.",0
For TRAIN we shuffle batches within the bucket,0
otherwise sequential,0
for specific case of rnn_packed need to be sorted,0
within the batch,0
Check if all tokens have features or none at all,0
Make features part of src like,0
"{'src': {'src': ..., 'feats': [...., ....]}}",0
at this point an example looks like:,0
"{'src': {'src': ..., 'feats': [....]},",0
"'tgt': {'tgt': ...},",0
"'src_original': ['tok1', ...'tokn'],",0
"'tgt_original': ['tok1', ...'tokm'],",0
'indices' : seq in bucket,0
"'align': ...,",0
},0
Need to add features in last dimensions,0
Keep it consistent with dynamic data,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
-*- coding: utf-8 -*-,0
temporary as long as translation_server and scoring_preparator still use lists,0
this is hack: if the special separator ｟newline｠is returned because of the,1
"""docify"" transform.get_specials we don't add it if the corresponding newline code",0
is already included in the sentencepiece or BPE-with-gpt2-pretok.,0
'src_original' and 'tgt_original' store the,0
original line before tokenization. These,0
fields are used later on in the feature,0
transforms.,0
NOTE: moved to dynamic_iterator.py cf process(),0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
bitsandbytes quantize weights when .cuda() is called,0
for huge models we need to save Ram,0
so we load the weights  module by module and transfer them to GPU for quantization,0
bitsandbytes quantize weights when .cuda() is called,0
for huge models we need to save Ram,0
so we load the weights  module by module and transfer them to GPU for quantization,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
This preserves backward-compat for models using customed layernorm,0
Force add_ffnbias to True if bias found in model w_1 keys,0
fix v2 compatibility,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
BPE training,0
SentencePiece training,0
!/usr/bin/env python,0
!/usr/bin/env python,0
Set sharing strategy manually instead of default based on the OS.,0
torch.multiprocessing.set_sharing_strategy('file_system'),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
magic indices,0
result caching,0
Here we set the decoder to start with self.start (BOS or EOS),0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
those two should be the same except feat dim,0
"batch['src'][perm[j], :, :])",0
trans.src,0
we rebuild a small batch made of the sub-segments,0
in the long segment.,0
new sub-batch ready to be translated,0
we re-insert the sub-batch in the initial translations,0
In the case of length_penalty = none we report the total logprobs,0
divided by the number of sentence to get an approximation of the,0
per sentence logprob. We also return the corresponding ppl,0
"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs",0
are normalized per token we report the per line per token logprob,0
"and the corresponding ""per word perplexity""",0
Turn any copied words into UNKs.,0
"Decoder forward, takes [batch, tgt_len, nfeats] as input",0
"and [batch, src_len, hidden] as enc_out",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
at this point scores is batch first (dim=0),0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``",0
Quick fix. Transformers return None as enc_states.,0
enc_states are only used later on to init decoder's state,0
"but are never used in Transformer decoder, so we can skip",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
it should be done in a better way,0
here dec_in is batch first,0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
"decoder_input = decode_strategy.current_predictions.view(1, -1,",0
1),0
Reorder states.,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
Auto import python files in this directory,0
"def custom_stopping_criteria(input_ids, score, **kwargs):",0
"stop_ids = [29871, 13, 13] # \n\n",0
return input_ids[-len(stop_ids)],0
Build the translator (along with the model),0
get prompt and make sure it fits,0
"def custom_stopping_criteria(input_ids, score, **kwargs):",0
"stop_ids = [29871, 13, 13] # \n\n",0
return input_ids[-len(stop_ids)],0
Build the translator (along with the model),0
Build the transforms (along with the tokenizer),0
get prompt and make sure it fits,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '6.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
"inf_type = ""ct2""",0
#####################,0
Inference with CT2 #,0
#####################,0
#####################,0
Inference with -py #,0
#####################,0
"ckpt_path = ""finetuned_llama7B/llama7B-vicuna-onmt_step_4000.pt""",0
we receive a text box content,0
might be good to split also based on full period (later),0
we reformat the transformed batch to be numericalized / tensorified,0
#####,0
UI #,0
#####,0
What are the 3 best french cities ?,0
Which one is better if I like outdoor activities ?,0
Which one is better if I like cultural outings?,0
What are the best neighborhoods in these 5 cities?,0
!/usr/bin/env python3,0
Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens,0
flake8: noqa,0
-*- coding: utf-8 -*-,0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: sentencepiece_model.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(module_scope),0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
from onmt.utils.misc import use_gpu,0
"Add in default model arguments, possibly added since training.",0
this patch is no longer needed included in converter,0
"if hasattr(model_opt, 'rnn_size'):",0
model_opt.hidden_size = model_opt.rnn_size,0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
Falcon stores QKV in one single tensor but it is not simply piled up Q+K+V,0
it is heads interleaved to we need to slice first,0
also it uses the HF rotary so we need to permute Q and K interleave,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python3,0
coding: utf-8,0
"In order to use this tool, please install comet first",0
https://github.com/Unbabel/COMET,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate source and reference sentences nbest times,0
for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
same for source.,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \,0
--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best comet score,0
when choosing a reference-less model no nbest-ref is required,0
for nbest in nbests:,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python mbr_bleu.py --nbest-hyp target.5.tl \,0
--nbest-order 5 --output target.mbr.tl,0
It will compare all hyp with eachother and output the max bleu,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
Now we can pipe the full file through the model using the Iterator,0
reminder a batch includes .src .tgt .indices and it is sorted,0
Compute and retrieve the loss for EACH sentence,0
Now we need to rearrange the batch of ppl,0
in the original order with indices,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate reference sentences nbest times for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \,0
--nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best bleu wrt reference,0
and output the max bleu,0
!/usr/bin/env python,0
with the two module = imp.load_source() below,0
we ghost the old torchtext.data.field and depercated,0
onmt.inputters.text_dataset,0
however this require some functions / classes to be,0
monkey patched for loading the old field/vocab objects.,0
"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
!/usr/bin/env python,0
redpajama stores QKV in one single tensor but it is not simply piled up Q+K+V,0
it is heads interleaved to we need to slice first,0
also it uses the HF rotary so we need to permute Q and K interleave,0
Avoid functionality on inference,0
weights are in the .pt file,0
weights are not in the .pt checkpoint but stored in the safetensors file,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
Remove old vocabulary associated embeddings,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
If new training initialize the model params,0
If update_vocab init also but checkpoint will overwrite old weights,0
ONLY for legacy fusedam with amp pytorch requires NOT to half the model,0
Update model embeddings with those from the checkpoint,0
after initialization,0
after this checkpoint contains no embeddings,0
when using LoRa or updating the vocab (no more embeddings in ckpt),0
=> strict=False when loading state_dict,0
weights are in the .pt file,0
weights are not in the .pt checkpoint but stored in the safetensors file,0
!/usr/bin/env python,0
if transform + options set in 'valid' we need to copy in main,0
transform / options for scoring considered as inference,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
Override checkpoint's freezing settings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
GPU,0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
Freeze Encoder and/or Decoder,0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
LoRa,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options related to source and target features,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Adding options related to Transforms,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Create a thread to listen for errors in the child processes.,0
Basic attributes.,0
Set model in training mode.,0
Let's clean the GPUs before training loop,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
raw_srcs = [],0
raw_refs = [],0
F-prop through the model.,0
Compute loss.,0
Compute validation metrics (at batch.dataset level),0
Compute stats,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
src lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Auto import python files in this directory,0
batch x len x dim,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
mask is now (batch x 1 x slen x slen),0
1 to be expanded to number of heads in MHA,0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
Help functions for Rotary Embeddings,0
https://arxiv.org/pdf/2104.09864.pdf,0
too convoluted to make maxseqlen a parameter.,0
we suppose src_seq_len at training and max_length at inference,0
are both < 2048 tokens.,0
"rope is now matrix [maxseqlen, dim/2]",0
Help functions for max_relative positions,0
https://arxiv.org/abs/1803.02155,0
Shift values to be >= 0,0
"now relative_position is in the range [0, inf)",0
half of the buckets are for exact increments in positions,0
The other half of the buckets are for logarithmically bigger bins in positions,0
up to max_distance,0
Help functions to split model dim per head,0
class MultiHeadedAttention(torch.jit.ScriptModule):,0
https://arxiv.org/pdf/1803.02155.pdf,0
in the paper they suggest either two embeds,0
relative_key / relative_value or only,0
relative_key. We implemented the same embed,0
for both.,0
@torch.jit.script_method,0
"1) Project key, value, and query.",0
as a reminder at training layer_cache[0] remains False,0
expand key on heads dimension when it's less than query heads (multi-query variant),0
expand value on heads dimension when it's less than query heads (multi-query variant),0
"2) When standard pos. enc. or rotary, use flash attention",0
batch x num_heads x query_len x key_len,0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
not 100% necessary but expand to nb of heads,0
now mask and scores have the same shape,0
3) Apply attention dropout and compute context vectors.,0
We use the same embeddings for key and value,0
--------------------------------------------------------------------------,0
copied and adapted https://github.com/microsoft/LoRA/,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License (MIT).,0
Support bnb quantization of nderlying layers,0
--------------------------------------------------------------------------,0
Optional dropout,0
Mark the weight as unmerged,0
LoRA implemented in a dense layer,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
we do not super().reset_parameters() save lot of time and useless when no grad.,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
cannot merge/unmerge quantized weigts with unquantized lora_X,0
Check if QLoraLinear has a custom __init__ method,0
Invoke the __init__ method of QLoraLinear,0
LoRA implemented in a dense layer,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
"for silu, see: https://arxiv.org/pdf/2002.05202.pdf",0
-*- coding: utf-8 -*-,0
class AverageAttention(torch.jit.ScriptModule):,0
@torch.jit.script,0
Code taken from bitsandbytes but modified with arg device to accept skipt_init,0
from torch.nn.utils => makes model building way faster.,0
"weights are cast automatically as Int8Params, but the bias has to be cast manually",0
reorder weight layout back from ampere/turing to row,0
"we only need to save SCB as extra data, because CB for quantized weights",0
is already stored in weight.data,0
"case 1: .cuda was called, SCB is in self.weight",0
"case 2: self.init_8bit_state was called, SCB is in self.state",0
"buffers not yet initialized, can't call them directly without",0
"weights are cast automatically as Int8Params, but the bias has to be cast manually",0
we converted 8-bit row major to turing/ampere format in the first inference pass,0
we no longer need the row-major weight,0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
We exclude tokenization for contractions in,0
order to avoid inconsistencies with pyonmtok's tokenization.,0
"(e.g. ""I ca n't"" with spacy, ""I can ' t"" with pyonmttok)",0
Use Spacy's stopwords to get rid of junk entries,0
Perform tokenization with spacy for consistency.,0
We ensure that the target lemma is present in the lemmatized,0
"target string, that the match is an exact match (there is",0
whitespace before or after the term),0
and we perform some bound checking.,0
Map the lemmatized string match index to,0
the lemmatized list index,0
We need to know if the term is multiword,0
Join multiword target lemmas with a unique separator so,0
we can treat them as single word and not change the indices.,0
Construct the final source from the lemmatized list,0
that contains the terms. We compare the tokens in the,0
term-augmented lemma list with the tokens in the original,0
"lemma list. If the lemma is the same, then we replace with",0
the token from the original tokenized source list. If they,0
"are not the same, it means the lemma has been augemented",0
"with a term, so we inject this in the final list.",0
Restore the spaces in multi-word terms,0
Skip half examples to improve performance. This means we set,0
"a hard limit for the `term_corpus_ratio` to 0.5, which is actually",0
quite high. TODO: We can add this (skipping examples) as an option,1
Filter out very short or very long sentences,0
from the TM for better performance,0
We split the `batch` and perform fuzzy matching,0
in smaller chunks of 10.000 examples in order to,0
reduce memory usage.,0
Perfomance is not affected.,0
Probably redundant but let's be safe,0
in case some examples are already fuzzied,0
(e.g. from another pipeline or workflow),0
We don't want exact matches,0
Apply a basic filtering to leave out very short or very long,0
sentences and speed up things a bit during fuzzy matching,0
Do nothing,0
We set the start number of tags to a random number from 1,0
to 12 + the number of subsequent tags that,0
will be added. We also apply weights to this choice so tags,0
"are more probable to start from 1, then from 2, etc.",0
This way we cover most scenarios met in real usage and,0
the system will learn to handle a fairly large number of,0
numbered tags (but not an excessively large number),0
Make sure we only search for exact matches (we don't want,0
to match part of words) and perform some bound checking,0
Create all possible tag forms. We inject a special,0
unicode char (∥) as a placeholder for whitespace in order,0
to keep the indices unaltered. This char is replaced with,0
spaces before we return the augmented examples.,0
Make a weighted choice between paired tags or single tags.,0
"We usually encounter, and thus here we favor, paired tags",0
with a ratio 1/3.,0
Check if the tags include the,0
"mandatory ""#"" number placeholder""",0
We split the user-defined tags in the # placeholder,0
in order to number them,0
Skip half examples to speed up the transform. This sets,0
"a hard limit of 0.5 to the `tags_corpus_ratio`, which is",0
excessive and should be avoided anyway.,0
normalize dict src/tgt for each dataset,0
"print(""src empty"")",0
"print(""too many same char in src"")",0
"print(""too many same word in src"")",0
"print(""avg token min"", len(src_str) / len(ex['src']))",0
"print(""avg token max"", len(src_str) / len(ex['src']))",0
"print(""text does not fully belong to wanted script"")",0
"print(""Some text belong to unwanted scripts"")",0
"print(""langid does not match"", _id(src_str))",0
"print(""src = tgt"")",0
"print(""tgt empty"")",0
"print(""src / tgt ratio "", len(src_str) / len(tgt_str))",0
"print(""too many same char in tgt"")",0
"print(""too many same word in tgt"")",0
"print(""avg token min"", len(tgt_str) / len(ex['tgt']))",0
"print(""avg token max"", len(tgt_str) / len(ex['tgt']))",0
"print(""text does not fully belong to wanted script"")",0
"print(""Some text belong to unwanted scripts"")",0
"print(""langid does not match"", _id(tgt_str))",0
"doc break we add it, restart new doc",0
case 1st ex is already longer,0
adding cur ex is too long we add cur doc,0
and reset doc to cur ex,0
we start the new doc with cur ex,0
we cumulate cur ex to cur doc,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
Load vocabulary file if provided and set threshold,0
Load Subword Model,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
prefix src/tgt for each dataset,0
prefix as general option for inference,0
suffix src/tgt for each dataset,0
suffix as general option for inference,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Most code taken from: https://github.com/alvations/sacremoses,0
Which in turn is based on the Moses punctuation normalizer.,0
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/,0
tokenizer/normalize-punctuation.perl,0
don't fix period at end of sentence,0
Regex substitutions from replace-unicode-punctuation.perl,0
https://github.com/moses-smt/mosesdecoder/blob/master/,0
scripts/tokenizer/replace-unicode-punctuation.perl,0
Adds the penn substitutions after extra_whitespace regexes.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
Actual normalization.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
normalize dict src/tgt for each dataset,0
One source feature expected but none given and no default provided,0
Provided default does not match required features,0
Data not properly annotated.,0
In this case we do not use the default as it might be an error,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
make sure the scalars are in the event accumulator tags,0
required arguments,0
transforms that require vocab will not create if not provide vocab,0
1. Init first transform in the pipe,0
2. Init second transform in the pipe,0
3. Sequential combine them into a transform pipe,0
4. apply transform pipe for example,0
"5. example after the pipe exceed the length limit, thus filtered",0
6. Transform statistics registed (here for filtertoolong),0
"7. after report, statistics become empty as a fresh start",0
filter_transform.warm_up(),0
test BPE-dropout:,0
1. disable bpe dropout for not training example,0
2. enable bpe dropout for training example,0
3. (NOTE) disable dropout won't take effect if already seen,0
this is caused by the cache mechanism in bpe:,0
return cached subword if the original token is seen when no dropout,0
test SP regularization:,0
1. enable regularization for training example,0
2. disable regularization for not training example,0
Not apply token drop for not training example,0
apply token drop for training example,0
Not apply token mask for not training example,0
apply token mask for training example,0
require vocabs to warm_up,0
Not apply token mask for not training example,0
apply token mask for training example,0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
random_ratio of inserted tokens are chosen in vocab,0
others are MASK_TOK,0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
all token are considered as an individual word,0
1. tokens are dropped when replace_length is 0,0
"print(f""token delete: {masked} / {tokens}"")",0
2. tokens are replaced by MASK when replace_length is 1,0
"print(f""token mask: {masked} / {tokens}"")",0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
"1. replace_length 0: ""words"" are dropped",0
"print(f""word delete: {masked} / {tokens}"")",0
"self.assertEqual(len(masked), n_words - n_masked)",0
"2. replace_length 1: ""words"" are replaced with a single MASK",0
"print(f""whole word single mask: {masked} / {tokens}"")",0
len(masked) depend on number of tokens in select word,0
"3. replace_length -1: all tokens in ""words"" are replaced with MASK",0
"print(f""whole word multi mask: {masked} / {tokens}"")",0
number of mask_tok depend on number of tokens in selected word,0
number of MASK_TOK can be greater than n_masked,0
"insert_ratio=0.5,",0
"random_ratio=0.3,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
n_words = sum(token_starts),0
n_masked = math.ceil(n_words * bart_noise.mask_ratio),0
"print(f""Text Span Infilling: {infillied} / {tokens}"")",0
"print(n_words, n_masked)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
we finish 3 hyps per example in this step,0
new beam 1 is old beam 3,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('hidden_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
"attns[""coverage""] is actually c^(t+1) of See et al(2017)",0
1-index shifted,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Auto import python files in this directory,0
src_len is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate src_len as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
BoolTensor was introduced in pytorch 1.2,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x s or t len),0
1 = heads to be expanded in MHA,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
first value set to True triggered by the beginning of decoding,0
layer_cache becomes active in the MultiHeadedAttention fwd,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x tlen),0
1 = heads to be expanded in MHA,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"print(filled, sz)",0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
"propagate exception to parent process, keeping original traceback",0
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use apex.amp,0
In this case use the old FusedAdam with,0
FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
should be: self._optimizer.zero_grad(set_to_none),0
but apex.amp is not up-to-date:,0
https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
placing this here make it easier to call logger.info,0
"from anywhere, just 'from onmt.utils.logging import logger'",0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
take into account here the tgt_shift_index (0 / 1 = LM/NMT),0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
in the case criterion reduction is None then we need,0
to sum the loss of each sentence in the batch,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check weight,0
Check features,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
It comes from training,0
TODO: needs to be added as inference opt,1
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
"this param init is overridden by model_builder, useless then.",0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
Helper functions,0
Keeps track of the original words/subwords,0
('prior_tokenization' option),0
In case there is a final case_markup when new_spacer is on,0
########## #,0
Translator #,0
########## #,0
Set translation options,0
Build translator from options,0
################### #,0
Validation iterator #,0
################### #,0
Reinstantiate the validation iterator,0
Retrieve raw references and sources,0
########### #,0
Predictions #,0
########### #,0
####### #,0
Outputs #,0
####### #,0
Flatten predictions,0
Save results,0
-*- coding: utf-8 -*-,0
this one is needed for Random Shuffler of batches,0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
we need to check the model path + any tokenizer path,0
patch to log stdout spawned processes of dataloader,0
bucket_size = batch_size,0
For TRAIN we need to group examples by length,0
"for faster performance, but otherwise, sequential.",0
For TRAIN we shuffle batches within the bucket,0
otherwise sequential,0
for specific case of rnn_packed need to be sorted,0
within the batch,0
Check if all tokens have features or none at all,0
Make features part of src like,0
"{'src': {'src': ..., 'feats': [...., ....]}}",0
at this point an example looks like:,0
"{'src': {'src': ..., 'feats': [....]},",0
"'tgt': {'tgt': ...},",0
"'src_original': ['tok1', ...'tokn'],",0
"'tgt_original': ['tok1', ...'tokm'],",0
'indices' : seq in bucket,0
"'align': ...,",0
},0
Need to add features in last dimensions,0
Keep it consistent with dynamic data,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
-*- coding: utf-8 -*-,0
temporary as long as translation_server and scoring_preparator still use lists,0
this is hack: if the special separator ｟newline｠is returned because of the,1
"""docify"" transform.get_specials we don't add it if the corresponding newline code",0
is already included in the sentencepiece or BPE-with-gpt2-pretok.,0
'src_original' and 'tgt_original' store the,0
original line before tokenization. These,0
fields are used later on in the feature,0
transforms.,0
NOTE: moved to dynamic_iterator.py cf process(),0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
bitsandbytes quantize weights when .cuda() is called,0
for huge models we need to save Ram,0
so we load the weights  module by module and transfer them to GPU for quantization,0
bitsandbytes quantize weights when .cuda() is called,0
for huge models we need to save Ram,0
so we load the weights  module by module and transfer them to GPU for quantization,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
This preserves backward-compat for models using customed layernorm,0
Force add_ffnbias to True if bias found in model w_1 keys,0
fix v2 compatibility,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
BPE training,0
SentencePiece training,0
!/usr/bin/env python,0
!/usr/bin/env python,0
Set sharing strategy manually instead of default based on the OS.,0
torch.multiprocessing.set_sharing_strategy('file_system'),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
magic indices,0
result caching,0
Here we set the decoder to start with self.start (BOS or EOS),0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
those two should be the same except feat dim,0
"batch['src'][perm[j], :, :])",0
trans.src,0
we rebuild a small batch made of the sub-segments,0
in the long segment.,0
new sub-batch ready to be translated,0
we re-insert the sub-batch in the initial translations,0
In the case of length_penalty = none we report the total logprobs,0
divided by the number of sentence to get an approximation of the,0
per sentence logprob. We also return the corresponding ppl,0
"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs",0
are normalized per token we report the per line per token logprob,0
"and the corresponding ""per word perplexity""",0
Turn any copied words into UNKs.,0
"Decoder forward, takes [batch, tgt_len, nfeats] as input",0
"and [batch, src_len, hidden] as enc_out",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
at this point scores is batch first (dim=0),0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``",0
Quick fix. Transformers return None as enc_states.,0
enc_states are only used later on to init decoder's state,0
"but are never used in Transformer decoder, so we can skip",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
it should be done in a better way,0
here dec_in is batch first,0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
"decoder_input = decode_strategy.current_predictions.view(1, -1,",0
1),0
Reorder states.,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
Auto import python files in this directory,0
"def custom_stopping_criteria(input_ids, score, **kwargs):",0
"stop_ids = [29871, 13, 13] # \n\n",0
return input_ids[-len(stop_ids)],0
Build the translator (along with the model),0
get prompt and make sure it fits,0
"def custom_stopping_criteria(input_ids, score, **kwargs):",0
"stop_ids = [29871, 13, 13] # \n\n",0
return input_ids[-len(stop_ids)],0
Build the translator (along with the model),0
Build the transforms (along with the tokenizer),0
get prompt and make sure it fits,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '6.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
"inf_type = ""ct2""",0
#####################,0
Inference with CT2 #,0
#####################,0
#####################,0
Inference with -py #,0
#####################,0
"ckpt_path = ""finetuned_llama7B/llama7B-vicuna-onmt_step_4000.pt""",0
we receive a text box content,0
might be good to split also based on full period (later),0
we reformat the transformed batch to be numericalized / tensorified,0
#####,0
UI #,0
#####,0
What are the 3 best french cities ?,0
Which one is better if I like outdoor activities ?,0
Which one is better if I like cultural outings?,0
What are the best neighborhoods in these 5 cities?,0
!/usr/bin/env python3,0
Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
from onmt.utils.misc import use_gpu,0
"Add in default model arguments, possibly added since training.",0
this patch is no longer needed included in converter,0
"if hasattr(model_opt, 'rnn_size'):",0
model_opt.hidden_size = model_opt.rnn_size,0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
Falcon stores QKV in one single tensor but it is not simply piled up Q+K+V,0
it is heads interleaved to we need to slice first,0
also it uses the HF rotary so we need to permute Q and K interleave,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python3,0
coding: utf-8,0
"In order to use this tool, please install comet first",0
https://github.com/Unbabel/COMET,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate source and reference sentences nbest times,0
for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
same for source.,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \,0
--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best comet score,0
when choosing a reference-less model no nbest-ref is required,0
for nbest in nbests:,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python mbr_bleu.py --nbest-hyp target.5.tl \,0
--nbest-order 5 --output target.mbr.tl,0
It will compare all hyp with eachother and output the max bleu,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
Now we can pipe the full file through the model using the Iterator,0
reminder a batch includes .src .tgt .indices and it is sorted,0
Compute and retrieve the loss for EACH sentence,0
Now we need to rearrange the batch of ppl,0
in the original order with indices,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate reference sentences nbest times for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \,0
--nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best bleu wrt reference,0
and output the max bleu,0
!/usr/bin/env python,0
with the two module = imp.load_source() below,0
we ghost the old torchtext.data.field and depercated,0
onmt.inputters.text_dataset,0
however this require some functions / classes to be,0
monkey patched for loading the old field/vocab objects.,0
"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
!/usr/bin/env python,0
redpajama stores QKV in one single tensor but it is not simply piled up Q+K+V,0
it is heads interleaved to we need to slice first,0
also it uses the HF rotary so we need to permute Q and K interleave,0
Avoid functionality on inference,0
weights are in the .pt file,0
weights are not in the .pt checkpoint but stored in the safetensors file,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
Remove old vocabulary associated embeddings,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
If new training initialize the model params,0
If update_vocab init also but checkpoint will overwrite old weights,0
ONLY for legacy fusedam with amp pytorch requires NOT to half the model,0
Update model embeddings with those from the checkpoint,0
after initialization,0
after this checkpoint contains no embeddings,0
when using LoRa or updating the vocab (no more embeddings in ckpt),0
=> strict=False when loading state_dict,0
weights are in the .pt file,0
weights are not in the .pt checkpoint but stored in the safetensors file,0
!/usr/bin/env python,0
if transform + options set in 'valid' we need to copy in main,0
transform / options for scoring considered as inference,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
Override checkpoint's freezing settings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
Freeze Encoder and/or Decoder,0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
GPU,0
LoRa,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options related to source and target features,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Adding options related to Transforms,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
Let's clean the GPUs before training loop,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Compute validation metrics (at batch.dataset level),0
Compute stats,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
Compute and save stats,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
src lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Auto import python files in this directory,0
batch x len x dim,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
mask is now (batch x 1 x slen x slen),0
1 to be expanded to number of heads in MHA,0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
Help functions for Rotary Embeddings,0
https://arxiv.org/pdf/2104.09864.pdf,0
too convoluted to make maxseqlen a parameter.,0
we suppose src_seq_len at training and max_length at inference,0
are both < 2048 tokens.,0
"rope is now matrix [maxseqlen, dim/2]",0
Help functions for max_relative positions,0
https://arxiv.org/abs/1803.02155,0
Shift values to be >= 0,0
Help functions to split model dim per head,0
class MultiHeadedAttention(torch.jit.ScriptModule):,0
https://arxiv.org/pdf/1803.02155.pdf,0
in the paper they suggest either two embeds,0
relative_key / relative_value or only,0
relative_key. We implemented the same embed,0
for both.,0
@torch.jit.script_method,0
"1) Project key, value, and query.",0
as a reminder at training layer_cache[0] remains False,0
2) Calculate and scale scores.,0
expand key on heads dimension when it's less than query heads (multi-query variant),0
batch x num_heads x query_len x key_len,0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
not 100% necessary but expand to nb of heads,0
now mask and scores have the same shape,0
3) Apply attention dropout and compute context vectors.,0
expand value on heads dimension when it's less than query heads (multi-query variant),0
We use the same embeddings for key and value,0
--------------------------------------------------------------------------,0
copied and adapted https://github.com/microsoft/LoRA/,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License (MIT).,0
Support bnb quantization of nderlying layers,0
--------------------------------------------------------------------------,0
Optional dropout,0
Mark the weight as unmerged,0
LoRA implemented in a dense layer,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
we do not super().reset_parameters() save lot of time and useless when no grad.,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
cannot merge/unmerge quantized weigts with unquantized lora_X,0
Check if QLoraLinear has a custom __init__ method,0
Invoke the __init__ method of QLoraLinear,0
LoRA implemented in a dense layer,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
"for silu, see: https://arxiv.org/pdf/2002.05202.pdf",0
-*- coding: utf-8 -*-,0
class AverageAttention(torch.jit.ScriptModule):,0
@torch.jit.script,0
Code taken from bitsandbytes but modified with arg device to accept skipt_init,0
from torch.nn.utils => makes model building way faster.,0
"weights are cast automatically as Int8Params, but the bias has to be cast manually",0
reorder weight layout back from ampere/turing to row,0
"we only need to save SCB as extra data, because CB for quantized weights",0
is already stored in weight.data,0
"case 1: .cuda was called, SCB is in self.weight",0
"case 2: self.init_8bit_state was called, SCB is in self.state",0
"buffers not yet initialized, can't call them directly without",0
"weights are cast automatically as Int8Params, but the bias has to be cast manually",0
we converted 8-bit row major to turing/ampere format in the first inference pass,0
we no longer need the row-major weight,0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
Filter out very short or very long sentences,0
from the TM for better performance,0
We split the `batch` and perform fuzzy matching,0
in smaller chunks of 10.000 examples in order to,0
reduce memory usage.,0
Perfomance is not affected.,0
Probably redundant but let's be safe,0
in case some examples are already fuzzied,0
(e.g. from another pipeline or workflow),0
We don't want exact matches,0
Apply a basic filtering to leave out very short or very long,0
sentences and speed up things a bit during fuzzy matching,0
Do nothing,0
We set the start number of tags to a random number from 1,0
to 12 + the number of subsequent tags that,0
will be added. We also apply weights to this choice so tags,0
"are more probable to start from 1, then from 2, etc.",0
This way we cover most scenarios met in real usage and,0
the system will learn to handle a fairly large number of,0
numbered tags (but not an excessively large number),0
Make sure we only search for exact matches (we don't want,0
to match part of words) and perform some bound checking,0
Create all possible tag forms. We inject a special,0
unicode char (∥) as a placeholder for whitespace in order,0
to keep the indices unaltered. This char is replaced with,0
spaces before we return the augmented examples.,0
Make a weighted choice between paired tags or single tags.,0
"We usually encounter, and thus here we favor, paired tags",0
with a ratio 1/3.,0
Check if the tags include the,0
"mandatory ""#"" number placeholder""",0
We split the user-defined tags in the # placeholder,0
in order to number them,0
normalize dict src/tgt for each dataset,0
"print(""src empty"")",0
"print(""too many same char in src"")",0
"print(""too many same word in src"")",0
"print(""avg token min"", len(src_str) / len(ex['src']))",0
"print(""avg token max"", len(src_str) / len(ex['src']))",0
"print(""text does not fully belong to wanted script"")",0
"print(""Some text belong to unwanted scripts"")",0
"print(""langid does not match"", _id(src_str))",0
"print(""src = tgt"")",0
"print(""tgt empty"")",0
"print(""src / tgt ratio "", len(src_str) / len(tgt_str))",0
"print(""too many same char in tgt"")",0
"print(""too many same word in tgt"")",0
"print(""avg token min"", len(tgt_str) / len(ex['tgt']))",0
"print(""avg token max"", len(tgt_str) / len(ex['tgt']))",0
"print(""text does not fully belong to wanted script"")",0
"print(""Some text belong to unwanted scripts"")",0
"print(""langid does not match"", _id(tgt_str))",0
"doc break we add it, restart new doc",0
case 1st ex is already longer,0
adding cur ex is too long we add cur doc,0
and reset doc to cur ex,0
we start the new doc with cur ex,0
we cumulate cur ex to cur doc,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
Load vocabulary file if provided and set threshold,0
Load Subword Model,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
prefix src/tgt for each dataset,0
prefix as general option for inference,0
suffix src/tgt for each dataset,0
suffix as general option for inference,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Most code taken from: https://github.com/alvations/sacremoses,0
Which in turn is based on the Moses punctuation normalizer.,0
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/,0
tokenizer/normalize-punctuation.perl,0
don't fix period at end of sentence,0
Regex substitutions from replace-unicode-punctuation.perl,0
https://github.com/moses-smt/mosesdecoder/blob/master/,0
scripts/tokenizer/replace-unicode-punctuation.perl,0
Adds the penn substitutions after extra_whitespace regexes.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
Actual normalization.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
normalize dict src/tgt for each dataset,0
One source feature expected but none given and no default provided,0
Provided default does not match required features,0
Data not properly annotated.,0
In this case we do not use the default as it might be an error,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
make sure the scalars are in the event accumulator tags,0
required arguments,0
transforms that require vocab will not create if not provide vocab,0
1. Init first transform in the pipe,0
2. Init second transform in the pipe,0
3. Sequential combine them into a transform pipe,0
4. apply transform pipe for example,0
"5. example after the pipe exceed the length limit, thus filtered",0
6. Transform statistics registed (here for filtertoolong),0
"7. after report, statistics become empty as a fresh start",0
filter_transform.warm_up(),0
test BPE-dropout:,0
1. disable bpe dropout for not training example,0
2. enable bpe dropout for training example,0
3. (NOTE) disable dropout won't take effect if already seen,0
this is caused by the cache mechanism in bpe:,0
return cached subword if the original token is seen when no dropout,0
test SP regularization:,0
1. enable regularization for training example,0
2. disable regularization for not training example,0
Not apply token drop for not training example,0
apply token drop for training example,0
Not apply token mask for not training example,0
apply token mask for training example,0
require vocabs to warm_up,0
Not apply token mask for not training example,0
apply token mask for training example,0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
random_ratio of inserted tokens are chosen in vocab,0
others are MASK_TOK,0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
all token are considered as an individual word,0
1. tokens are dropped when replace_length is 0,0
"print(f""token delete: {masked} / {tokens}"")",0
2. tokens are replaced by MASK when replace_length is 1,0
"print(f""token mask: {masked} / {tokens}"")",0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
"1. replace_length 0: ""words"" are dropped",0
"print(f""word delete: {masked} / {tokens}"")",0
"self.assertEqual(len(masked), n_words - n_masked)",0
"2. replace_length 1: ""words"" are replaced with a single MASK",0
"print(f""whole word single mask: {masked} / {tokens}"")",0
len(masked) depend on number of tokens in select word,0
"3. replace_length -1: all tokens in ""words"" are replaced with MASK",0
"print(f""whole word multi mask: {masked} / {tokens}"")",0
number of mask_tok depend on number of tokens in selected word,0
number of MASK_TOK can be greater than n_masked,0
"insert_ratio=0.5,",0
"random_ratio=0.3,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
n_words = sum(token_starts),0
n_masked = math.ceil(n_words * bart_noise.mask_ratio),0
"print(f""Text Span Infilling: {infillied} / {tokens}"")",0
"print(n_words, n_masked)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
we finish 3 hyps per example in this step,0
new beam 1 is old beam 3,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('hidden_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
"attns[""coverage""] is actually c^(t+1) of See et al(2017)",0
1-index shifted,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Auto import python files in this directory,0
src_len is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate src_len as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
BoolTensor was introduced in pytorch 1.2,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x s or t len),0
1 = heads to be expanded in MHA,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
first value set to True triggered by the beginning of decoding,0
layer_cache becomes active in the MultiHeadedAttention fwd,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x tlen),0
1 = heads to be expanded in MHA,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"print(filled, sz)",0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use apex.amp,0
In this case use the old FusedAdam with,0
FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
should be: self._optimizer.zero_grad(set_to_none),0
but apex.amp is not up-to-date:,0
https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
placing this here make it easier to call logger.info,0
"from anywhere, just 'from onmt.utils.logging import logger'",0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
take into account here the tgt_shift_index (0 / 1 = LM/NMT),0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
in the case criterion reduction is None then we need,0
to sum the loss of each sentence in the batch,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check weight,0
Check features,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
It comes from training,0
TODO: needs to be added as inference opt,1
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
"this param init is overridden by model_builder, useless then.",0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
Helper functions,0
Keeps track of the original words/subwords,0
('prior_tokenization' option),0
In case there is a final case_markup when new_spacer is on,0
translate,0
for validation we build an infer_iter per batch,0
in order to avoid oom issues because there is no,0
batching strategy in `textbatch_to_tensor`,0
apply_reverse refs,0
flatten preds,0
save results,0
-*- coding: utf-8 -*-,0
this one is needed for Random Shuffler of batches,0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
we need to check the model path + any tokenizer path,0
patch to log stdout spawned processes of dataloader,0
bucket_size = batch_size,0
For TRAIN we need to group examples by length,0
"for faster performance, but otherwise, sequential.",0
For TRAIN we shuffle batches within the bucket,0
otherwise sequential,0
for specific case of rnn_packed need to be sorted,0
within the batch,0
Check if all tokens have features or none at all,0
Make features part of src like,0
"{'src': {'src': ..., 'feats': [...., ....]}}",0
at this point an example looks like:,0
"{'src': {'src': ..., 'feats': [....]},",0
"'tgt': {'tgt': ...},",0
"'src_original': ['tok1', ...'tokn'],",0
"'tgt_original': ['tok1', ...'tokm'],",0
'indices' : seq in bucket,0
"'align': ...,",0
},0
Need to add features in last dimensions,0
Keep it consistent with dynamic data,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
-*- coding: utf-8 -*-,0
temporary as long as translation_server and scoring_preparator still use lists,0
this is hack: if the special separator ｟newline｠is returned because of the,1
"""docify"" transform.get_specials we don't add it if the corresponding newline code",0
is already included in the sentencepiece or BPE-with-gpt2-pretok.,0
'src_original' and 'tgt_original' store the,0
original line before tokenization. These,0
fields are used later on in the feature,0
transforms.,0
NOTE: moved to dynamic_iterator.py cf process(),0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
bitsandbytes quantize weights when .cuda() is called,0
for huge models we need to save Ram,0
so we load the weights  module by module and transfer them to GPU for quantization,0
bitsandbytes quantize weights when .cuda() is called,0
for huge models we need to save Ram,0
so we load the weights  module by module and transfer them to GPU for quantization,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
This preserves backward-compat for models using customed layernorm,0
Force add_ffnbias to True if bias found in model w_1 keys,0
fix v2 compatibility,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
BPE training,0
SentencePiece training,0
!/usr/bin/env python,0
!/usr/bin/env python,0
Set sharing strategy manually instead of default based on the OS.,0
torch.multiprocessing.set_sharing_strategy('file_system'),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
magic indices,0
result caching,0
Here we set the decoder to start with self.start (BOS or EOS),0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
those two should be the same except feat dim,0
"batch['src'][perm[j], :, :])",0
trans.src,0
we rebuild a small batch made of the sub-segments,0
in the long segment.,0
new sub-batch ready to be translated,0
we re-insert the sub-batch in the initial translations,0
In the case of length_penalty = none we report the total logprobs,0
divided by the number of sentence to get an approximation of the,0
per sentence logprob. We also return the corresponding ppl,0
"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs",0
are normalized per token we report the per line per token logprob,0
"and the corresponding ""per word perplexity""",0
Turn any copied words into UNKs.,0
"Decoder forward, takes [batch, tgt_len, nfeats] as input",0
"and [batch, src_len, hidden] as enc_out",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
at this point scores is batch first (dim=0),0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``",0
Quick fix. Transformers return None as enc_states.,0
enc_states are only used later on to init decoder's state,0
"but are never used in Transformer decoder, so we can skip",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
it should be done in a better way,0
here dec_in is batch first,0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
"decoder_input = decode_strategy.current_predictions.view(1, -1,",0
1),0
Reorder states.,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
Auto import python files in this directory,0
"def custom_stopping_criteria(input_ids, score, **kwargs):",0
"stop_ids = [29871, 13, 13] # \n\n",0
return input_ids[-len(stop_ids)],0
Build the translator (along with the model),0
Build the transforms (along with the tokenizer),0
get prompt and make sure it fits,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '6.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
"inf_type = ""ct2""",0
#####################,0
Inference with CT2 #,0
#####################,0
#####################,0
Inference with -py #,0
#####################,0
"ckpt_path = ""finetuned_llama7B/llama7B-vicuna-onmt_step_4000.pt""",0
we receive a text box content,0
might be good to split also based on full period (later),0
we reformat the transformed batch to be numericalized / tensorified,0
#####,0
UI #,0
#####,0
What are the 3 best french cities ?,0
Which one is better if I like outdoor activities ?,0
Which one is better if I like cultural outings?,0
What are the best neighborhoods in these 5 cities?,0
!/usr/bin/env python3,0
Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
from onmt.utils.misc import use_gpu,0
"Add in default model arguments, possibly added since training.",0
this patch is no longer needed included in converter,0
"if hasattr(model_opt, 'rnn_size'):",0
model_opt.hidden_size = model_opt.rnn_size,0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
flake8: noqa,0
Falcon stores QKV in one single tensor but it is not simply piled up Q+K+V,0
it is heads interleaved to we need to slice first,0
also it uses the HF rotary so we need to permute Q and K interleave,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python3,0
coding: utf-8,0
"In order to use this tool, please install comet first",0
https://github.com/Unbabel/COMET,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate source and reference sentences nbest times,0
for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
same for source.,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \,0
--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best comet score,0
when choosing a reference-less model no nbest-ref is required,0
for nbest in nbests:,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python mbr_bleu.py --nbest-hyp target.5.tl \,0
--nbest-order 5 --output target.mbr.tl,0
It will compare all hyp with eachother and output the max bleu,0
!/usr/bin/env python,0
flake8: noqa,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
Now we can pipe the full file through the model using the Iterator,0
reminder a batch includes .src .tgt .indices and it is sorted,0
Compute and retrieve the loss for EACH sentence,0
Now we need to rearrange the batch of ppl,0
in the original order with indices,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
flake8: noqa,0
!/usr/bin/env python,0
!/usr/bin/env python,0
flake8: noqa,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate reference sentences nbest times for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \,0
--nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best bleu wrt reference,0
and output the max bleu,0
!/usr/bin/env python,0
with the two module = imp.load_source() below,0
we ghost the old torchtext.data.field and depercated,0
onmt.inputters.text_dataset,0
however this require some functions / classes to be,0
monkey patched for loading the old field/vocab objects.,0
"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
!/usr/bin/env python,0
flake8: noqa,0
redpajama stores QKV in one single tensor but it is not simply piled up Q+K+V,0
it is heads interleaved to we need to slice first,0
also it uses the HF rotary so we need to permute Q and K interleave,0
Avoid functionality on inference,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
Remove old vocabulary associated embeddings,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
If new training initialize the model params,0
If update_vocab init also but checkpoint will overwrite old weights,0
ONLY for legacy fusedam with amp pytorch requires NOT to half the model,0
Update model embeddings with those from the checkpoint,0
after initialization,0
after this checkpoint contains no embeddings,0
when using LoRa or updating the vocab (no more embeddings in ckpt),0
=> strict=False when loading state_dict,0
!/usr/bin/env python,0
if transform + options set in 'valid' we need to copy in main,0
transform / options for scoring considered as inference,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
Override checkpoint's freezing settings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
Freeze Encoder and/or Decoder,0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
GPU,0
LoRa,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options related to source and target features,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Adding options related to Transforms,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
Let's clean the GPUs before training loop,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Compute validation metrics (at batch.dataset level),0
Compute stats,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
Compute and save stats,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
src lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
batch x len x dim,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
mask is now (batch x 1 x slen x slen),0
1 to be expanded to number of heads in MHA,0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
Help functions for Rotary Embeddings,0
https://arxiv.org/pdf/2104.09864.pdf,0
too convoluted to make maxseqlen a parameter.,0
we suppose src_seq_len at training and max_length at inference,0
are both < 2048 tokens.,0
"rope is now matrix [maxseqlen, dim/2]",0
Help functions for max_relative positions,0
https://arxiv.org/abs/1803.02155,0
Shift values to be >= 0,0
Help functions to split model dim per head,0
class MultiHeadedAttention(torch.jit.ScriptModule):,0
https://arxiv.org/pdf/1803.02155.pdf,0
in the paper they suggest either two embeds,0
relative_key / relative_value or only,0
relative_key. We implemented the same embed,0
for both.,0
@torch.jit.script_method,0
"1) Project key, value, and query.",0
as a reminder at training layer_cache[0] remains False,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
not 100% necessary but expand to nb of heads,0
now mask and scores have the same shape,0
3) Apply attention dropout and compute context vectors.,0
We use the same embeddings for key and value,0
--------------------------------------------------------------------------,0
copied and adapted https://github.com/microsoft/LoRA/,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License (MIT).,0
Support bnb quantization of nderlying layers,0
--------------------------------------------------------------------------,0
Optional dropout,0
Mark the weight as unmerged,0
LoRA implemented in a dense layer,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
we do not super().reset_parameters() save lot of time and useless when no grad.,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
cannot merge/unmerge quantized weigts with unquantized lora_X,0
Check if QLoraLinear has a custom __init__ method,0
Invoke the __init__ method of QLoraLinear,0
LoRA implemented in a dense layer,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
"for silu, see: https://arxiv.org/pdf/2002.05202.pdf",0
-*- coding: utf-8 -*-,0
class AverageAttention(torch.jit.ScriptModule):,0
@torch.jit.script,0
Code taken from bitsandbytes but modified with arg device to accept skipt_init,0
from torch.nn.utils => makes model building way faster.,0
"weights are cast automatically as Int8Params, but the bias has to be cast manually",0
reorder weight layout back from ampere/turing to row,0
"we only need to save SCB as extra data, because CB for quantized weights",0
is already stored in weight.data,0
"case 1: .cuda was called, SCB is in self.weight",0
"case 2: self.init_8bit_state was called, SCB is in self.state",0
"buffers not yet initialized, can't call them directly without",0
"weights are cast automatically as Int8Params, but the bias has to be cast manually",0
we converted 8-bit row major to turing/ampere format in the first inference pass,0
we no longer need the row-major weight,0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
Filter out very short or very long sentences,0
from the TM for better performance,0
We split the `batch` and perform fuzzy matching,0
in smaller chunks of 10.000 examples in order to,0
reduce memory usage.,0
Perfomance is not affected.,0
Probably redundant but let's be safe,0
in case some examples are already fuzzied,0
(e.g. from another pipeline or workflow),0
We don't want exact matches,0
Apply a basic filtering to leave out very short or very long,0
sentences and speed up things a bit during fuzzy matching,0
Do nothing,0
We set the start number of tags to a random number from 1,0
to 12 + the number of subsequent tags that,0
will be added. We also apply weights to this choice so tags,0
"are more probable to start from 1, then from 2, etc.",0
This way we cover most scenarios met in real usage and,0
the system will learn to handle a fairly large number of,0
numbered tags (but not an excessively large number),0
Make sure we only search for exact matches (we don't want,0
to match part of words) and perform some bound checking,0
Create all possible tag forms. We inject a special,0
unicode char (∥) as a placeholder for whitespace in order,0
to keep the indices unaltered. This char is replaced with,0
spaces before we return the augmented examples.,0
Make a weighted choice between paired tags or single tags.,0
"We usually encounter, and thus here we favor, paired tags",0
with a ratio 1/3.,0
Check if the tags include the,0
"mandatory ""#"" number placeholder""",0
We split the user-defined tags in the # placeholder,0
in order to number them,0
normalize dict src/tgt for each dataset,0
"print(""src empty"")",0
"print(""too many same char in src"")",0
"print(""too many same word in src"")",0
"print(""avg token min"", len(src_str) / len(ex['src']))",0
"print(""avg token max"", len(src_str) / len(ex['src']))",0
"print(""text does not fully belong to wanted script"")",0
"print(""Some text belong to unwanted scripts"")",0
"print(""langid does not match"", _id(src_str))",0
"print(""src = tgt"")",0
"print(""tgt empty"")",0
"print(""src / tgt ratio "", len(src_str) / len(tgt_str))",0
"print(""too many same char in tgt"")",0
"print(""too many same word in tgt"")",0
"print(""avg token min"", len(tgt_str) / len(ex['tgt']))",0
"print(""avg token max"", len(tgt_str) / len(ex['tgt']))",0
"print(""text does not fully belong to wanted script"")",0
"print(""Some text belong to unwanted scripts"")",0
"print(""langid does not match"", _id(tgt_str))",0
"doc break we add it, restart new doc",0
case 1st ex is already longer,0
adding cur ex is too long we add cur doc,0
and reset doc to cur ex,0
we start the new doc with cur ex,0
we cumulate cur ex to cur doc,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
Load vocabulary file if provided and set threshold,0
Load Subword Model,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
prefix src/tgt for each dataset,0
prefix as general option for inference,0
suffix src/tgt for each dataset,0
suffix as general option for inference,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Most code taken from: https://github.com/alvations/sacremoses,0
Which in turn is based on the Moses punctuation normalizer.,0
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/,0
tokenizer/normalize-punctuation.perl,0
don't fix period at end of sentence,0
Regex substitutions from replace-unicode-punctuation.perl,0
https://github.com/moses-smt/mosesdecoder/blob/master/,0
scripts/tokenizer/replace-unicode-punctuation.perl,0
Adds the penn substitutions after extra_whitespace regexes.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
Actual normalization.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
normalize dict src/tgt for each dataset,0
One source feature expected but none given and no default provided,0
Provided default does not match required features,0
Data not properly annotated.,0
In this case we do not use the default as it might be an error,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
make sure the scalars are in the event accumulator tags,0
required arguments,0
transforms that require vocab will not create if not provide vocab,0
1. Init first transform in the pipe,0
2. Init second transform in the pipe,0
3. Sequential combine them into a transform pipe,0
4. apply transform pipe for example,0
"5. example after the pipe exceed the length limit, thus filtered",0
6. Transform statistics registed (here for filtertoolong),0
"7. after report, statistics become empty as a fresh start",0
filter_transform.warm_up(),0
test BPE-dropout:,0
1. disable bpe dropout for not training example,0
2. enable bpe dropout for training example,0
3. (NOTE) disable dropout won't take effect if already seen,0
this is caused by the cache mechanism in bpe:,0
return cached subword if the original token is seen when no dropout,0
test SP regularization:,0
1. enable regularization for training example,0
2. disable regularization for not training example,0
Not apply token drop for not training example,0
apply token drop for training example,0
Not apply token mask for not training example,0
apply token mask for training example,0
require vocabs to warm_up,0
Not apply token mask for not training example,0
apply token mask for training example,0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
random_ratio of inserted tokens are chosen in vocab,0
others are MASK_TOK,0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
all token are considered as an individual word,0
1. tokens are dropped when replace_length is 0,0
"print(f""token delete: {masked} / {tokens}"")",0
2. tokens are replaced by MASK when replace_length is 1,0
"print(f""token mask: {masked} / {tokens}"")",0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
"1. replace_length 0: ""words"" are dropped",0
"print(f""word delete: {masked} / {tokens}"")",0
"self.assertEqual(len(masked), n_words - n_masked)",0
"2. replace_length 1: ""words"" are replaced with a single MASK",0
"print(f""whole word single mask: {masked} / {tokens}"")",0
len(masked) depend on number of tokens in select word,0
"3. replace_length -1: all tokens in ""words"" are replaced with MASK",0
"print(f""whole word multi mask: {masked} / {tokens}"")",0
number of mask_tok depend on number of tokens in selected word,0
number of MASK_TOK can be greater than n_masked,0
"insert_ratio=0.5,",0
"random_ratio=0.3,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
n_words = sum(token_starts),0
n_masked = math.ceil(n_words * bart_noise.mask_ratio),0
"print(f""Text Span Infilling: {infillied} / {tokens}"")",0
"print(n_words, n_masked)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
we finish 3 hyps per example in this step,0
new beam 1 is old beam 3,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('hidden_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
"attns[""coverage""] is actually c^(t+1) of See et al(2017)",0
1-index shifted,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
src_len is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate src_len as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
BoolTensor was introduced in pytorch 1.2,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x s or t len),0
1 = heads to be expanded in MHA,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
first value set to True triggered by the beginning of decoding,0
layer_cache becomes active in the MultiHeadedAttention fwd,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x tlen),0
1 = heads to be expanded in MHA,0
"feed_forward applies residual, so we remove and apply residual with un-normed",0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"print(filled, sz)",0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use apex.amp,0
In this case use the old FusedAdam with,0
FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
should be: self._optimizer.zero_grad(set_to_none),0
but apex.amp is not up-to-date:,0
https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
placing this here make it easier to call logger.info,0
"from anywhere, just 'from onmt.utils.logging import logger'",0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
take into account here the tgt_shift_index (0 / 1 = LM/NMT),0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
in the case criterion reduction is None then we need,0
to sum the loss of each sentence in the batch,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check weight,0
Check features,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
It comes from training,0
TODO: needs to be added as inference opt,1
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
"this param init is overridden by model_builder, useless then.",0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
Helper functions,0
Keeps track of the original words/subwords,0
('prior_tokenization' option),0
In case there is a final case_markup when new_spacer is on,0
translate,0
for validation we build an infer_iter per batch,0
in order to avoid oom issues because there is no,0
batching strategy in `textbatch_to_tensor`,0
apply_reverse refs,0
flatten preds,0
save results,0
-*- coding: utf-8 -*-,0
this one is needed for Random Shuffler of batches,0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
we need to check the model path + any tokenizer path,0
patch to log stdout spawned processes of dataloader,0
bucket_size = batch_size,0
For TRAIN we need to group examples by length,0
"for faster performance, but otherwise, sequential.",0
For TRAIN we shuffle batches within the bucket,0
otherwise sequential,0
for specific case of rnn_packed need to be sorted,0
within the batch,0
Check if all tokens have features or none at all,0
Make features part of src like,0
"{'src': {'src': ..., 'feats': [...., ....]}}",0
at this point an example looks like:,0
"{'src': {'src': ..., 'feats': [....]},",0
"'tgt': {'tgt': ...},",0
"'src_original': ['tok1', ...'tokn'],",0
"'tgt_original': ['tok1', ...'tokm'],",0
'indices' : seq in bucket,0
"'align': ...,",0
},0
Need to add features in last dimensions,0
Keep it consistent with dynamic data,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
-*- coding: utf-8 -*-,0
this is hack: if the special separator ｟newline｠is returned because of the,1
"""docify"" transform.get_specials we don't add it if the corresponding newline code",0
is already included in the sentencepiece or BPE-with-gpt2-pretok.,0
'src_original' and 'tgt_original' store the,0
original line before tokenization. These,0
fields are used later on in the feature,0
transforms.,0
NOTE: moved to dynamic_iterator.py cf process(),0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
bitsandbytes quantize weights when .cuda() is called,0
for huge models we need to save Ram,0
so we load the weights  module by module and transfer them to GPU for quantization,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
This preserves backward-compat for models using customed layernorm,0
Force add_ffnbias to True if bias found in model w_1 keys,0
fix v2 compatibility,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
BPE training,0
SentencePiece training,0
!/usr/bin/env python,0
!/usr/bin/env python,0
Set sharing strategy manually instead of default based on the OS.,0
torch.multiprocessing.set_sharing_strategy('file_system'),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
magic indices,0
result caching,0
Here we set the decoder to start with self.start (BOS or EOS),0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
those two should be the same except feat dim,0
"batch['src'][perm[j], :, :])",0
trans.src,0
we rebuild a small batch made of the sub-segments,0
in the long segment.,0
new sub-batch ready to be translated,0
we re-insert the sub-batch in the initial translations,0
In the case of length_penalty = none we report the total logprobs,0
divided by the number of sentence to get an approximation of the,0
per sentence logprob. We also return the corresponding ppl,0
"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs",0
are normalized per token we report the per line per token logprob,0
"and the corresponding ""per word perplexity""",0
Turn any copied words into UNKs.,0
"Decoder forward, takes [batch, tgt_len, nfeats] as input",0
"and [batch, src_len, hidden] as enc_out",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
at this point scores is batch first (dim=0),0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``",0
Quick fix. Transformers return None as enc_states.,0
enc_states are only used later on to init decoder's state,0
"but are never used in Transformer decoder, so we can skip",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
it should be done in a better way,0
here dec_in is batch first,0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
"decoder_input = decode_strategy.current_predictions.view(1, -1,",0
1),0
Reorder states.,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
Auto import python files in this directory,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '6.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
What are the 3 best french cities ?,0
Which one is better if I like outdoor activities ?,0
Which one is better if I like cultural outings?,0
What are the best neighborhoods in these 5 cities?,0
!/usr/bin/env python3,0
Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
from onmt.utils.misc import use_gpu,0
"Add in default model arguments, possibly added since training.",0
this patch is no longer needed included in converter,0
"if hasattr(model_opt, 'rnn_size'):",0
model_opt.hidden_size = model_opt.rnn_size,0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python3,0
coding: utf-8,0
"In order to use this tool, please install comet first",0
https://github.com/Unbabel/COMET,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate source and reference sentences nbest times,0
for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
same for source.,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \,0
--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best comet score,0
when choosing a reference-less model no nbest-ref is required,0
for nbest in nbests:,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python mbr_bleu.py --nbest-hyp target.5.tl \,0
--nbest-order 5 --output target.mbr.tl,0
It will compare all hyp with eachother and output the max bleu,0
!/usr/bin/env python,0
flake8: noqa,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
Now we can pipe the full file through the model using the Iterator,0
reminder a batch includes .src .tgt .indices and it is sorted,0
Compute and retrieve the loss for EACH sentence,0
Now we need to rearrange the batch of ppl,0
in the original order with indices,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
flake8: noqa,0
!/usr/bin/env python,0
!/usr/bin/env python,0
flake8: noqa,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate reference sentences nbest times for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \,0
--nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best bleu wrt reference,0
and output the max bleu,0
!/usr/bin/env python,0
with the two module = imp.load_source() below,0
we ghost the old torchtext.data.field and depercated,0
onmt.inputters.text_dataset,0
however this require some functions / classes to be,0
monkey patched for loading the old field/vocab objects.,0
"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
!/usr/bin/env python,0
flake8: noqa,0
redpajama stores QKV in one single tensor but it is not simply piled up Q+K+V,0
it is heads interleaved to we need to slice first,0
also it uses the HF rotary so we need to permute Q and K interleave,0
Avoid functionality on inference,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
Remove old vocabulary associated embeddings,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
Update model embeddings with those from the checkpoint,0
after initialization,0
when using LoRa or updating the vocab (no more embeddings in ckpt),0
=> strict=False when loading state_dict,0
!/usr/bin/env python,0
if transform + options set in 'valid' we need to copy in main,0
transform / options for scoring considered as inference,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
Override checkpoint's freezing settings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
Freeze Encoder and/or Decoder,0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
GPU,0
LoRa,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options related to source and target features,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Adding options related to Transforms,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
Let's clean the GPUs before training loop,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Compute validation metrics (at batch.dataset level),0
Compute stats,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
Compute and save stats,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
src lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
batch x len x dim,0
mask is now (batch x 1 x slen x slen),0
1 to be expanded to number of heads in MHA,0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
Help functions for Rotary Embeddings,0
https://arxiv.org/pdf/2104.09864.pdf,0
too convoluted to make maxseqlen a parameter.,0
we suppose src_seq_len at training and max_length at inference,0
are both < 2048 tokens.,0
"rope is now matrix [maxseqlen, dim/2]",0
Help functions for max_relative positions,0
https://arxiv.org/abs/1803.02155,0
Shift values to be >= 0,0
Help functions to split model dim per head,0
class MultiHeadedAttention(torch.jit.ScriptModule):,0
https://arxiv.org/pdf/1803.02155.pdf,0
in the paper they suggest either two embeds,0
relative_key / relative_value or only,0
relative_key. We implemented the same embed,0
for both.,0
@torch.jit.script_method,0
"1) Project key, value, and query.",0
as a reminder at training layer_cache[0] remains False,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
not 100% necessary but expand to nb of heads,0
now mask and scores have the same shape,0
3) Apply attention dropout and compute context vectors.,0
We use the same embeddings for key and value,0
--------------------------------------------------------------------------,0
Mostly copied from https://github.com/microsoft/LoRA/,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License (MIT).,0
,0
--------------------------------------------------------------------------,0
Optional dropout,0
Mark the weight as unmerged,0
LoRA implemented in a dense layer,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
LoRA implemented in a dense layer,0
Set this to True if the layer to replace stores,0
"weight like (fan_in, fan_out)",0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
LoRA implemented in a dense layer,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
Compute the indices,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
"for silu, see: https://arxiv.org/pdf/2002.05202.pdf",0
-*- coding: utf-8 -*-,0
class AverageAttention(torch.jit.ScriptModule):,0
@torch.jit.script,0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
Filter out very short or very long sentences,0
from the TM for better performance,0
We split the `batch` and perform fuzzy matching,0
in smaller chunks of 10.000 examples in order to,0
reduce memory usage.,0
Perfomance is not affected.,0
Probably redundant but let's be safe,0
in case some examples are already fuzzied,0
(e.g. from another pipeline or workflow),0
We don't want exact matches,0
Apply a basic filtering to leave out very short or very long,0
sentences and speed up things a bit during fuzzy matching,0
Do nothing,0
We set the start number of tags to a random number from 1,0
to 12 + the number of subsequent tags that,0
will be added. We also apply weights to this choice so tags,0
"are more probable to start from 1, then from 2, etc.",0
This way we cover most scenarios met in real usage and,0
the system will learn to handle a fairly large number of,0
numbered tags (but not an excessively large number),0
Make sure we only search for exact matches (we don't want,0
to match part of words) and perform some bound checking,0
Create all possible tag forms. We inject a special,0
unicode char (∥) as a placeholder for whitespace in order,0
to keep the indices unaltered. This char is replaced with,0
spaces before we return the augmented examples.,0
Make a weighted choice between paired tags or single tags.,0
"We usually encounter, and thus here we favor, paired tags",0
with a ratio 1/3.,0
Check if the tags include the,0
"mandatory ""#"" number placeholder""",0
We split the user-defined tags in the # placeholder,0
in order to number them,0
normalize dict src/tgt for each dataset,0
"print(""src empty"")",0
"print(""too many same char in src"")",0
"print(""too many same word in src"")",0
"print(""avg token min"", len(src_str) / len(ex['src']))",0
"print(""avg token max"", len(src_str) / len(ex['src']))",0
"print(""text does not fully belong to wanted script"")",0
"print(""Some text belong to unwanted scripts"")",0
"print(""langid does not match"", _id(src_str))",0
"print(""src = tgt"")",0
"print(""tgt empty"")",0
"print(""src / tgt ratio "", len(src_str) / len(tgt_str))",0
"print(""too many same char in tgt"")",0
"print(""too many same word in tgt"")",0
"print(""avg token min"", len(tgt_str) / len(ex['tgt']))",0
"print(""avg token max"", len(tgt_str) / len(ex['tgt']))",0
"print(""text does not fully belong to wanted script"")",0
"print(""Some text belong to unwanted scripts"")",0
"print(""langid does not match"", _id(tgt_str))",0
"doc break we add it, restart new doc",0
case 1st ex is already longer,0
adding cur ex is too long we add cur doc,0
and reset doc to cur ex,0
we start the new doc with cur ex,0
we cumulate cur ex to cur doc,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
Load vocabulary file if provided and set threshold,0
Load Subword Model,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
prefix src/tgt for each dataset,0
prefix as general option for inference,0
suffix src/tgt for each dataset,0
suffix as general option for inference,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Most code taken from: https://github.com/alvations/sacremoses,0
Which in turn is based on the Moses punctuation normalizer.,0
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/,0
tokenizer/normalize-punctuation.perl,0
don't fix period at end of sentence,0
Regex substitutions from replace-unicode-punctuation.perl,0
https://github.com/moses-smt/mosesdecoder/blob/master/,0
scripts/tokenizer/replace-unicode-punctuation.perl,0
Adds the penn substitutions after extra_whitespace regexes.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
Actual normalization.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
normalize dict src/tgt for each dataset,0
One source feature expected but none given and no default provided,0
Provided default does not match required features,0
Data not properly annotated.,0
In this case we do not use the default as it might be an error,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
make sure the scalars are in the event accumulator tags,0
required arguments,0
transforms that require vocab will not create if not provide vocab,0
1. Init first transform in the pipe,0
2. Init second transform in the pipe,0
3. Sequential combine them into a transform pipe,0
4. apply transform pipe for example,0
"5. example after the pipe exceed the length limit, thus filtered",0
6. Transform statistics registed (here for filtertoolong),0
"7. after report, statistics become empty as a fresh start",0
filter_transform.warm_up(),0
test BPE-dropout:,0
1. disable bpe dropout for not training example,0
2. enable bpe dropout for training example,0
3. (NOTE) disable dropout won't take effect if already seen,0
this is caused by the cache mechanism in bpe:,0
return cached subword if the original token is seen when no dropout,0
test SP regularization:,0
1. enable regularization for training example,0
2. disable regularization for not training example,0
Not apply token drop for not training example,0
apply token drop for training example,0
Not apply token mask for not training example,0
apply token mask for training example,0
require vocabs to warm_up,0
Not apply token mask for not training example,0
apply token mask for training example,0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
random_ratio of inserted tokens are chosen in vocab,0
others are MASK_TOK,0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
all token are considered as an individual word,0
1. tokens are dropped when replace_length is 0,0
"print(f""token delete: {masked} / {tokens}"")",0
2. tokens are replaced by MASK when replace_length is 1,0
"print(f""token mask: {masked} / {tokens}"")",0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
"1. replace_length 0: ""words"" are dropped",0
"print(f""word delete: {masked} / {tokens}"")",0
"self.assertEqual(len(masked), n_words - n_masked)",0
"2. replace_length 1: ""words"" are replaced with a single MASK",0
"print(f""whole word single mask: {masked} / {tokens}"")",0
len(masked) depend on number of tokens in select word,0
"3. replace_length -1: all tokens in ""words"" are replaced with MASK",0
"print(f""whole word multi mask: {masked} / {tokens}"")",0
number of mask_tok depend on number of tokens in selected word,0
number of MASK_TOK can be greater than n_masked,0
"insert_ratio=0.5,",0
"random_ratio=0.3,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
n_words = sum(token_starts),0
n_masked = math.ceil(n_words * bart_noise.mask_ratio),0
"print(f""Text Span Infilling: {infillied} / {tokens}"")",0
"print(n_words, n_masked)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
we finish 3 hyps per example in this step,0
new beam 1 is old beam 3,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('hidden_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
"attns[""coverage""] is actually c^(t+1) of See et al(2017)",0
1-index shifted,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
src_len is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate src_len as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
BoolTensor was introduced in pytorch 1.2,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x s or t len),0
1 = heads to be expanded in MHA,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
first value set to True triggered by the beginning of decoding,0
layer_cache becomes active in the MultiHeadedAttention fwd,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x tlen),0
1 = heads to be expanded in MHA,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"print(filled, sz)",0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use apex.amp,0
In this case use the old FusedAdam with,0
FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
should be: self._optimizer.zero_grad(set_to_none),0
but apex.amp is not up-to-date:,0
https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
placing this here make it easier to call logger.info,0
"from anywhere, just 'from onmt.utils.logging import logger'",0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
take into account here the tgt_shift_index (0 / 1 = LM/NMT),0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
in the case criterion reduction is None then we need,0
to sum the loss of each sentence in the batch,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check weight,0
Check features,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
It comes from training,0
TODO: needs to be added as inference opt,1
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
"this param init is overridden by model_builder, useless then.",0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
Helper functions,0
Keeps track of the original words/subwords,0
('prior_tokenization' option),0
In case there is a final case_markup when new_spacer is on,0
translate,0
for validation we build an infer_iter per batch,0
in order to avoid oom issues because there is no,0
batching strategy in `textbatch_to_tensor`,0
apply_reverse refs,0
flatten preds,0
save results,0
-*- coding: utf-8 -*-,0
this one is needed for Random Shuffler of batches,0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
we need to check the model path + any tokenizer path,0
patch to log stdout spawned processes of dataloader,0
bucket_size = batch_size,0
For TRAIN we need to group examples by length,0
"for faster performance, but otherwise, sequential.",0
For TRAIN we shuffle batches within the bucket,0
otherwise sequential,0
for specific case of rnn_packed need to be sorted,0
within the batch,0
Check if all tokens have features or none at all,0
Make features part of src like,0
"{'src': {'src': ..., 'feats': [...., ....]}}",0
at this point an example looks like:,0
"{'src': {'src': ..., 'feats': [....]},",0
"'tgt': {'tgt': ...},",0
"'src_original': ['tok1', ...'tokn'],",0
"'tgt_original': ['tok1', ...'tokm'],",0
'indices' : seq in bucket,0
"'align': ...,",0
},0
Need to add features in last dimensions,0
Keep it consistent with dynamic data,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
-*- coding: utf-8 -*-,0
'src_original' and 'tgt_original' store the,0
original line before tokenization. These,0
fields are used later on in the feature,0
transforms.,0
NOTE: moved to dynamic_iterator.py cf process(),0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
BPE training,0
SentencePiece training,0
!/usr/bin/env python,0
!/usr/bin/env python,0
Set sharing strategy manually instead of default based on the OS.,0
torch.multiprocessing.set_sharing_strategy('file_system'),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
magic indices,0
result caching,0
Here we set the decoder to start with self.start (BOS or EOS),0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
those two should be the same except feat dim,0
"batch['src'][perm[j], :, :])",0
trans.src,0
we rebuild a small batch made of the sub-segments,0
in the long segment.,0
new sub-batch ready to be translated,0
we re-insert the sub-batch in the initial translations,0
In the case of length_penalty = none we report the total logprobs,0
divided by the number of sentence to get an approximation of the,0
per sentence logprob. We also return the corresponding ppl,0
"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs",0
are normalized per token we report the per line per token logprob,0
"and the corresponding ""per word perplexity""",0
Turn any copied words into UNKs.,0
"Decoder forward, takes [batch, tgt_len, nfeats] as input",0
"and [batch, src_len, hidden] as enc_out",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
at this point scores is batch first (dim=0),0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``",0
Quick fix. Transformers return None as enc_states.,0
enc_states are only used later on to init decoder's state,0
"but are never used in Transformer decoder, so we can skip",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
it should be done in a better way,0
here dec_in is batch first,0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
"decoder_input = decode_strategy.current_predictions.view(1, -1,",0
1),0
Reorder states.,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
Auto import python files in this directory,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '6.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python3,0
Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
from onmt.utils.misc import use_gpu,0
"Add in default model arguments, possibly added since training.",0
this patch is no longer needed included in converter,0
"if hasattr(model_opt, 'rnn_size'):",0
model_opt.hidden_size = model_opt.rnn_size,0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python3,0
coding: utf-8,0
"In order to use this tool, please install comet first",0
https://github.com/Unbabel/COMET,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate source and reference sentences nbest times,0
for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
same for source.,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \,0
--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best comet score,0
when choosing a reference-less model no nbest-ref is required,0
for nbest in nbests:,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python mbr_bleu.py --nbest-hyp target.5.tl \,0
--nbest-order 5 --output target.mbr.tl,0
It will compare all hyp with eachother and output the max bleu,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
Now we can pipe the full file through the model using the Iterator,0
reminder a batch includes .src .tgt .indices and it is sorted,0
Compute and retrieve the loss for EACH sentence,0
Now we need to rearrange the batch of ppl,0
in the original order with indices,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
flake8: noqa,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate reference sentences nbest times for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \,0
--nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best bleu wrt reference,0
and output the max bleu,0
!/usr/bin/env python,0
with the two module = imp.load_source() below,0
we ghost the old torchtext.data.field and depercated,0
onmt.inputters.text_dataset,0
however this require some functions / classes to be,0
monkey patched for loading the old field/vocab objects.,0
"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
Avoid functionality on inference,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
Remove old vocabulary associated embeddings,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
Update model embeddings with those from the checkpoint,0
after initialization,0
when using LoRa or updating the vocab (no more embeddings in ckpt),0
=> strict=False when loading state_dict,0
!/usr/bin/env python,0
if transform + options set in 'valid' we need to copy in main,0
transform / options for scoring considered as inference,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
Override checkpoint's freezing settings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
Freeze Encoder and/or Decoder,0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
GPU,0
LoRa,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options related to source and target features,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Adding options related to Transforms,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
Let's clean the GPUs before training loop,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Compute validation metrics (at batch.dataset level),0
Compute stats,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
Compute and save stats,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
src lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
batch x len x dim,0
mask is now (batch x 1 x slen x slen),0
1 to be expanded to number of heads in MHA,0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
Help functions for Rotary Embeddings,0
https://arxiv.org/pdf/2104.09864.pdf,0
too convoluted to make maxseqlen a parameter.,0
we suppose src_seq_len at training and max_length at inference,0
are both < 2048 tokens.,0
"rope is now matrix [maxseqlen, dim/2]",0
Help functions for max_relative positions,0
https://arxiv.org/abs/1803.02155,0
Shift values to be >= 0,0
Help functions to split model dim per head,0
class MultiHeadedAttention(torch.jit.ScriptModule):,0
https://arxiv.org/pdf/1803.02155.pdf,0
in the paper they suggest either two embeds,0
relative_key / relative_value or only,0
relative_key. We implemented the same embed,0
for both.,0
@torch.jit.script_method,0
"1) Project key, value, and query.",0
as a reminder at training layer_cache[0] remains False,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
not 100% necessary but expand to nb of heads,0
now mask and scores have the same shape,0
3) Apply attention dropout and compute context vectors.,0
We use the same embeddings for key and value,0
--------------------------------------------------------------------------,0
Mostly copied from https://github.com/microsoft/LoRA/,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License (MIT).,0
,0
--------------------------------------------------------------------------,0
Optional dropout,0
Mark the weight as unmerged,0
LoRA implemented in a dense layer,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
LoRA implemented in a dense layer,0
Set this to True if the layer to replace stores,0
"weight like (fan_in, fan_out)",0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
LoRA implemented in a dense layer,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
Compute the indices,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
"for silu, see: https://arxiv.org/pdf/2002.05202.pdf",0
-*- coding: utf-8 -*-,0
class AverageAttention(torch.jit.ScriptModule):,0
@torch.jit.script,0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
Filter out very short or very long sentences,0
from the TM for better performance,0
We split the `batch` and perform fuzzy matching,0
in smaller chunks of 10.000 examples in order to,0
reduce memory usage.,0
Perfomance is not affected.,0
Probably redundant but let's be safe,0
in case some examples are already fuzzied,0
(e.g. from another pipeline or workflow),0
We don't want exact matches,0
Apply a basic filtering to leave out very short or very long,0
sentences and speed up things a bit during fuzzy matching,0
Do nothing,0
We set the start number of tags to a random number from 1,0
to 12 + the number of subsequent tags that,0
will be added. We also apply weights to this choice so tags,0
"are more probable to start from 1, then from 2, etc.",0
This way we cover most scenarios met in real usage and,0
the system will learn to handle a fairly large number of,0
numbered tags (but not an excessively large number),0
Make sure we only search for exact matches (we don't want,0
to match part of words) and perform some bound checking,0
Create all possible tag forms. We inject a special,0
unicode char (∥) as a placeholder for whitespace in order,0
to keep the indices unaltered. This char is replaced with,0
spaces before we return the augmented examples.,0
Make a weighted choice between paired tags or single tags.,0
"We usually encounter, and thus here we favor, paired tags",0
with a ratio 1/3.,0
Check if the tags include the,0
"mandatory ""#"" number placeholder""",0
We split the user-defined tags in the # placeholder,0
in order to number them,0
normalize dict src/tgt for each dataset,0
"print(""src empty"")",0
"print(""too many same char in src"")",0
"print(""too many same word in src"")",0
"print(""avg token min"", len(src_str) / len(ex['src']))",0
"print(""avg token max"", len(src_str) / len(ex['src']))",0
"print(""text does not fully belong to wanted script"")",0
"print(""Some text belong to unwanted scripts"")",0
"print(""langid does not match"", _id(src_str))",0
"print(""src = tgt"")",0
"print(""tgt empty"")",0
"print(""src / tgt ratio "", len(src_str) / len(tgt_str))",0
"print(""too many same char in tgt"")",0
"print(""too many same word in tgt"")",0
"print(""avg token min"", len(tgt_str) / len(ex['tgt']))",0
"print(""avg token max"", len(tgt_str) / len(ex['tgt']))",0
"print(""text does not fully belong to wanted script"")",0
"print(""Some text belong to unwanted scripts"")",0
"print(""langid does not match"", _id(tgt_str))",0
"doc break we add it, restart new doc",0
case 1st ex is already longer,0
adding cur ex is too long we add cur doc,0
and reset doc to cur ex,0
we start the new doc with cur ex,0
we cumulate cur ex to cur doc,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
Load vocabulary file if provided and set threshold,0
Load Subword Model,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
prefix src/tgt for each dataset,0
prefix as general option for inference,0
suffix src/tgt for each dataset,0
suffix as general option for inference,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Most code taken from: https://github.com/alvations/sacremoses,0
Which in turn is based on the Moses punctuation normalizer.,0
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/,0
tokenizer/normalize-punctuation.perl,0
don't fix period at end of sentence,0
Regex substitutions from replace-unicode-punctuation.perl,0
https://github.com/moses-smt/mosesdecoder/blob/master/,0
scripts/tokenizer/replace-unicode-punctuation.perl,0
Adds the penn substitutions after extra_whitespace regexes.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
Actual normalization.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
normalize dict src/tgt for each dataset,0
One source feature expected but none given and no default provided,0
Provided default does not match required features,0
Data not properly annotated.,0
In this case we do not use the default as it might be an error,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
make sure the scalars are in the event accumulator tags,0
required arguments,0
transforms that require vocab will not create if not provide vocab,0
1. Init first transform in the pipe,0
2. Init second transform in the pipe,0
3. Sequential combine them into a transform pipe,0
4. apply transform pipe for example,0
"5. example after the pipe exceed the length limit, thus filtered",0
6. Transform statistics registed (here for filtertoolong),0
"7. after report, statistics become empty as a fresh start",0
filter_transform.warm_up(),0
test BPE-dropout:,0
1. disable bpe dropout for not training example,0
2. enable bpe dropout for training example,0
3. (NOTE) disable dropout won't take effect if already seen,0
this is caused by the cache mechanism in bpe:,0
return cached subword if the original token is seen when no dropout,0
test SP regularization:,0
1. enable regularization for training example,0
2. disable regularization for not training example,0
Not apply token drop for not training example,0
apply token drop for training example,0
Not apply token mask for not training example,0
apply token mask for training example,0
require vocabs to warm_up,0
Not apply token mask for not training example,0
apply token mask for training example,0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
random_ratio of inserted tokens are chosen in vocab,0
others are MASK_TOK,0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
all token are considered as an individual word,0
1. tokens are dropped when replace_length is 0,0
"print(f""token delete: {masked} / {tokens}"")",0
2. tokens are replaced by MASK when replace_length is 1,0
"print(f""token mask: {masked} / {tokens}"")",0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
"1. replace_length 0: ""words"" are dropped",0
"print(f""word delete: {masked} / {tokens}"")",0
"self.assertEqual(len(masked), n_words - n_masked)",0
"2. replace_length 1: ""words"" are replaced with a single MASK",0
"print(f""whole word single mask: {masked} / {tokens}"")",0
len(masked) depend on number of tokens in select word,0
"3. replace_length -1: all tokens in ""words"" are replaced with MASK",0
"print(f""whole word multi mask: {masked} / {tokens}"")",0
number of mask_tok depend on number of tokens in selected word,0
number of MASK_TOK can be greater than n_masked,0
"insert_ratio=0.5,",0
"random_ratio=0.3,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
n_words = sum(token_starts),0
n_masked = math.ceil(n_words * bart_noise.mask_ratio),0
"print(f""Text Span Infilling: {infillied} / {tokens}"")",0
"print(n_words, n_masked)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
we finish 3 hyps per example in this step,0
new beam 1 is old beam 3,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('hidden_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
"attns[""coverage""] is actually c^(t+1) of See et al(2017)",0
1-index shifted,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
src_len is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate src_len as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
BoolTensor was introduced in pytorch 1.2,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x s or t len),0
1 = heads to be expanded in MHA,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
first value set to True triggered by the beginning of decoding,0
layer_cache becomes active in the MultiHeadedAttention fwd,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x tlen),0
1 = heads to be expanded in MHA,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"print(filled, sz)",0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use apex.amp,0
In this case use the old FusedAdam with,0
FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
should be: self._optimizer.zero_grad(set_to_none),0
but apex.amp is not up-to-date:,0
https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
placing this here make it easier to call logger.info,0
"from anywhere, just 'from onmt.utils.logging import logger'",0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
take into account here the tgt_shift_index (0 / 1 = LM/NMT),0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
in the case criterion reduction is None then we need,0
to sum the loss of each sentence in the batch,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check weight,0
Check features,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
It comes from training,0
TODO: needs to be added as inference opt,1
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
"this param init is overridden by model_builder, useless then.",0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
Helper functions,0
Keeps track of the original words/subwords,0
('prior_tokenization' option),0
In case there is a final case_markup when new_spacer is on,0
translate,0
for validation we build an infer_iter per batch,0
in order to avoid oom issues because there is no,0
batching strategy in `textbatch_to_tensor`,0
apply_reverse refs,0
flatten preds,0
save results,0
-*- coding: utf-8 -*-,0
this one is needed for Random Shuffler of batches,0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
we need to check the model path + any tokenizer path,0
patch to log stdout spawned processes of dataloader,0
bucket_size = batch_size,0
For TRAIN we need to group examples by length,0
"for faster performance, but otherwise, sequential.",0
For TRAIN we shuffle batches within the bucket,0
otherwise sequential,0
for specific case of rnn_packed need to be sorted,0
within the batch,0
Check if all tokens have features or none at all,0
Make features part of src like,0
"{'src': {'src': ..., 'feats': [...., ....]}}",0
at this point an example looks like:,0
"{'src': {'src': ..., 'feats': [....]},",0
"'tgt': {'tgt': ...},",0
"'src_original': ['tok1', ...'tokn'],",0
"'tgt_original': ['tok1', ...'tokm'],",0
'indices' : seq in bucket,0
"'align': ...,",0
},0
Need to add features in last dimensions,0
Keep it consistent with dynamic data,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
-*- coding: utf-8 -*-,0
'src_original' and 'tgt_original' store the,0
original line before tokenization. These,0
fields are used later on in the feature,0
transforms.,0
NOTE: moved to dynamic_iterator.py cf process(),0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
BPE training,0
SentencePiece training,0
!/usr/bin/env python,0
!/usr/bin/env python,0
Set sharing strategy manually instead of default based on the OS.,0
torch.multiprocessing.set_sharing_strategy('file_system'),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
magic indices,0
result caching,0
Here we set the decoder to start with self.start (BOS or EOS),0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
those two should be the same except feat dim,0
"batch['src'][perm[j], :, :])",0
trans.src,0
we rebuild a small batch made of the sub-segments,0
in the long segment.,0
new sub-batch ready to be translated,0
we re-insert the sub-batch in the initial translations,0
In the case of length_penalty = none we report the total logprobs,0
divided by the number of sentence to get an approximation of the,0
per sentence logprob. We also return the corresponding ppl,0
"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs",0
are normalized per token we report the per line per token logprob,0
"and the corresponding ""per word perplexity""",0
Turn any copied words into UNKs.,0
"Decoder forward, takes [batch, tgt_len, nfeats] as input",0
"and [batch, src_len, hidden] as enc_out",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
at this point scores is batch first (dim=0),0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``",0
Quick fix. Transformers return None as enc_states.,0
enc_states are only used later on to init decoder's state,0
"but are never used in Transformer decoder, so we can skip",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
it should be done in a better way,0
here dec_in is batch first,0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
"decoder_input = decode_strategy.current_predictions.view(1, -1,",0
1),0
Reorder states.,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
Auto import python files in this directory,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '6.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python3,0
Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
"Add in default model arguments, possibly added since training.",0
this patch is no longer needed included in converter,0
"if hasattr(model_opt, 'rnn_size'):",0
model_opt.hidden_size = model_opt.rnn_size,0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python3,0
coding: utf-8,0
"In order to use this tool, please install comet first",0
https://github.com/Unbabel/COMET,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate source and reference sentences nbest times,0
for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
same for source.,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \,0
--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best comet score,0
when choosing a reference-less model no nbest-ref is required,0
for nbest in nbests:,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python mbr_bleu.py --nbest-hyp target.5.tl \,0
--nbest-order 5 --output target.mbr.tl,0
It will compare all hyp with eachother and output the max bleu,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
Now we can pipe the full file through the model using the Iterator,0
reminder a batch includes .src .tgt .indices and it is sorted,0
Compute and retrieve the loss for EACH sentence,0
Now we need to rearrange the batch of ppl,0
in the original order with indices,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate reference sentences nbest times for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \,0
--nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best bleu wrt reference,0
and output the max bleu,0
!/usr/bin/env python,0
with the two module = imp.load_source() below,0
we ghost the old torchtext.data.field and depercated,0
onmt.inputters.text_dataset,0
however this require some functions / classes to be,0
monkey patched for loading the old field/vocab objects.,0
"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
Patch for NLLB200 model loading,0
Avoid functionality on inference,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
Remove old vocabulary associated embeddings,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
Update model embeddings with those from the checkpoint,0
after initialization,0
!/usr/bin/env python,0
if transform + options set in 'valid' we need to copy in main,0
transform / options for scoring considered as inference,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
Override checkpoint's freezing settings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
Freeze Encoder and/or Decoder,0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
GPU,0
LoRa,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options related to source and target features,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Adding options related to Transforms,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
Let's clean the GPUs before training loop,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Compute validation metrics (at batch.dataset level),0
Compute stats,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
Compute and save stats,0
in theory we should divide by accum_count and bptt,0
to rescale for each sub batch,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
src lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
batch x len x dim,0
mask is now (batch x 1 x slen x slen),0
1 to be expanded to number of heads in MHA,0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
Shift values to be >= 0,0
class MultiHeadedAttention(torch.jit.ScriptModule):,0
https://arxiv.org/pdf/1803.02155.pdf,0
in the paper they suggest either two embeds,0
relative_key / relative_value or only,0
relative_key. We implemented the same embed,0
for both.,0
@torch.jit.script_method,0
"1) Project key, value, and query.",0
as a reminder at training layer_cache[0] remains False,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
not 100% necessary but expand to nb of heads,0
now mask and scores have the same shape,0
3) Apply attention dropout and compute context vectors.,0
We use the same embeddings for key and value,0
--------------------------------------------------------------------------,0
MOstly copied from https://github.com/microsoft/LoRA/,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License (MIT).,0
--------------------------------------------------------------------------,0
Optional dropout,0
Mark the weight as unmerged,0
LoRA implemented in a dense layer,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
LoRA implemented in a dense layer,0
Set this to True if the layer to replace stores,0
"weight like (fan_in, fan_out)",0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
LoRA implemented in a dense layer,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
Compute the indices,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
class AverageAttention(torch.jit.ScriptModule):,0
@torch.jit.script,0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
Filter out very short or very long sentences,0
from the TM for better performance,0
We split the `batch` and perform fuzzy matching,0
in smaller chunks of 10.000 examples in order to,0
reduce memory usage.,0
Perfomance is not affected.,0
Probably redundant but let's be safe,0
in case some examples are already fuzzied,0
(e.g. from another pipeline or workflow),0
We don't want exact matches,0
Apply a basic filtering to leave out very short or very long,0
sentences and speed up things a bit during fuzzy matching,0
Do nothing,0
We set the start number of tags to a random number from 1,0
to 12 + the number of subsequent tags that,0
will be added. We also apply weights to this choice so tags,0
"are more probable to start from 1, then from 2, etc.",0
This way we cover most scenarios met in real usage and,0
the system will learn to handle a fairly large number of,0
numbered tags (but not an excessively large number),0
Make sure we only search for exact matches (we don't want,0
to match part of words) and perform some bound checking,0
Make a weighted choice between paired tags or single tags.,0
"We usually encounter, and thus here we favor, paired tags",0
with a ratio 1/3.,0
Check if the tags include the,0
"mandatory ""#"" number placeholder""",0
We split the user-defined tags in the # placeholder,0
in order to number them,0
"doc break we add it, restart new doc",0
case 1st ex is already longer,0
adding cur ex is too long we add cur doc,0
and reset doc to cur ex,0
we start the new doc with cur ex,0
we cumulate cur ex to cur doc,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
Load vocabulary file if provided and set threshold,0
Load Subword Model,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
prefix src/tgt for each dataset,0
prefix as general option for inference,0
suffix src/tgt for each dataset,0
suffix as general option for inference,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Most code taken from: https://github.com/alvations/sacremoses,0
Which in turn is based on the Moses punctuation normalizer.,0
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/,0
tokenizer/normalize-punctuation.perl,0
don't fix period at end of sentence,0
Regex substitutions from replace-unicode-punctuation.perl,0
https://github.com/moses-smt/mosesdecoder/blob/master/,0
scripts/tokenizer/replace-unicode-punctuation.perl,0
Adds the penn substitutions after extra_whitespace regexes.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
Actual normalization.,0
"print(regexp, substitution)",0
print(text),0
"Optionally, replace unicode puncts BEFORE normalization.",0
One source feature expected but none given and no default provided,0
Provided default does not match required features,0
Data not properly annotated.,0
In this case we do not use the default as it might be an error,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
make sure the scalars are in the event accumulator tags,0
required arguments,0
transforms that require vocab will not create if not provide vocab,0
1. Init first transform in the pipe,0
2. Init second transform in the pipe,0
3. Sequential combine them into a transform pipe,0
4. apply transform pipe for example,0
"5. example after the pipe exceed the length limit, thus filtered",0
6. Transform statistics registed (here for filtertoolong),0
"7. after report, statistics become empty as a fresh start",0
filter_transform.warm_up(),0
test BPE-dropout:,0
1. disable bpe dropout for not training example,0
2. enable bpe dropout for training example,0
3. (NOTE) disable dropout won't take effect if already seen,0
this is caused by the cache mechanism in bpe:,0
return cached subword if the original token is seen when no dropout,0
test SP regularization:,0
1. enable regularization for training example,0
2. disable regularization for not training example,0
Not apply token drop for not training example,0
apply token drop for training example,0
Not apply token mask for not training example,0
apply token mask for training example,0
require vocabs to warm_up,0
Not apply token mask for not training example,0
apply token mask for training example,0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
random_ratio of inserted tokens are chosen in vocab,0
others are MASK_TOK,0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
all token are considered as an individual word,0
1. tokens are dropped when replace_length is 0,0
"print(f""token delete: {masked} / {tokens}"")",0
2. tokens are replaced by MASK when replace_length is 1,0
"print(f""token mask: {masked} / {tokens}"")",0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
"1. replace_length 0: ""words"" are dropped",0
"print(f""word delete: {masked} / {tokens}"")",0
"self.assertEqual(len(masked), n_words - n_masked)",0
"2. replace_length 1: ""words"" are replaced with a single MASK",0
"print(f""whole word single mask: {masked} / {tokens}"")",0
len(masked) depend on number of tokens in select word,0
"3. replace_length -1: all tokens in ""words"" are replaced with MASK",0
"print(f""whole word multi mask: {masked} / {tokens}"")",0
number of mask_tok depend on number of tokens in selected word,0
number of MASK_TOK can be greater than n_masked,0
"insert_ratio=0.5,",0
"random_ratio=0.3,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
n_words = sum(token_starts),0
n_masked = math.ceil(n_words * bart_noise.mask_ratio),0
"print(f""Text Span Infilling: {infillied} / {tokens}"")",0
"print(n_words, n_masked)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
we finish 3 hyps per example in this step,0
new beam 1 is old beam 3,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('hidden_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
"attns[""coverage""] is actually c^(t+1) of See et al(2017)",0
1-index shifted,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
src_len is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate src_len as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
BoolTensor was introduced in pytorch 1.2,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x s or t len),0
1 = heads to be expanded in MHA,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
first value set to True triggered by the beginning of decoding,0
layer_cache becomes active in the MultiHeadedAttention fwd,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x tlen),0
1 = heads to be expanded in MHA,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"print(filled, sz)",0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use apex.amp,0
In this case use the old FusedAdam with,0
FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
should be: self._optimizer.zero_grad(set_to_none),0
but apex.amp is not up-to-date:,0
https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
placing this here make it easier to call logger.info,0
"from anywhere, just 'from onmt.utils.logging import logger'",0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
take into account here the tgt_shift_index (0 / 1 = LM/NMT),0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
in the case criterion reduction is None then we need,0
to sum the loss of each sentence in the batch,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check weight,0
Check features,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
It comes from training,0
TODO: needs to be added as inference opt,1
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
"this param init is overridden by model_builder, useless then.",0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
Helper functions,0
Keeps track of the original words/subwords,0
('prior_tokenization' option),0
In case there is a final case_markup when new_spacer is on,0
translate,0
for validation we build an infer_iter per batch,0
in order to avoid oom issues because there is no,0
batching strategy in `textbatch_to_tensor`,0
apply_reverse refs,0
flatten preds,0
save results,0
-*- coding: utf-8 -*-,0
this one is needed for Random Shuffler of batches,0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
we need to check the model path + any tokenizer path,0
patch to log stdout spawned processes of dataloader,0
bucket_size = batch_size,0
For TRAIN we need to group examples by length,0
"for faster performance, but otherwise, sequential.",0
For TRAIN we shuffle batches within the bucket,0
otherwise sequential,0
for specific case of rnn_packed need to be sorted,0
within the batch,0
Check if all tokens have features or none at all,0
Make features part of src like,0
"{'src': {'src': ..., 'feats': [...., ....]}}",0
at this point an example looks like:,0
"{'src': {'src': ..., 'feats': [....]},",0
"'tgt': {'tgt': ...},",0
"'src_original': ['tok1', ...'tokn'],",0
"'tgt_original': ['tok1', ...'tokm'],",0
'indices' : seq in bucket,0
"'align': ...,",0
},0
Need to add features in last dimensions,0
Keep it consistent with dynamic data,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
-*- coding: utf-8 -*-,0
'src_original' and 'tgt_original' store the,0
original line before tokenization. These,0
fields are used later on in the feature,0
transforms.,0
NOTE: moved to dynamic_iterator.py cf process(),0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
for future use?,0
if we want to save the LoRa model state_dict only,0
"model_state_dict = lora_state_dict(model, bias='lora_only')",0
and comment the line below,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
"for side in [""src"", ""tgt""]:",0
keys_to_pop = [],0
"if hasattr(vocab[side], ""fields""):",0
unk_token = vocab[side].fields[0][1].vocab.itos[0],0
"for key, value in vocab[side].fields[0][1].vocab.stoi.items():",0
if value == 0 and key != unk_token:,0
keys_to_pop.append(key),0
for key in keys_to_pop:,0
"vocab[side].fields[0][1].vocab.stoi.pop(key, None)",0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
Set sharing strategy manually instead of default based on the OS.,0
torch.multiprocessing.set_sharing_strategy('file_system'),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
magic indices,0
result caching,0
Here we set the decoder to start with self.start (BOS or EOS),0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
Here we handle the cases of mismatch in number of segments,0
between source and target. We re-translate seg by seg.,0
those two should be the same except feat dim,0
"batch['src'][perm[j], :, :])",0
trans.src,0
we rebuild a small batch made of the sub-segments,0
in the long segment.,0
new sub-batch ready to be translated,0
we re-insert the sub-batch in the initial translations,0
In the case of length_penalty = none we report the total logprobs,0
divided by the number of sentence to get an approximation of the,0
per sentence logprob. We also return the corresponding ppl,0
"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs",0
are normalized per token we report the per line per token logprob,0
"and the corresponding ""per word perplexity""",0
Turn any copied words into UNKs.,0
"Decoder forward, takes [batch, tgt_len, nfeats] as input",0
"and [batch, src_len, hidden] as enc_out",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
at this point scores is batch first (dim=0),0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``",0
Quick fix. Transformers return None as enc_states.,0
enc_states are only used later on to init decoder's state,0
"but are never used in Transformer decoder, so we can skip",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
it should be done in a better way,0
here dec_in is batch first,0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
"decoder_input = decode_strategy.current_predictions.view(1, -1,",0
1),0
Reorder states.,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
Auto import python files in this directory,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '6.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python3,0
Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
"Add in default model arguments, possibly added since training.",0
this patch is no longer needed included in converter,0
"if hasattr(model_opt, 'rnn_size'):",0
model_opt.hidden_size = model_opt.rnn_size,0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python3,0
coding: utf-8,0
"In order to use this tool, please install comet first",0
https://github.com/Unbabel/COMET,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate source and reference sentences nbest times,0
for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
same for source.,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \,0
--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best comet score,0
when choosing a reference-less model no nbest-ref is required,0
for nbest in nbests:,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python mbr_bleu.py --nbest-hyp target.5.tl \,0
--nbest-order 5 --output target.mbr.tl,0
It will compare all hyp with eachother and output the max bleu,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
Now we can pipe the full file through the model using the Iterator,0
reminder a batch includes .src .tgt .indices and it is sorted,0
Compute and retrieve the loss for EACH sentence,0
Now we need to rearrange the batch of ppl,0
in the original order with indices,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate reference sentences nbest times for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \,0
--nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best bleu wrt reference,0
and output the max bleu,0
!/usr/bin/env python,0
with the two module = imp.load_source() below,0
we ghost the old torchtext.data.field and depercated,0
onmt.inputters.text_dataset,0
however this require some functions / classes to be,0
monkey patched for loading the old field/vocab objects.,0
"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
Patch for NLLB200 model loading,0
Avoid functionality on inference,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
Remove old vocabulary associated embeddings,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
Update model embeddings with those from the checkpoint,0
after initialization,0
!/usr/bin/env python,0
if transform + options set in 'valid' we need to copy in main,0
transform / options for scoring considered as inference,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
Override checkpoint's freezing settings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
Freeze Encoder and/or Decoder,0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
GPU,0
LoRa,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options related to source and target features,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Adding options related to Transforms,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
Let's clean the GPUs before training loop,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Compute validation metrics (at batch.dataset level),0
Compute stats,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
Compute and save stats,0
in theory we should divide by accum_count and bptt,0
to rescale for each sub batch,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
src lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
batch x len x dim,0
mask is now (batch x 1 x slen x slen),0
1 to be expanded to number of heads in MHA,0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
Shift values to be >= 0,0
class MultiHeadedAttention(torch.jit.ScriptModule):,0
https://arxiv.org/pdf/1803.02155.pdf,0
in the paper they suggest either two embeds,0
relative_key / relative_value or only,0
relative_key. We implemented the same embed,0
for both.,0
@torch.jit.script_method,0
"1) Project key, value, and query.",0
as a reminder at training layer_cache[0] remains False,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
not 100% necessary but expand to nb of heads,0
now mask and scores have the same shape,0
3) Apply attention dropout and compute context vectors.,0
We use the same embeddings for key and value,0
--------------------------------------------------------------------------,0
MOstly copied from https://github.com/microsoft/LoRA/,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License (MIT).,0
--------------------------------------------------------------------------,0
Optional dropout,0
Mark the weight as unmerged,0
LoRA implemented in a dense layer,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
LoRA implemented in a dense layer,0
Set this to True if the layer to replace stores,0
"weight like (fan_in, fan_out)",0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
LoRA implemented in a dense layer,0
Actual trainable parameters,0
Freezing the pre-trained weight matrix,0
Compute the indices,0
initialize A the same way as the default,0
for nn.Linear and B to zero,0
Make sure that the weights are not merged,0
Merge the weights and mark it,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
class AverageAttention(torch.jit.ScriptModule):,0
@torch.jit.script,0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
Filter out very short or very long sentences,0
from the TM for better performance,0
We split the `batch` and perform fuzzy matching,0
in smaller chunks of 10.000 examples in order to,0
reduce memory usage.,0
Perfomance is not affected.,0
Probably redundant but let's be safe,0
in case some examples are already fuzzied,0
(e.g. from another pipeline or workflow),0
We don't want exact matches,0
Apply a basic filtering to leave out very short or very long,0
sentences and speed up things a bit during fuzzy matching,0
Do nothing,0
We set the start number of tags to a random number from 1,0
to 12 + the number of subsequent tags that,0
will be added. We also apply weights to this choice so tags,0
"are more probable to start from 1, then from 2, etc.",0
This way we cover most scenarios met in real usage and,0
the system will learn to handle a fairly large number of,0
numbered tags (but not an excessively large number),0
Make sure we only search for exact matches (we don't want,0
to match part of words) and perform some bound checking,0
Make a weighted choice between paired tags or single tags.,0
"We usually encounter, and thus here we favor, paired tags",0
with a ratio 1/3.,0
Check if the tags include the,0
"mandatory ""#"" number placeholder""",0
We split the user-defined tags in the # placeholder,0
in order to number them,0
"doc break we add it, restart new doc",0
case 1st ex is already longer,0
adding cur ex is too long we add cur doc,0
and reset doc to cur ex,0
we start the new doc with cur ex,0
we cumulate cur ex to cur doc,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
Load vocabulary file if provided and set threshold,0
Load Subword Model,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
prefix src/tgt for each dataset,0
prefix as general option for inference,0
suffix src/tgt for each dataset,0
suffix as general option for inference,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Most code taken from: https://github.com/alvations/sacremoses,0
Which in turn is based on the Moses punctuation normalizer.,0
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/,0
tokenizer/normalize-punctuation.perl,0
don't fix period at end of sentence,0
Regex substitutions from replace-unicode-punctuation.perl,0
https://github.com/moses-smt/mosesdecoder/blob/master/,0
scripts/tokenizer/replace-unicode-punctuation.perl,0
Adds the penn substitutions after extra_whitespace regexes.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
Actual normalization.,0
"print(regexp, substitution)",0
print(text),0
"Optionally, replace unicode puncts BEFORE normalization.",0
One source feature expected but none given and no default provided,0
Provided default does not match required features,0
Data not properly annotated.,0
In this case we do not use the default as it might be an error,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
make sure the scalars are in the event accumulator tags,0
required arguments,0
transforms that require vocab will not create if not provide vocab,0
1. Init first transform in the pipe,0
2. Init second transform in the pipe,0
3. Sequential combine them into a transform pipe,0
4. apply transform pipe for example,0
"5. example after the pipe exceed the length limit, thus filtered",0
6. Transform statistics registed (here for filtertoolong),0
"7. after report, statistics become empty as a fresh start",0
filter_transform.warm_up(),0
test BPE-dropout:,0
1. disable bpe dropout for not training example,0
2. enable bpe dropout for training example,0
3. (NOTE) disable dropout won't take effect if already seen,0
this is caused by the cache mechanism in bpe:,0
return cached subword if the original token is seen when no dropout,0
test SP regularization:,0
1. enable regularization for training example,0
2. disable regularization for not training example,0
Not apply token drop for not training example,0
apply token drop for training example,0
Not apply token mask for not training example,0
apply token mask for training example,0
require vocabs to warm_up,0
Not apply token mask for not training example,0
apply token mask for training example,0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
random_ratio of inserted tokens are chosen in vocab,0
others are MASK_TOK,0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
all token are considered as an individual word,0
1. tokens are dropped when replace_length is 0,0
"print(f""token delete: {masked} / {tokens}"")",0
2. tokens are replaced by MASK when replace_length is 1,0
"print(f""token mask: {masked} / {tokens}"")",0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
"1. replace_length 0: ""words"" are dropped",0
"print(f""word delete: {masked} / {tokens}"")",0
"self.assertEqual(len(masked), n_words - n_masked)",0
"2. replace_length 1: ""words"" are replaced with a single MASK",0
"print(f""whole word single mask: {masked} / {tokens}"")",0
len(masked) depend on number of tokens in select word,0
"3. replace_length -1: all tokens in ""words"" are replaced with MASK",0
"print(f""whole word multi mask: {masked} / {tokens}"")",0
number of mask_tok depend on number of tokens in selected word,0
number of MASK_TOK can be greater than n_masked,0
"insert_ratio=0.5,",0
"random_ratio=0.3,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
n_words = sum(token_starts),0
n_masked = math.ceil(n_words * bart_noise.mask_ratio),0
"print(f""Text Span Infilling: {infillied} / {tokens}"")",0
"print(n_words, n_masked)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
we finish 3 hyps per example in this step,0
new beam 1 is old beam 3,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('hidden_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
"attns[""coverage""] is actually c^(t+1) of See et al(2017)",0
1-index shifted,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
src_len is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate src_len as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
BoolTensor was introduced in pytorch 1.2,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x s or t len),0
1 = heads to be expanded in MHA,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
first value set to True triggered by the beginning of decoding,0
layer_cache becomes active in the MultiHeadedAttention fwd,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x tlen),0
1 = heads to be expanded in MHA,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"print(filled, sz)",0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use apex.amp,0
In this case use the old FusedAdam with,0
FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
should be: self._optimizer.zero_grad(set_to_none),0
but apex.amp is not up-to-date:,0
https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
placing this here make it easier to call logger.info,0
"from anywhere, just 'from onmt.utils.logging import logger'",0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
take into account here the tgt_shift_index (0 / 1 = LM/NMT),0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
in the case criterion reduction is None then we need,0
to sum the loss of each sentence in the batch,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check weight,0
Check features,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
It comes from training,0
TODO: needs to be added as inference opt,1
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
"this param init is overridden by model_builder, useless then.",0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
Helper functions,0
Keeps track of the original words/subwords,0
('prior_tokenization' option),0
In case there is a final case_markup when new_spacer is on,0
translate,0
for validation we build an infer_iter per batch,0
in order to avoid oom issues because there is no,0
batching strategy in `textbatch_to_tensor`,0
apply_reverse refs,0
flatten preds,0
save results,0
-*- coding: utf-8 -*-,0
this one is needed for Random Shuffler of batches,0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
we need to check the model path + any tokenizer path,0
patch to log stdout spawned processes of dataloader,0
bucket_size = batch_size,0
For TRAIN we need to group examples by length,0
"for faster performance, but otherwise, sequential.",0
For TRAIN we shuffle batches within the bucket,0
otherwise sequential,0
for specific case of rnn_packed need to be sorted,0
within the batch,0
Check if all tokens have features or none at all,0
Make features part of src like,0
"{'src': {'src': ..., 'feats': [...., ....]}}",0
at this point an example looks like:,0
"{'src': {'src': ..., 'feats': [....]},",0
"'tgt': {'tgt': ...},",0
"'src_original': ['tok1', ...'tokn'],",0
"'tgt_original': ['tok1', ...'tokm'],",0
'indices' : seq in bucket,0
"'align': ...,",0
},0
Need to add features in last dimensions,0
Keep it consistent with dynamic data,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
-*- coding: utf-8 -*-,0
'src_original' and 'tgt_original' store the,0
original line before tokenization. These,0
fields are used later on in the feature,0
transforms.,0
NOTE: moved to dynamic_iterator.py cf process(),0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
for future use?,0
if we want to save the LoRa model state_dict only,0
"model_state_dict = lora_state_dict(model, bias='lora_only')",0
and comment the line below,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
"for side in [""src"", ""tgt""]:",0
keys_to_pop = [],0
"if hasattr(vocab[side], ""fields""):",0
unk_token = vocab[side].fields[0][1].vocab.itos[0],0
"for key, value in vocab[side].fields[0][1].vocab.stoi.items():",0
if value == 0 and key != unk_token:,0
keys_to_pop.append(key),0
for key in keys_to_pop:,0
"vocab[side].fields[0][1].vocab.stoi.pop(key, None)",0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
Set sharing strategy manually instead of default based on the OS.,0
torch.multiprocessing.set_sharing_strategy('file_system'),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
magic indices,0
result caching,0
Here we set the decoder to start with self.start (BOS or EOS),0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
Here we handle the cases of mismatch in number of segments,0
between source and target. We re-translate seg by seg.,0
those two should be the same except feat dim,0
"batch['src'][perm[j], :, :])",0
trans.src,0
we rebuild a small batch made of the sub-segments,0
in the long segment.,0
new sub-batch ready to be translated,0
we re-insert the sub-batch in the initial translations,0
In the case of length_penalty = none we report the total logprobs,0
divided by the number of sentence to get an approximation of the,0
per sentence logprob. We also return the corresponding ppl,0
"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs",0
are normalized per token we report the per line per token logprob,0
"and the corresponding ""per word perplexity""",0
Turn any copied words into UNKs.,0
"Decoder forward, takes [batch, tgt_len, nfeats] as input",0
"and [batch, src_len, hidden] as enc_out",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
at this point scores is batch first (dim=0),0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``",0
Quick fix. Transformers return None as enc_states.,0
enc_states are only used later on to init decoder's state,0
"but are never used in Transformer decoder, so we can skip",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
it should be done in a better way,0
here dec_in is batch first,0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
"decoder_input = decode_strategy.current_predictions.view(1, -1,",0
1),0
Reorder states.,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
Auto import python files in this directory,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
"Add in default model arguments, possibly added since training.",0
this patch is no longer needed included in converter,0
"if hasattr(model_opt, 'rnn_size'):",0
model_opt.hidden_size = model_opt.rnn_size,0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python3,0
coding: utf-8,0
"In order to use this tool, please install comet first",0
https://github.com/Unbabel/COMET,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate source and reference sentences nbest times,0
for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
same for source.,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \,0
--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best comet score,0
when choosing a reference-less model no nbest-ref is required,0
for nbest in nbests:,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python mbr_bleu.py --nbest-hyp target.5.tl \,0
--nbest-order 5 --output target.mbr.tl,0
It will compare all hyp with eachother and output the max bleu,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
Now we can pipe the full file through the model using the Iterator,0
reminder a batch includes .src .tgt .indices and it is sorted,0
Compute and retrieve the loss for EACH sentence,0
Now we need to rearrange the batch of ppl,0
in the original order with indices,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate reference sentences nbest times for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \,0
--nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best bleu wrt reference,0
and output the max bleu,0
!/usr/bin/env python,0
with the two module = imp.load_source() below,0
we ghost the old torchtext.data.field and depercated,0
onmt.inputters.text_dataset,0
however this require some functions / classes to be,0
monkey patched for loading the old field/vocab objects.,0
"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
Patch for NLLB200 model loading,0
Avoid functionality on inference,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
Remove old vocabulary associated embeddings,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
Update model embeddings with those from the checkpoint,0
after initialization,0
!/usr/bin/env python,0
if transform + options set in 'valid' we need to copy in main,0
transform / options for scoring considered as inference,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
Override checkpoint's freezing settings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
Freeze Encoder and/or Decoder,0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
GPU,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Adding options related to Transforms,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
Let's clean the GPUs before training loop,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Compute validation metrics (at batch.dataset level),0
Compute stats,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
Compute and save stats,0
in theory we should divide by accum_count and bptt,0
to rescale for each sub batch,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
src lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
batch x len x dim,0
mask is now (batch x 1 x slen x slen),0
1 to be expanded to number of heads in MHA,0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
Shift values to be >= 0,0
class MultiHeadedAttention(torch.jit.ScriptModule):,0
https://arxiv.org/pdf/1803.02155.pdf,0
in the paper they suggest either two embeds,0
relative_key / relative_value or only,0
relative_key. We implemented the same embed,0
for both.,0
@torch.jit.script_method,0
"1) Project key, value, and query.",0
as a reminder at training layer_cache[0] remains False,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
not 100% necessary but expand to nb of heads,0
now mask and scores have the same shape,0
3) Apply attention dropout and compute context vectors.,0
We use the same embeddings for key and value,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
class AverageAttention(torch.jit.ScriptModule):,0
@torch.jit.script,0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
Filter out very short or very long sentences,0
from the TM for better performance,0
We split the `batch` and perform fuzzy matching,0
in smaller chunks of 10.000 examples in order to,0
reduce memory usage.,0
Perfomance is not affected.,0
Probably redundant but let's be safe,0
in case some examples are already fuzzied,0
(e.g. from another pipeline or workflow),0
We don't want exact matches,0
Apply a basic filtering to leave out very short or very long,0
sentences and speed up things a bit during fuzzy matching,0
Do nothing,0
Do nothing,0
Punctuation only,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
Load vocabulary file if provided and set threshold,0
Load Subword Model,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Most code taken from: https://github.com/alvations/sacremoses,0
Which in turn is based on the Moses punctuation normalizer.,0
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/,0
tokenizer/normalize-punctuation.perl,0
don't fix period at end of sentence,0
Regex substitutions from replace-unicode-punctuation.perl,0
https://github.com/moses-smt/mosesdecoder/blob/master/,0
scripts/tokenizer/replace-unicode-punctuation.perl,0
Adds the penn substitutions after extra_whitespace regexes.,0
"Optionally, replace unicode puncts BEFORE normalization.",0
Actual normalization.,0
"print(regexp, substitution)",0
print(text),0
"Optionally, replace unicode puncts BEFORE normalization.",0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
make sure the scalars are in the event accumulator tags,0
required arguments,0
transforms that require vocab will not create if not provide vocab,0
1. Init first transform in the pipe,0
2. Init second transform in the pipe,0
3. Sequential combine them into a transform pipe,0
4. apply transform pipe for example,0
"5. example after the pipe exceed the length limit, thus filtered",0
6. Transform statistics registed (here for filtertoolong),0
"7. after report, statistics become empty as a fresh start",0
filter_transform.warm_up(),0
test BPE-dropout:,0
1. disable bpe dropout for not training example,0
2. enable bpe dropout for training example,0
3. (NOTE) disable dropout won't take effect if already seen,0
this is caused by the cache mechanism in bpe:,0
return cached subword if the original token is seen when no dropout,0
test SP regularization:,0
1. enable regularization for training example,0
2. disable regularization for not training example,0
Not apply token drop for not training example,0
apply token drop for training example,0
Not apply token mask for not training example,0
apply token mask for training example,0
require vocabs to warm_up,0
Not apply token mask for not training example,0
apply token mask for training example,0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
random_ratio of inserted tokens are chosen in vocab,0
others are MASK_TOK,0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
all token are considered as an individual word,0
1. tokens are dropped when replace_length is 0,0
"print(f""token delete: {masked} / {tokens}"")",0
2. tokens are replaced by MASK when replace_length is 1,0
"print(f""token mask: {masked} / {tokens}"")",0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
"1. replace_length 0: ""words"" are dropped",0
"print(f""word delete: {masked} / {tokens}"")",0
"self.assertEqual(len(masked), n_words - n_masked)",0
"2. replace_length 1: ""words"" are replaced with a single MASK",0
"print(f""whole word single mask: {masked} / {tokens}"")",0
len(masked) depend on number of tokens in select word,0
"3. replace_length -1: all tokens in ""words"" are replaced with MASK",0
"print(f""whole word multi mask: {masked} / {tokens}"")",0
number of mask_tok depend on number of tokens in selected word,0
number of MASK_TOK can be greater than n_masked,0
"insert_ratio=0.5,",0
"random_ratio=0.3,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
n_words = sum(token_starts),0
n_masked = math.ceil(n_words * bart_noise.mask_ratio),0
"print(f""Text Span Infilling: {infillied} / {tokens}"")",0
"print(n_words, n_masked)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
we finish 3 hyps per example in this step,0
new beam 1 is old beam 3,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('hidden_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
"attns[""coverage""] is actually c^(t+1) of See et al(2017)",0
1-index shifted,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
src_len is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate src_len as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
BoolTensor was introduced in pytorch 1.2,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x s or t len),0
1 = heads to be expanded in MHA,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
first value set to True triggered by the beginning of decoding,0
layer_cache becomes active in the MultiHeadedAttention fwd,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x tlen),0
1 = heads to be expanded in MHA,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"print(filled, sz)",0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use apex.amp,0
In this case use the old FusedAdam with,0
FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
should be: self._optimizer.zero_grad(set_to_none),0
but apex.amp is not up-to-date:,0
https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
placing this here make it easier to call logger.info,0
"from anywhere, just 'from onmt.utils.logging import logger'",0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
take into account here the tgt_shift_index (0 / 1 = LM/NMT),0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
in the case criterion reduction is None then we need,0
to sum the loss of each sentence in the batch,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check prefix: will be used when use prefix transform,0
Check weight,0
Check features,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
It comes from training,0
TODO: needs to be added as inference opt,1
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
"this param init is overridden by model_builder, useless then.",0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
Helper functions,0
Keeps track of the original words/subwords,0
('prior_tokenization' option),0
In case there is a final case_markup when new_spacer is on,0
translate,0
for validation we build an infer_iter per batch,0
in order to avoid oom issues because there is no,0
batching strategy in `textbatch_to_tensor`,0
apply_reverse refs,0
flatten preds,0
save results,0
-*- coding: utf-8 -*-,0
this one is needed for Random Shuffler of batches,0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
we need to check the model path + any tokenizer path,0
patch to log stdout spawned processes of dataloader,0
bucket_size = batch_size,0
For TRAIN we need to group examples by length,0
"for faster performance, but otherwise, sequential.",0
For TRAIN we shuffle batches within the bucket,0
otherwise sequential,0
for specific case of rnn_packed need to be sorted,0
within the batch,0
Make features part of src like,0
"{'src': {'src': ..., 'feat1': ...., 'feat2': ....}}",0
We apply the same TransformPipe to all the bucket,0
at this point an example looks like:,0
"{'src': {'src': ..., 'feat1': ...., 'feat2': ....},",0
"'tgt': {'tgt': ...},",0
"'src_original': ['tok1', ...'tokn'],",0
"'tgt_original': ['tok1', ...'tokm'],",0
'indices' : seq in bucket,0
"'align': ...,",0
},0
we'll need to change this if we introduce tgt feat,0
Need to add features in last dimensions,0
Need to add features also in 'src',0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
-*- coding: utf-8 -*-,0
'src_original' and 'tgt_original' store the,0
original line before tokenization. These,0
fields are used later on in the feature,0
transforms.,0
NOTE: moved to dynamic_iterator.py cf process(),0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
"for side in [""src"", ""tgt""]:",0
keys_to_pop = [],0
"if hasattr(vocab[side], ""fields""):",0
unk_token = vocab[side].fields[0][1].vocab.itos[0],0
"for key, value in vocab[side].fields[0][1].vocab.stoi.items():",0
if value == 0 and key != unk_token:,0
keys_to_pop.append(key),0
for key in keys_to_pop:,0
"vocab[side].fields[0][1].vocab.stoi.pop(key, None)",0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
Just for debugging purposes,0
It appends features to subwords when dumping to file,0
!/usr/bin/env python,0
!/usr/bin/env python,0
Set sharing strategy manually instead of default based on the OS.,0
torch.multiprocessing.set_sharing_strategy('file_system'),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
magic indices,0
result caching,0
Here we set the decoder to start with self.start (BOS or EOS),0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
In the case of length_penalty = none we report the total logprobs,0
divided by the number of sentence to get an approximation of the,0
per sentence logprob. We also return the corresponding ppl,0
"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs",0
are normalized per token we report the per line per token logprob,0
"and the corresponding ""per word perplexity""",0
Turn any copied words into UNKs.,0
"Decoder forward, takes [batch, tgt_len, nfeats] as input",0
"and [batch, src_len, hidden] as enc_out",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
at this point scores is batch first (dim=0),0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
it should be done in a better way,0
here dec_in is batch first,0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
"decoder_input = decode_strategy.current_predictions.view(1, -1,",0
1),0
Reorder states.,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
Auto import python files in this directory,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
"Add in default model arguments, possibly added since training.",0
this patch is no longer needed included in converter,0
"if hasattr(model_opt, 'rnn_size'):",0
model_opt.hidden_size = model_opt.rnn_size,0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python3,0
coding: utf-8,0
"In order to use this tool, please install comet first",0
https://github.com/Unbabel/COMET,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate source and reference sentences nbest times,0
for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
same for source.,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \,0
--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best comet score,0
when choosing a reference-less model no nbest-ref is required,0
for nbest in nbests:,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python mbr_bleu.py --nbest-hyp target.5.tl \,0
--nbest-order 5 --output target.mbr.tl,0
It will compare all hyp with eachother and output the max bleu,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
Now we can pipe the full file through the model using the Iterator,0
reminder a batch includes .src .tgt .indices and it is sorted,0
Compute and retrieve the loss for EACH sentence,0
loss is returned normalized by tokens,0
we unnormalize to cumulate at doc level,0
Now we need to rearrange the batch of ppl,0
in the original order with indices,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate reference sentences nbest times for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \,0
--nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best bleu wrt reference,0
and output the max bleu,0
!/usr/bin/env python,0
with the two module = imp.load_source() below,0
we ghost the old torchtext.data.field and depercated,0
onmt.inputters.text_dataset,0
however this require some functions / classes to be,0
monkey patched for loading the old field/vocab objects.,0
"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
"this patch is no longer needed, included in converter",0
"if hasattr(model_opt, 'rnn_size'):",0
model_opt.hidden_size = model_opt.rnn_size,0
Avoid functionality on inference,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
Remove old vocabulary associated embeddings,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
Update model embeddings with those from the checkpoint,0
after initialization,0
!/usr/bin/env python,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
Override checkpoint's freezing settings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
Freeze Encoder and/or Decoder,0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
GPU,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Adding options related to Transforms,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
Let's clean the GPUs before training loop,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Compute validation metrics (at batch.dataset level),0
Compute stats,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
Compute and save stats,0
in theory we should divide by accum_count and bptt,0
to rescale for each sub batch,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
src lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
batch x len x dim,0
mask is now (batch x 1 x slen x slen),0
1 to be expanded to number of heads in MHA,0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
Shift values to be >= 0,0
class MultiHeadedAttention(torch.jit.ScriptModule):,0
https://arxiv.org/pdf/1803.02155.pdf,0
in the paper they suggest either two embeds,0
relative_key / relative_value or only,0
relative_key. We implemented the same embed,0
for both.,0
@torch.jit.script_method,0
"1) Project key, value, and query.",0
as a reminder at training layer_cache[0] remains False,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
not 100% necessary but expand to nb of heads,0
now mask and scores have the same shape,0
3) Apply attention dropout and compute context vectors.,0
We use the same embeddings for key and value,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
class AverageAttention(torch.jit.ScriptModule):,0
@torch.jit.script,0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
Do nothing,0
Do nothing,0
Punctuation only,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
Load vocabulary file if provided and set threshold,0
Load Subword Model,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
make sure the scalars are in the event accumulator tags,0
required arguments,0
transforms that require vocab will not create if not provide vocab,0
1. Init first transform in the pipe,0
2. Init second transform in the pipe,0
3. Sequential combine them into a transform pipe,0
4. apply transform pipe for example,0
"5. example after the pipe exceed the length limit, thus filtered",0
6. Transform statistics registed (here for filtertoolong),0
"7. after report, statistics become empty as a fresh start",0
filter_transform.warm_up(),0
test BPE-dropout:,0
1. disable bpe dropout for not training example,0
2. enable bpe dropout for training example,0
3. (NOTE) disable dropout won't take effect if already seen,0
this is caused by the cache mechanism in bpe:,0
return cached subword if the original token is seen when no dropout,0
test SP regularization:,0
1. enable regularization for training example,0
2. disable regularization for not training example,0
Not apply token drop for not training example,0
apply token drop for training example,0
Not apply token mask for not training example,0
apply token mask for training example,0
require vocabs to warm_up,0
Not apply token mask for not training example,0
apply token mask for training example,0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
random_ratio of inserted tokens are chosen in vocab,0
others are MASK_TOK,0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
all token are considered as an individual word,0
1. tokens are dropped when replace_length is 0,0
"print(f""token delete: {masked} / {tokens}"")",0
2. tokens are replaced by MASK when replace_length is 1,0
"print(f""token mask: {masked} / {tokens}"")",0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
"1. replace_length 0: ""words"" are dropped",0
"print(f""word delete: {masked} / {tokens}"")",0
"self.assertEqual(len(masked), n_words - n_masked)",0
"2. replace_length 1: ""words"" are replaced with a single MASK",0
"print(f""whole word single mask: {masked} / {tokens}"")",0
len(masked) depend on number of tokens in select word,0
"3. replace_length -1: all tokens in ""words"" are replaced with MASK",0
"print(f""whole word multi mask: {masked} / {tokens}"")",0
number of mask_tok depend on number of tokens in selected word,0
number of MASK_TOK can be greater than n_masked,0
"insert_ratio=0.5,",0
"random_ratio=0.3,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
n_words = sum(token_starts),0
n_masked = math.ceil(n_words * bart_noise.mask_ratio),0
"print(f""Text Span Infilling: {infillied} / {tokens}"")",0
"print(n_words, n_masked)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
we finish 3 hyps per example in this step,0
new beam 1 is old beam 3,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('hidden_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
"attns[""coverage""] is actually c^(t+1) of See et al(2017)",0
1-index shifted,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
src_len is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate src_len as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
BoolTensor was introduced in pytorch 1.2,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x s or t len),0
1 = heads to be expanded in MHA,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
first value set to True triggered by the beginning of decoding,0
layer_cache becomes active in the MultiHeadedAttention fwd,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x tlen),0
1 = heads to be expanded in MHA,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"print(filled, sz)",0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use apex.amp,0
In this case use the old FusedAdam with,0
FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
should be: self._optimizer.zero_grad(set_to_none),0
but apex.amp is not up-to-date:,0
https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
rescale with tau (temperature) and apply the log_softmax.,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
take into account here the tgt_shift_index (0 / 1 = LM/NMT),0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
in the case criterion reduction is None then we need,0
to sum the loss of each sentence in the batch,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check prefix: will be used when use prefix transform,0
Check weight,0
Check features,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
It comes from training,0
TODO: needs to be added as inference opt,1
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
"this param init is overridden by model_builder, useless then.",0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
Helper functions,0
Keeps track of the original words/subwords,0
('prior_tokenization' option),0
In case there is a final case_markup when new_spacer is on,0
translate,0
for validation we build an infer_iter per batch,0
in order to avoid oom issues because there is no,0
batching strategy in `textbatch_to_tensor`,0
apply_reverse refs,0
flatten preds,0
save results,0
We deactivate the decoder's cache,0
as we use teacher forcing at training time.,0
-*- coding: utf-8 -*-,0
this one is needed for Random Shuffler of batches,0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
we need to check the model path + any tokenizer path,0
bucket_size = batch_size,0
We only support,0
For TRAIN we need to group examples by length,0
"for faster performance, but otherwise, sequential.",0
For TRAIN we shuffle batches within the bucket,0
otherwise sequential,0
for specific case of rnn_packed need to be sorted,0
within the batch,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
Make features part of src like,0
"{'src': {'src': ..., 'feat1': ...., 'feat2': ....}}",0
We apply the same TransformPipe to all the bucket,0
at this point an example looks like:,0
"{'src': {'src': ..., 'feat1': ...., 'feat2': ....},",0
"'tgt': {'tgt': ...},",0
"'src_original': ['tok1', ...'tokn'],",0
"'tgt_original': ['tok1', ...'tokm'],",0
'indices' : seq in bucket,0
"'align': ...,",0
},0
we'll need to change this if we introduce tgt feat,0
Need to add features in last dimensions,0
Need to add features also in 'src',0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
-*- coding: utf-8 -*-,0
'src_original' and 'tgt_original' store the,0
original line before tokenization. These,0
fields are used later on in the feature,0
transforms.,0
NOTE: moved to dynamic_iterator.py cf process(),0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
"for side in [""src"", ""tgt""]:",0
keys_to_pop = [],0
"if hasattr(vocab[side], ""fields""):",0
unk_token = vocab[side].fields[0][1].vocab.itos[0],0
"for key, value in vocab[side].fields[0][1].vocab.stoi.items():",0
if value == 0 and key != unk_token:,0
keys_to_pop.append(key),0
for key in keys_to_pop:,0
"vocab[side].fields[0][1].vocab.stoi.pop(key, None)",0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
Just for debugging purposes,0
It appends features to subwords when dumping to file,0
!/usr/bin/env python,0
!/usr/bin/env python,0
Set sharing strategy manually instead of default based on the OS.,0
torch.multiprocessing.set_sharing_strategy('file_system'),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
magic indices,0
result caching,0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
In the case of length_penalty = none we report the total logprobs,0
divided by the number of sentence to get an approximation of the,0
per sentence logprob. We also return the corresponding ppl,0
"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs",0
are normalized per token we report the per line per token logprob,0
"and the corresponding ""per word perplexity""",0
Turn any copied words into UNKs.,0
"Decoder forward, takes [batch, tgt_len, nfeats] as input",0
"and [batch, src_len, hidden] as enc_out",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
at this point scores is batch first (dim=0),0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
it should be done in a better way,0
here dec_in is batch first,0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
"decoder_input = decode_strategy.current_predictions.view(1, -1,",0
1),0
Reorder states.,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
Auto import python files in this directory,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
"Add in default model arguments, possibly added since training.",0
this patch is no longer needed included in converter,0
"if hasattr(model_opt, 'rnn_size'):",0
model_opt.hidden_size = model_opt.rnn_size,0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python3,0
coding: utf-8,0
"In order to use this tool, please install comet first",0
https://github.com/Unbabel/COMET,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate source and reference sentences nbest times,0
for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
same for source.,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \,0
--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best comet score,0
when choosing a reference-less model no nbest-ref is required,0
for nbest in nbests:,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python mbr_bleu.py --nbest-hyp target.5.tl \,0
--nbest-order 5 --output target.mbr.tl,0
It will compare all hyp with eachother and output the max bleu,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
Now we can pipe the full file through the model using the Iterator,0
reminder a batch includes .src .tgt .indices and it is sorted,0
Compute and retrieve the loss for EACH sentence,0
loss is returned normalized by tokens,0
we unnormalize to cumulate at doc level,0
Now we need to rearrange the batch of ppl,0
in the original order with indices,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate reference sentences nbest times for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \,0
--nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best bleu wrt reference,0
and output the max bleu,0
!/usr/bin/env python,0
with the two module = imp.load_source() below,0
we ghost the old torchtext.data.field and depercated,0
onmt.inputters.text_dataset,0
however this require some functions / classes to be,0
monkey patched for loading the old field/vocab objects.,0
"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
"this patch is no longer needed, included in converter",0
"if hasattr(model_opt, 'rnn_size'):",0
model_opt.hidden_size = model_opt.rnn_size,0
Avoid functionality on inference,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
Remove old vocabulary associated embeddings,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
Update model embeddings with those from the checkpoint,0
after initialization,0
!/usr/bin/env python,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
Override checkpoint's freezing settings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
Freeze Encoder and/or Decoder,0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
GPU,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Adding options related to Transforms,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Compute validation metrics (at batch.dataset level),0
Compute stats,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
Compute and save stats,0
in theory we should divide by accum_count and bptt,0
to rescale for each sub batch,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
src lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
batch x len x dim,0
mask is now (batch x 1 x slen x slen),0
1 to be expanded to number of heads in MHA,0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
Shift values to be >= 0,0
class MultiHeadedAttention(torch.jit.ScriptModule):,0
https://arxiv.org/pdf/1803.02155.pdf,0
in the paper they suggest either two embeds,0
relative_key / relative_value or only,0
relative_key. We implemented the same embed,0
for both.,0
@torch.jit.script_method,0
"1) Project key, value, and query.",0
as a reminder at training layer_cache[0] remains False,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
not 100% necessary but expand to nb of heads,0
now mask and scores have the same shape,0
3) Apply attention dropout and compute context vectors.,0
We use the same embeddings for key and value,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
class AverageAttention(torch.jit.ScriptModule):,0
@torch.jit.script,0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
Do nothing,0
Do nothing,0
Punctuation only,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
Load vocabulary file if provided and set threshold,0
Load Subword Model,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
make sure the scalars are in the event accumulator tags,0
required arguments,0
transforms that require vocab will not create if not provide vocab,0
1. Init first transform in the pipe,0
2. Init second transform in the pipe,0
3. Sequential combine them into a transform pipe,0
4. apply transform pipe for example,0
"5. example after the pipe exceed the length limit, thus filtered",0
6. Transform statistics registed (here for filtertoolong),0
"7. after report, statistics become empty as a fresh start",0
filter_transform.warm_up(),0
test BPE-dropout:,0
1. disable bpe dropout for not training example,0
2. enable bpe dropout for training example,0
3. (NOTE) disable dropout won't take effect if already seen,0
this is caused by the cache mechanism in bpe:,0
return cached subword if the original token is seen when no dropout,0
test SP regularization:,0
1. enable regularization for training example,0
2. disable regularization for not training example,0
Not apply token drop for not training example,0
apply token drop for training example,0
Not apply token mask for not training example,0
apply token mask for training example,0
require vocabs to warm_up,0
Not apply token mask for not training example,0
apply token mask for training example,0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
random_ratio of inserted tokens are chosen in vocab,0
others are MASK_TOK,0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
all token are considered as an individual word,0
1. tokens are dropped when replace_length is 0,0
"print(f""token delete: {masked} / {tokens}"")",0
2. tokens are replaced by MASK when replace_length is 1,0
"print(f""token mask: {masked} / {tokens}"")",0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
"1. replace_length 0: ""words"" are dropped",0
"print(f""word delete: {masked} / {tokens}"")",0
"self.assertEqual(len(masked), n_words - n_masked)",0
"2. replace_length 1: ""words"" are replaced with a single MASK",0
"print(f""whole word single mask: {masked} / {tokens}"")",0
len(masked) depend on number of tokens in select word,0
"3. replace_length -1: all tokens in ""words"" are replaced with MASK",0
"print(f""whole word multi mask: {masked} / {tokens}"")",0
number of mask_tok depend on number of tokens in selected word,0
number of MASK_TOK can be greater than n_masked,0
"insert_ratio=0.5,",0
"random_ratio=0.3,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
n_words = sum(token_starts),0
n_masked = math.ceil(n_words * bart_noise.mask_ratio),0
"print(f""Text Span Infilling: {infillied} / {tokens}"")",0
"print(n_words, n_masked)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
we finish 3 hyps per example in this step,0
new beam 1 is old beam 3,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('hidden_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
src_len is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate src_len as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
BoolTensor was introduced in pytorch 1.2,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x s or t len),0
1 = heads to be expanded in MHA,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
first value set to True triggered by the beginning of decoding,0
layer_cache becomes active in the MultiHeadedAttention fwd,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x tlen),0
1 = heads to be expanded in MHA,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"print(filled, sz)",0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use apex.amp,0
In this case use the old FusedAdam with,0
FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
should be: self._optimizer.zero_grad(set_to_none),0
but apex.amp is not up-to-date:,0
https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead,0
"of probabilities, only the first part of the generator needs to",0
"be passed to the NMTLossCompute. At the moment, the only",0
supported loss function of this kind is the sparsemax loss.,0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
"we use the raw logits, rescale with tau (temperature) and",0
apply the log_softmax. reminder generator[0] is just the nn.Linear,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
"we use the raw logits, rescale with tau (temperature) and",0
apply the log_softmax. reminder generator[0] is just the nn.Linear,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
take into account here the tgt_shift_index (0 / 1 = LM/NMT),0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
in the case criterion reduction is None then we need,0
to sum the loss of each sentence in the batch,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check prefix: will be used when use prefix transform,0
Check weight,0
Check features,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
It comes from training,0
TODO: needs to be added as inference opt,1
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
"this param init is overridden by model_builder, useless then.",0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
Helper functions,0
Keeps track of the original words/subwords,0
('prior_tokenization' option),0
In case there is a final case_markup when new_spacer is on,0
for validation we build an infer_iter per batch,0
in order to avoid oom issues because there is no,0
batching strategy in `textbatch_to_tensor`,0
flatten sources (for validation),0
we deactivate the decoder's cache,0
as we use teacher forcing at training time.,0
-*- coding: utf-8 -*-,0
this one is needed for Random Shuffler of batches,0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
we need to check the model path + any tokenizer path,0
bucket_size = batch_size,0
We only support,0
For TRAIN we need to group examples by length,0
"for faster performance, but otherwise, sequential.",0
For TRAIN we shuffle batches within the bucket,0
otherwise sequential,0
for specific case of rnn_packed need to be sorted,0
within the batch,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
Make features part of src like,0
"{'src': {'src': ..., 'feat1': ...., 'feat2': ....}}",0
We apply the same TransformPipe to all the bucket,0
at this point an example looks like:,0
"{'src': {'src': ..., 'feat1': ...., 'feat2': ....},",0
"'tgt': {'tgt': ...},",0
"'src_original': ['tok1', ...'tokn'],",0
"'tgt_original': ['tok1', ...'tokm'],",0
'indices' : seq in bucket,0
"'align': ...,",0
},0
we'll need to change this if we introduce tgt feat,0
Need to add features in last dimensions,0
Need to add features also in 'src',0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
-*- coding: utf-8 -*-,0
'src_original' and 'tgt_original' store the,0
original line before tokenization. These,0
fields are used later on in the feature,0
transforms.,0
NOTE: moved to dynamic_iterator.py cf process(),0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
"for side in [""src"", ""tgt""]:",0
keys_to_pop = [],0
"if hasattr(vocab[side], ""fields""):",0
unk_token = vocab[side].fields[0][1].vocab.itos[0],0
"for key, value in vocab[side].fields[0][1].vocab.stoi.items():",0
if value == 0 and key != unk_token:,0
keys_to_pop.append(key),0
for key in keys_to_pop:,0
"vocab[side].fields[0][1].vocab.stoi.pop(key, None)",0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
Just for debugging purposes,0
It appends features to subwords when dumping to file,0
!/usr/bin/env python,0
!/usr/bin/env python,0
Set sharing strategy manually instead of default based on the OS.,0
torch.multiprocessing.set_sharing_strategy('file_system'),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
magic indices,0
result caching,0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
In the case of length_penalty = none we report the total logprobs,0
divided by the number of sentence to get an approximation of the,0
per sentence logprob. We also return the corresponding ppl,0
"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs",0
are normalized per token we report the per line per token logprob,0
"and the corresponding ""per word perplexity""",0
Turn any copied words into UNKs.,0
"Decoder forward, takes [batch, tgt_len, nfeats] as input",0
"and [batch, src_len, hidden] as enc_out",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
at this point scores is batch first (dim=0),0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
it should be done in a better way,0
here dec_in is batch first,0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
"decoder_input = decode_strategy.current_predictions.view(1, -1,",0
1),0
Reorder states.,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
src_raw = self.data.examples[inds[b]].src[0],0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
Auto import python files in this directory,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
"Add in default model arguments, possibly added since training.",0
this patch is no longer needed included in converter,0
"if hasattr(model_opt, 'rnn_size'):",0
model_opt.hidden_size = model_opt.rnn_size,0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python3,0
coding: utf-8,0
"In order to use this tool, please install comet first",0
https://github.com/Unbabel/COMET,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate source and reference sentences nbest times,0
for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
same for source.,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \,0
--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best comet score,0
when choosing a reference-less model no nbest-ref is required,0
for nbest in nbests:,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python mbr_bleu.py --nbest-hyp target.5.tl \,0
--nbest-order 5 --output target.mbr.tl,0
It will compare all hyp with eachother and output the max bleu,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
Now we can pipe the full file through the model using the Iterator,0
reminder a batch includes .src .tgt .indices and it is sorted,0
Compute and retrieve the loss for EACH sentence,0
loss is returned normalized by tokens,0
we unnormalize to cumulate at doc level,0
Now we need to rearrange the batch of ppl,0
in the original order with indices,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate reference sentences nbest times for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \,0
--nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best bleu wrt reference,0
and output the max bleu,0
!/usr/bin/env python,0
with the two module = imp.load_source() below,0
we ghost the old torchtext.data.field and depercated,0
onmt.inputters.text_dataset,0
however this require some functions / classes to be,0
monkey patched for loading the old field/vocab objects.,0
"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
"this patch is no longer needed, included in converter",0
"if hasattr(model_opt, 'rnn_size'):",0
model_opt.hidden_size = model_opt.rnn_size,0
Avoid functionality on inference,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
Remove old vocabulary associated embeddings,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
Update model embeddings with those from the checkpoint,0
after initialization,0
!/usr/bin/env python,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
Override checkpoint's freezing settings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
Freeze Encoder and/or Decoder,0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
GPU,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Adding options related to Transforms,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Compute validation metrics (at batch.dataset level),0
Compute stats,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
Compute and save stats,0
in theory we should divide by accum_count and bptt,0
to rescale for each sub batch,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
src lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
batch x len x dim,0
mask is now (batch x 1 x slen x slen),0
1 to be expanded to number of heads in MHA,0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
Shift values to be >= 0,0
class MultiHeadedAttention(torch.jit.ScriptModule):,0
https://arxiv.org/pdf/1803.02155.pdf,0
in the paper they suggest either two embeds,0
relative_key / relative_value or only,0
relative_key. We implemented the same embed,0
for both.,0
@torch.jit.script_method,0
"1) Project key, value, and query.",0
as a reminder at training layer_cache[0] remains False,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
not 100% necessary but expand to nb of heads,0
now mask and scores have the same shape,0
3) Apply attention dropout and compute context vectors.,0
We use the same embeddings for key and value,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
class AverageAttention(torch.jit.ScriptModule):,0
@torch.jit.script,0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
Do nothing,0
Do nothing,0
Punctuation only,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
Load vocabulary file if provided and set threshold,0
Load Subword Model,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
transforms that require vocab will not create if not provide vocab,0
1. Init first transform in the pipe,0
2. Init second transform in the pipe,0
3. Sequential combine them into a transform pipe,0
4. apply transform pipe for example,0
"5. example after the pipe exceed the length limit, thus filtered",0
6. Transform statistics registed (here for filtertoolong),0
"7. after report, statistics become empty as a fresh start",0
filter_transform.warm_up(),0
test BPE-dropout:,0
1. disable bpe dropout for not training example,0
2. enable bpe dropout for training example,0
3. (NOTE) disable dropout won't take effect if already seen,0
this is caused by the cache mechanism in bpe:,0
return cached subword if the original token is seen when no dropout,0
test SP regularization:,0
1. enable regularization for training example,0
2. disable regularization for not training example,0
Not apply token drop for not training example,0
apply token drop for training example,0
Not apply token mask for not training example,0
apply token mask for training example,0
require vocabs to warm_up,0
Not apply token mask for not training example,0
apply token mask for training example,0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
random_ratio of inserted tokens are chosen in vocab,0
others are MASK_TOK,0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
all token are considered as an individual word,0
1. tokens are dropped when replace_length is 0,0
"print(f""token delete: {masked} / {tokens}"")",0
2. tokens are replaced by MASK when replace_length is 1,0
"print(f""token mask: {masked} / {tokens}"")",0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
"1. replace_length 0: ""words"" are dropped",0
"print(f""word delete: {masked} / {tokens}"")",0
"self.assertEqual(len(masked), n_words - n_masked)",0
"2. replace_length 1: ""words"" are replaced with a single MASK",0
"print(f""whole word single mask: {masked} / {tokens}"")",0
len(masked) depend on number of tokens in select word,0
"3. replace_length -1: all tokens in ""words"" are replaced with MASK",0
"print(f""whole word multi mask: {masked} / {tokens}"")",0
number of mask_tok depend on number of tokens in selected word,0
number of MASK_TOK can be greater than n_masked,0
"insert_ratio=0.5,",0
"random_ratio=0.3,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
n_words = sum(token_starts),0
n_masked = math.ceil(n_words * bart_noise.mask_ratio),0
"print(f""Text Span Infilling: {infillied} / {tokens}"")",0
"print(n_words, n_masked)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
we finish 3 hyps per example in this step,0
new beam 1 is old beam 3,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('hidden_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
src_len is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate src_len as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
BoolTensor was introduced in pytorch 1.2,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x s or t len),0
1 = heads to be expanded in MHA,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
first value set to True triggered by the beginning of decoding,0
layer_cache becomes active in the MultiHeadedAttention fwd,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x tlen),0
1 = heads to be expanded in MHA,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"print(filled, sz)",0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use apex.amp,0
In this case use the old FusedAdam with,0
FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
should be: self._optimizer.zero_grad(set_to_none),0
but apex.amp is not up-to-date:,0
https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead,0
"of probabilities, only the first part of the generator needs to",0
"be passed to the NMTLossCompute. At the moment, the only",0
supported loss function of this kind is the sparsemax loss.,0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
"we use the raw logits, rescale with tau (temperature) and",0
apply the log_softmax. reminder generator[0] is just the nn.Linear,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
"we use the raw logits, rescale with tau (temperature) and",0
apply the log_softmax. reminder generator[0] is just the nn.Linear,0
ct2 expects src with lengths without padding,0
again we use raw probs to rescale with tau and apply log_softmax,0
lm_scores are in log space so log_target=True,0
take into account here the tgt_shift_index (0 / 1 = LM/NMT),0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
in the case criterion reduction is None then we need,0
to sum the loss of each sentence in the batch,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check prefix: will be used when use prefix transform,0
Check weight,0
Check features,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
It comes from training,0
TODO: needs to be added as inference opt,1
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
"this param init is overridden by model_builder, useless then.",0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
Helper functions,0
Keeps track of the original words/subwords,0
('prior_tokenization' option),0
In case there is a final case_markup when new_spacer is on,0
batch_side.shape[0] sentences to rebuild,0
batch_side.shape[1] tokens per sentence,0
we deactivate the decoder's cache,0
as we use teacher forcing at training time.,0
-*- coding: utf-8 -*-,0
this one is needed for Random Shuffler of batches,0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
we need to check the model path + any tokenizer path,0
bucket_size = batch_size,0
We only support,0
For TRAIN we need to group examples by length,0
"for faster performance, but otherwise, sequential.",0
For TRAIN we shuffle batches within the bucket,0
otherwise sequential,0
for specific case of rnn_packed need to be sorted,0
within the batch,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
this is a hack: appears quicker to apply it here,1
than in the ParallelCorpusIterator,0
Make features part of src like,0
"{'src': {'src': ..., 'feat1': ...., 'feat2': ....}}",0
at this point an example looks like:,0
"{'src': {'src': ..., 'feat1': ...., 'feat2': ....},",0
"'tgt': {'tgt': ...},",0
"'src_original': ['tok1', ...'tokn'],",0
"'tgt_original': ['tok1', ...'tokm'],",0
'indices' : seq in bucket,0
"'align': ...,",0
},0
we'll need to change this if we introduce tgt feat,0
Need to add features in last dimensions,0
Need to add features also in 'src',0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
-*- coding: utf-8 -*-,0
'src_original' and 'tgt_original' store the,0
original line before tokenization. These,0
fields are used later on in the feature,0
transforms.,0
NOTE: moved to dynamic_iterator.py cf process(),0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
"for side in [""src"", ""tgt""]:",0
keys_to_pop = [],0
"if hasattr(vocab[side], ""fields""):",0
unk_token = vocab[side].fields[0][1].vocab.itos[0],0
"for key, value in vocab[side].fields[0][1].vocab.stoi.items():",0
if value == 0 and key != unk_token:,0
keys_to_pop.append(key),0
for key in keys_to_pop:,0
"vocab[side].fields[0][1].vocab.stoi.pop(key, None)",0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
Just for debugging purposes,0
It appends features to subwords when dumping to file,0
!/usr/bin/env python,0
!/usr/bin/env python,0
Set sharing strategy manually instead of default based on the OS.,0
torch.multiprocessing.set_sharing_strategy('file_system'),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
magic indices,0
result caching,0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
In the case of length_penalty = none we report the total logprobs,0
divided by the number of sentence to get an approximation of the,0
per sentence logprob. We also return the corresponding ppl,0
"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs",0
are normalized per token we report the per line per token logprob,0
"and the corresponding ""per word perplexity""",0
Turn any copied words into UNKs.,0
"Decoder forward, takes [batch, tgt_len, nfeats] as input",0
"and [batch, src_len, hidden] as enc_out",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
at this point scores is batch first (dim=0),0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
it should be done in a better way,0
here dec_in is batch first,0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
"decoder_input = decode_strategy.current_predictions.view(1, -1,",0
1),0
Reorder states.,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
src_raw = self.data.examples[inds[b]].src[0],0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
Auto import python files in this directory,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
"Add in default model arguments, possibly added since training.",0
this patch is no longer needed included in converter,0
"if hasattr(model_opt, 'rnn_size'):",0
model_opt.hidden_size = model_opt.rnn_size,0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python3,0
coding: utf-8,0
"In order to use this tool, please install comet first",0
https://github.com/Unbabel/COMET,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate source and reference sentences nbest times,0
for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
same for source.,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \,0
--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best comet score,0
when choosing a reference-less model no nbest-ref is required,0
for nbest in nbests:,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python mbr_bleu.py --nbest-hyp target.5.tl \,0
--nbest-order 5 --output target.mbr.tl,0
It will compare all hyp with eachother and output the max bleu,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
Now we can pipe the full file through the model using the Iterator,0
reminder a batch includes .src .tgt .indices and it is sorted,0
Compute and retrieve the loss for EACH sentence,0
Now we need to rearrange the batch of ppl,0
in the original order with indices,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate reference sentences nbest times for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \,0
--nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best bleu wrt reference,0
and output the max bleu,0
!/usr/bin/env python,0
with the two module = imp.load_source() below,0
we ghost the old torchtext.data.field and depercated,0
onmt.inputters.text_dataset,0
however this require some functions / classes to be,0
monkey patched for loading the old field/vocab objects.,0
"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),",0
"key=lambda x: (-x[1], x[0]))).keys()",0
"this patch is no longer needed, included in converter",0
"if hasattr(model_opt, 'rnn_size'):",0
model_opt.hidden_size = model_opt.rnn_size,0
Avoid functionality on inference,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
Remove old vocabulary associated embeddings,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
Update model embeddings with those from the checkpoint,0
after initialization,0
!/usr/bin/env python,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
Override checkpoint's freezing settings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
Freeze Encoder and/or Decoder,0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
GPU,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Adding options related to Transforms,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Compute validation metrics (at batch.dataset level),0
Compute stats,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
Compute and save stats,0
in theory we should divide by accum_count and bptt,0
to rescale for each sub batch,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
src lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
batch x len x dim,0
mask is now (batch x 1 x slen x slen),0
1 to be expanded to number of heads in MHA,0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
Shift values to be >= 0,0
class MultiHeadedAttention(torch.jit.ScriptModule):,0
https://arxiv.org/pdf/1803.02155.pdf,0
in the paper they suggest either two embeds,0
relative_key / relative_value or only,0
relative_key. We implemented the same embed,0
for both.,0
@torch.jit.script_method,0
"1) Project key, value, and query.",0
as a reminder at training layer_cache[0] remains False,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
not 100% necessary but expand to nb of heads,0
now mask and scores have the same shape,0
3) Apply attention dropout and compute context vectors.,0
We use the same embeddings for key and value,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
class AverageAttention(torch.jit.ScriptModule):,0
@torch.jit.script,0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
Do nothing,0
Do nothing,0
Punctuation only,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
Load vocabulary file if provided and set threshold,0
Load Subword Model,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
transforms that require vocab will not create if not provide vocab,0
1. Init first transform in the pipe,0
2. Init second transform in the pipe,0
3. Sequential combine them into a transform pipe,0
4. apply transform pipe for example,0
"5. example after the pipe exceed the length limit, thus filtered",0
6. Transform statistics registed (here for filtertoolong),0
"7. after report, statistics become empty as a fresh start",0
filter_transform.warm_up(),0
test BPE-dropout:,0
1. disable bpe dropout for not training example,0
2. enable bpe dropout for training example,0
3. (NOTE) disable dropout won't take effect if already seen,0
this is caused by the cache mechanism in bpe:,0
return cached subword if the original token is seen when no dropout,0
test SP regularization:,0
1. enable regularization for training example,0
2. disable regularization for not training example,0
Not apply token drop for not training example,0
apply token drop for training example,0
Not apply token mask for not training example,0
apply token mask for training example,0
require vocabs to warm_up,0
Not apply token mask for not training example,0
apply token mask for training example,0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
random_ratio of inserted tokens are chosen in vocab,0
others are MASK_TOK,0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
all token are considered as an individual word,0
1. tokens are dropped when replace_length is 0,0
"print(f""token delete: {masked} / {tokens}"")",0
2. tokens are replaced by MASK when replace_length is 1,0
"print(f""token mask: {masked} / {tokens}"")",0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
"1. replace_length 0: ""words"" are dropped",0
"print(f""word delete: {masked} / {tokens}"")",0
"self.assertEqual(len(masked), n_words - n_masked)",0
"2. replace_length 1: ""words"" are replaced with a single MASK",0
"print(f""whole word single mask: {masked} / {tokens}"")",0
len(masked) depend on number of tokens in select word,0
"3. replace_length -1: all tokens in ""words"" are replaced with MASK",0
"print(f""whole word multi mask: {masked} / {tokens}"")",0
number of mask_tok depend on number of tokens in selected word,0
number of MASK_TOK can be greater than n_masked,0
"insert_ratio=0.5,",0
"random_ratio=0.3,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
n_words = sum(token_starts),0
n_masked = math.ceil(n_words * bart_noise.mask_ratio),0
"print(f""Text Span Infilling: {infillied} / {tokens}"")",0
"print(n_words, n_masked)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
we finish 3 hyps per example in this step,0
new beam 1 is old beam 3,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('hidden_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
src_len is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate src_len as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
BoolTensor was introduced in pytorch 1.2,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x s or t len),0
1 = heads to be expanded in MHA,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
first value set to True triggered by the beginning of decoding,0
layer_cache becomes active in the MultiHeadedAttention fwd,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
mask now are (batch x 1 x tlen x tlen),0
1 = heads to be expanded in MHA,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"print(filled, sz)",0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use here a FusedAdam() copy of an old Apex repo,0
In this case use the old FusedAdam with FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead,0
"of probabilities, only the first part of the generator needs to",0
"be passed to the NMTLossCompute. At the moment, the only",0
supported loss function of this kind is the sparsemax loss.,0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
take into account here the tgt_shift_index (0 / 1 = LM/NMT),0
in the case criterion reduction is None then we need,0
to sum the loss of each sentence in the batch,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check prefix: will be used when use prefix transform,0
Check weight,0
Check features,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
It comes from training,0
TODO: needs to be added as inference opt,1
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
"this param init is overridden by model_builder, useless then.",0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
Helper functions,0
Keeps track of the original words/subwords,0
('prior_tokenization' option),0
In case there is a final case_markup when new_spacer is on,0
-*- coding: utf-8 -*-,0
this one is needed for Random Shuffler of batches,0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
we need to check the model path + any tokenizer path,0
bucket_size = batch_size,0
We only support,0
For TRAIN we need to group examples by length,0
"for faster performance, but otherwise, sequential.",0
For TRAIN we shuffle batches within the bucket,0
otherwise sequential,0
for specific case of rnn_packed need to be sorted,0
within the batch,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
this is a hack: appears quicker to apply it here,1
than in the ParallelCorpusIterator,0
Make features part of src like,0
"{'src': {'src': ..., 'feat1': ...., 'feat2': ....}}",0
at this point an example looks like:,0
"{'src': {'src': ..., 'feat1': ...., 'feat2': ....},",0
"'tgt': {'tgt': ...},",0
"'src_original': ['tok1', ...'tokn'],",0
"'tgt_original': ['tok1', ...'tokm'],",0
'indices' : seq in bucket,0
"'align': ...,",0
},0
we'll need to change this if we introduce tgt feat,0
Need to add features in last dimensions,0
Need to add features also in 'src',0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
-*- coding: utf-8 -*-,0
'src_original' and 'tgt_original' store the,0
original line before tokenization. These,0
fields are used later on in the feature,0
transforms.,0
NOTE: moved to dynamic_iterator.py cf process(),0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
RNN uses enc_final_hs,0
CNN uses enc_out and enc_final_hs,0
transformer uses src,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
"for side in [""src"", ""tgt""]:",0
keys_to_pop = [],0
"if hasattr(vocab[side], ""fields""):",0
unk_token = vocab[side].fields[0][1].vocab.itos[0],0
"for key, value in vocab[side].fields[0][1].vocab.stoi.items():",0
if value == 0 and key != unk_token:,0
keys_to_pop.append(key),0
for key in keys_to_pop:,0
"vocab[side].fields[0][1].vocab.stoi.pop(key, None)",0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
Just for debugging purposes,0
It appends features to subwords when dumping to file,0
!/usr/bin/env python,0
!/usr/bin/env python,0
Set sharing strategy manually instead of default based on the OS.,0
torch.multiprocessing.set_sharing_strategy('file_system'),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
magic indices,0
result caching,0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
In the case of length_penalty = none we report the total logprobs,0
divided by the number of sentence to get an approximation of the,0
per sentence logprob. We also return the corresponding ppl,0
"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs",0
are normalized per token we report the per line per token logprob,0
"and the corresponding ""per word perplexity""",0
Turn any copied words into UNKs.,0
"Decoder forward, takes [batch, tgt_len, nfeats] as input",0
"and [batch, src_len, hidden] as enc_out",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
at this point scores is batch first (dim=0),0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [batch_size, tgt_len, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
it should be done in a better way,0
here dec_in is batch first,0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
"decoder_input = decode_strategy.current_predictions.view(1, -1,",0
1),0
Reorder states.,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
in LM task src_len is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
src_raw = self.data.examples[inds[b]].src[0],0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
Auto import python files in this directory,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
"Add in default model arguments, possibly added since training.",0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python3,0
coding: utf-8,0
"In order to use this tool, please install comet first",0
https://github.com/Unbabel/COMET,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate source and reference sentences nbest times,0
for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
same for source.,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \,0
--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best comet score,0
when choosing a reference-less model no nbest-ref is required,0
for nbest in nbests:,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python mbr_bleu.py --nbest-hyp target.5.tl \,0
--nbest-order 5 --output target.mbr.tl,0
It will compare all hyp with eachother and output the max bleu,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
Load model for inference,0
Build transforms,0
Build datareader based on src AND tgt (should be equal),0
Cannot use build_loss_compute() we need reduction 'none' in the criterion,0
to get the loss of each sentence instead of the loss of the full batch,0
Now we can pipe the full file through the model using the Iterator,0
reminder a batch includes .src .tgt .indices and it is sorted,0
Compute and retrieve the loss for EACH sentence,0
Now we need to rearrange the batch of ppl,0
in the original order with indices,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
coding: utf-8,0
Let's say you have a source file with N sentences in SL - eg: source.sl,0
and the corresponding references (N sentences) reference.tl,0
Translate your file in TL with the -n_best nbest options nbest being,0
then number of hypotheses and output the target to -output target.nbest.tl,0
Then you need to duplicate reference sentences nbest times for this script.,0
for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \,0
> reference.5.tl,0
This script can be run (for instance with nbest = 5) as follows:,0
python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \,0
--nbest-order 5 --output target.maxbleu.tl,0
It will search in all hyp the best bleu wrt reference,0
and output the max bleu,0
Avoid functionality on inference,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
Remove old vocabulary associated embeddings,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
Update model embeddings with those from the checkpoint,0
after initialization,0
!/usr/bin/env python,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
Override checkpoint's freezing settings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Move batch to specified device,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
Freeze Encoder and/or Decoder,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
GPU,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Adding options related to Transforms,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
gather stats unuseful on a single gpu,0
valid_stats = self._maybe_gather_stats(valid_stats),0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Compute validation metrics (at batch.dataset level),0
Compute stats,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
Compute and save stats,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return multi-head attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
Do nothing,0
Do nothing,0
Punctuation only,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
Load vocabulary file if provided and set threshold,0
Load Subword Model,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
transforms that require vocab will not create if not provide vocab,0
1. Init first transform in the pipe,0
2. Init second transform in the pipe,0
3. Sequential combine them into a transform pipe,0
4. apply transform pipe for example,0
"5. example after the pipe exceed the length limit, thus filtered",0
6. Transform statistics registed (here for filtertoolong),0
"7. after report, statistics become empty as a fresh start",0
filter_transform.warm_up(),0
test BPE-dropout:,0
1. disable bpe dropout for not training example,0
2. enable bpe dropout for training example,0
3. (NOTE) disable dropout won't take effect if already seen,0
this is caused by the cache mechanism in bpe:,0
return cached subword if the original token is seen when no dropout,0
test SP regularization:,0
1. enable regularization for training example,0
2. disable regularization for not training example,0
Not apply token drop for not training example,0
apply token drop for training example,0
Not apply token mask for not training example,0
apply token mask for training example,0
require vocabs to warm_up,0
Not apply token mask for not training example,0
apply token mask for training example,0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
random_ratio of inserted tokens are chosen in vocab,0
others are MASK_TOK,0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
all token are considered as an individual word,0
1. tokens are dropped when replace_length is 0,0
"print(f""token delete: {masked} / {tokens}"")",0
2. tokens are replaced by MASK when replace_length is 1,0
"print(f""token mask: {masked} / {tokens}"")",0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
"1. replace_length 0: ""words"" are dropped",0
"print(f""word delete: {masked} / {tokens}"")",0
"self.assertEqual(len(masked), n_words - n_masked)",0
"2. replace_length 1: ""words"" are replaced with a single MASK",0
"print(f""whole word single mask: {masked} / {tokens}"")",0
len(masked) depend on number of tokens in select word,0
"3. replace_length -1: all tokens in ""words"" are replaced with MASK",0
"print(f""whole word multi mask: {masked} / {tokens}"")",0
number of mask_tok depend on number of tokens in selected word,0
number of MASK_TOK can be greater than n_masked,0
"insert_ratio=0.5,",0
"random_ratio=0.3,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
n_words = sum(token_starts),0
n_masked = math.ceil(n_words * bart_noise.mask_ratio),0
"print(f""Text Span Infilling: {infillied} / {tokens}"")",0
"print(n_words, n_masked)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
we finish 3 hyps per example in this step,0
new beam 1 is old beam 3,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
BoolTensor was introduced in pytorch 1.2,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
NOTE: stride (if needed) is handled at the,0
generator (train_iter) level,0
Move batch to correspond device_id when consumer iterate,0
hack to dodge unpicklable `dict_keys`,1
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use here a FusedAdam() copy of an old Apex repo,0
In this case use the old FusedAdam with FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
in the case criterion reduction is None then we need,0
to sum the loss of each sentence in the batch,0
"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)",0
"align_idx should be a Tensor in size([N, 3]), N is total number",0
"of align src-tgt pair in current batch, each as",0
"['sent_N°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)",0
NOTE: tgt-src ref alignement that in range_ of shard,0
(coherent with batch.tgt),0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check prefix: will be used when use prefix transform,0
Check weight,0
Check features,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
It comes from training,0
TODO: needs to be added as inference opt,1
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
Helper functions,0
Keeps track of the original words/subwords,0
('prior_tokenization' option),0
In case there is a final case_markup when new_spacer is on,0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
we need to check the model path + any tokenizer path,0
fast-forward if loaded from state,0
NOTE: `rnn.pack_padded_sequence` requires that a,0
"minibatch be sorted by decreasing order, which",0
requires reversing relative to typical sort keys,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
add init_token and eos_token according to src construction,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
fields needs to have only keys that examples have as attrs,0
avoid infinite recursion when fields isn't defined,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
fields needs to have only keys that examples have as attrs,0
avoid infinite recursion when fields isn't defined,0
this is a hack: appears quicker to apply it here,1
than in the ParallelCorpusIterator,0
Make features part of src as in TextMultiField,0
"{'src': {'src': ..., 'feat1': ...., 'feat2': ....}}",0
'src_original' and 'tgt_original' store the,0
original line before tokenization. These,0
fields are used later on in the feature,0
transforms.,0
NOTE: moved to DatasetAdapter._process method in iterator.py,0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
Just for debugging purposes,0
It appends features to subwords when dumping to file,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"+1 for tgt side to keep coherent after ""bos"" padding,",0
"register ['N°_in_batch', 'tgt_id+1', 'src_id']",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
return vocab to dump with standard name,0
empty train_dataset_files so that vocab is only loaded from,0
"given paths in src_vocab_path, tgt_vocab_path",0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
NOTE: not support tgt feats yet,0
-*- coding: utf-8 -*-,0
Make features part of src as in TextMultiField,0
"{'src': {'src': ..., 'feat1': ...., 'feat2': ....}}",0
Cleanup,0
Legacy function. Currently it only truncates input if truncate is set.,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
Base field,0
Feats fields,0
"Legacy function, it is not really necessary",0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Build transforms,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
import onmt.opts as opts,0
Set sharing strategy manually instead of default based on the OS.,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
Report src and tgt vocab sizes,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"This does not work if we merge with the first loop, not sure why",1
Get the iterator to generate from,0
"Once training is done, we can terminate the producers",0
magic indices,0
result caching,0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task memory_lengths is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
max_tgt_in_batch = 0,0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
In the case of length_penalty = none we report the total logprobs,0
divided by the number of sentence to get an approximation of the,0
per sentence logprob. We also return the corresponding ppl,0
"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs",0
are normalized per token we report the per line per token logprob,0
"and the corresponding ""per word perplexity""",0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
Reorder states.,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task memory_lengths is associated with currently generated src,0
and therefore needs to follow the generation,0
in LM task memory_lengths is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
Auto import python files in this directory,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
"Add in default model arguments, possibly added since training.",0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
Avoid functionality on inference,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
Remove old vocabulary associated embeddings,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
Update model embeddings with those from the checkpoint,0
after initialization,0
!/usr/bin/env python,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Move batch to specified device,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
GPU,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return multi-head attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
Do nothing,0
Do nothing,0
If case markup placeholder,0
Punctuation only (assumes joiner is also some punctuation token),0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
Load vocabulary file if provided and set threshold,0
Load Subword Model,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
transforms that require vocab will not create if not provide vocab,0
1. Init first transform in the pipe,0
2. Init second transform in the pipe,0
3. Sequential combine them into a transform pipe,0
4. apply transform pipe for example,0
"5. example after the pipe exceed the length limit, thus filtered",0
6. Transform statistics registed (here for filtertoolong),0
"7. after report, statistics become empty as a fresh start",0
filter_transform.warm_up(),0
test BPE-dropout:,0
1. disable bpe dropout for not training example,0
2. enable bpe dropout for training example,0
3. (NOTE) disable dropout won't take effect if already seen,0
this is caused by the cache mechanism in bpe:,0
return cached subword if the original token is seen when no dropout,0
test SP regularization:,0
1. enable regularization for training example,0
2. disable regularization for not training example,0
Not apply token drop for not training example,0
apply token drop for training example,0
Not apply token mask for not training example,0
apply token mask for training example,0
require vocabs to warm_up,0
Not apply token mask for not training example,0
apply token mask for training example,0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
random_ratio of inserted tokens are chosen in vocab,0
others are MASK_TOK,0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
all token are considered as an individual word,0
1. tokens are dropped when replace_length is 0,0
"print(f""token delete: {masked} / {tokens}"")",0
2. tokens are replaced by MASK when replace_length is 1,0
"print(f""token mask: {masked} / {tokens}"")",0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
"1. replace_length 0: ""words"" are dropped",0
"print(f""word delete: {masked} / {tokens}"")",0
"self.assertEqual(len(masked), n_words - n_masked)",0
"2. replace_length 1: ""words"" are replaced with a single MASK",0
"print(f""whole word single mask: {masked} / {tokens}"")",0
len(masked) depend on number of tokens in select word,0
"3. replace_length -1: all tokens in ""words"" are replaced with MASK",0
"print(f""whole word multi mask: {masked} / {tokens}"")",0
number of mask_tok depend on number of tokens in selected word,0
number of MASK_TOK can be greater than n_masked,0
"insert_ratio=0.5,",0
"random_ratio=0.3,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
n_words = sum(token_starts),0
n_masked = math.ceil(n_words * bart_noise.mask_ratio),0
"print(f""Text Span Infilling: {infillied} / {tokens}"")",0
"print(n_words, n_masked)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
BoolTensor was introduced in pytorch 1.2,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
NOTE: stride (if needed) is handled at the,0
generator (train_iter) level,0
Move batch to correspond device_id when consumer iterate,0
hack to dodge unpicklable `dict_keys`,1
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use here a FusedAdam() copy of an old Apex repo,0
In this case use the old FusedAdam with FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)",0
"align_idx should be a Tensor in size([N, 3]), N is total number",0
"of align src-tgt pair in current batch, each as",0
"['sent_N°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)",0
NOTE: tgt-src ref alignement that in range_ of shard,0
(coherent with batch.tgt),0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check prefix: will be used when use prefix transform,0
Check weight,0
Check features,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
In case there is a final case_markup when new_spacer is on,0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
we need to check the model path + any tokenizer path,0
fast-forward if loaded from state,0
NOTE: `rnn.pack_padded_sequence` requires that a,0
"minibatch be sorted by decreasing order, which",0
requires reversing relative to typical sort keys,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
add init_token and eos_token according to src construction,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
fields needs to have only keys that examples have as attrs,0
avoid infinite recursion when fields isn't defined,0
this is a hack: appears quicker to apply it here,1
than in the ParallelCorpusIterator,0
Make features part of src as in TextMultiField,0
"{'src': {'src': ..., 'feat1': ...., 'feat2': ....}}",0
NOTE: moved to DatasetAdapter._process method in iterator.py,0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"+1 for tgt side to keep coherent after ""bos"" padding,",0
"register ['N°_in_batch', 'tgt_id+1', 'src_id']",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
return vocab to dump with standard name,0
empty train_dataset_files so that vocab is only loaded from,0
"given paths in src_vocab_path, tgt_vocab_path",0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
NOTE: not support tgt feats yet,0
-*- coding: utf-8 -*-,0
Legacy function. Currently it only truncates input if truncate is set.,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
Base field,0
Feats fields,0
"Legacy function, it is not really necessary",0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
import onmt.opts as opts,0
Set sharing strategy manually instead of default based on the OS.,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
Report src and tgt vocab sizes,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"This does not work if we merge with the first loop, not sure why",1
Get the iterator to generate from,0
"Once training is done, we can terminate the producers",0
magic indices,0
result caching,0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task memory_lengths is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
max_tgt_in_batch = 0,0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
Reorder states.,0
TODO: support these blacklisted features,0
hack [min_len_batch-1:] because expect <bos>,1
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task memory_lengths is associated with currently generated src,0
and therefore needs to follow the generation,0
in LM task memory_lengths is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
"Add in default model arguments, possibly added since training.",0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
Avoid functionality on inference,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
Remove old vocabulary associated embeddings,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
Update model embeddings with those from the checkpoint,0
after initialization,0
!/usr/bin/env python,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Move batch to specified device,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
GPU,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return multi-head attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
Load vocabulary file if provided and set threshold,0
Load Subword Model,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
transforms that require vocab will not create if not provide vocab,0
filter_transform.warm_up(),0
test BPE-dropout:,0
1. disable bpe dropout for not training example,0
2. enable bpe dropout for training example,0
3. (NOTE) disable dropout won't take effect if already seen,0
this is caused by the cache mechanism in bpe:,0
return cached subword if the original token is seen when no dropout,0
test SP regularization:,0
1. enable regularization for training example,0
2. disable regularization for not training example,0
Not apply token drop for not training example,0
apply token drop for training example,0
Not apply token mask for not training example,0
apply token mask for training example,0
require vocabs to warm_up,0
Not apply token mask for not training example,0
apply token mask for training example,0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
random_ratio of inserted tokens are chosen in vocab,0
others are MASK_TOK,0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
all token are considered as an individual word,0
1. tokens are dropped when replace_length is 0,0
"print(f""token delete: {masked} / {tokens}"")",0
2. tokens are replaced by MASK when replace_length is 1,0
"print(f""token mask: {masked} / {tokens}"")",0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
"1. replace_length 0: ""words"" are dropped",0
"print(f""word delete: {masked} / {tokens}"")",0
"self.assertEqual(len(masked), n_words - n_masked)",0
"2. replace_length 1: ""words"" are replaced with a single MASK",0
"print(f""whole word single mask: {masked} / {tokens}"")",0
len(masked) depend on number of tokens in select word,0
"3. replace_length -1: all tokens in ""words"" are replaced with MASK",0
"print(f""whole word multi mask: {masked} / {tokens}"")",0
number of mask_tok depend on number of tokens in selected word,0
number of MASK_TOK can be greater than n_masked,0
"insert_ratio=0.5,",0
"random_ratio=0.3,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
n_words = sum(token_starts),0
n_masked = math.ceil(n_words * bart_noise.mask_ratio),0
"print(f""Text Span Infilling: {infillied} / {tokens}"")",0
"print(n_words, n_masked)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
BoolTensor was introduced in pytorch 1.2,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
NOTE: stride (if needed) is handled at the,0
generator (train_iter) level,0
Move batch to correspond device_id when consumer iterate,0
hack to dodge unpicklable `dict_keys`,1
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use here a FusedAdam() copy of an old Apex repo,0
In this case use the old FusedAdam with FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)",0
"align_idx should be a Tensor in size([N, 3]), N is total number",0
"of align src-tgt pair in current batch, each as",0
"['sent_N°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)",0
NOTE: tgt-src ref alignement that in range_ of shard,0
(coherent with batch.tgt),0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check prefix: will be used when use prefix transform,0
Check weight,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
we need to check the model path + any tokenizer path,0
fast-forward if loaded from state,0
NOTE: `rnn.pack_padded_sequence` requires that a,0
"minibatch be sorted by decreasing order, which",0
requires reversing relative to typical sort keys,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
add init_token and eos_token according to src construction,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
fields needs to have only keys that examples have as attrs,0
avoid infinite recursion when fields isn't defined,0
this is a hack: appears quicker to apply it here,1
than in the ParallelCorpusIterator,0
NOTE: moved to DatasetAdapter._process method in iterator.py,0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"+1 for tgt side to keep coherent after ""bos"" padding,",0
"register ['N°_in_batch', 'tgt_id+1', 'src_id']",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
return vocab to dump with standard name,0
empty train_dataset_files so that vocab is only loaded from,0
"given paths in src_vocab_path, tgt_vocab_path",0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
NOTE: not support nfeats > 0 yet,0
-*- coding: utf-8 -*-,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
import onmt.opts as opts,0
Set sharing strategy manually instead of default based on the OS.,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
Report src and tgt vocab sizes,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"This does not work if we merge with the first loop, not sure why",1
Get the iterator to generate from,0
"Once training is done, we can terminate the producers",0
magic indices,0
result caching,0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task memory_lengths is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
max_tgt_in_batch = 0,0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
Reorder states.,0
TODO: support these blacklisted features,0
hack [min_len_batch-1:] because expect <bos>,1
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task memory_lengths is associated with currently generated src,0
and therefore needs to follow the generation,0
in LM task memory_lengths is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
"Add in default model arguments, possibly added since training.",0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
Avoid functionality on inference,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
Update model embeddings with those from the checkpoint after initialization,0
Remove old vocabulary associated embeddings,0
Embedding layers,0
!/usr/bin/env python,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Move batch to specified device,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
GPU,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return multi-head attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
Load vocabulary file if provided and set threshold,0
Load Subword Model,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
transforms that require vocab will not create if not provide vocab,0
filter_transform.warm_up(),0
test BPE-dropout:,0
1. disable bpe dropout for not training example,0
2. enable bpe dropout for training example,0
3. (NOTE) disable dropout won't take effect if already seen,0
this is caused by the cache mechanism in bpe:,0
return cached subword if the original token is seen when no dropout,0
test SP regularization:,0
1. enable regularization for training example,0
2. disable regularization for not training example,0
Not apply token drop for not training example,0
apply token drop for training example,0
Not apply token mask for not training example,0
apply token mask for training example,0
require vocabs to warm_up,0
Not apply token mask for not training example,0
apply token mask for training example,0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
random_ratio of inserted tokens are chosen in vocab,0
others are MASK_TOK,0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
all token are considered as an individual word,0
1. tokens are dropped when replace_length is 0,0
"print(f""token delete: {masked} / {tokens}"")",0
2. tokens are replaced by MASK when replace_length is 1,0
"print(f""token mask: {masked} / {tokens}"")",0
"insert_ratio=0.0,",0
"random_ratio=0.0,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
"1. replace_length 0: ""words"" are dropped",0
"print(f""word delete: {masked} / {tokens}"")",0
"self.assertEqual(len(masked), n_words - n_masked)",0
"2. replace_length 1: ""words"" are replaced with a single MASK",0
"print(f""whole word single mask: {masked} / {tokens}"")",0
len(masked) depend on number of tokens in select word,0
"3. replace_length -1: all tokens in ""words"" are replaced with MASK",0
"print(f""whole word multi mask: {masked} / {tokens}"")",0
number of mask_tok depend on number of tokens in selected word,0
number of MASK_TOK can be greater than n_masked,0
"insert_ratio=0.5,",0
"random_ratio=0.3,",0
"Defalt: full_stop_token=[""."", ""?"", ""!""]",0
start token of word are identified using subword marker,0
n_words = sum(token_starts),0
n_masked = math.ceil(n_words * bart_noise.mask_ratio),0
"print(f""Text Span Infilling: {infillied} / {tokens}"")",0
"print(n_words, n_masked)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
BoolTensor was introduced in pytorch 1.2,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
NOTE: stride (if needed) is handled at the,0
generator (train_iter) level,0
Move batch to correspond device_id when consumer iterate,0
hack to dodge unpicklable `dict_keys`,1
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use here a FusedAdam() copy of an old Apex repo,0
In this case use the old FusedAdam with FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)",0
"align_idx should be a Tensor in size([N, 3]), N is total number",0
"of align src-tgt pair in current batch, each as",0
"['sent_N°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)",0
NOTE: tgt-src ref alignement that in range_ of shard,0
(coherent with batch.tgt),0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check prefix: will be used when use prefix transform,0
Check weight,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
we need to check the model path + any tokenizer path,0
fast-forward if loaded from state,0
NOTE: `rnn.pack_padded_sequence` requires that a,0
"minibatch be sorted by decreasing order, which",0
requires reversing relative to typical sort keys,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
add init_token and eos_token according to src construction,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
fields needs to have only keys that examples have as attrs,0
avoid infinite recursion when fields isn't defined,0
this is a hack: appears quicker to apply it here,1
than in the ParallelCorpusIterator,0
NOTE: moved to DatasetAdapter._process method in iterator.py,0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"+1 for tgt side to keep coherent after ""bos"" padding,",0
"register ['N°_in_batch', 'tgt_id+1', 'src_id']",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
return vocab to dump with standard name,0
empty train_dataset_files so that vocab is only loaded from,0
"given paths in src_vocab_path, tgt_vocab_path",0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
NOTE: not support nfeats > 0 yet,0
-*- coding: utf-8 -*-,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
import onmt.opts as opts,0
Set sharing strategy manually instead of default based on the OS.,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
Report src and tgt vocab sizes,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"This does not work if we merge with the first loop, not sure why",1
Get the iterator to generate from,0
"Once training is done, we can terminate the producers",0
magic indices,0
result caching,0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task memory_lengths is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
max_tgt_in_batch = 0,0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
Reorder states.,0
TODO: support these blacklisted features,0
hack [min_len_batch-1:] because expect <bos>,1
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task memory_lengths is associated with currently generated src,0
and therefore needs to follow the generation,0
in LM task memory_lengths is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
"Add in default model arguments, possibly added since training.",0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
Avoid functionality on inference,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Update vocabulary embeddings with checkpoint embeddings,0
Embedding layers,0
Just for debugging purposes,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
Update model embeddings with those from the checkpoint after initialization,0
Remove old vocabulary associated embeddings,0
Embedding layers,0
!/usr/bin/env python,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
Override checkpoint's update_embeddings as it defaults to false,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Move batch to specified device,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
GPU,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return multi-head attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
no dummy prefix,0
no dummy prefix,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
BoolTensor was introduced in pytorch 1.2,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
NOTE: stride (if needed) is handled at the,0
generator (train_iter) level,0
Move batch to correspond device_id when consumer iterate,0
hack to dodge unpicklable `dict_keys`,1
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use here a FusedAdam() copy of an old Apex repo,0
In this case use the old FusedAdam with FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)",0
"align_idx should be a Tensor in size([N, 3]), N is total number",0
"of align src-tgt pair in current batch, each as",0
"['sent_N°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)",0
NOTE: tgt-src ref alignement that in range_ of shard,0
(coherent with batch.tgt),0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check prefix: will be used when use prefix transform,0
Check weight,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
we need to check the model path + any tokenizer path,0
fast-forward if loaded from state,0
NOTE: `rnn.pack_padded_sequence` requires that a,0
"minibatch be sorted by decreasing order, which",0
requires reversing relative to typical sort keys,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
add init_token and eos_token according to src construction,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
fields needs to have only keys that examples have as attrs,0
avoid infinite recursion when fields isn't defined,0
this is a hack: appears quicker to apply it here,1
than in the ParallelCorpusIterator,0
NOTE: moved to DatasetAdapter._process method in iterator.py,0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"+1 for tgt side to keep coherent after ""bos"" padding,",0
"register ['N°_in_batch', 'tgt_id+1', 'src_id']",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
return vocab to dump with standard name,0
empty train_dataset_files so that vocab is only loaded from,0
"given paths in src_vocab_path, tgt_vocab_path",0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
NOTE: not support nfeats > 0 yet,0
-*- coding: utf-8 -*-,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
import onmt.opts as opts,0
Set sharing strategy manually instead of default based on the OS.,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
Report src and tgt vocab sizes,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"This does not work if we merge with the first loop, not sure why",1
Get the iterator to generate from,0
"Once training is done, we can terminate the producers",0
magic indices,0
result caching,0
fix length constraint and remove eos from count,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task memory_lengths is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
max_tgt_in_batch = 0,0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
Reorder states.,0
TODO: support these blacklisted features,0
hack [min_len_batch-1:] because expect <bos>,1
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task memory_lengths is associated with currently generated src,0
and therefore needs to follow the generation,0
in LM task memory_lengths is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
"Add in default model arguments, possibly added since training.",0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Move batch to specified device,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
The following options (bridge_extra_node to n_steps) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
GPU,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Length penalty options,0
Coverage penalty options,0
Decoding Length constraint,0
Decoding content constraint,0
Adding options relate to decoding strategy,0
Adding option for logging,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Token embedding,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return multi-head attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
BoolTensor was introduced in pytorch 1.2,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
NOTE: stride (if needed) is handled at the,0
generator (train_iter) level,0
Move batch to correspond device_id when consumer iterate,0
hack to dodge unpicklable `dict_keys`,1
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use here a FusedAdam() copy of an old Apex repo,0
In this case use the old FusedAdam with FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)",0
"align_idx should be a Tensor in size([N, 3]), N is total number",0
"of align src-tgt pair in current batch, each as",0
"['sent_N°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)",0
NOTE: tgt-src ref alignement that in range_ of shard,0
(coherent with batch.tgt),0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check prefix: will be used when use prefix transform,0
Check weight,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
we need to check the model path + any tokenizer path,0
fast-forward if loaded from state,0
NOTE: `rnn.pack_padded_sequence` requires that a,0
"minibatch be sorted by decreasing order, which",0
requires reversing relative to typical sort keys,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
add init_token and eos_token according to src construction,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
fields needs to have only keys that examples have as attrs,0
avoid infinite recursion when fields isn't defined,0
this is a hack: appears quicker to apply it here,1
than in the ParallelCorpusIterator,0
NOTE: moved to DatasetAdapter._process method in iterator.py,0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"+1 for tgt side to keep coherent after ""bos"" padding,",0
"register ['N°_in_batch', 'tgt_id+1', 'src_id']",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
return vocab to dump with standard name,0
empty train_dataset_files so that vocab is only loaded from,0
"given paths in src_vocab_path, tgt_vocab_path",0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
NOTE: not support nfeats > 0 yet,0
-*- coding: utf-8 -*-,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
import onmt.opts as opts,0
Set sharing strategy manually instead of default based on the OS.,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
Report src and tgt vocab sizes,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"This does not work if we merge with the first loop, not sure why",1
Get the iterator to generate from,0
"Once training is done, we can terminate the producers",0
magic indices,0
result caching,0
fix length constraint,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task memory_lengths is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
max_tgt_in_batch = 0,0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
Reorder states.,0
TODO: support these blacklisted features,0
hack [min_len_batch-1:] because expect <bos>,1
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task memory_lengths is associated with currently generated src,0
and therefore needs to follow the generation,0
in LM task memory_lengths is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
"Add in default model arguments, possibly added since training.",0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
Build embeddings.,0
Build encoder.,0
Build embeddings.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
for back compat when attention_dropout was not defined,0
Build Model,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
ensure tensorboard output is written in the directory,0
of previous checkpoints,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Move batch to specified device,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
Embedding Options,0
Model Task Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
The following options (bridge_extra_node to src_vocab) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
GPU,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Adding options relate to decoding strategy,0
Adding option for logging,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return multi-head attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
finish one beam,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
finish example in last batch,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
BoolTensor was introduced in pytorch 1.2,0
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
T: could be 1 in the case of stepwise decoding or tgt_len,0
masking is necessary when sequence length is greater than one,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
NOTE: stride (if needed) is handled at the,0
generator (train_iter) level,0
Move batch to correspond device_id when consumer iterate,0
hack to dodge unpicklable `dict_keys`,1
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use here a FusedAdam() copy of an old Apex repo,0
In this case use the old FusedAdam with FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)",0
"align_idx should be a Tensor in size([N, 3]), N is total number",0
"of align src-tgt pair in current batch, each as",0
"['sent_N°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)",0
NOTE: tgt-src ref alignement that in range_ of shard,0
(coherent with batch.tgt),0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Check Transforms,0
Check path,0
tgt is src for LM task,0
Check prefix: will be used when use prefix transform,0
Check weight,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
we need to check the model path + any tokenizer path,0
fast-forward if loaded from state,0
NOTE: `rnn.pack_padded_sequence` requires that a,0
"minibatch be sorted by decreasing order, which",0
requires reversing relative to typical sort keys,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
add init_token and eos_token according to src construction,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
fields needs to have only keys that examples have as attrs,0
avoid infinite recursion when fields isn't defined,0
this is a hack: appears quicker to apply it here,1
than in the ParallelCorpusIterator,0
NOTE: moved to DatasetAdapter._process method in iterator.py,0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"+1 for tgt side to keep coherent after ""bos"" padding,",0
"register ['N°_in_batch', 'tgt_id+1', 'src_id']",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
return vocab to dump with standard name,0
empty train_dataset_files so that vocab is only loaded from,0
"given paths in src_vocab_path, tgt_vocab_path",0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
NOTE: not support nfeats > 0 yet,0
-*- coding: utf-8 -*-,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
"No encoder in LM, seq2seq count formatting kept",0
_check_save_model_path,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
import onmt.opts as opts,0
Set sharing strategy manually instead of default based on the OS.,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
Report src and tgt vocab sizes,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"This does not work if we merge with the first loop, not sure why",1
Get the iterator to generate from,0
"Once training is done, we can terminate the producers",0
magic indices,0
result caching,0
fix length constraint,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
keep indices until overflowing p,0
Set all logits that are not in the top-p to -10000.,0
This puts the probabilities close to 0.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
in LM task memory_lengths is associated with currently generated src,0
and therefore needs to follow the generation,0
!/usr/bin/env python,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
max_tgt_in_batch = 0,0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
Reorder states.,0
TODO: support these blacklisted features,0
hack [min_len_batch-1:] because expect <bos>,1
(0) Prep the components of the search.,0
(1) split src into src and target_prefix to avoid padding.,0
(2) init decoder,0
(3) prep decode_strategy. Possibly repeat src objects.,0
(4) Begin decoding step by step:,0
Reorder states.,0
select indexes in model state/cache,0
beam parameters,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
in LM task memory_lengths is associated with currently generated src,0
and therefore needs to follow the generation,0
in LM task memory_lengths is associated with currently generated src,0
and therefore needs to follow the generation,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
"Add in default model arguments, possibly added since training.",0
build_base_model expects updated and validated opts,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
for back compat when attention_dropout was not defined,0
Build embeddings.,0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Move batch to specified device,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
Embedding Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
The following options (bridge_extra_node to src_vocab) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
GPU,0
Init options,0
Pretrained word vectors,0
Freeze word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Adding options relate to decoding strategy,0
Adding option for logging,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return multi-head attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
subword vocabulary restriction options:,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are freezed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
T: could be 1 in the case of stepwise decoding or tgt_len,0
BoolTensor was introduced in pytorch 1.2,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
NOTE: stride (if needed) is handled at the,0
generator (train_iter) level,0
Move batch to correspond device_id when consumer iterate,0
hack to dodge unpicklable `dict_keys`,1
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use here a FusedAdam() copy of an old Apex repo,0
In this case use the old FusedAdam with FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)",0
"align_idx should be a Tensor in size([N, 3]), N is total number",0
"of align src-tgt pair in current batch, each as",0
"['sent_N°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)",0
NOTE: tgt-src ref alignement that in range_ of shard,0
(coherent with batch.tgt),0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Check Transforms,0
Check path,0
Check prefix: will be used when use prefix transform,0
Check weight,0
validation when train:,0
Check embeddings stuff,0
"Backward compatibility with ""fix_word_vecs_*"" opts",0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
we need to check the model path + any tokenizer path,0
fast-forward if loaded from state,0
NOTE: `rnn.pack_padded_sequence` requires that a,0
"minibatch be sorted by decreasing order, which",0
requires reversing relative to typical sort keys,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
fields needs to have only keys that examples have as attrs,0
avoid infinite recursion when fields isn't defined,0
this is a hack: appears quicker to apply it here,1
than in the ParallelCorpusIterator,0
NOTE: moved to DatasetAdapter._process method in iterator.py,0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"+1 for tgt side to keep coherent after ""bos"" padding,",0
"register ['N°_in_batch', 'tgt_id+1', 'src_id']",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
return vocab to dump with standard name,0
empty train_dataset_files so that vocab is only loaded from,0
"given paths in src_vocab_path, tgt_vocab_path",0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
NOTE: not support nfeats > 0 yet,0
-*- coding: utf-8 -*-,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
_check_save_model_path,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
import onmt.opts as opts,0
Set sharing strategy manually instead of default based on the OS.,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
Report src and tgt vocab sizes,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"This does not work if we merge with the first loop, not sure why",1
Get the iterator to generate from,0
"Once training is done, we can terminate the producers",0
magic indices,0
result caching,0
fix length constraint,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
!/usr/bin/env python,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
max_tgt_in_batch = 0,0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
Reorder states.,0
beam parameters,0
result caching,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
converts a SentencePiece vocabulary to the format expected by dynamic data,0
"(essentially converts float expected counts to ""fixed precision"" int pseudo",0
counts),0
"Add in default model arguments, possibly added since training.",0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
for back compat when attention_dropout was not defined,0
Build embeddings.,0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Move batch to specified device,0
Use Tensorboard for visualization during training,0
Options only during inference,0
"Truncation options, for text corpus",0
"as for False, this will be added in _add_train_general_opts",0
Embedding Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
The following options (bridge_extra_node to src_vocab) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
Attention options,0
Alignement options,0
Generator and loss options.,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
options relate to data preprare,0
options relate to train,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Adding options relate to decoding strategy,0
Adding option for logging,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
Some utilitary functions for pretrained embeddings,0
is this reachable?,0
Write to file,0
set the opt in place,0
set the opt in place,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return multi-head attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
Auto import python files in this directory,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. sample corrupted values,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. Drop token on chosen position,0
1. sample number of tokens to corrupt,0
2. sample positions to corrput,0
3. mask word on chosen position,0
"Sharing options among `TokenizerTransform`s, same name conflict in",0
this scope will be resolved by remove previous occurrence in parser,0
subword regularization(or BPE dropout) options:,0
derterministic subwording,0
subword sampling when nbest_size > 1 or -1,0
alpha should be 0.0 < alpha < 1.0,0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
view each subword as word start / input is word level token,0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert is_word_start[-1] == 0,0
assert tokens_length - 1 not in indices,0
"keep index, but replace it with [MASK]",0
"acts as a long length, so spans don't go over the end of doc",0
next position from each word_start,0
delete token: 1 mask/remove per span,0
"keep index, but replace it with [MASK]: 1 mask per token",0
A bit faster when all lengths are 1,0
to cover whole token,0
delete token,0
"keep index, but replace it with [MASK]",0
assert tokens_length - 1 not in indices,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Inject some dummy training options that may needed when build fields,0
Remove the generated *pt files.,0
Remove the generated data samples,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are fixed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
T: could be 1 in the case of stepwise decoding or tgt_len,0
BoolTensor was introduced in pytorch 1.2,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
NOTE: stride (if needed) is handled at the,0
generator (train_iter) level,0
Move batch to correspond device_id when consumer iterate,0
hack to dodge unpicklable `dict_keys`,1
"propagate exception to parent process, keeping original traceback",0
TODO: Find a better way to check for sparse gradients.,1
we use here a FusedAdam() copy of an old Apex repo,0
In this case use the old FusedAdam with FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)",0
"align_idx should be a Tensor in size([N, 3]), N is total number",0
"of align src-tgt pair in current batch, each as",0
"['sent_N°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)",0
NOTE: tgt-src ref alignement that in range_ of shard,0
(coherent with batch.tgt),0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Check Transforms,0
Check path,0
Check prefix: will be used when use prefix transform,0
Check weight,0
Check embeddings stuff,0
encoder and decoder should be same sizes,0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
This one is needed for various tranfroms,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
we need to check the model path + any tokenizer path,0
fast-forward if loaded from state,0
NOTE: `rnn.pack_padded_sequence` requires that a,0
"minibatch be sorted by decreasing order, which",0
requires reversing relative to typical sort keys,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
fields needs to have only keys that examples have as attrs,0
avoid infinite recursion when fields isn't defined,0
this is a hack: appears quicker to apply it here,1
than in the ParallelCorpusIterator,0
NOTE: moved to DatasetAdapter._process method in iterator.py,0
item = self.transform.apply(,0
"example, is_train=self.infinitely, corpus_name=self.cid)",0
empty example: skip,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"+1 for tgt side to keep coherent after ""bos"" padding,",0
"register ['N°_in_batch', 'tgt_id+1', 'src_id']",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
return vocab to dump with standard name,0
empty train_dataset_files so that vocab is only loaded from,0
"given paths in src_vocab_path, tgt_vocab_path",0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
NOTE: not support nfeats > 0 yet,0
-*- coding: utf-8 -*-,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
_check_save_model_path,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
import onmt.opts as opts,0
Set sharing strategy manually instead of default based on the OS.,0
"maybe prepare pretrained embeddings, if any",0
Load checkpoint if we resume from a previous training.,0
Report src and tgt vocab sizes,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"This does not work if we merge with the first loop, not sure why",1
Get the iterator to generate from,0
"Once training is done, we can terminate the producers",0
magic indices,0
result caching,0
fix length constraint,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
!/usr/bin/env python,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
max_tgt_in_batch = 0,0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
for debugging,0
TODO: maybe add dynamic part,1
Statistics,0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
Reorder states.,0
beam parameters,0
result caching,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
degenerate case,0
cache the features,0
mp queues don't work well between procs unless they're from a manager,0
each device has its own saver so that reconstructing is easier,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"Add in default model arguments, possibly added since training.",0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
for back compat when attention_dropout was not defined,0
Build embeddings.,0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Load checkpoint if we resume from a previous training.,0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
patch for fields that may be missing in old data/model,0
"Report src and tgt vocab sizes, including for features",0
Build model.,0
Build optimizer.,0
Build model saver,0
Move batch to specified device,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
The following options (bridge_extra_node to src_vocab) are used,0
for training with --encoder_type ggnn (Gated Graph Neural Network).,0
The ggnn uses src_vocab during training because the graph is built,0
using edge information which requires parsing the input sequence.,0
Attention options,0
Alignement options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"if you want to pass an existing vocab.pt file, pass it to",0
-src_vocab alone as it already contains tgt vocab.,0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Option most relevant to image input,0
Options for experimental source noising (BART style),0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use Tensorboard for visualization during training,0
Options most relevant to speech,0
Option most relevant to image input,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
Option most relevant to image input,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
incoming and outgoing edge embedding,0
Find vocab data for tree builting,0
Propogation Model,0
Initialize the bridge layer,0
Initialize graph using formatted input sequence,0
Number of flagged nodes defines node count for this sample,0
"(Nodes can have no flags on them, but must be in 'flags' list).",0
The total number of integers in the vocab should allow,0
for all features and edges to be defined.,0
Use first extra node as only source for decoder init,0
Average all nodes to get bridge input,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
The encoder hidden is  (layers*directions) x batch x dim.,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
why is the model_opt.__dict__ check necessary?,1
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return multi-head attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
noise_skip = batch.noise_skip,0
aeq(len(batch.noise_skip) == source.size(1)),0
source is [src_len x bs x feats],0
source might increase length so we need to resize the whole,0
tensor,0
remove useless pad,0
"def s(self, tokens):",0
prob = self.prob,0
r = torch.rand([len(tokens)]),0
mask = False,0
masked = [],0
"for i, tok in enumerate(tokens):",0
if tok.startswith(subword_prefix):,0
if r[i].item() <= prob:,0
masked.append(mask_tok),0
mask = True,0
else:,0
masked.append(tok),0
mask = False,0
else:,0
if mask:,0
pass,0
else:,0
masked.append(tok),0
return masked,0
"aeq(source.size(0), length)",0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
"aeq(source.size(0), length)",0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
fairseq/data/denoising_dataset.py,0
"print(""src size: "", source.size())",0
"print(""ws size: "", self.word_start_mask.size())",0
"print(""max: "", source.max())",0
assert source.max() < self.word_start_mask.size(0),0
assert source.min() >= 0,0
assert source.size() == is_word_start.size(),0
"aeq(source.eq(self.pad_idx).long().sum(), 0)",0
we manually add this hypothesis since it's required for the rest,0
of the function and kindof make sense,0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert (lengths > 0).all(),0
assert is_word_start[-1] == 0,0
TODO why?,1
assert source_length - 1 not in indices,0
"acts as a long length, so spans don't go over the end of doc",0
"keep index, but replace it with [MASK]",0
random ratio disabled,0
source[indices[mask_random]] = torch.randint(,0
"1, len(self.vocab), size=(mask_random.sum(),))",0
if self.mask_span_distribution is not None:,0
assert len(lengths.size()) == 1,0
assert lengths.size() == indices.size(),0
assert lengths.size() == indices.size(),0
mask_random = mask_random[uncompleted],0
delete token,0
"keep index, but replace it with [MASK]",0
random ratio disabled,0
source[indices[mask_random]] = torch.randint(,0
"1, len(self.vocab), size=(mask_random.sum(),))",0
else:,0
# A bit faster when all lengths are 1,0
while indices.size(0) > 0:,0
uncompleted = is_word_start[indices + 1] == 0,0
indices = indices[uncompleted] + 1,0
mask_random = mask_random[uncompleted],0
if self.replace_length != -1:,0
# delete token,0
to_keep[indices] = 0,0
else:,0
"# keep index, but replace it with [MASK]",0
source[indices] = self.mask_idx,0
source[indices[mask_random]] = torch.randint(,0
"1, len(self.vocab), size=(mask_random.sum(),))",0
assert source_length - 1 not in indices,0
"aeq(source.eq(self.pad_idx).long().sum(), 0)",0
random ratio disabled,0
num_random = int(math.ceil(n * self.random_ratio)),0
result[noise_indices[:num_random]] = torch.randint(,0
"low=1, high=len(self.vocab), size=(num_random,))",0
assert (result >= 0).all(),0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
-*- coding: utf-8 -*-,0
tests pad and numericalize integration,0
tests pad and numericalize integration,0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to audio data,0
file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names),0
it's ok if non-audio files co-exist with audio files in the data dir,0
"dividing gets the noise in [-1, 1]",0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to image data,0
file to hold image paths relative to _IMG_DATA_DIR (i.e. file names),0
it's ok if non-image files co-exist with image files in the data dir,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
"when reasonable, set audio_enc_pooling to 2",0
Need lengths >= audio_enc_pooling**n_layers.,0
"That condition is unrealistic for large n_layers,",0
so leave audio_enc_pooling at 1.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are fixed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
Test image preprocessing,0
Test audio preprocessing,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
T: could be 1 in the case of stepwise decoding or tgt_len,0
BoolTensor was introduced in pytorch 1.2,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
TODO: Find a better way to check for sparse gradients.,1
we use here a FusedAdam() copy of an old Apex repo,0
In this case use the old FusedAdam with FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
"unscaled optimizer's gradients (already done therefore skip),",0
skips optimizer.step() if gradients contain infs/NaNs.,0
Updates the scale for next iteration.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)",0
"align_idx should be a Tensor in size([N, 3]), N is total number",0
"of align src-tgt pair in current batch, each as",0
"['sent_N°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)",0
NOTE: tgt-src ref alignement that in range_ of shard,0
(coherent with batch.tgt),0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
this check is here because audio allows the encoder and decoder to,0
"be different sizes, but other model types do not yet",0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
we need to check the model path + any tokenizer path,0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
fields needs to have only keys that examples have as attrs,0
avoid infinite recursion when fields isn't defined,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"+1 for tgt side to keep coherent after ""bos"" padding,",0
"register ['N°_in_batch', 'tgt_id+1', 'src_id']",0
"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"Dict[str, List[Tuple[str, Field]]]",0
doesn't change structure - don't return early.,0
"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"if tgt isn't using TextMultiField, then no text field is.",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
return vocab to dump with standard name,0
empty train_dataset_files so that vocab is only loaded from,0
"given paths in src_vocab_path, tgt_vocab_path",0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
fast-forward if loaded from state,0
NOTE: `rnn.pack_padded_sequence` requires that a,0
"minibatch be sorted by decreasing order, which",0
requires reversing relative to typical sort keys,0
self.weights = opt.data_weights,0
Temporarily load one shard to retrieve sort_key for data_type,0
"NOTE: This is causing some issues for consumer/producer,",0
as we may still have some of those examples in some queue,0
cur_dataset.examples = None,0
gc.collect(),0
del cur_dataset,0
gc.collect(),0
Cycle through the shards indefinitely.,0
"When the dataset is not repeated, we might need to ensure that",0
the number of returned batches is the multiple of a given value.,0
This is important for multi GPU training to ensure that all,0
workers have the same number of batches to process.,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
-*- coding: utf-8 -*-,0
imports of datatype-specific dependencies,0
torchaudio loading options recently changed. It's probably,0
straightforward to rewrite the audio handling to make use of,0
"up-to-date torchaudio, but in the meantime there is a legacy",0
method which uses the old defaults,0
STFT,0
-*- coding: utf-8 -*-,0
domain specific dependencies,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
-*- coding: utf-8 -*-,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
create one counter per shard,0
"every corpus has shards, no new one",0
patch corpus_id,0
!/usr/bin/env python,0
!/usr/bin/env python,0
Fix CPU tensor sharing strategy,0
Load checkpoint if we resume from a previous training.,0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
patch for fields that may be missing in old data/model,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
generator_to_serve = iter(generator_to_serve),0
hack to dodge unpicklable `dict_keys`,1
"propagate exception to parent process, keeping original traceback",0
magic indices,0
result caching,0
fix length constraint,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"pickups: Tensor where specified index were set to 1, others 0",0
"dropdowns: opposite of pickups, 1 for those shouldn't pick",0
Minus dropdowns to log_probs making probabilities of,0
unspecified index close to 0,0
"prediction step have surpass length of given target_prefix,",0
no need to further change this attr,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
maybe fix some prediction at this step by modifying log_probs,0
"shape: (sum(~ self.is_finished), 1)",0
!/usr/bin/env python,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
max_tgt_in_batch = 0,0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
for debugging,0
corpus_id field is useless here,0
Statistics,0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
Reorder states.,0
beam parameters,0
result caching,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
maybe fix some prediction at this step by modifying log_probs,0
Flatten probs into a list of possibilities.,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Pick up candidate token by curr_scores,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
load can be called multiple times: modify copy,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
degenerate case,0
cache the features,0
mp queues don't work well between procs unless they're from a manager,0
each device has its own saver so that reconstructing is easier,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"Add in default model arguments, possibly added since training.",0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
for back compat when attention_dropout was not defined,0
Build embeddings.,0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Load checkpoint if we resume from a previous training.,0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
"Report src and tgt vocab sizes, including for features",0
Build model.,0
Build optimizer.,0
Build model saver,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Alignement options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"if you want to pass an existing vocab.pt file, pass it to",0
-src_vocab alone as it already contains tgt vocab.,0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Option most relevant to image input,0
Options for experimental source noising (BART style),0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use Tensorboard for visualization during training,0
Options most relevant to speech,0
Option most relevant to image input,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
Option most relevant to image input,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
The encoder hidden is  (layers*directions) x batch x dim.,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
why is the model_opt.__dict__ check necessary?,1
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return multi-head attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
noise_skip = batch.noise_skip,0
aeq(len(batch.noise_skip) == source.size(1)),0
source is [src_len x bs x feats],0
source might increase length so we need to resize the whole,0
tensor,0
remove useless pad,0
"def s(self, tokens):",0
prob = self.prob,0
r = torch.rand([len(tokens)]),0
mask = False,0
masked = [],0
"for i, tok in enumerate(tokens):",0
if tok.startswith(subword_prefix):,0
if r[i].item() <= prob:,0
masked.append(mask_tok),0
mask = True,0
else:,0
masked.append(tok),0
mask = False,0
else:,0
if mask:,0
pass,0
else:,0
masked.append(tok),0
return masked,0
"aeq(source.size(0), length)",0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
"aeq(source.size(0), length)",0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
fairseq/data/denoising_dataset.py,0
"print(""src size: "", source.size())",0
"print(""ws size: "", self.word_start_mask.size())",0
"print(""max: "", source.max())",0
assert source.max() < self.word_start_mask.size(0),0
assert source.min() >= 0,0
assert source.size() == is_word_start.size(),0
"aeq(source.eq(self.pad_idx).long().sum(), 0)",0
we manually add this hypothesis since it's required for the rest,0
of the function and kindof make sense,0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert (lengths > 0).all(),0
assert is_word_start[-1] == 0,0
TODO why?,1
assert source_length - 1 not in indices,0
"acts as a long length, so spans don't go over the end of doc",0
"keep index, but replace it with [MASK]",0
random ratio disabled,0
source[indices[mask_random]] = torch.randint(,0
"1, len(self.vocab), size=(mask_random.sum(),))",0
if self.mask_span_distribution is not None:,0
assert len(lengths.size()) == 1,0
assert lengths.size() == indices.size(),0
assert lengths.size() == indices.size(),0
mask_random = mask_random[uncompleted],0
delete token,0
"keep index, but replace it with [MASK]",0
random ratio disabled,0
source[indices[mask_random]] = torch.randint(,0
"1, len(self.vocab), size=(mask_random.sum(),))",0
else:,0
# A bit faster when all lengths are 1,0
while indices.size(0) > 0:,0
uncompleted = is_word_start[indices + 1] == 0,0
indices = indices[uncompleted] + 1,0
mask_random = mask_random[uncompleted],0
if self.replace_length != -1:,0
# delete token,0
to_keep[indices] = 0,0
else:,0
"# keep index, but replace it with [MASK]",0
source[indices] = self.mask_idx,0
source[indices[mask_random]] = torch.randint(,0
"1, len(self.vocab), size=(mask_random.sum(),))",0
assert source_length - 1 not in indices,0
"aeq(source.eq(self.pad_idx).long().sum(), 0)",0
random ratio disabled,0
num_random = int(math.ceil(n * self.random_ratio)),0
result[noise_indices[:num_random]] = torch.randint(,0
"low=1, high=len(self.vocab), size=(num_random,))",0
assert (result >= 0).all(),0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
-*- coding: utf-8 -*-,0
tests pad and numericalize integration,0
tests pad and numericalize integration,0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to audio data,0
file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names),0
it's ok if non-audio files co-exist with audio files in the data dir,0
"dividing gets the noise in [-1, 1]",0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to image data,0
file to hold image paths relative to _IMG_DATA_DIR (i.e. file names),0
it's ok if non-image files co-exist with image files in the data dir,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
"when reasonable, set audio_enc_pooling to 2",0
Need lengths >= audio_enc_pooling**n_layers.,0
"That condition is unrealistic for large n_layers,",0
so leave audio_enc_pooling at 1.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are fixed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
Test image preprocessing,0
Test audio preprocessing,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
T: could be 1 in the case of stepwise decoding or tgt_len,0
BoolTensor was introduced in pytorch 1.2,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
TODO: Find a better way to check for sparse gradients.,1
we use here a FusedAdam() copy of an old Apex repo,0
In this case use the new AMP API from apex,0
In this case use the old FusedAdam with FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)",0
"align_idx should be a Tensor in size([N, 3]), N is total number",0
"of align src-tgt pair in current batch, each as",0
"['sent_N°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)",0
NOTE: tgt-src ref alignement that in range_ of shard,0
(coherent with batch.tgt),0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
this check is here because audio allows the encoder and decoder to,0
"be different sizes, but other model types do not yet",0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
we need to check the model path + any tokenizer path,0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
fields needs to have only keys that examples have as attrs,0
avoid infinite recursion when fields isn't defined,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"+1 for tgt side to keep coherent after ""bos"" padding,",0
"register ['N°_in_batch', 'tgt_id+1', 'src_id']",0
"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"Dict[str, List[Tuple[str, Field]]]",0
doesn't change structure - don't return early.,0
"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"if tgt isn't using TextMultiField, then no text field is.",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
return vocab to dump with standard name,0
empty train_dataset_files so that vocab is only loaded from,0
"given paths in src_vocab_path, tgt_vocab_path",0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
fast-forward if loaded from state,0
NOTE: `rnn.pack_padded_sequence` requires that a,0
"minibatch be sorted by decreasing order, which",0
requires reversing relative to typical sort keys,0
Temporarily load one shard to retrieve sort_key for data_type,0
"NOTE: This is causing some issues for consumer/producer,",0
as we may still have some of those examples in some queue,0
cur_dataset.examples = None,0
gc.collect(),0
del cur_dataset,0
gc.collect(),0
Cycle through the shards indefinitely.,0
"When the dataset is not repeated, we might need to ensure that",0
the number of returned batches is the multiple of a given value.,0
This is important for multi GPU training to ensure that all,0
workers have the same number of batches to process.,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
-*- coding: utf-8 -*-,0
imports of datatype-specific dependencies,0
torchaudio loading options recently changed. It's probably,0
straightforward to rewrite the audio handling to make use of,0
"up-to-date torchaudio, but in the meantime there is a legacy",0
method which uses the old defaults,0
STFT,0
-*- coding: utf-8 -*-,0
domain specific dependencies,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
-*- coding: utf-8 -*-,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
create one counter per shard,0
"every corpus has shards, no new one",0
!/usr/bin/env python,0
!/usr/bin/env python,0
Load checkpoint if we resume from a previous training.,0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
generator_to_serve = iter(generator_to_serve),0
hack to dodge unpicklable `dict_keys`,1
"propagate exception to parent process, keeping original traceback",0
magic indices,0
result caching,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"shape: (sum(~ self.is_finished), 1)",0
!/usr/bin/env python,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
max_tgt_in_batch = 0,0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
for debugging,0
corpus_id field is useless here,0
Statistics,0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
Reorder states.,0
beam parameters,0
result caching,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
load can be called multiple times: modify copy,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
degenerate case,0
cache the features,0
mp queues don't work well between procs unless they're from a manager,0
each device has its own saver so that reconstructing is easier,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"Add in default model arguments, possibly added since training.",0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
for back compat when attention_dropout was not defined,0
Build embeddings.,0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Load checkpoint if we resume from a previous training.,0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
"Report src and tgt vocab sizes, including for features",0
Build model.,0
Build optimizer.,0
Build model saver,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Alignement options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"if you want to pass an existing vocab.pt file, pass it to",0
-src_vocab alone as it already contains tgt vocab.,0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Option most relevant to image input,0
Options for experimental source noising (BART style),0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use Tensorboard for visualization during training,0
Options most relevant to speech,0
Option most relevant to image input,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
Option most relevant to image input,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
The encoder hidden is  (layers*directions) x batch x dim.,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
why is the model_opt.__dict__ check necessary?,1
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return multi-head attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
noise_skip = batch.noise_skip,0
aeq(len(batch.noise_skip) == source.size(1)),0
source is [src_len x bs x feats],0
source might increase length so we need to resize the whole,0
tensor,0
remove useless pad,0
"def s(self, tokens):",0
prob = self.prob,0
r = torch.rand([len(tokens)]),0
mask = False,0
masked = [],0
"for i, tok in enumerate(tokens):",0
if tok.startswith(subword_prefix):,0
if r[i].item() <= prob:,0
masked.append(mask_tok),0
mask = True,0
else:,0
masked.append(tok),0
mask = False,0
else:,0
if mask:,0
pass,0
else:,0
masked.append(tok),0
return masked,0
"aeq(source.size(0), length)",0
Pretend it ends with a full stop so last span is a sentence,0
"Tokens that are full stops, where the previous token is not",0
"aeq(source.size(0), length)",0
-1: keep everything (i.e. 1 mask per token),0
0: replace everything (i.e. no mask),0
1: 1 mask per span,0
fairseq/data/denoising_dataset.py,0
"print(""src size: "", source.size())",0
"print(""ws size: "", self.word_start_mask.size())",0
"print(""max: "", source.max())",0
assert source.max() < self.word_start_mask.size(0),0
assert source.min() >= 0,0
assert source.size() == is_word_start.size(),0
"aeq(source.eq(self.pad_idx).long().sum(), 0)",0
we manually add this hypothesis since it's required for the rest,0
of the function and kindof make sense,0
Make sure we have enough to mask,0
Trim to masking budget,0
Handle 0-length mask (inserts) separately,0
assert (lengths > 0).all(),0
assert is_word_start[-1] == 0,0
TODO why?,1
assert source_length - 1 not in indices,0
"acts as a long length, so spans don't go over the end of doc",0
"keep index, but replace it with [MASK]",0
random ratio disabled,0
source[indices[mask_random]] = torch.randint(,0
"1, len(self.vocab), size=(mask_random.sum(),))",0
if self.mask_span_distribution is not None:,0
assert len(lengths.size()) == 1,0
assert lengths.size() == indices.size(),0
assert lengths.size() == indices.size(),0
mask_random = mask_random[uncompleted],0
delete token,0
"keep index, but replace it with [MASK]",0
random ratio disabled,0
source[indices[mask_random]] = torch.randint(,0
"1, len(self.vocab), size=(mask_random.sum(),))",0
else:,0
# A bit faster when all lengths are 1,0
while indices.size(0) > 0:,0
uncompleted = is_word_start[indices + 1] == 0,0
indices = indices[uncompleted] + 1,0
mask_random = mask_random[uncompleted],0
if self.replace_length != -1:,0
# delete token,0
to_keep[indices] = 0,0
else:,0
"# keep index, but replace it with [MASK]",0
source[indices] = self.mask_idx,0
source[indices[mask_random]] = torch.randint(,0
"1, len(self.vocab), size=(mask_random.sum(),))",0
assert source_length - 1 not in indices,0
"aeq(source.eq(self.pad_idx).long().sum(), 0)",0
random ratio disabled,0
num_random = int(math.ceil(n * self.random_ratio)),0
result[noise_indices[:num_random]] = torch.randint(,0
"low=1, high=len(self.vocab), size=(num_random,))",0
assert (result >= 0).all(),0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
-*- coding: utf-8 -*-,0
tests pad and numericalize integration,0
tests pad and numericalize integration,0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to audio data,0
file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names),0
it's ok if non-audio files co-exist with audio files in the data dir,0
"dividing gets the noise in [-1, 1]",0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to image data,0
file to hold image paths relative to _IMG_DATA_DIR (i.e. file names),0
it's ok if non-image files co-exist with image files in the data dir,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
"when reasonable, set audio_enc_pooling to 2",0
Need lengths >= audio_enc_pooling**n_layers.,0
"That condition is unrealistic for large n_layers,",0
so leave audio_enc_pooling at 1.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are fixed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
Test image preprocessing,0
Test audio preprocessing,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
T: could be 1 in the case of stepwise decoding or tgt_len,0
BoolTensor was introduced in pytorch 1.2,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
TODO: Find a better way to check for sparse gradients.,1
we use here a FusedAdam() copy of an old Apex repo,0
In this case use the new AMP API from apex,0
In this case use the old FusedAdam with FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)",0
"align_idx should be a Tensor in size([N, 3]), N is total number",0
"of align src-tgt pair in current batch, each as",0
"['sent_N°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)",0
NOTE: tgt-src ref alignement that in range_ of shard,0
(coherent with batch.tgt),0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
this check is here because audio allows the encoder and decoder to,0
"be different sizes, but other model types do not yet",0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
No alignment if not exist valid tgt token,0
get valid alignment (sub-matrix from full paded aligment matrix),0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
we need to check the model path + any tokenizer path,0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
fields needs to have only keys that examples have as attrs,0
avoid infinite recursion when fields isn't defined,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"+1 for tgt side to keep coherent after ""bos"" padding,",0
"register ['N°_in_batch', 'tgt_id+1', 'src_id']",0
"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"Dict[str, List[Tuple[str, Field]]]",0
doesn't change structure - don't return early.,0
"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"if tgt isn't using TextMultiField, then no text field is.",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
return vocab to dump with standard name,0
empty train_dataset_files so that vocab is only loaded from,0
"given paths in src_vocab_path, tgt_vocab_path",0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
fast-forward if loaded from state,0
NOTE: `rnn.pack_padded_sequence` requires that a,0
"minibatch be sorted by decreasing order, which",0
requires reversing relative to typical sort keys,0
Temporarily load one shard to retrieve sort_key for data_type,0
"NOTE: This is causing some issues for consumer/producer,",0
as we may still have some of those examples in some queue,0
cur_dataset.examples = None,0
gc.collect(),0
del cur_dataset,0
gc.collect(),0
Cycle through the shards indefinitely.,0
"When the dataset is not repeated, we might need to ensure that",0
the number of returned batches is the multiple of a given value.,0
This is important for multi GPU training to ensure that all,0
workers have the same number of batches to process.,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
-*- coding: utf-8 -*-,0
imports of datatype-specific dependencies,0
torchaudio loading options recently changed. It's probably,0
straightforward to rewrite the audio handling to make use of,0
"up-to-date torchaudio, but in the meantime there is a legacy",0
method which uses the old defaults,0
STFT,0
-*- coding: utf-8 -*-,0
domain specific dependencies,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
-*- coding: utf-8 -*-,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
create one counter per shard,0
"every corpus has shards, no new one",0
!/usr/bin/env python,0
!/usr/bin/env python,0
Load checkpoint if we resume from a previous training.,0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
generator_to_serve = iter(generator_to_serve),0
hack to dodge unpicklable `dict_keys`,1
"propagate exception to parent process, keeping original traceback",0
magic indices,0
result caching,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"shape: (sum(~ self.is_finished), 1)",0
!/usr/bin/env python,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
max_tgt_in_batch = 0,0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
for debugging,0
corpus_id field is useless here,0
Statistics,0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
Reorder states.,0
beam parameters,0
result caching,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
perform a first request to initialize everything,0
backwards compatibility for confs,0
load can be called multiple times: modify copy,0
every segment becomes a dict for flexibility purposes,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
degenerate case,0
cache the features,0
mp queues don't work well between procs unless they're from a manager,0
each device has its own saver so that reconstructing is easier,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"Add in default model arguments, possibly added since training.",0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
for back compat when attention_dropout was not defined,0
Build embeddings.,0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Load checkpoint if we resume from a previous training.,0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
"Report src and tgt vocab sizes, including for features",0
Build model.,0
Build optimizer.,0
Build model saver,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Alignement options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"if you want to pass an existing vocab.pt file, pass it to",0
-src_vocab alone as it already contains tgt vocab.,0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Option most relevant to image input,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use Tensorboard for visualization during training,0
Options most relevant to speech,0
Option most relevant to image input,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
Option most relevant to image input,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
The encoder hidden is  (layers*directions) x batch x dim.,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
why is the model_opt.__dict__ check necessary?,1
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return multi-head attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
-*- coding: utf-8 -*-,0
tests pad and numericalize integration,0
tests pad and numericalize integration,0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to audio data,0
file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names),0
it's ok if non-audio files co-exist with audio files in the data dir,0
"dividing gets the noise in [-1, 1]",0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to image data,0
file to hold image paths relative to _IMG_DATA_DIR (i.e. file names),0
it's ok if non-image files co-exist with image files in the data dir,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
"when reasonable, set audio_enc_pooling to 2",0
Need lengths >= audio_enc_pooling**n_layers.,0
"That condition is unrealistic for large n_layers,",0
so leave audio_enc_pooling at 1.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are fixed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
Test image preprocessing,0
Test audio preprocessing,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
T: could be 1 in the case of stepwise decoding or tgt_len,0
BoolTensor was introduced in pytorch 1.2,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
TODO: Find a better way to check for sparse gradients.,1
we use here a FusedAdam() copy of an old Apex repo,0
In this case use the new AMP API from apex,0
In this case use the old FusedAdam with FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)",0
"align_idx should be a Tensor in size([N, 3]), N is total number",0
"of align src-tgt pair in current batch, each as",0
"['sent_N°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)",0
NOTE: tgt-src ref alignement that in range_ of shard,0
(coherent with batch.tgt),0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
this check is here because audio allows the encoder and decoder to,0
"be different sizes, but other model types do not yet",0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
get valid alignment (sub-matrix from full paded aligment matrix),0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
we need to check the model path + any tokenizer path,0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
fields needs to have only keys that examples have as attrs,0
avoid infinite recursion when fields isn't defined,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"+1 for tgt side to keep coherent after ""bos"" padding,",0
"register ['N°_in_batch', 'tgt_id+1', 'src_id']",0
"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"Dict[str, List[Tuple[str, Field]]]",0
doesn't change structure - don't return early.,0
"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"if tgt isn't using TextMultiField, then no text field is.",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
return vocab to dump with standard name,0
empty train_dataset_files so that vocab is only loaded from,0
"given paths in src_vocab_path, tgt_vocab_path",0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
fast-forward if loaded from state,0
NOTE: `rnn.pack_padded_sequence` requires that a,0
"minibatch be sorted by decreasing order, which",0
requires reversing relative to typical sort keys,0
Temporarily load one shard to retrieve sort_key for data_type,0
"NOTE: This is causing some issues for consumer/producer,",0
as we may still have some of those examples in some queue,0
cur_dataset.examples = None,0
gc.collect(),0
del cur_dataset,0
gc.collect(),0
Cycle through the shards indefinitely.,0
"When the dataset is not repeated, we might need to ensure that",0
the number of returned batches is the multiple of a given value.,0
This is important for multi GPU training to ensure that all,0
workers have the same number of batches to process.,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
-*- coding: utf-8 -*-,0
imports of datatype-specific dependencies,0
torchaudio loading options recently changed. It's probably,0
straightforward to rewrite the audio handling to make use of,0
"up-to-date torchaudio, but in the meantime there is a legacy",0
method which uses the old defaults,0
STFT,0
-*- coding: utf-8 -*-,0
domain specific dependencies,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
-*- coding: utf-8 -*-,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
create one counter per shard,0
"every corpus has shards, no new one",0
!/usr/bin/env python,0
!/usr/bin/env python,0
Load checkpoint if we resume from a previous training.,0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
generator_to_serve = iter(generator_to_serve),0
hack to dodge unpicklable `dict_keys`,1
"propagate exception to parent process, keeping original traceback",0
magic indices,0
result caching,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"shape: (sum(~ self.is_finished), 1)",0
!/usr/bin/env python,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
max_tgt_in_batch = 0,0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
for debugging,0
Statistics,0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
Reorder states.,0
beam parameters,0
result caching,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
backwards compatibility for confs,0
load can be called multiple times: modify copy,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
degenerate case,0
cache the features,0
mp queues don't work well between procs unless they're from a manager,0
each device has its own saver so that reconstructing is easier,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"Add in default model arguments, possibly added since training.",0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
for back compat when attention_dropout was not defined,0
Build embeddings.,0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Load checkpoint if we resume from a previous training.,0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
"Report src and tgt vocab sizes, including for features",0
Build model.,0
Build optimizer.,0
Build model saver,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Alignement options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"if you want to pass an existing vocab.pt file, pass it to",0
-src_vocab alone as it already contains tgt vocab.,0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Option most relevant to image input,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use Tensorboard for visualization during training,0
Options most relevant to speech,0
Option most relevant to image input,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
Option most relevant to image input,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
The encoder hidden is  (layers*directions) x batch x dim.,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
why is the model_opt.__dict__ check necessary?,1
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return multi-head attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
-*- coding: utf-8 -*-,0
tests pad and numericalize integration,0
tests pad and numericalize integration,0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to audio data,0
file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names),0
it's ok if non-audio files co-exist with audio files in the data dir,0
"dividing gets the noise in [-1, 1]",0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to image data,0
file to hold image paths relative to _IMG_DATA_DIR (i.e. file names),0
it's ok if non-image files co-exist with image files in the data dir,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
"when reasonable, set audio_enc_pooling to 2",0
Need lengths >= audio_enc_pooling**n_layers.,0
"That condition is unrealistic for large n_layers,",0
so leave audio_enc_pooling at 1.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are fixed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
Test image preprocessing,0
Test audio preprocessing,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
T: could be 1 in the case of stepwise decoding or tgt_len,0
BoolTensor was introduced in pytorch 1.2,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
TODO: Find a better way to check for sparse gradients.,1
we use here a FusedAdam() copy of an old Apex repo,0
In this case use the new AMP API from apex,0
In this case use the old FusedAdam with FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)",0
"align_idx should be a Tensor in size([N, 3]), N is total number",0
"of align src-tgt pair in current batch, each as",0
"['sent_N°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)",0
NOTE: tgt-src ref alignement that in range_ of shard,0
(coherent with batch.tgt),0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
this check is here because audio allows the encoder and decoder to,0
"be different sizes, but other model types do not yet",0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
get valid alignment (sub-matrix from full paded aligment matrix),0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
we need to check the model path + any tokenizer path,0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
fields needs to have only keys that examples have as attrs,0
avoid infinite recursion when fields isn't defined,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"+1 for tgt side to keep coherent after ""bos"" padding,",0
"register ['N°_in_batch', 'tgt_id+1', 'src_id']",0
"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"Dict[str, List[Tuple[str, Field]]]",0
doesn't change structure - don't return early.,0
"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"if tgt isn't using TextMultiField, then no text field is.",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
return vocab to dump with standard name,0
empty train_dataset_files so that vocab is only loaded from,0
"given paths in src_vocab_path, tgt_vocab_path",0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
fast-forward if loaded from state,0
NOTE: `rnn.pack_padded_sequence` requires that a,0
"minibatch be sorted by decreasing order, which",0
requires reversing relative to typical sort keys,0
Temporarily load one shard to retrieve sort_key for data_type,0
"NOTE: This is causing some issues for consumer/producer,",0
as we may still have some of those examples in some queue,0
cur_dataset.examples = None,0
gc.collect(),0
del cur_dataset,0
gc.collect(),0
Cycle through the shards indefinitely.,0
"When the dataset is not repeated, we might need to ensure that",0
the number of returned batches is the multiple of a given value.,0
This is important for multi GPU training to ensure that all,0
workers have the same number of batches to process.,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
-*- coding: utf-8 -*-,0
imports of datatype-specific dependencies,0
torchaudio loading options recently changed. It's probably,0
straightforward to rewrite the audio handling to make use of,0
"up-to-date torchaudio, but in the meantime there is a legacy",0
method which uses the old defaults,0
STFT,0
-*- coding: utf-8 -*-,0
domain specific dependencies,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
-*- coding: utf-8 -*-,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
create one counter per shard,0
"every corpus has shards, no new one",0
!/usr/bin/env python,0
!/usr/bin/env python,0
Load checkpoint if we resume from a previous training.,0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
generator_to_serve = iter(generator_to_serve),0
hack to dodge unpicklable `dict_keys`,1
"propagate exception to parent process, keeping original traceback",0
magic indices,0
result caching,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"shape: (sum(~ self.is_finished), 1)",0
!/usr/bin/env python,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
max_tgt_in_batch = 0,0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
for debugging,0
Statistics,0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
Reorder states.,0
beam parameters,0
result caching,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
backwards compatibility for confs,0
load can be called multiple times: modify copy,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
degenerate case,0
cache the features,0
mp queues don't work well between procs unless they're from a manager,0
each device has its own saver so that reconstructing is easier,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"Add in default model arguments, possibly added since training.",0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
for back compat when attention_dropout was not defined,0
Build embeddings.,0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Load checkpoint if we resume from a previous training.,0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
"Report src and tgt vocab sizes, including for features",0
Build model.,0
Build optimizer.,0
Build model saver,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Alignement options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"if you want to pass an existing vocab.pt file, pass it to",0
-src_vocab alone as it already contains tgt vocab.,0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Option most relevant to image input,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use Tensorboard for visualization during training,0
Options most relevant to speech,0
Option most relevant to image input,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
Option most relevant to image input,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
swap model params w/ moving average,0
(and keep the original parameters),0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
The encoder hidden is  (layers*directions) x batch x dim.,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
why is the model_opt.__dict__ check necessary?,1
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return multi-head attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
-*- coding: utf-8 -*-,0
tests pad and numericalize integration,0
tests pad and numericalize integration,0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to audio data,0
file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names),0
it's ok if non-audio files co-exist with audio files in the data dir,0
"dividing gets the noise in [-1, 1]",0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to image data,0
file to hold image paths relative to _IMG_DATA_DIR (i.e. file names),0
it's ok if non-image files co-exist with image files in the data dir,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
"before repeat, scores are either 0 or -inf",0
"on repeat, `repeat_idx` score is BLOCKED_SCORE",0
"(but it's still the best score, thus we have",0
"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]",0
repetitions keeps maximizing score,0
"index 0 has been blocked, so repeating=>+0.0 score",0
other indexes are -inf so repeating=>BLOCKED_SCORE,0
which is higher,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"inp_lens is tiled in initialize, reassign to make attn match",0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
"when reasonable, set audio_enc_pooling to 2",0
Need lengths >= audio_enc_pooling**n_layers.,0
"That condition is unrealistic for large n_layers,",0
so leave audio_enc_pooling at 1.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are fixed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
Test image preprocessing,0
Test audio preprocessing,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"return _, (B, Q_len, K_len)",0
"layer average attention across heads, get ``(B, Q, K)``",0
"Case 1: no full_context, no align heads -> layer avg baseline",0
"Case 2: no full_context, 1 align heads -> guided align",0
"Case 3: full_context, 1 align heads -> full cte guided align",0
TODO: change 1 to T as T could be 1 or tgt_len,1
BoolTensor was introduced in pytorch 1.2,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg",0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
TODO: Find a better way to check for sparse gradients.,1
we use here a FusedAdam() copy of an old Apex repo,0
In this case use the new AMP API from apex,0
In this case use the old FusedAdam with FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)",0
"align_idx should be a Tensor in size([N, 3]), N is total number",0
"of align src-tgt pair in current batch, each as",0
"['sent_N°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)",0
NOTE: tgt-src ref alignement that in range_ of shard,0
(coherent with batch.tgt),0
"align_head contains value in [0, 1) presenting attn prob,",0
0 was resulted by the context attention src_pad_mask,0
"So, the correspand position in ref_align should also be 0",0
"Therefore, clip align_head to > 1e-18 should be bias free.",0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
this check is here because audio allows the encoder and decoder to,0
"be different sizes, but other model types do not yet",0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
threshold on 1 to avoid div by 0,0
treat alignment matrix one by one as each have different lengths,0
get valid alignment (sub-matrix from full paded aligment matrix),0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
we need to check the model path + any tokenizer path,0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
fields needs to have only keys that examples have as attrs,0
avoid infinite recursion when fields isn't defined,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"+1 for tgt side to keep coherent after ""bos"" padding,",0
"register ['N°_in_batch', 'tgt_id+1', 'src_id']",0
"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"Dict[str, List[Tuple[str, Field]]]",0
doesn't change structure - don't return early.,0
"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"if tgt isn't using TextMultiField, then no text field is.",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
return vocab to dump with standard name,0
empty train_dataset_files so that vocab is only loaded from,0
"given paths in src_vocab_path, tgt_vocab_path",0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
fast-forward if loaded from state,0
NOTE: `rnn.pack_padded_sequence` requires that a,0
"minibatch be sorted by decreasing order, which",0
requires reversing relative to typical sort keys,0
Temporarily load one shard to retrieve sort_key for data_type,0
"NOTE: This is causing some issues for consumer/producer,",0
as we may still have some of those examples in some queue,0
cur_dataset.examples = None,0
gc.collect(),0
del cur_dataset,0
gc.collect(),0
Cycle through the shards indefinitely.,0
"When the dataset is not repeated, we might need to ensure that",0
the number of returned batches is the multiple of a given value.,0
This is important for multi GPU training to ensure that all,0
workers have the same number of batches to process.,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
-*- coding: utf-8 -*-,0
imports of datatype-specific dependencies,0
torchaudio loading options recently changed. It's probably,0
straightforward to rewrite the audio handling to make use of,0
"up-to-date torchaudio, but in the meantime there is a legacy",0
method which uses the old defaults,0
STFT,0
-*- coding: utf-8 -*-,0
domain specific dependencies,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
-*- coding: utf-8 -*-,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
create one counter per shard,0
"every corpus has shards, no new one",0
!/usr/bin/env python,0
!/usr/bin/env python,0
Load checkpoint if we resume from a previous training.,0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
generator_to_serve = iter(generator_to_serve),0
hack to dodge unpicklable `dict_keys`,1
"propagate exception to parent process, keeping original traceback",0
magic indices,0
result caching,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
we don't block nothing if the user doesn't want it,0
we can't block nothing beam's too short,0
we check paths one by one,0
we don't forbid nothing if the user doesn't want it,0
we can't forbid nothing if beam's too short,0
Reordering forbidden_tokens following beam selection,0
We rebuild a dict to ensure we get the value and not the pointer,0
Grabing the newly selected tokens and associated ngram,0
skip the blocking if any token in current_ngram is excluded,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"shape: (sum(~ self.is_finished), 1)",0
!/usr/bin/env python,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
max_tgt_in_batch = 0,0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
for debugging,0
Statistics,0
(0) add BOS and padding to tgt prediction,0
(1) Encoder forward.,0
(2) Repeat src objects `n_best` times.,0
"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``",0
"(3) Init decoder with n_best src,",0
"reshape tgt to ``(len, batch * n_best, nfeat)``",0
masked_select,0
get aligned src id for each prediction's valid tgt tokens,0
TODO: support these blacklisted features,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) prep decode_strategy. Possibly repeat src objects.,0
(3) Begin decoding step by step:,0
Reorder states.,0
beam parameters,0
result caching,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
"""global state"" of the old beam",0
buffers for the topk scores and 'backpointer',0
for testing,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Avoid any direction that would repeat unwanted ngrams,0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
backwards compatibility for confs,0
load can be called multiple times: modify copy,0
NOTE: translator returns lists of `n_best` list,0
build back results with empty texts,0
output contain alignment,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
degenerate case,0
cache the features,0
mp queues don't work well between procs unless they're from a manager,0
each device has its own saver so that reconstructing is easier,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"Add in default model arguments, possibly added since training.",0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
for back compat when attention_dropout was not defined,0
Build embeddings.,0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Load checkpoint if we resume from a previous training.,0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
"Report src and tgt vocab sizes, including for features",0
Build model.,0
Build optimizer.,0
Build model saver,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"if you want to pass an existing vocab.pt file, pass it to",0
-src_vocab alone as it already contains tgt vocab.,0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Option most relevant to image input,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use Tensorboard for visualization during training,0
Options most relevant to speech,0
Option most relevant to image input,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
Option most relevant to image input,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
The encoder hidden is  (layers*directions) x batch x dim.,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
why is the model_opt.__dict__ check necessary?,1
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return one attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
first beam finished had length beam.min_length,0
first beam finished was 0,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
"[2, 5, 3, 6, 0], so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010]",0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0], so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
predict repeat_idx over and over again,0
"batch 0 and 7 will repeat, the rest will advance",0
predict the same thing in batch 0 and 7 every i,0
push around what the other batches predict,0
now batch 0 and 7 die,0
"batch 0 will repeat excluded idx, batch 1 will repeat",0
now batch 1 dies,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
-*- coding: utf-8 -*-,0
tests pad and numericalize integration,0
tests pad and numericalize integration,0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to audio data,0
file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names),0
it's ok if non-audio files co-exist with audio files in the data dir,0
"dividing gets the noise in [-1, 1]",0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to image data,0
file to hold image paths relative to _IMG_DATA_DIR (i.e. file names),0
it's ok if non-image files co-exist with image files in the data dir,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
"when reasonable, set audio_enc_pooling to 2",0
Need lengths >= audio_enc_pooling**n_layers.,0
"That condition is unrealistic for large n_layers,",0
so leave audio_enc_pooling at 1.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are fixed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
Test image preprocessing,0
Test audio preprocessing,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
BoolTensor was introduced in pytorch 1.2,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
TODO: Find a better way to check for sparse gradients.,1
we use here a FusedAdam() copy of an old Apex repo,0
In this case use the new AMP API from apex,0
In this case use the old FusedAdam with FP16_optimizer wrapper,0
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
backward compatibility,0
assuming a list/generator of parameter means single group,0
compute combined scale factor for this group,0
norm is in fact norm*scale,0
note: p.grad should not ever be set for correct operation of,0
mixed precision optimizer that sometimes sends None gradients,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Log the progress using the number of batches on the x-axis.,0
this check is here because audio allows the encoder and decoder to,0
"be different sizes, but other model types do not yet",0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
fields needs to have only keys that examples have as attrs,0
avoid infinite recursion when fields isn't defined,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"Dict[str, List[Tuple[str, Field]]]",0
doesn't change structure - don't return early.,0
"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"if tgt isn't using TextMultiField, then no text field is.",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
return vocab to dump with standard name,0
empty train_dataset_files so that vocab is only loaded from,0
"given paths in src_vocab_path, tgt_vocab_path",0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
fast-forward if loaded from state,0
NOTE: `rnn.pack_padded_sequence` requires that a,0
"minibatch be sorted by decreasing order, which",0
requires reversing relative to typical sort keys,0
Temporarily load one shard to retrieve sort_key for data_type,0
"NOTE: This is causing some issues for consumer/producer,",0
as we may still have some of those examples in some queue,0
cur_dataset.examples = None,0
gc.collect(),0
del cur_dataset,0
gc.collect(),0
Cycle through the shards indefinitely.,0
"When the dataset is not repeated, we might need to ensure that",0
the number of returned batches is the multiple of a given value.,0
This is important for multi GPU training to ensure that all,0
workers have the same number of batches to process.,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
-*- coding: utf-8 -*-,0
imports of datatype-specific dependencies,0
torchaudio loading options recently changed. It's probably,0
straightforward to rewrite the audio handling to make use of,0
"up-to-date torchaudio, but in the meantime there is a legacy",0
method which uses the old defaults,0
STFT,0
-*- coding: utf-8 -*-,0
domain specific dependencies,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
-*- coding: utf-8 -*-,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
create one counter per shard,0
"every corpus has shards, no new one",0
!/usr/bin/env python,0
Load checkpoint if we resume from a previous training.,0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
generator_to_serve = iter(generator_to_serve),0
hack to dodge unpicklable `dict_keys`,1
"propagate exception to parent process, keeping original traceback",0
The score for each translation on the beam.,0
The backpointers at each time-step.,0
The outputs at each time-step.,0
Has EOS topped the beam yet.,0
The attentions (matrix) for each time.,0
Time and k pair for finished.,0
Information for global scoring.,0
Minimum prediction length,0
Apply Penalty at every step,0
force the output to be longer than self.min_length,0
assumes there are len(word_probs) predictions OTHER,0
than EOS that are greater than -1e20,0
Sum the previous scores.,0
Don't let EOS have children.,0
Block ngram repeats,0
"Last n tokens, n = block_ngram_repeat",0
Skip the blocking if it is in the exclusion list,0
"best_scores_id is flattened beam x word array, so calculate which",0
word and beam each score came from,0
End condition is when top-of-beam is EOS and no global score.,0
Add from beam until we have minimum outputs.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"shape: (sum(~ self.is_finished), 1)",0
magic indices,0
result caching,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
skip BOS,0
"Last n tokens, n = block_ngram_repeat",0
skip the blocking if any token in gram is excluded,0
!/usr/bin/env python,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
max_tgt_in_batch = 0,0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
for debugging,0
Statistics,0
TODO: support these blacklisted features.,0
Encoder forward.,0
"Shape: (1, B, 1)",0
Reorder states.,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
TODO: support these blacklisted features.,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
We use batch_size x beam_size,0
"(0) pt 2, prep the beam object",0
Reorder states.,0
"This is left in the code for now, but unsued",0
(0) Prep each of the components of the search.,0
And helper method for reducing verbosity.,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
We use now  batch_size x beam_size (same as fast mode),0
"(3) run the decoder to generate sentences, using beam search.",0
(a) Construct batch x beam_size nxt words.,0
Get all the pending current beam words and arrange for forward.,0
(b) Decode and forward,0
(c) Advance each beam.,0
Loop over the batch_size number of beam,0
(4) Extract sentences from beam.,0
Rollback pointer to the beginning.,0
beam parameters,0
result caching,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
buffers for the topk scores and 'backpointer',0
"""global state"" of the old beam",0
for testing,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
backwards compatibility for confs,0
load can be called multiple times: modify copy,0
NOTE: translator returns lists of `n_best` list,0
we can ignore that (i.e. flatten lists) only because,0
we restrict `n_best=1`,0
build back results with empty texts,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
Load checkpoint if we resume from a previous training.,0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
generator_to_serve = iter(generator_to_serve),0
hack to dodge unpicklable `dict_keys`,1
"propagate exception to parent process, keeping original traceback",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
degenerate case,0
cache the features,0
mp queues don't work well between procs unless they're from a manager,0
each device has its own saver so that reconstructing is easier,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"Add in default model arguments, possibly added since training.",0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
for back compat when attention_dropout was not defined,0
Build embeddings.,0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Load checkpoint if we resume from a previous training.,0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
"Report src and tgt vocab sizes, including for features",0
Build model.,0
Build optimizer.,0
Build model saver,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"if you want to pass an existing vocab.pt file, pass it to",0
-src_vocab alone as it already contains tgt vocab.,0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Option most relevant to image input,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use Tensorboard for visualization during training,0
Options most relevant to speech,0
Option most relevant to image input,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
Option most relevant to image input,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
The encoder hidden is  (layers*directions) x batch x dim.,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
why is the model_opt.__dict__ check necessary?,1
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return one attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
first beam finished had length beam.min_length,0
first beam finished was 0,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
"[2, 5, 3, 6, 0], so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010]",0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0], so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
predict repeat_idx over and over again,0
"batch 0 and 7 will repeat, the rest will advance",0
predict the same thing in batch 0 and 7 every i,0
push around what the other batches predict,0
now batch 0 and 7 die,0
"batch 0 will repeat excluded idx, batch 1 will repeat",0
now batch 1 dies,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
-*- coding: utf-8 -*-,0
tests pad and numericalize integration,0
tests pad and numericalize integration,0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to audio data,0
file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names),0
it's ok if non-audio files co-exist with audio files in the data dir,0
"dividing gets the noise in [-1, 1]",0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to image data,0
file to hold image paths relative to _IMG_DATA_DIR (i.e. file names),0
it's ok if non-image files co-exist with image files in the data dir,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
"when reasonable, set audio_enc_pooling to 2",0
Need lengths >= audio_enc_pooling**n_layers.,0
"That condition is unrealistic for large n_layers,",0
so leave audio_enc_pooling at 1.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are fixed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
Test image preprocessing,0
Test audio preprocessing,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
BoolTensor was introduced in pytorch 1.2,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
TODO: Find a better way to check for sparse gradients.,1
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Log the progress using the number of batches on the x-axis.,0
this check is here because audio allows the encoder and decoder to,0
"be different sizes, but other model types do not yet",0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
fields needs to have only keys that examples have as attrs,0
avoid infinite recursion when fields isn't defined,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"Dict[str, List[Tuple[str, Field]]]",0
doesn't change structure - don't return early.,0
"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"if tgt isn't using TextMultiField, then no text field is.",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
return vocab to dump with standard name,0
empty train_dataset_files so that vocab is only loaded from,0
"given paths in src_vocab_path, tgt_vocab_path",0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
fast-forward if loaded from state,0
NOTE: `rnn.pack_padded_sequence` requires that a,0
"minibatch be sorted by decreasing order, which",0
requires reversing relative to typical sort keys,0
Temporarily load one shard to retrieve sort_key for data_type,0
"NOTE: This is causing some issues for consumer/producer,",0
as we may still have some of those examples in some queue,0
cur_dataset.examples = None,0
gc.collect(),0
del cur_dataset,0
gc.collect(),0
Cycle through the shards indefinitely.,0
"When the dataset is not repeated, we might need to ensure that",0
the number of returned batches is the multiple of a given value.,0
This is important for multi GPU training to ensure that all,0
workers have the same number of batches to process.,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
-*- coding: utf-8 -*-,0
imports of datatype-specific dependencies,0
torchaudio loading options recently changed. It's probably,0
straightforward to rewrite the audio handling to make use of,0
"up-to-date torchaudio, but in the meantime there is a legacy",0
method which uses the old defaults,0
STFT,0
-*- coding: utf-8 -*-,0
domain specific dependencies,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
-*- coding: utf-8 -*-,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
The score for each translation on the beam.,0
The backpointers at each time-step.,0
The outputs at each time-step.,0
Has EOS topped the beam yet.,0
The attentions (matrix) for each time.,0
Time and k pair for finished.,0
Information for global scoring.,0
Minimum prediction length,0
Apply Penalty at every step,0
force the output to be longer than self.min_length,0
assumes there are len(word_probs) predictions OTHER,0
than EOS that are greater than -1e20,0
Sum the previous scores.,0
Don't let EOS have children.,0
Block ngram repeats,0
"Last n tokens, n = block_ngram_repeat",0
Skip the blocking if it is in the exclusion list,0
"best_scores_id is flattened beam x word array, so calculate which",0
word and beam each score came from,0
End condition is when top-of-beam is EOS and no global score.,0
Add from beam until we have minimum outputs.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"shape: (sum(~ self.is_finished), 1)",0
magic indices,0
result caching,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
skip BOS,0
"Last n tokens, n = block_ngram_repeat",0
skip the blocking if any token in gram is excluded,0
!/usr/bin/env python,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
max_tgt_in_batch = 0,0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
for debugging,0
Statistics,0
TODO: support these blacklisted features.,0
Encoder forward.,0
"Shape: (1, B, 1)",0
Reorder states.,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
TODO: support these blacklisted features.,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
We use batch_size x beam_size,0
"(0) pt 2, prep the beam object",0
Reorder states.,0
"This is left in the code for now, but unsued",0
(0) Prep each of the components of the search.,0
And helper method for reducing verbosity.,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
We use now  batch_size x beam_size (same as fast mode),0
"(3) run the decoder to generate sentences, using beam search.",0
(a) Construct batch x beam_size nxt words.,0
Get all the pending current beam words and arrange for forward.,0
(b) Decode and forward,0
(c) Advance each beam.,0
Loop over the batch_size number of beam,0
(4) Extract sentences from beam.,0
Rollback pointer to the beginning.,0
beam parameters,0
result caching,0
beam state,0
BoolTensor was introduced in pytorch 1.2,0
buffers for the topk scores and 'backpointer',0
"""global state"" of the old beam",0
for testing,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
backwards compatibility for confs,0
load can be called multiple times: modify copy,0
NOTE: translator returns lists of `n_best` list,0
we can ignore that (i.e. flatten lists) only because,0
we restrict `n_best=1`,0
build back results with empty texts,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
Chinese segmentation,0
Chinese simplify -> Chinese traditional standard,0
Chinese simplify -> Chinese traditional (HongKong),0
Chinese simplify -> Chinese traditional (Taiwan),0
Chinese traditional -> Chinese simplify (v1),0
Chinese traditional -> Chinese simplify (v2),0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
Load checkpoint if we resume from a previous training.,0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
generator_to_serve = iter(generator_to_serve),0
hack to dodge unpicklable `dict_keys`,1
"propagate exception to parent process, keeping original traceback",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"Add in default model arguments, possibly added since training.",0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
Build embeddings.,0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Load checkpoint if we resume from a previous training.,0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
"Report src and tgt vocab sizes, including for features",0
Build model.,0
Build optimizer.,0
Build model saver,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"if you want to pass an existing vocab.pt file, pass it to",0
-src_vocab alone as it already contains tgt vocab.,0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Option most relevant to image input,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use TensorboardX for visualization during training,0
Options most relevant to speech,0
Option most relevant to image input,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
Option most relevant to image input,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
The encoder hidden is  (layers*directions) x batch x dim.,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
why is the model_opt.__dict__ check necessary?,1
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return one attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
first beam finished had length beam.min_length,0
first beam finished was 0,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
"[2, 5, 3, 6, 0], so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010]",0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0], so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
predict repeat_idx over and over again,0
"batch 0 and 7 will repeat, the rest will advance",0
predict the same thing in batch 0 and 7 every i,0
push around what the other batches predict,0
now batch 0 and 7 die,0
"batch 0 will repeat excluded idx, batch 1 will repeat",0
now batch 1 dies,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
-*- coding: utf-8 -*-,0
tests pad and numericalize integration,0
tests pad and numericalize integration,0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to audio data,0
file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names),0
it's ok if non-audio files co-exist with audio files in the data dir,0
"dividing gets the noise in [-1, 1]",0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to image data,0
file to hold image paths relative to _IMG_DATA_DIR (i.e. file names),0
it's ok if non-image files co-exist with image files in the data dir,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
"when reasonable, set audio_enc_pooling to 2",0
Need lengths >= audio_enc_pooling**n_layers.,0
"That condition is unrealistic for large n_layers,",0
so leave audio_enc_pooling at 1.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are fixed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
Test image preprocessing,0
Test audio preprocessing,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
TODO: Find a better way to check for sparse gradients.,1
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Log the progress using the number of batches on the x-axis.,0
this check is here because audio allows the encoder and decoder to,0
"be different sizes, but other model types do not yet",0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
fields needs to have only keys that examples have as attrs,0
avoid infinite recursion when fields isn't defined,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"Dict[str, List[Tuple[str, Field]]]",0
doesn't change structure - don't return early.,0
"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"if tgt isn't using TextMultiField, then no text field is.",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
return vocab to dump with standard name,0
empty train_dataset_files so that vocab is only loaded from,0
"given paths in src_vocab_path, tgt_vocab_path",0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
fast-forward if loaded from state,0
NOTE: `rnn.pack_padded_sequence` requires that a,0
"minibatch be sorted by decreasing order, which",0
requires reversing relative to typical sort keys,0
Temporarily load one shard to retrieve sort_key for data_type,0
"NOTE: This is causing some issues for consumer/producer,",0
as we may still have some of those examples in some queue,0
cur_dataset.examples = None,0
gc.collect(),0
del cur_dataset,0
gc.collect(),0
Cycle through the shards indefinitely.,0
"When the dataset is not repeated, we might need to ensure that",0
the number of returned batches is the multiple of a given value.,0
This is important for multi GPU training to ensure that all,0
workers have the same number of batches to process.,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
-*- coding: utf-8 -*-,0
imports of datatype-specific dependencies,0
torchaudio loading options recently changed. It's probably,0
straightforward to rewrite the audio handling to make use of,0
"up-to-date torchaudio, but in the meantime there is a legacy",0
method which uses the old defaults,0
STFT,0
-*- coding: utf-8 -*-,0
domain specific dependencies,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
-*- coding: utf-8 -*-,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
NOTE: We need to trim the vocab to remove any unk tokens that,0
were not originally here.,0
The score for each translation on the beam.,0
The backpointers at each time-step.,0
The outputs at each time-step.,0
Has EOS topped the beam yet.,0
The attentions (matrix) for each time.,0
Time and k pair for finished.,0
Information for global scoring.,0
Minimum prediction length,0
Apply Penalty at every step,0
force the output to be longer than self.min_length,0
assumes there are len(word_probs) predictions OTHER,0
than EOS that are greater than -1e20,0
Sum the previous scores.,0
Don't let EOS have children.,0
Block ngram repeats,0
"Last n tokens, n = block_ngram_repeat",0
Skip the blocking if it is in the exclusion list,0
"best_scores_id is flattened beam x word array, so calculate which",0
word and beam each score came from,0
End condition is when top-of-beam is EOS and no global score.,0
Add from beam until we have minimum outputs.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"shape: (sum(~ self.is_finished), 1)",0
magic indices,0
result caching,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
skip BOS,0
"Last n tokens, n = block_ngram_repeat",0
skip the blocking if any token in gram is excluded,0
!/usr/bin/env python,0
for debugging,0
Statistics,0
TODO: support these blacklisted features.,0
Encoder forward.,0
"Shape: (1, B, 1)",0
Reorder states.,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
TODO: support these blacklisted features.,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
We use batch_size x beam_size,0
"(0) pt 2, prep the beam object",0
Reorder states.,0
"This is left in the code for now, but unsued",0
(0) Prep each of the components of the search.,0
And helper method for reducing verbosity.,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
We use now  batch_size x beam_size (same as fast mode),0
"(3) run the decoder to generate sentences, using beam search.",0
(a) Construct batch x beam_size nxt words.,0
Get all the pending current beam words and arrange for forward.,0
(b) Decode and forward,0
(c) Advance each beam.,0
Loop over the batch_size number of beam,0
(4) Extract sentences from beam.,0
Rollback pointer to the beginning.,0
beam parameters,0
result caching,0
beam state,0
buffers for the topk scores and 'backpointer',0
"""global state"" of the old beam",0
for testing,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
backwards compatibility for confs,0
load can be called multiple times: modify copy,0
NOTE: translator returns lists of `n_best` list,0
we can ignore that (i.e. flatten lists) only because,0
we restrict `n_best=1`,0
build back results with empty texts,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"propagate exception to parent process, keeping original traceback",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
Read in embeddings,0
Write to file,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"Add in default model arguments, possibly added since training.",0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
Build embeddings.,0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Load checkpoint if we resume from a previous training.,0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
"Report src and tgt vocab sizes, including for features",0
Build model.,0
Build optimizer.,0
Build model saver,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"if you want to pass an existing vocab.pt file, pass it to",0
-src_vocab alone as it already contains tgt vocab.,0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Option most relevant to image input,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use TensorboardX for visualization during training,0
Options most relevant to speech,0
Option most relevant to image input,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
Option most relevant to image input,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
UPDATE DROPOUT,0
Run patience mechanism,0
"If the patience has reached the limit, stop training",0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
we avoid padding while mean pooling,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
The encoder hidden is  (layers*directions) x batch x dim.,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
why is the model_opt.__dict__ check necessary?,1
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return one attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
first beam finished had length beam.min_length,0
first beam finished was 0,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
"[2, 5, 3, 6, 0], so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010]",0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0], so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
predict repeat_idx over and over again,0
"batch 0 and 7 will repeat, the rest will advance",0
predict the same thing in batch 0 and 7 every i,0
push around what the other batches predict,0
now batch 0 and 7 die,0
"batch 0 will repeat excluded idx, batch 1 will repeat",0
now batch 1 dies,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
-*- coding: utf-8 -*-,0
tests pad and numericalize integration,0
tests pad and numericalize integration,0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to audio data,0
file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names),0
it's ok if non-audio files co-exist with audio files in the data dir,0
"dividing gets the noise in [-1, 1]",0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to image data,0
file to hold image paths relative to _IMG_DATA_DIR (i.e. file names),0
it's ok if non-image files co-exist with image files in the data dir,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
"when reasonable, set audio_enc_pooling to 2",0
Need lengths >= audio_enc_pooling**n_layers.,0
"That condition is unrealistic for large n_layers,",0
so leave audio_enc_pooling at 1.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are fixed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
Test image preprocessing,0
Test audio preprocessing,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
TODO: Find a better way to check for sparse gradients.,1
TODO: clean this up when APEX unify its optimizer API.,1
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Log the progress using the number of batches on the x-axis.,0
this check is here because audio allows the encoder and decoder to,0
"be different sizes, but other model types do not yet",0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
Don't do anything,0
Update best score of each criteria,0
Reset tolerance,0
Update current status,0
Decrease tolerance,0
Log,0
Log,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
fields needs to have only keys that examples have as attrs,0
avoid infinite recursion when fields isn't defined,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"Dict[str, List[Tuple[str, Field]]]",0
doesn't change structure - don't return early.,0
"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"if tgt isn't using TextMultiField, then no text field is.",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
return vocab to dump with standard name,0
empty train_dataset_files so that vocab is only loaded from,0
"given paths in src_vocab_path, tgt_vocab_path",0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
fast-forward if loaded from state,0
NOTE: `rnn.pack_padded_sequence` requires that a,0
"minibatch be sorted by decreasing order, which",0
requires reversing relative to typical sort keys,0
Temporarily load one shard to retrieve sort_key for data_type,0
Cycle through the shards indefinitely.,0
"When the dataset is not repeated, we might need to ensure that",0
the number of returned batches is the multiple of a given value.,0
This is important for multi GPU training to ensure that all,0
workers have the same number of batches to process.,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
-*- coding: utf-8 -*-,0
imports of datatype-specific dependencies,0
torchaudio loading options recently changed. It's probably,0
straightforward to rewrite the audio handling to make use of,0
"up-to-date torchaudio, but in the meantime there is a legacy",0
method which uses the old defaults,0
STFT,0
-*- coding: utf-8 -*-,0
domain specific dependencies,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
-*- coding: utf-8 -*-,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
The score for each translation on the beam.,0
The backpointers at each time-step.,0
The outputs at each time-step.,0
Has EOS topped the beam yet.,0
The attentions (matrix) for each time.,0
Time and k pair for finished.,0
Information for global scoring.,0
Minimum prediction length,0
Apply Penalty at every step,0
force the output to be longer than self.min_length,0
assumes there are len(word_probs) predictions OTHER,0
than EOS that are greater than -1e20,0
Sum the previous scores.,0
Don't let EOS have children.,0
Block ngram repeats,0
"Last n tokens, n = block_ngram_repeat",0
Skip the blocking if it is in the exclusion list,0
"best_scores_id is flattened beam x word array, so calculate which",0
word and beam each score came from,0
End condition is when top-of-beam is EOS and no global score.,0
Add from beam until we have minimum outputs.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"shape: (sum(~ self.is_finished), 1)",0
magic indices,0
result caching,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
skip BOS,0
"Last n tokens, n = block_ngram_repeat",0
skip the blocking if any token in gram is excluded,0
!/usr/bin/env python,0
for debugging,0
Statistics,0
TODO: support these blacklisted features.,0
Encoder forward.,0
"Shape: (1, B, 1)",0
Reorder states.,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
TODO: support these blacklisted features.,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
We use batch_size x beam_size,0
"(0) pt 2, prep the beam object",0
Reorder states.,0
"This is left in the code for now, but unsued",0
(0) Prep each of the components of the search.,0
And helper method for reducing verbosity.,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
We use now  batch_size x beam_size (same as fast mode),0
"(3) run the decoder to generate sentences, using beam search.",0
(a) Construct batch x beam_size nxt words.,0
Get all the pending current beam words and arrange for forward.,0
(b) Decode and forward,0
(c) Advance each beam.,0
Loop over the batch_size number of beam,0
(4) Extract sentences from beam.,0
Rollback pointer to the beginning.,0
beam parameters,0
result caching,0
beam state,0
buffers for the topk scores and 'backpointer',0
"""global state"" of the old beam",0
for testing,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
backwards compatibility for confs,0
load can be called multiple times: modify copy,0
NOTE: translator returns lists of `n_best` list,0
we can ignore that (i.e. flatten lists) only because,0
we restrict `n_best=1`,0
build back results with empty texts,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"propagate exception to parent process, keeping original traceback",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
Show base classes,0
"Use ""variables"" section for Attributes instead of weird block things",0
mimicking the function style.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"Add in default model arguments, possibly added since training.",0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
Build embeddings.,0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
NOTE: It's important that ``opt`` has been validated and updated,0
at this point.,0
Load checkpoint if we resume from a previous training.,0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
"Report src and tgt vocab sizes, including for features",0
Build model.,0
Build optimizer.,0
Build model saver,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Option most relevant to image input,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use TensorboardX for visualization during training,0
Options most relevant to speech,0
Option most relevant to image input,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
Option most relevant to image input,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
The encoder hidden is  (layers*directions) x batch x dim.,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
why is the model_opt.__dict__ check necessary?,1
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return one attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
first beam finished had length beam.min_length,0
first beam finished was 0,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
"[2, 5, 3, 6, 0], so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010]",0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0], so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
predict repeat_idx over and over again,0
"batch 0 and 7 will repeat, the rest will advance",0
predict the same thing in batch 0 and 7 every i,0
push around what the other batches predict,0
now batch 0 and 7 die,0
"batch 0 will repeat excluded idx, batch 1 will repeat",0
now batch 1 dies,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
this could be considered an integration test because it touches,0
the filesystem for the config file (and the models),0
-*- coding: utf-8 -*-,0
tests pad and numericalize integration,0
tests pad and numericalize integration,0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to audio data,0
file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names),0
it's ok if non-audio files co-exist with audio files in the data dir,0
"dividing gets the noise in [-1, 1]",0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to image data,0
file to hold image paths relative to _IMG_DATA_DIR (i.e. file names),0
it's ok if non-image files co-exist with image files in the data dir,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
"when reasonable, set audio_enc_pooling to 2",0
Need lengths >= audio_enc_pooling**n_layers.,0
"That condition is unrealistic for large n_layers,",0
so leave audio_enc_pooling at 1.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are fixed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
Test image preprocessing,0
Test audio preprocessing,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
TODO: Find a better way to check for sparse gradients.,1
TODO: clean this up when APEX unify its optimizer API.,1
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Log the progress using the number of batches on the x-axis.,0
this check is here because audio allows the encoder and decoder to,0
"be different sizes, but other model types do not yet",0
"Load default opt values, then overwrite with the opts in",0
"the checkpoint. That way, if there are new options added,",0
the defaults are used.,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
fields needs to have only keys that examples have as attrs,0
avoid infinite recursion when fields isn't defined,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"Dict[str, List[Tuple[str, Field]]]",0
doesn't change structure - don't return early.,0
"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]",0
"-> dict[str, Field]",0
"if tgt isn't using TextMultiField, then no text field is.",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
Cycle through the shards indefinitely.,0
"When the dataset is not repeated, we might need to ensure that",0
the number of returned batches is the multiple of a given value.,0
This is important for multi GPU training to ensure that all,0
workers have the same number of batches to process.,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
-*- coding: utf-8 -*-,0
imports of datatype-specific dependencies,0
torchaudio loading options recently changed. It's probably,0
straightforward to rewrite the audio handling to make use of,0
"up-to-date torchaudio, but in the meantime there is a legacy",0
method which uses the old defaults,0
STFT,0
-*- coding: utf-8 -*-,0
domain specific dependencies,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
-*- coding: utf-8 -*-,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
The score for each translation on the beam.,0
The backpointers at each time-step.,0
The outputs at each time-step.,0
Has EOS topped the beam yet.,0
The attentions (matrix) for each time.,0
Time and k pair for finished.,0
Information for global scoring.,0
Minimum prediction length,0
Apply Penalty at every step,0
force the output to be longer than self.min_length,0
assumes there are len(word_probs) predictions OTHER,0
than EOS that are greater than -1e20,0
Sum the previous scores.,0
Don't let EOS have children.,0
Block ngram repeats,0
"Last n tokens, n = block_ngram_repeat",0
Skip the blocking if it is in the exclusion list,0
"best_scores_id is flattened beam x word array, so calculate which",0
word and beam each score came from,0
End condition is when top-of-beam is EOS and no global score.,0
Add from beam until we have minimum outputs.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"shape: (sum(~ self.is_finished), 1)",0
magic indices,0
result caching,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
skip BOS,0
"Last n tokens, n = block_ngram_repeat",0
skip the blocking if any token in gram is excluded,0
!/usr/bin/env python,0
for debugging,0
Statistics,0
TODO: support these blacklisted features.,0
Encoder forward.,0
"Shape: (1, B, 1)",0
Reorder states.,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
TODO: support these blacklisted features.,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
We use batch_size x beam_size,0
"(0) pt 2, prep the beam object",0
Reorder states.,0
"This is left in the code for now, but unsued",0
(0) Prep each of the components of the search.,0
And helper method for reducing verbosity.,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
We use now  batch_size x beam_size (same as fast mode),0
"(3) run the decoder to generate sentences, using beam search.",0
(a) Construct batch x beam_size nxt words.,0
Get all the pending current beam words and arrange for forward.,0
(b) Decode and forward,0
(c) Advance each beam.,0
Loop over the batch_size number of beam,0
(4) Extract sentences from beam.,0
Rollback pointer to the beginning.,0
beam parameters,0
result caching,0
beam state,0
buffers for the topk scores and 'backpointer',0
"""global state"" of the old beam",0
for testing,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
!/usr/bin/env python,0
semaphore doesn't have a timeout arg in Python 2.7,0
backwards compatibility for confs,0
load can be called multiple times: modify copy,0
NOTE: translator returns lists of `n_best` list,0
we can ignore that (i.e. flatten lists) only because,0
we restrict `n_best=1`,0
build back results with empty texts,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"propagate exception to parent process, keeping original traceback",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"Add in default model arguments, possibly added since training.",0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
for backward compatibility,0
Build embeddings.,0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
this check is here because audio allows the encoder and decoder to,0
"be different sizes, but other model types do not yet",0
Load checkpoint if we resume from a previous training.,0
Load default opts values then overwrite it with opts from,0
the checkpoint. It's usefull in order to re-train a model,0
after adding a new option (not set in checkpoint),0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
"Report src and tgt vocab sizes, including for features",0
Build model.,0
Build optimizer.,0
Build model saver,0
this line is kind of a temporary kludge because different objects expect,1
fields to have a different structure,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Option most relevant to image input,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use TensorboardX for visualization during training,0
Options most relevant to speech,0
Option most relevant to image input,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
Option most relevant to image input,0
MARKDOWN boilerplate,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
**section heading**:,0
# **--argument-one**,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
The encoder hidden is  (layers*directions) x batch x dim.,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
why is the model_opt.__dict__ check necessary?,1
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return one attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
first beam finished had length beam.min_length,0
first beam finished was 0,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
"[2, 5, 3, 6, 0], so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010]",0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0], so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
predict repeat_idx over and over again,0
"batch 0 and 7 will repeat, the rest will advance",0
predict the same thing in batch 0 and 7 every i,0
push around what the other batches predict,0
now batch 0 and 7 die,0
"batch 0 will repeat excluded idx, batch 1 will repeat",0
now batch 1 dies,0
batch 0 will always predict EOS. The other batches will predict,0
non-eos scores.,0
"""best"" prediction is eos - that should be blocked",0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
now batch 0 has ended and no others have,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
initial step,0
batch 0 dies on step 0,0
include at least one prediction OTHER than EOS,0
that is greater than -1e20,0
step 2,0
(old) batch 8 dies on step 1,0
step 3,0
everything dies,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
-*- coding: utf-8 -*-,0
tests pad and numericalize integration,0
tests pad and numericalize integration,0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to audio data,0
file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names),0
it's ok if non-audio files co-exist with audio files in the data dir,0
"dividing gets the noise in [-1, 1]",0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to image data,0
file to hold image paths relative to _IMG_DATA_DIR (i.e. file names),0
it's ok if non-image files co-exist with image files in the data dir,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
no top beams are finished yet,0
beam 1 dies on min_length,0
no top beams are finished yet,0
beam 0 dies on the step after beam 1 dies,0
top beam is finished now so there are attentions,0
two beams are finished in each batch,0
second dim is cut down to the non-padded src length,0
first dim is equal to the time of death,0
(beam 0 died at current step - adjust for SOS),0
(beam 1 died at last step - adjust for SOS),0
behavior gets weird when beam is already done so just stop,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are fixed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
Test image preprocessing,0
Test audio preprocessing,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
TODO change the way attns is returned dict => list or tuple (onnx),1
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
TODO: Find a better way to check for sparse gradients.,1
TODO: clean this up when APEX unify its optimizer API.,1
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Log the progress using the number of batches on the x-axis.,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
the dataset's self.fields should have the same attributes as examples,0
avoid infinite recursion when fields isn't defined,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"if tgt isn't using TextMultiField, then no text field is.",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
Cycle through the shards indefinitely.,0
"When the dataset is not repeated, we might need to ensure that",0
the number of returned batches is the multiple of a given value.,0
This is important for multi GPU training to ensure that all,0
workers have the same number of batches to process.,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
-*- coding: utf-8 -*-,0
imports of datatype-specific dependencies,0
torchaudio loading options recently changed. It's probably,0
straightforward to rewrite the audio handling to make use of,0
"up-to-date torchaudio, but in the meantime there is a legacy",0
method which uses the old defaults,0
STFT,0
-*- coding: utf-8 -*-,0
domain specific dependencies,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
-*- coding: utf-8 -*-,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
The score for each translation on the beam.,0
The backpointers at each time-step.,0
The outputs at each time-step.,0
Has EOS topped the beam yet.,0
The attentions (matrix) for each time.,0
Time and k pair for finished.,0
Information for global scoring.,0
Minimum prediction length,0
Apply Penalty at every step,0
force the output to be longer than self.min_length,0
assumes there are len(word_probs) predictions OTHER,0
than EOS that are greater than -1e20,0
Sum the previous scores.,0
Don't let EOS have children.,0
Block ngram repeats,0
"Last n tokens, n = block_ngram_repeat",0
Skip the blocking if it is in the exclusion list,0
"best_scores_id is flattened beam x word array, so calculate which",0
word and beam each score came from,0
End condition is when top-of-beam is EOS and no global score.,0
Add from beam until we have minimum outputs.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
Set all logits that are not in the top-k to -10000.,0
This puts the probabilities close to 0.,0
"shape: (sum(~ self.is_finished), 1)",0
magic indices,0
result caching,0
add one to account for BOS. Don't account for EOS because hitting,0
this implies it hasn't been found.,0
skip BOS,0
"Last n tokens, n = block_ngram_repeat",0
skip the blocking if any token in gram is excluded,0
!/usr/bin/env python,0
for debugging,0
Statistics,0
TODO: support these blacklisted features.,0
Encoder forward.,0
"Shape: (1, B, 1)",0
Reorder states.,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
TODO: support these blacklisted features.,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
We use batch_size x beam_size,0
"(0) pt 2, prep the beam object",0
Reorder states.,0
"This is left in the code for now, but unsued",0
(0) Prep each of the components of the search.,0
And helper method for reducing verbosity.,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
We use now  batch_size x beam_size (same as fast mode),0
"(3) run the decoder to generate sentences, using beam search.",0
(a) Construct batch x beam_size nxt words.,0
Get all the pending current beam words and arrange for forward.,0
(b) Decode and forward,0
(c) Advance each beam.,0
Loop over the batch_size number of beam,0
(4) Extract sentences from beam.,0
Rollback pointer to the beginning.,0
beam parameters,0
result caching,0
beam state,0
buffers for the topk scores and 'backpointer',0
"""global state"" of the old beam",0
for testing,0
using integer division to get an integer _B without casting,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and map to batch index flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
!/usr/bin/env python,0
backwards compatibility for confs,0
load can be called multiple times: modify copy,0
NOTE: translator returns lists of `n_best` list,0
we can ignore that (i.e. flatten lists) only because,0
we restrict `n_best=1`,0
build back results with empty texts,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"propagate exception to parent process, keeping original traceback",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"Add in default model arguments, possibly added since training.",0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
for backward compatibility,0
Build embeddings.,0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
this check is here because audio allows the encoder and decoder to,0
"be different sizes, but other model types do not yet",0
Load checkpoint if we resume from a previous training.,0
Load default opts values then overwrite it with opts from,0
the checkpoint. It's usefull in order to re-train a model,0
after adding a new option (not set in checkpoint),0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
"Report src and tgt vocab sizes, including for features",0
Build model.,0
Build optimizer.,0
Build model saver,0
this line is kind of a temporary kludge because different objects expect,1
fields to have a different structure,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Option most relevant to image input,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use TensorboardX for visualization during training,0
Options most relevant to speech,0
Option most relevant to image input,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
Option most relevant to image input,0
MARKDOWN boilerplate,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
**section heading**:,0
# **--argument-one**,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
The encoder hidden is  (layers*directions) x batch x dim.,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
why is the model_opt.__dict__ check necessary?,1
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
1 or key_len x key_len,0
1 or key_len x key_len x dim_per_head,0
1 or key_len x key_len x dim_per_head,0
2) Calculate and scale scores.,0
batch x num_heads x query_len x key_len,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return one attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
beam includes start token in cur_len count.,0
Add one to its min_length to compensate,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
first beam finished had length beam.min_length,0
first beam finished was 0,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
beam includes start token in cur_len count.,0
Add one to its min_length to compensate,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
"[2, 5, 3, 6, 0], so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010]",0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0], so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
initialize fields at the top of each unit test to prevent,0
any undesired stateful effects,0
"this test touches the file system, so it could be considered an",0
integration test,0
write utf-8 bytes,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
-*- coding: utf-8 -*-,0
tests pad and numericalize integration,0
tests pad and numericalize integration,0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to audio data,0
file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names),0
it's ok if non-audio files co-exist with audio files in the data dir,0
"dividing gets the noise in [-1, 1]",0
"this test touches the file system, so it could be considered an",0
integration test,0
file to hold full paths to image data,0
file to hold image paths relative to _IMG_DATA_DIR (i.e. file names),0
it's ok if non-image files co-exist with image files in the data dir,0
all beams repeat (beam >= 1 repeat dummy scores),0
predict repeat_idx over and over again,0
beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores),0
non-interesting beams are going to get dummy values,0
"on initial round, only predicted scores for beam 0",0
matter. Make two predictions. Top one will be repeated,0
"in beam zero, second one will live on in beam 1.",0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
"now beam 0 dies (along with the others), beam 1 -> beam 0",0
beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx),0
non-interesting beams are going to get dummy values,0
predict the same thing in beam 0,0
continue pushing around what beam 1 predicts,0
predict the allowed-repeat again in beam 2,0
"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1",0
and the rest die,0
"since all preds after i=0 are 0, we can check",0
that the beam is the correct idx by checking that,0
the curr score is the initial score,0
beam 0 will always predict EOS. The other beams will predict,0
non-eos scores.,0
beam includes start token in cur_len count.,0
Add one to its min_length to compensate,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 0,0
provide beam_sz other good predictions,0
now the top beam has ended and no others have,0
"not of interest, but want to make sure it keeps running",0
since only beam 0 terminates and n_best = 2,0
"this is also a test that when block_ngram_repeat=0,",0
repeating is acceptable,0
beam includes start token in cur_len count.,0
Add one to its min_length to compensate,0
non-interesting beams are going to get dummy values,0
"""best"" prediction is eos - that should be blocked",0
include at least beam_sz predictions OTHER than EOS,0
that are greater than -1e20,0
predict eos in beam 1,0
provide beam_sz other good predictions in other beams,0
provide beam_sz other good predictions in other beams,0
beam 1 dies on min_length,0
beam 0 dies on the step after beam 1 dies,0
this is just test_beam.TestBeamAgainstReferenceCase repeated,0
in each batch.,0
"init_preds: [4, 3, 5, 6, 7] - no EOS's",0
no EOS's yet,0
"[5, 3, 2, 6, 0], so beam 2 predicts EOS!",0
assumes beam 2 finished on last step,0
ended beam 2 shouldn't continue,0
"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!",0
"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ",0
another beam is finished in all batches,0
new beam 0 finished,0
new beam 0 is old beam 3,0
assumes beam 0 finished on last step,0
"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!",0
new beam 1 finished,0
new beam 1 is old beam 4,0
this could be considered an integration test because it tests,0
interactions between the GNMT scorer and the beam,0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are fixed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
Test image preprocessing,0
Test audio preprocessing,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
TODO change the way attns is returned dict => list or tuple (onnx),1
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
TODO: Find a better way to check for sparse gradients.,1
TODO: clean this up when APEX unify its optimizer API.,1
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Log the progress using the number of batches on the x-axis.,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
These ensure same initialization in multi gpu mode,0
Shift values to be >= 0,0
coding: utf-8,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
the dataset's self.fields should have the same attributes as examples,0
avoid infinite recursion when fields isn't defined,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
monkey-patch to make torchtext Vocab's pickleable,0
"if tgt isn't using TextMultiField, then no text field is.",0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
Cycle through the shards indefinitely.,0
"When the dataset is not repeated, we might need to ensure that",0
the number of returned batches is the multiple of a given value.,0
This is important for multi GPU training to ensure that all,0
workers have the same number of batches to process.,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wM <eos>],0
-*- coding: utf-8 -*-,0
imports of datatype-specific dependencies,0
torchaudio loading options recently changed. It's probably,0
straightforward to rewrite the audio handling to make use of,0
"up-to-date torchaudio, but in the meantime there is a legacy",0
method which uses the old defaults,0
STFT,0
-*- coding: utf-8 -*-,0
domain specific dependencies,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
-*- coding: utf-8 -*-,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
The score for each translation on the beam.,0
The backpointers at each time-step.,0
The outputs at each time-step.,0
Has EOS topped the beam yet.,0
The attentions (matrix) for each time.,0
Time and k pair for finished.,0
Information for global scoring.,0
Minimum prediction length,0
Apply Penalty at every step,0
force the output to be longer than self.min_length,0
assumes there are len(word_probs) predictions OTHER,0
than EOS that are greater than -1e20,0
Sum the previous scores.,0
Don't let EOS have children.,0
Block ngram repeats,0
"Last n tokens, n = block_ngram_repeat",0
Skip the blocking if it is in the exclusion list,0
"best_scores_id is flattened beam x word array, so calculate which",0
word and beam each score came from,0
End condition is when top-of-beam is EOS and no global score.,0
Add from beam until we have minimum outputs.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
these warnings indicate that either the alpha/beta,0
"forces a penalty to be a no-op, or a penalty is a no-op but",0
the alpha/beta would suggest otherwise.,0
using some length penalty,0
using some coverage penalty,0
!/usr/bin/env python,0
for debugging,0
Statistics,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
Set all logits that are not in the top-k to -1000.,0
This puts the probabilities close to 0.,0
TODO: support these blacklisted features.,0
Encoder forward.,0
"seq_so_far contains chosen tokens; on each step, dim 1 grows by one.",0
Note that what this code calls log_probs are actually logits.,0
Append last prediction.,0
"Store finished hypotheses for this batch. Unlike in beam search,",0
there will only ever be 1 hypothesis per example.,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
TODO: support these blacklisted features.,0
(0) Prep the components of the search.,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
We use batch_size x beam_size,0
"(0) pt 2, prep the beam object",0
Reorder states.,0
"This is left in the code for now, but unsued",0
(0) Prep each of the components of the search.,0
And helper method for reducing verbosity.,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
We use now  batch_size x beam_size (same as fast mode),0
"(3) run the decoder to generate sentences, using beam search.",0
(a) Construct batch x beam_size nxt words.,0
Get all the pending current beam words and arrange for forward.,0
(b) Decode and forward,0
(c) Advance each beam.,0
Loop over the batch_size number of beam,0
(4) Extract sentences from beam.,0
Rollback pointer to the beginning.,0
magic indices,0
beam parameters,0
result caching,0
beam state,0
"""global state"" of the old beam",0
for testing,0
force the output to be longer than self.min_length,0
Multiply probs by the beam probability.,0
block ngram repeats,0
"iterate over all batches, over all beams",0
"Last n tokens, n = block_ngram_repeat",0
skip the blocking if any token in gram is excluded,0
"if the sequence ends now, then the penalty is the current",0
"length + 1, to include the EOS token",0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Length penalty is just a scalar. It doesn't matter if it's applied,0
before or after the topk.,0
Resolve beam origin and true word ids.,0
Map beam_index to batch_index in the flat representation.,0
Append last prediction.,0
update global state (step == 1),0
update global state (step > 1),0
"shape: (batch_size x beam_size, 1)",0
Penalize beams that finished.,0
"on real data (newstest2017) with the pretrained transformer,",0
it's faster to not move this back to the original device,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
!/usr/bin/env python,0
backwards compatibility for confs,0
load can be called multiple times: modify copy,0
NOTE: translator returns lists of `n_best` list,0
we can ignore that (i.e. flatten lists) only because,0
we restrict `n_best=1`,0
build back results with empty texts,0
Below are all the different penalty terms implemented so far.,0
Subtract coverage penalty from topk log probs.,0
Divide topk log probs by length penalty.,0
Sorting,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"propagate exception to parent process, keeping original traceback",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"Add in default model arguments, possibly added since training.",0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
for backward compatibility,0
Build embeddings.,0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
this check is here because audio allows the encoder and decoder to,0
"be different sizes, but other model types do not yet",0
Load checkpoint if we resume from a previous training.,0
Load default opts values then overwrite it with opts from,0
the checkpoint. It's usefull in order to re-train a model,0
after adding a new option (not set in checkpoint),0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
"Report src and tgt vocab sizes, including for features",0
Build model.,0
Build optimizer.,0
Build model saver,0
this line is kind of a temporary kludge because different objects expect,1
fields to have a different structure,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Option most relevant to image input,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use TensorboardX for visualization during training,0
Options most relevant to speech,0
Option most relevant to image input,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
Option most relevant to image input,0
MARKDOWN boilerplate,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
**section heading**:,0
# **--argument-one**,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
The encoder hidden is  (layers*directions) x batch x dim.,0
"s_len, batch, emb_dim = emb.size()",0
Run the forward pass of every layer of the tranformer.,0
why is the model_opt.__dict__ check necessary?,1
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
features must use word_vec_size,0
features will use feat_vec_size,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
2) Calculate and scale scores.,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return one attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
first check there's nothing unexpectedly not trainable,0
ok: word embeddings shouldn't be trainable,0
if word vecs are fixed,0
ok: positional encodings shouldn't be trainable,0
then check nothing unexpectedly trainable,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
Test image preprocessing,0
Test audio preprocessing,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
TODO change the way attns is returned dict => list or tuple (onnx),1
Check,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Decoder State,0
CNNDecoder has its own attention mechanism.,0
Set up a separate copy attention layer if needed.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"Register self.mask as a buffer in TransformerDecoderLayer, so",0
it gets TransformerDecoderLayer's cuda behavior automatically.,0
Decoder State,0
"previously, there was a GlobalAttention module here for copy",0
"attention. But it was never actually used -- the ""copy"" attention",0
just reuses the context attention.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
TODO: Find a better way to check for sparse gradients.,1
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Log the progress using the number of batches on the x-axis.,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
These ensure same initialization in multi gpu mode,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
this assumes src_field and tgt_field are both text,0
the dataset's self.fields should have the same attributes as examples,0
avoid infinite recursion when fields isn't defined,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
"if tgt isn't using TextMultiField, then no text field is.",0
"there is a truncate argument as well, but it was never set to",0
anything besides None before,0
the second conjunct means nothing will be filtered at translation time,0
if there is no target data,0
this is basically copy-pasted from torchtext.,0
counters changes in place,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: [<bos> w1 ... wN <eos>],0
Tgt: [w1 ... wN <eos>],0
-*- coding: utf-8 -*-,0
imports of datatype-specific dependencies,0
torchaudio loading options recently changed. It's probably,0
straightforward to rewrite the audio handling to make use of,0
"up-to-date torchaudio, but in the meantime there is a legacy",0
method which uses the old defaults,0
STFT,0
-*- coding: utf-8 -*-,0
domain specific dependencies,0
-*- coding: utf-8 -*-,0
mix this with partial,0
batch (list(list(list))): batch_size x len(self.fields) x seq_len,0
lengths: batch_size,0
data: seq_len x batch_size x len(self.fields),0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
The score for each translation on the beam.,0
The backpointers at each time-step.,0
The outputs at each time-step.,0
Has EOS topped the beam yet.,0
The attentions (matrix) for each time.,0
Time and k pair for finished.,0
Information for global scoring.,0
Minimum prediction length,0
Apply Penalty at every step,0
force the output to be longer than self.min_length,0
Sum the previous scores.,0
Don't let EOS have children.,0
Block ngram repeats,0
"Last n tokens, n = block_ngram_repeat",0
Skip the blocking if it is in the exclusion list,0
"best_scores_id is flattened beam x word array, so calculate which",0
word and beam each score came from,0
End condition is when top-of-beam is EOS and no global score.,0
Add from beam until we have minimum outputs.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
!/usr/bin/env python,0
for debugging,0
Statistics,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
Set all logits that are not in the top-k to -1000.,0
This puts the probabilities close to 0.,0
TODO: support these blacklisted features.,0
Encoder forward.,0
"seq_so_far contains chosen tokens; on each step, dim 1 grows by one.",0
Note that what this code calls log_probs are actually logits.,0
Append last prediction.,0
"Store finished hypotheses for this batch. Unlike in beam search,",0
there will only ever be 1 hypothesis per example.,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
TODO: support these blacklisted features.,0
Encoder forward.,0
Tile states and memory beam_size times.,0
Give full probability to the first beam on the first step.,0
Structure that holds finished hypotheses.,0
Multiply probs by the beam probability.,0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Resolve beam origin and true word ids.,0
Map beam_index to batch_index in the flat representation.,0
Append last prediction.,0
Save finished hypotheses.,0
Penalize beams that finished.,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
Reorder states.,0
(0) Prep each of the components of the search.,0
And helper method for reducing verbosity.,0
Define a set of tokens to exclude from ngram-blocking,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
We use now  batch_size x beam_size (same as fast mode),0
"(3) run the decoder to generate sentences, using beam search.",0
(a) Construct batch x beam_size nxt words.,0
Get all the pending current beam words and arrange for forward.,0
(b) Decode and forward,0
(c) Advance each beam.,0
Loop over the batch_size number of beam,0
(4) Extract sentences from beam.,0
Rollback pointer to the beginning.,0
!/usr/bin/env python,0
backwards compatibility for confs,0
load can be called multiple times: modify copy,0
NOTE: translator returns lists of `n_best` list,0
we can ignore that (i.e. flatten lists) only because,0
we restrict `n_best=1`,0
build back results with empty texts,0
Sorting,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"propagate exception to parent process, keeping original traceback",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
is this reachable?,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"Add in default model arguments, possibly added since training.",0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
for backward compatibility,0
Build encoder.,0
why is build_encoder not used here?,0
why is the model_opt.__dict__ check necessary?,1
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
this check is here because audio allows the encoder and decoder to,0
"be different sizes, but other model types do not yet",0
Load checkpoint if we resume from a previous training.,0
Load default opts values then overwrite it with opts from,0
the checkpoint. It's usefull in order to re-train a model,0
after adding a new option (not set in checkpoint),0
check for code where vocab is saved instead of fields,0
(in the future this will be done in a smarter way),0
"Report src and tgt vocab sizes, including for features",0
Build model.,0
Build optimizer.,0
Build model saver,0
this line is kind of a temporary kludge because different objects expect,1
fields to have a different structure,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Option most relevant to image input,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use TensorboardX for visualization during training,0
Options most relevant to speech,0
Option most relevant to image input,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
Option most relevant to image input,0
MARKDOWN boilerplate,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
**section heading**:,0
# **--argument-one**,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
this method unsqueezes its input,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
The encoder hidden is  (layers*directions) x batch x dim.,0
"s_len, batch, emb_dim = emb.size()",0
from onmt.utils.misc import aeq,0
Run the forward pass of every layer of the tranformer.,0
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
2) Calculate and scale scores.,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return one attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
Test image preprocessing,0
Test audio preprocessing,0
Basic attributes.,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
"Set up a separated copy attention layer, if needed.",0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Run the forward pass of the RNN.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
TODO change the way attns is returned dict => list or tuple (onnx),1
Initialize local and return variables.,0
Run the forward pass of the RNN.,0
Check,0
END,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Initialize local and return variables.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Run the forward pass of the copy attention layer.,0
Return result.,0
Basic attributes.,0
Decoder State,0
Build the CNN.,0
CNNDecoder has its own attention mechanism.,0
"Set up a separated copy attention layer, if needed.",0
NOTE: memory_lengths is only here for compatibility reasons,0
with onmt.modules.RNNDecoderBase.forward(),0
Initialize return variables.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Run the forward pass of the CNNDecoder.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"Register self.mask as a buffer in TransformerDecoderLayer, so",0
it gets TransformerDecoderLayer's cuda behavior automatically.,0
Basic attributes.,0
Decoder State,0
Build TransformerDecoder.,0
TransformerDecoder has its own attention mechanism.,0
"Set up a separated copy attention layer, if needed.",0
Initialize return variables.,0
Run the forward pass of the TransformerDecoder.,0
Process the result and update the attentions.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
TODO: Find a better way to check for sparse gradients.,1
Load everything from the checkpoint.,0
Build everything from scratch.,0
"Reset optimizer, keep options.",0
"Reset options, keep optimizer.",0
State can be partially restored.,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Log the progress using the number of batches on the x-axis.,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
These ensure same initialization in multi gpu mode,0
coding: utf-8,0
several data readers need optional dependencies. There's no,0
appropriate builtin exception,0
self.src_vocabs is used in collapse_copy_scores and Translator.py,0
the dataset's self.fields should have the same attributes as examples,0
avoid infinite recursion when fields isn't defined,0
make a small vocab containing just the tokens in the source sequence,0
Map source tokens to indices in the dynamic dict.,0
-*- coding: utf-8 -*-,0
backwards compatibility,0
"cat together layers, producing a 3d output tensor for src text",0
and for tgt (which is assumed to be text),0
"there is a truncate argument as well, but it was never set to",0
anything besides None before,0
the second conjunct means nothing will be filtered at translation time,0
if there is no target data,0
this is basically copy-pasted from torchtext.,0
Load vocabulary,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
Drop the none-using from memory but keep the last,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
temporary fix: See #1196,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: <bos> w1 ... wN <eos>,0
Tgt: w1 ... wN <eos>,0
-*- coding: utf-8 -*-,0
imports of datatype-specific dependencies,0
torchaudio loading options recently changed. It's probably,0
straightforward to rewrite the audio handling to make use of,0
"up-to-date torchaudio, but in the meantime there is a legacy",0
method which uses the old defaults,0
STFT,0
-*- coding: utf-8 -*-,0
domain specific dependencies,0
-*- coding: utf-8 -*-,0
mix this with partial,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
The score for each translation on the beam.,0
The backpointers at each time-step.,0
The outputs at each time-step.,0
Has EOS topped the beam yet.,0
The attentions (matrix) for each time.,0
Time and k pair for finished.,0
Information for global scoring.,0
Minimum prediction length,0
Apply Penalty at every step,0
force the output to be longer than self.min_length,0
Sum the previous scores.,0
Don't let EOS have children.,0
Block ngram repeats,0
"Last n tokens, n = block_ngram_repeat",0
Skip the blocking if it is in the exclusion list,0
"best_scores_id is flattened beam x word array, so calculate which",0
word and beam each score came from,0
End condition is when top-of-beam is EOS and no global score.,0
Add from beam until we have minimum outputs.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
!/usr/bin/env python,0
for debugging,0
Statistics,0
"For temp=0.0, take the argmax to avoid divide-by-zero errors.",0
keep_topk=1 is also equivalent to argmax.,0
Set all logits that are not in the top-k to -1000.,0
This puts the probabilities close to 0.,0
TODO: support these blacklisted features.,0
Encoder forward.,0
"seq_so_far contains chosen tokens; on each step, dim 1 grows by one.",0
Note that what this code calls log_probs are actually logits.,0
Append last prediction.,0
"Store finished hypotheses for this batch. Unlike in beam search,",0
there will only ever be 1 hypothesis per example.,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
TODO: support these blacklisted features.,0
Encoder forward.,0
Tile states and memory beam_size times.,0
Give full probability to the first beam on the first step.,0
Structure that holds finished hypotheses.,0
Multiply probs by the beam probability.,0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Resolve beam origin and true word ids.,0
Map beam_index to batch_index in the flat representation.,0
Append last prediction.,0
Save finished hypotheses.,0
Penalize beams that finished.,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
Reorder states.,0
(0) Prep each of the components of the search.,0
And helper method for reducing verbosity.,0
Define a set of tokens to exclude from ngram-blocking,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
We use now  batch_size x beam_size (same as fast mode),0
"(3) run the decoder to generate sentences, using beam search.",0
(a) Construct batch x beam_size nxt words.,0
Get all the pending current beam words and arrange for forward.,0
(b) Decode and forward,0
(c) Advance each beam.,0
Loop over the batch_size number of beam,0
(4) Extract sentences from beam.,0
Rollback pointer to the beginning.,0
!/usr/bin/env python,0
backwards compatibility for confs,0
load can be called multiple times: modify copy,0
NOTE: translator returns lists of `n_best` list,0
we can ignore that (i.e. flatten lists) only because,0
we restrict `n_best=1`,0
build back results with empty texts,0
Sorting,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"propagate exception to parent process, keeping original traceback",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"the vocab object is a list of tuple (name, torchtext.Vocab)",0
we iterate over this list and associate vocabularies based on the name,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"Add in default model arguments, possibly added since training.",0
"the vocab object is a list of tuple (name, torchtext.Vocab)",0
we iterate over this list and associate vocabularies based on the name,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
for backward compatibility,0
Build encoder.,0
why is build_encoder not used here?,0
why is the model_opt.__dict__ check necessary?,1
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
!/usr/bin/env python,0
this check is here because audio allows the encoder and decoder to,0
"be different sizes, but other model types do not yet",0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
These ensure same initialization in multi gpu mode,0
Load checkpoint if we resume from a previous training.,0
Load default opts values then overwrite it with opts from,0
the checkpoint. It's usefull in order to re-train a model,0
after adding a new option (not set in checkpoint),0
Load a shard dataset to determine the data_type.,0
(All datasets have the same data_type).,0
this should be refactored out of existence reasonably soon,0
Load fields generated from preprocess phase.,0
Report src/tgt features.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Option most relevant to image input,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use TensorboardX for visualization during training,0
Options most relevant to speech,0
Option most relevant to image input,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
Option most relevant to image input,0
MARKDOWN boilerplate,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
**section heading**:,0
# **--argument-one**,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
dec_state = None,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss in shards for memory efficiency.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
The encoder hidden is  (layers*directions) x batch x dim.,0
"s_len, batch, emb_dim = emb.size()",0
from onmt.utils.misc import aeq,0
Run the forward pass of every layer of the tranformer.,0
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
2) Calculate and scale scores.,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return one attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probability of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
"-data option is required, but not used in this test, so dummy.",0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
Test image preprocessing,0
Test audio preprocessing,0
Basic attributes.,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
"Set up a separated copy attention layer, if needed.",0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Run the forward pass of the RNN.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
TODO change the way attns is returned dict => list or tuple (onnx),1
Initialize local and return variables.,0
Run the forward pass of the RNN.,0
Check,0
END,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Initialize local and return variables.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Run the forward pass of the copy attention layer.,0
Return result.,0
Basic attributes.,0
Decoder State,0
Build the CNN.,0
CNNDecoder has its own attention mechanism.,0
"Set up a separated copy attention layer, if needed.",0
NOTE: memory_lengths is only here for compatibility reasons,0
with onmt.modules.RNNDecoderBase.forward(),0
Initialize return variables.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Run the forward pass of the CNNDecoder.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"Register self.mask as a buffer in TransformerDecoderLayer, so",0
it gets TransformerDecoderLayer's cuda behavior automatically.,0
Basic attributes.,0
Decoder State,0
Build TransformerDecoder.,0
TransformerDecoder has its own attention mechanism.,0
"Set up a separated copy attention layer, if needed.",0
Initialize return variables.,0
Run the forward pass of the TransformerDecoder.,0
Process the result and update the attentions.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
We need to save a copy of optim.optimizer.state_dict() for setting,0
"the, optimizer state later on in Stage 2 in this method, since",0
the method optim.set_parameters(model) will overwrite,0
"optim.optimizer, and with ith the values stored in",0
optim.optimizer.state_dict(),0
Stage 1:,0
Essentially optim.set_parameters (re-)creates and optimizer using,0
model.paramters() as parameters that will be stored in the,0
optim.optimizer.param_groups field of the torch optimizer class.,0
"Importantly, this method does not yet load the optimizer state, as",0
essentially it builds a new optimizer with empty optimizer state and,0
parameters from the model.,0
"Stage 2: In this stage, which is only performed when loading an",0
"optimizer from a checkpoint, we load the saved_optimizer_state_dict",0
"into the re-created optimizer, to set the optim.optimizer.state",0
"field, which was previously empty. For this, we use the optimizer",0
"state saved in the ""saved_optimizer_state_dict"" variable for",0
this purpose.,0
See also: https://github.com/pytorch/pytorch/issues/2830,0
Convert back the state values to cuda type if applicable,0
We want to make sure that indeed we have a non-empty optimizer state,0
when we loaded an existing model. This should be at least the case,0
"for Adam, which saves ""exp_avg"" and ""exp_avg_sq"" state",0
(Exponential moving average of gradient and squared gradient values),0
TODO: Find a better way to check for sparse gradients.,1
Decay method used in tensor2tensor.,0
Decay based on start_decay_steps every decay_steps,0
Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf,0
inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch,0
default value from paper,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Log the progress using the number of batches on the x-axis.,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
coding: utf-8,0
This is a hack. Something is broken with torch pickle.,1
Each element of an example is a dictionary whose keys represents,0
at minimum the src tokens and their indices and potentially also,0
the src and tgt features and alignment information.,0
self.src_vocabs is used in collapse_copy_scores and in Translator.py,0
Peek at the first to see which fields are used.,0
why do we need to use different keys from the ones passed in?,0
why does this exist?,0
it would not be necessary to pass unk and pad if the method were,0
called after fields becomes an attribute of self,0
Map source tokens to indices in the dynamic dict.,0
-*- coding: utf-8 -*-,0
only audio has src_lengths,0
everything except audio has src_map and alignment,0
below this: things defined no matter what the data source type is,0
"there is a truncate argument as well, but it was never set to",0
anything besides None before,0
the second conjunct means nothing will be filtered at translation time,0
if there is no target data,0
Prop src from field to get lower memory using when training with image,0
Load vocabulary,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
Drop the none-using from memory but keep the last,0
"All datasets have same num of n_tgt_features,",0
getting the last one is OK.,0
"All datasets have same num of n_src_features,",0
getting the last one is OK.,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
"in the long run, shouldn't it be possible to do this by calling",0
build_vocab with both the src and tgt data?,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: <bos> w1 ... wN <eos>,0
Tgt: w1 ... wN <eos>,0
-*- coding: utf-8 -*-,0
torchaudio loading options recently changed. It's probably,0
straightforward to rewrite the audio handling to make use of,0
"up-to-date torchaudio, but in the meantime there is a legacy",0
method which uses the old defaults,0
STFT,0
-*- coding: utf-8 -*-,0
-*- coding: utf-8 -*-,0
the implicit assumption here is that data that does not come,0
"from a file is already at least semi-tokenized, i.e. split on",0
whitespace. We cannot do modular/user-specified tokenization,0
until that is no longer the case. The fields should handle this.,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
The score for each translation on the beam.,0
The backpointers at each time-step.,0
The outputs at each time-step.,0
Has EOS topped the beam yet.,0
The attentions (matrix) for each time.,0
Time and k pair for finished.,0
Information for global scoring.,0
Minimum prediction length,0
Apply Penalty at every step,0
force the output to be longer than self.min_length,0
Sum the previous scores.,0
Don't let EOS have children.,0
Block ngram repeats,0
"Last n tokens, n = block_ngram_repeat",0
Skip the blocking if it is in the exclusion list,0
"best_scores_id is flattened beam x word array, so calculate which",0
word and beam each score came from,0
End condition is when top-of-beam is EOS and no global score.,0
Add from beam until we have minimum outputs.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
!/usr/bin/env python,0
for debugging,0
Statistics,0
Turn any copied words into UNKs.,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
TODO: faster code path for beam_size == 1.,1
TODO: support these blacklisted features.,0
Encoder forward.,0
Tile states and memory beam_size times.,0
Give full probability to the first beam on the first step.,0
Structure that holds finished hypotheses.,0
Multiply probs by the beam probability.,0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Resolve beam origin and true word ids.,0
Map beam_index to batch_index in the flat representation.,0
Append last prediction.,0
Save finished hypotheses.,0
Penalize beams that finished.,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
Reorder states.,0
(0) Prep each of the components of the search.,0
And helper method for reducing verbosity.,0
Define a set of tokens to exclude from ngram-blocking,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
We use now  batch_size x beam_size (same as fast mode),0
"(3) run the decoder to generate sentences, using beam search.",0
(a) Construct batch x beam_size nxt words.,0
Get all the pending current beam words and arrange for forward.,0
(b) Decode and forward,0
(c) Advance each beam.,0
Loop over the batch_size number of beam,0
(4) Extract sentences from beam.,0
Rollback pointer to the beginning.,0
!/usr/bin/env python,0
backwards compatibility for confs,0
load can be called multiple times: modify copy,0
NOTE: translator returns lists of `n_best` list,0
we can ignore that (i.e. flatten lists) only because,0
we restrict `n_best=1`,0
build back results with empty texts,0
Sorting,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
We will use glob.glob() to find sharded {train|valid}.[0-9]*.pt,0
"when training, so check to avoid tampering with existing pt files",0
or mixing them up.,0
"We save fields in vocab.pt seperately, so make it empty.",0
"For data_type == 'img' or 'audio', currently we don't do",0
preprocess sharding. We only build a monolithic dataset.,0
"But since the interfaces are uniform, it would be not hard",0
to do this should users need this feature.,0
"We save fields in vocab.pt seperately, so make it empty.",0
"Can't save fields, so remove/reconstruct at training time.",0
!/usr/bin/env python,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"propagate exception to parent process, keeping original traceback",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"the vocab object is a list of tuple (name, torchtext.Vocab)",0
we iterate over this list and associate vocabularies based on the name,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"Add in default model arguments, possibly added since training.",0
"the vocab object is a list of tuple (name, torchtext.Vocab)",0
we iterate over this list and associate vocabularies based on the name,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
"""rnn"" or ""brnn""",0
for backward compatibility,0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
This preserves backward-compat for models using customed layernorm,0
end of patch for backward compatibility,0
Add generator to model (this registers it as parameter of model).,0
!/usr/bin/env python,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
These ensure same initialization in multi gpu mode,0
Load checkpoint if we resume from a previous training.,0
Load default opts values then overwrite it with opts from,0
the checkpoint. It's usefull in order to re-train a model,0
after adding a new option (not set in checkpoint),0
Peek the first dataset to determine the data_type.,0
(All datasets have the same data_type).,0
Load fields generated from preprocess phase.,0
Report src/tgt features.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Do training.,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add('--residual', '-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Option most relevant to image input,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use TensorboardX for visualization during training,0
Options most relevant to speech,0
Option most relevant to image input,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
Option most relevant to image input,0
MARKDOWN boilerplate,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
**section heading**:,0
# **--argument-one**,0
"Get the key 'value' in the dict, or just use 'value'",0
Basic attributes.,0
Set model in training mode.,0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
dec_state = None,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss in shards for memory efficiency.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
TO CHECK,0
if dec_state is not None:,0
dec_state.detach(),0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
The encoder hidden is  (layers*directions) x batch x dim.,0
"s_len, batch, emb_dim = emb.size()",0
from onmt.utils.misc import aeq,0
Run the forward pass of every layer of the tranformer.,0
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
2) Calculate and scale scores.,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return one attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probibility of not copying: p_{word}(w) * (1 - p(z)),0
probabilities assigned by the model to the gold targets,0
probability of tokens copied from source,0
Set scores for unk to 0 and add eps,0
find the indices in which you do not use the copy mechanism,0
Drop padding.,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
this block does not depend on the loss value computed above,0
and is used only for stats,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
this part looks like it belongs in CopyGeneratorLoss,0
Compute Loss as NLL divided by seq length,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
"-data option is required, but not used in this test, so dummy.",0
Helper to generate a vocabulary,0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
4 specicials + 2 words (since we pass 2 to merge_vocabs),0
Test image preprocessing,0
Test audio preprocessing,0
Basic attributes.,0
Decoder state,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
"Set up a separated copy attention layer, if needed.",0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Init the input feed.,0
Run the forward pass of the RNN.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
TODO change the way attns is returned dict => list or tuple (onnx),1
Initialize local and return variables.,0
Run the forward pass of the RNN.,0
Check,0
END,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Initialize local and return variables.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Run the forward pass of the copy attention layer.,0
Return result.,0
Basic attributes.,0
Decoder State,0
Build the CNN.,0
CNNDecoder has its own attention mechanism.,0
"Set up a separated copy attention layer, if needed.",0
NOTE: memory_lengths is only here for compatibility reasons,0
with onmt.modules.RNNDecoderBase.forward(),0
Initialize return variables.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Run the forward pass of the CNNDecoder.,0
Process the result and update the attentions.,0
Update the state.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"Register self.mask as a buffer in TransformerDecoderLayer, so",0
it gets TransformerDecoderLayer's cuda behavior automatically.,0
Basic attributes.,0
Decoder State,0
Build TransformerDecoder.,0
TransformerDecoder has its own attention mechanism.,0
"Set up a separated copy attention layer, if needed.",0
Initialize return variables.,0
Run the forward pass of the TransformerDecoder.,0
Process the result and update the attentions.,0
TODO change the way attns is returned dict => list or tuple (onnx),1
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
We need to save a copy of optim.optimizer.state_dict() for setting,0
"the, optimizer state later on in Stage 2 in this method, since",0
the method optim.set_parameters(model.parameters()) will overwrite,0
"optim.optimizer, and with ith the values stored in",0
optim.optimizer.state_dict(),0
Stage 1:,0
Essentially optim.set_parameters (re-)creates and optimizer using,0
model.paramters() as parameters that will be stored in the,0
optim.optimizer.param_groups field of the torch optimizer class.,0
"Importantly, this method does not yet load the optimizer state, as",0
essentially it builds a new optimizer with empty optimizer state and,0
parameters from the model.,0
"Stage 2: In this stage, which is only performed when loading an",0
"optimizer from a checkpoint, we load the saved_optimizer_state_dict",0
"into the re-created optimizer, to set the optim.optimizer.state",0
"field, which was previously empty. For this, we use the optimizer",0
"state saved in the ""saved_optimizer_state_dict"" variable for",0
this purpose.,0
See also: https://github.com/pytorch/pytorch/issues/2830,0
Convert back the state values to cuda type if applicable,0
We want to make sure that indeed we have a non-empty optimizer state,0
when we loaded an existing model. This should be at least the case,0
"for Adam, which saves ""exp_avg"" and ""exp_avg_sq"" state",0
(Exponential moving average of gradient and squared gradient values),0
Decay method used in tensor2tensor.,0
Decay based on start_decay_steps every decay_steps,0
-*- coding: utf-8 -*-,0
if the loss function operates on vectors of raw logits instead of,0
"probabilities, only the first part of the generator needs to be",0
"passed to the NMTLossCompute. At the moment, the only supported",0
loss function of this kind is the sparsemax loss.,0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Log the progress using the number of batches on the x-axis.,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
coding: utf-8,0
Below are helper functions for intra-class use only.,0
-*- coding: utf-8 -*-,0
Hack. Can't pickle defaultdict :(,1
"For all data types, the tgt side corpus is in form of text.",0
Prop src from field to get lower memory using when training with image,0
Load vocabulary,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
Drop the none-using from memory but keep the last,0
"All datasets have same num of n_tgt_features,",0
getting the last one is OK.,0
"All datasets have same num of n_src_features,",0
getting the last one is OK.,0
Merge the input and output vocabularies.,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
We have at least one dataset.,0
"We return the len of cur_dataset, otherwise we need to load",0
"all datasets to determine the real len, which loses the benefit",0
of lazy loading.,0
Drop the current dataset for decreasing memory,0
"We clear `fields` when saving, restore when loading.",0
Sort batch by decreasing lengths of sentence required by pytorch.,0
"sort=False means ""Use dataset's sortkey instead of iterator's"".",0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: <bos> w1 ... wN <eos>,0
Tgt: w1 ... wN <eos>,0
Sort the glob output by file name (by increasing indexes).,0
"Only one inputters.*Dataset, simple!",0
-*- coding: utf-8 -*-,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
STFT,0
"The codecs module seems to have bugs with seek()/tell(),",0
so we use io.open().,0
"We have associate iterator, just yields tuples",0
util we run parallel with it.,0
Yield tuples util this shard's size reaches the threshold.,0
-*- coding: utf-8 -*-,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
-*- coding: utf-8 -*-,0
"self.src_vocabs: mutated in dynamic_dict, used in",0
collapse_copy_scores and in Translator.py,0
Each element of an example is a dictionary whose keys represents,0
at minimum the src tokens and their indices and potentially also,0
the src and tgt features and alignment information.,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
"Default to a balanced sort, prioritizing tgt len match.",0
TODO: make this configurable.,1
"All examples have same number of features, so we peek first one",0
to get the num_feats.,0
Chain back the first element - we only want to peek it.,0
Below are helper functions for intra-class use only.,0
Mapping source tokens to indices in the dynamic dict.,0
"The codecs module seems to have bugs with seek()/tell(),",0
so we use io.open().,0
"We have associate iterator, just yields tuples",0
util we run parallel with it.,0
Yield tuples util this shard's size reaches the threshold.,0
This part of check is time consuming on Py2 (but,0
"it is quite fast on Py3, weird!). So we don't bother",0
to check for very line. Instead we chekc every 64,0
lines. Thus we are not dividing exactly per,0
"`shard_size`, but it is not too much difference.",0
All examples must have same number of features.,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
The score for each translation on the beam.,0
The backpointers at each time-step.,0
The outputs at each time-step.,0
Has EOS topped the beam yet.,0
The attentions (matrix) for each time.,0
Time and k pair for finished.,0
Information for global scoring.,0
Minimum prediction length,0
Apply Penalty at every step,0
force the output to be longer than self.min_length,0
Sum the previous scores.,0
Don't let EOS have children.,0
Block ngram repeats,0
"Last n tokens, n = block_ngram_repeat",0
Skip the blocking if it is in the exclusion list,0
"best_scores_id is flattened beam x word array, so calculate which",0
word and beam each score came from,0
End condition is when top-of-beam is EOS and no global score.,0
Add from beam until we have minimum outputs.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
!/usr/bin/env python,0
for debugging,0
Statistics,0
Debug attention.,0
Turn any copied words to UNKs (index 0).,0
"Decoder forward, takes [tgt_len, batch, nfeats] as input",0
"and [src_len, batch, hidden] as memory_bank",0
"in case of inference tgt_len = 1, batch = beam times batch_size",0
"in case of Gold Scoring tgt_len = actual length, batch = 1 batch",0
Generator forward.,0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]",0
"returns [(batch_size x beam_size) , vocab ] when 1 step",0
"or [ tgt_len, batch_size, vocab ] when full sentence",0
TODO: faster code path for beam_size == 1.,1
TODO: support these blacklisted features.,0
Encoder forward.,0
Tile states and memory beam_size times.,0
Give full probability to the first beam on the first step.,0
Structure that holds finished hypotheses.,0
Multiply probs by the beam probability.,0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Resolve beam origin and true word ids.,0
Map beam_index to batch_index in the flat representation.,0
Append last prediction.,0
Save finished hypotheses.,0
Penalize beams that finished.,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
Reorder states.,0
(0) Prep each of the components of the search.,0
And helper method for reducing verbosity.,0
Define a list of tokens to exclude from ngram-blocking,0
"exclusion_list = [""<t>"", ""</t>"", "".""]",0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
We use now  batch_size x beam_size (same as fast mode),0
"(3) run the decoder to generate sentences, using beam search.",0
(a) Construct batch x beam_size nxt words.,0
Get all the pending current beam words and arrange for forward.,0
(b) Decode and forward,0
(c) Advance each beam.,0
Loop over the batch_size number of beam,0
(4) Extract sentences from beam.,0
Rollback pointer to the beginning.,0
!/usr/bin/env python,0
backwards compatibility for confs,0
load can be called multiple times: modify copy,0
NOTE: translator returns lists of `n_best` list,0
we can ignore that (i.e. flatten lists) only because,0
we restrict `n_best=1`,0
build back results with empty texts,0
Sorting,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
We will use glob.glob() to find sharded {train|valid}.[0-9]*.pt,0
"when training, so check to avoid tampering with existing pt files",0
or mixing them up.,0
"We save fields in vocab.pt seperately, so make it empty.",0
"For data_type == 'img' or 'audio', currently we don't do",0
preprocess sharding. We only build a monolithic dataset.,0
"But since the interfaces are uniform, it would be not hard",0
to do this should users need this feature.,0
"We save fields in vocab.pt seperately, so make it empty.",0
"Can't save fields, so remove/reconstruct at training time.",0
!/usr/bin/env python,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"propagate exception to parent process, keeping original traceback",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"the vocab object is a list of tuple (name, torchtext.Vocab)",0
we iterate over this list and associate vocabularies based on the name,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"Add in default model arguments, possibly added since training.",0
"the vocab object is a list of tuple (name, torchtext.Vocab)",0
we iterate over this list and associate vocabularies based on the name,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
"""rnn"" or ""brnn""",0
for backward compatibility,0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
Add generator to model (this registers it as parameter of model).,0
!/usr/bin/env python,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
These ensure same initialization in multi gpu mode,0
Load checkpoint if we resume from a previous training.,0
Peek the first dataset to determine the data_type.,0
(All datasets have the same data_type).,0
Load fields generated from preprocess phase.,0
Report src/tgt features.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Do training.,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add_argument('-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Option most relevant to image input,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use TensorboardX for visualization during training,0
Options most relevant to speech,0
Option most relevant to image input,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
Option most relevant to image input,0
MARKDOWN boilerplate,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
**section heading**:,0
# **--argument-one**,0
Basic attributes.,0
Set model in training mode.,0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss in shards for memory efficiency.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
The encoder hidden is  (layers*directions) x batch x dim.,0
"s_len, batch, emb_dim = emb.size()",0
from onmt.utils.misc import aeq,0
Run the forward pass of every layer of the tranformer.,0
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
2) Calculate and scale scores.,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return one attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probibility of not copying: p_{word}(w) * (1 - p(z)),0
Compute unks in align and target for readability,0
Copy probability of tokens in source,0
Set scores for unk to 0 and add eps,0
Get scores for tokens in target,0
Regular prob (no unks and unks that can't be copied),0
Add score for non-unks in target,0
Add score for when word is unk in both align and tgt,0
Forced copy. Add only probability for not-copied tokens,0
Drop padding.,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
Compute Loss as NLL divided by seq length,0
Compute Sequence Lengths,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
"-data option is required, but not used in this test, so dummy.",0
Helper to generate a vocabulary,0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
4 specicials + 2 words (since we pass 2 to merge_vocabs),0
Test image preprocessing,0
Test audio preprocessing,0
Basic attributes.,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
"Set up a separated copy attention layer, if needed.",0
Check,0
tgt.size() returns tgt length and batch,0
END,0
Run the forward pass of the RNN.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: decoder_outputs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Initialize local and return variables.,0
Run the forward pass of the RNN.,0
Check,0
END,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Initialize local and return variables.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Run the forward pass of the copy attention layer.,0
Return result.,0
Init the input feed.,0
Basic attributes.,0
Build the CNN.,0
CNNDecoder has its own attention mechanism.,0
"Set up a separated copy attention layer, if needed.",0
NOTE: memory_lengths is only here for compatibility reasons,0
with onmt.modules.RNNDecoderBase.forward(),0
CHECKS,0
END CHECKS,0
Initialize return variables.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Run the forward pass of the CNNDecoder.,0
Process the result and update the attentions.,0
Update the state.,0
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"Register self.mask as a buffer in TransformerDecoderLayer, so",0
it gets TransformerDecoderLayer's cuda behavior automatically.,0
Basic attributes.,0
Build TransformerDecoder.,0
TransformerDecoder has its own attention mechanism.,0
"Set up a separated copy attention layer, if needed.",0
Initialize return variables.,0
Run the forward pass of the TransformerDecoder.,0
Process the result and update the attentions.,0
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
We need to save a copy of optim.optimizer.state_dict() for setting,0
"the, optimizer state later on in Stage 2 in this method, since",0
the method optim.set_parameters(model.parameters()) will overwrite,0
"optim.optimizer, and with ith the values stored in",0
optim.optimizer.state_dict(),0
Stage 1:,0
Essentially optim.set_parameters (re-)creates and optimizer using,0
model.paramters() as parameters that will be stored in the,0
optim.optimizer.param_groups field of the torch optimizer class.,0
"Importantly, this method does not yet load the optimizer state, as",0
essentially it builds a new optimizer with empty optimizer state and,0
parameters from the model.,0
"Stage 2: In this stage, which is only performed when loading an",0
"optimizer from a checkpoint, we load the saved_optimizer_state_dict",0
"into the re-created optimizer, to set the optim.optimizer.state",0
"field, which was previously empty. For this, we use the optimizer",0
"state saved in the ""saved_optimizer_state_dict"" variable for",0
this purpose.,0
See also: https://github.com/pytorch/pytorch/issues/2830,0
Convert back the state values to cuda type if applicable,0
We want to make sure that indeed we have a non-empty optimizer state,0
when we loaded an existing model. This should be at least the case,0
"for Adam, which saves ""exp_avg"" and ""exp_avg_sq"" state",0
(Exponential moving average of gradient and squared gradient values),0
Decay method used in tensor2tensor.,0
Decay based on start_decay_steps every decay_steps,0
-*- coding: utf-8 -*-,0
"for sparsemax loss, the loss function operates on the raw output",0
"vector, not a probability vector. Hence it's only necessary to",0
apply the first part of the generator here.,0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Log the progress using the number of batches on the x-axis.,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
coding: utf-8,0
Below are helper functions for intra-class use only.,0
-*- coding: utf-8 -*-,0
Hack. Can't pickle defaultdict :(,1
"For all data types, the tgt side corpus is in form of text.",0
Prop src from field to get lower memory using when training with image,0
Load vocabulary,0
keep the order of tokens specified in the vocab file by,0
adding them to the counter with decreasing counting values,0
Drop the none-using from memory but keep the last,0
"All datasets have same num of n_tgt_features,",0
getting the last one is OK.,0
"All datasets have same num of n_src_features,",0
getting the last one is OK.,0
Merge the input and output vocabularies.,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
We have at least one dataset.,0
"We return the len of cur_dataset, otherwise we need to load",0
"all datasets to determine the real len, which loses the benefit",0
of lazy loading.,0
Drop the current dataset for decreasing memory,0
"We clear `fields` when saving, restore when loading.",0
Sort batch by decreasing lengths of sentence required by pytorch.,0
"sort=False means ""Use dataset's sortkey instead of iterator's"".",0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: <bos> w1 ... wN <eos>,0
Tgt: w1 ... wN <eos>,0
Sort the glob output by file name (by increasing indexes).,0
"Only one inputters.*Dataset, simple!",0
-*- coding: utf-8 -*-,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
STFT,0
"The codecs module seems to have bugs with seek()/tell(),",0
so we use io.open().,0
"We have associate iterator, just yields tuples",0
util we run parallel with it.,0
Yield tuples util this shard's size reaches the threshold.,0
-*- coding: utf-8 -*-,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
-*- coding: utf-8 -*-,0
"self.src_vocabs: mutated in dynamic_dict, used in",0
collapse_copy_scores and in Translator.py,0
Each element of an example is a dictionary whose keys represents,0
at minimum the src tokens and their indices and potentially also,0
the src and tgt features and alignment information.,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
"Default to a balanced sort, prioritizing tgt len match.",0
TODO: make this configurable.,1
"All examples have same number of features, so we peek first one",0
to get the num_feats.,0
Chain back the first element - we only want to peek it.,0
Below are helper functions for intra-class use only.,0
Mapping source tokens to indices in the dynamic dict.,0
"The codecs module seems to have bugs with seek()/tell(),",0
so we use io.open().,0
"We have associate iterator, just yields tuples",0
util we run parallel with it.,0
Yield tuples util this shard's size reaches the threshold.,0
This part of check is time consuming on Py2 (but,0
"it is quite fast on Py3, weird!). So we don't bother",0
to check for very line. Instead we chekc every 64,0
lines. Thus we are not dividing exactly per,0
"`shard_size`, but it is not too much difference.",0
All examples must have same number of features.,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
Not yet supported on multi-gpu,0
The score for each translation on the beam.,0
The backpointers at each time-step.,0
The outputs at each time-step.,0
Has EOS topped the beam yet.,0
The attentions (matrix) for each time.,0
Time and k pair for finished.,0
Information for global scoring.,0
Minimum prediction length,0
Apply Penalty at every step,0
force the output to be longer than self.min_length,0
Sum the previous scores.,0
Don't let EOS have children.,0
Block ngram repeats,0
"Last n tokens, n = block_ngram_repeat",0
Skip the blocking if it is in the exclusion list,0
"best_scores_id is flattened beam x word array, so calculate which",0
word and beam each score came from,0
End condition is when top-of-beam is EOS and no global score.,0
Add from beam until we have minimum outputs.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
!/usr/bin/env python,0
use ensemble decoding if more than one model is specified,0
for debugging,0
Statistics,0
Debug attention.,0
TODO: faster code path for beam_size == 1.,1
TODO: support these blacklisted features.,0
Encoder forward.,0
Tile states and memory beam_size times.,0
Give full probability to the first beam on the first step.,0
Structure that holds finished hypotheses.,0
Decoder forward.,0
Generator forward.,0
Multiply probs by the beam probability.,0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Resolve beam origin and true word ids.,0
Map beam_index to batch_index in the flat representation.,0
Append last prediction.,0
Save finished hypotheses.,0
Penalize beams that finished.,0
Store finished hypotheses for this batch.,0
End condition is the top beam finished and we can return,0
n_best hypotheses.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
Reorder states.,0
(0) Prep each of the components of the search.,0
And helper method for reducing verbosity.,0
Define a list of tokens to exclude from ngram-blocking,0
"exclusion_list = [""<t>"", ""</t>"", "".""]",0
Help functions for working with beams and batches,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
"(3) run the decoder to generate sentences, using beam search.",0
Construct batch x beam_size nxt words.,0
Get all the pending current beam words and arrange for forward.,0
Turn any copied words to UNKs,0
0 is unk,0
Temporary kludge solution to handle changed dim expectation,1
in the decoder,0
Run one step.,0
dec_out: beam x rnn_size,0
(b) Compute a vector of batch x beam word scores.,0
beam x tgt_vocab,0
beam x (tgt_vocab + extra_vocab),0
beam x tgt_vocab,0
(c) Advance each beam.,0
(4) Extract sentences from beam.,0
(1) run the encoder on the src,0
"(2) if a target is specified, compute the 'goldScore'",0
(i.e. log likelihood) of the target under the model,0
Log prob of each word.,0
Rollback pointer to the beginning.,0
!/usr/bin/env python,0
backwards compatibility for confs,0
load can be called multiple times: modify copy,0
NOTE: translator returns lists of `n_best` list,0
we can ignore that (i.e. flatten lists) only because,0
we restrict `n_best=1`,0
build back results with empty texts,0
Sorting,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
We will use glob.glob() to find sharded {train|valid}.[0-9]*.pt,0
"when training, so check to avoid tampering with existing pt files",0
or mixing them up.,0
"We save fields in vocab.pt seperately, so make it empty.",0
"For data_type == 'img' or 'audio', currently we don't do",0
preprocess sharding. We only build a monolithic dataset.,0
"But since the interfaces are uniform, it would be not hard",0
to do this should users need this feature.,0
"We save fields in vocab.pt seperately, so make it empty.",0
"Can't save fields, so remove/reconstruct at training time.",0
!/usr/bin/env python,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"propagate exception to parent process, keeping original traceback",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"the vocab object is a list of tuple (name, torchtext.Vocab)",0
we iterate over this list and associate vocabularies based on the name,0
"Add in default model arguments, possibly added since training.",0
"the vocab object is a list of tuple (name, torchtext.Vocab)",0
we iterate over this list and associate vocabularies based on the name,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
"""rnn"" or ""brnn""",0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
Add generator to model (this registers it as parameter of model).,0
!/usr/bin/env python,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
These ensure same initialization in multi gpu mode,0
Load checkpoint if we resume from a previous training.,0
Peek the first dataset to determine the data_type.,0
(All datasets have the same data_type).,0
Load fields generated from preprocess phase.,0
Report src/tgt features.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Do training.,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add_argument('-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Option most relevant to image input,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use TensorboardX for visualization during training,0
Options most relevant to speech,0
Option most relevant to image input,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
Option most relevant to image input,0
MARKDOWN boilerplate,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
**section heading**:,0
# **--argument-one**,0
Basic attributes.,0
Set model in training mode.,0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss in shards for memory efficiency.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
The encoder hidden is  (layers*directions) x batch x dim.,0
"s_len, batch, emb_dim = emb.size()",0
from onmt.utils.misc import aeq,0
Run the forward pass of every layer of the tranformer.,0
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
2) Calculate and scale scores.,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return one attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probibility of not copying: p_{word}(w) * (1 - p(z)),0
Compute unks in align and target for readability,0
Copy probability of tokens in source,0
Set scores for unk to 0 and add eps,0
Get scores for tokens in target,0
Regular prob (no unks and unks that can't be copied),0
Add score for non-unks in target,0
Add score for when word is unk in both align and tgt,0
Forced copy. Add only probability for not-copied tokens,0
Drop padding.,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
Compute Loss as NLL divided by seq length,0
Compute Sequence Lengths,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
"-data option is required, but not used in this test, so dummy.",0
Helper to generate a vocabulary,0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
4 specicials + 2 words (since we pass 2 to merge_vocabs),0
Test image preprocessing,0
Test audio preprocessing,0
Basic attributes.,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
"Set up a separated copy attention layer, if needed.",0
Check,0
tgt.size() returns tgt length and batch,0
END,0
Run the forward pass of the RNN.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: decoder_outputs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Initialize local and return variables.,0
Run the forward pass of the RNN.,0
Check,0
END,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Initialize local and return variables.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Run the forward pass of the copy attention layer.,0
Return result.,0
Init the input feed.,0
Basic attributes.,0
Build the CNN.,0
CNNDecoder has its own attention mechanism.,0
"Set up a separated copy attention layer, if needed.",0
NOTE: memory_lengths is only here for compatibility reasons,0
with onmt.modules.RNNDecoderBase.forward(),0
CHECKS,0
END CHECKS,0
Initialize return variables.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Run the forward pass of the CNNDecoder.,0
Process the result and update the attentions.,0
Update the state.,0
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"Register self.mask as a buffer in TransformerDecoderLayer, so",0
it gets TransformerDecoderLayer's cuda behavior automatically.,0
Basic attributes.,0
Build TransformerDecoder.,0
TransformerDecoder has its own attention mechanism.,0
"Set up a separated copy attention layer, if needed.",0
Initialize return variables.,0
Run the forward pass of the TransformerDecoder.,0
Process the result and update the attentions.,0
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
We need to save a copy of optim.optimizer.state_dict() for setting,0
"the, optimizer state later on in Stage 2 in this method, since",0
the method optim.set_parameters(model.parameters()) will overwrite,0
"optim.optimizer, and with ith the values stored in",0
optim.optimizer.state_dict(),0
Stage 1:,0
Essentially optim.set_parameters (re-)creates and optimizer using,0
model.paramters() as parameters that will be stored in the,0
optim.optimizer.param_groups field of the torch optimizer class.,0
"Importantly, this method does not yet load the optimizer state, as",0
essentially it builds a new optimizer with empty optimizer state and,0
parameters from the model.,0
"Stage 2: In this stage, which is only performed when loading an",0
"optimizer from a checkpoint, we load the saved_optimizer_state_dict",0
"into the re-created optimizer, to set the optim.optimizer.state",0
"field, which was previously empty. For this, we use the optimizer",0
"state saved in the ""saved_optimizer_state_dict"" variable for",0
this purpose.,0
See also: https://github.com/pytorch/pytorch/issues/2830,0
Convert back the state values to cuda type if applicable,0
We want to make sure that indeed we have a non-empty optimizer state,0
when we loaded an existing model. This should be at least the case,0
"for Adam, which saves ""exp_avg"" and ""exp_avg_sq"" state",0
(Exponential moving average of gradient and squared gradient values),0
Decay method used in tensor2tensor.,0
Decay based on start_decay_steps every decay_steps,0
-*- coding: utf-8 -*-,0
"for sparsemax loss, the loss function operates on the raw output",0
"vector, not a probability vector. Hence it's only necessary to",0
apply the first part of the generator here.,0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Log the progress using the number of batches on the x-axis.,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
coding: utf-8,0
Below are helper functions for intra-class use only.,0
-*- coding: utf-8 -*-,0
Hack. Can't pickle defaultdict :(,1
"For all data types, the tgt side corpus is in form of text.",0
Prop src from field to get lower memory using when training with image,0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"All datasets have same num of n_tgt_features,",0
getting the last one is OK.,0
"All datasets have same num of n_src_features,",0
getting the last one is OK.,0
Merge the input and output vocabularies.,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
We have at least one dataset.,0
"We return the len of cur_dataset, otherwise we need to load",0
"all datasets to determine the real len, which loses the benefit",0
of lazy loading.,0
Drop the current dataset for decreasing memory,0
"We clear `fields` when saving, restore when loading.",0
Sort batch by decreasing lengths of sentence required by pytorch.,0
"sort=False means ""Use dataset's sortkey instead of iterator's"".",0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: <bos> w1 ... wN <eos>,0
Tgt: w1 ... wN <eos>,0
Sort the glob output by file name (by increasing indexes).,0
"Only one inputters.*Dataset, simple!",0
-*- coding: utf-8 -*-,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
STFT,0
"The codecs module seems to have bugs with seek()/tell(),",0
so we use io.open().,0
"We have associate iterator, just yields tuples",0
util we run parallel with it.,0
Yield tuples util this shard's size reaches the threshold.,0
-*- coding: utf-8 -*-,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
-*- coding: utf-8 -*-,0
"self.src_vocabs: mutated in dynamic_dict, used in",0
collapse_copy_scores and in Translator.py,0
Each element of an example is a dictionary whose keys represents,0
at minimum the src tokens and their indices and potentially also,0
the src and tgt features and alignment information.,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
"Default to a balanced sort, prioritizing tgt len match.",0
TODO: make this configurable.,1
"All examples have same number of features, so we peek first one",0
to get the num_feats.,0
Chain back the first element - we only want to peek it.,0
Below are helper functions for intra-class use only.,0
Mapping source tokens to indices in the dynamic dict.,0
"The codecs module seems to have bugs with seek()/tell(),",0
so we use io.open().,0
"We have associate iterator, just yields tuples",0
util we run parallel with it.,0
Yield tuples util this shard's size reaches the threshold.,0
This part of check is time consuming on Py2 (but,0
"it is quite fast on Py3, weird!). So we don't bother",0
to check for very line. Instead we chekc every 64,0
lines. Thus we are not dividing exactly per,0
"`shard_size`, but it is not too much difference.",0
All examples must have same number of features.,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
Not yet supported on multi-gpu,0
The score for each translation on the beam.,0
The backpointers at each time-step.,0
The outputs at each time-step.,0
Has EOS topped the beam yet.,0
The attentions (matrix) for each time.,0
Time and k pair for finished.,0
Information for global scoring.,0
Minimum prediction length,0
Apply Penalty at every step,0
force the output to be longer than self.min_length,0
Sum the previous scores.,0
Don't let EOS have children.,0
Block ngram repeats,0
"Last n tokens, n = block_ngram_repeat",0
Skip the blocking if it is in the exclusion list,0
"best_scores_id is flattened beam x word array, so calculate which",0
word and beam each score came from,0
End condition is when top-of-beam is EOS and no global score.,0
Add from beam until we have minimum outputs.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
!/usr/bin/env python,0
use ensemble decoding if more than one model is specified,0
for debugging,0
Statistics,0
Debug attention.,0
TODO: faster code path for beam_size == 1.,1
TODO: support these blacklisted features.,0
Encoder forward.,0
Tile states and memory beam_size times.,0
Give full probability to the first beam on the first step.,0
Structure that holds finished hypotheses.,0
Decoder forward.,0
Generator forward.,0
Multiply probs by the beam probability.,0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Resolve beam origin and true word ids.,0
Map beam_index to batch_index in the flat representation.,0
Append last prediction.,0
End condition is top beam is finished.,0
Save finished hypotheses.,0
Store finished hypotheses for this batch.,0
"If the batch reached the end, save the n_best hypotheses.",0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
Reorder states.,0
(0) Prep each of the components of the search.,0
And helper method for reducing verbosity.,0
Define a list of tokens to exclude from ngram-blocking,0
"exclusion_list = [""<t>"", ""</t>"", "".""]",0
Help functions for working with beams and batches,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
"(3) run the decoder to generate sentences, using beam search.",0
Construct batch x beam_size nxt words.,0
Get all the pending current beam words and arrange for forward.,0
Turn any copied words to UNKs,0
0 is unk,0
Temporary kludge solution to handle changed dim expectation,1
in the decoder,0
Run one step.,0
dec_out: beam x rnn_size,0
(b) Compute a vector of batch x beam word scores.,0
beam x tgt_vocab,0
beam x (tgt_vocab + extra_vocab),0
beam x tgt_vocab,0
(c) Advance each beam.,0
(4) Extract sentences from beam.,0
(1) run the encoder on the src,0
"(2) if a target is specified, compute the 'goldScore'",0
(i.e. log likelihood) of the target under the model,0
Log prob of each word.,0
Rollback pointer to the beginning.,0
!/usr/bin/env python,0
backwards compatibility for confs,0
load can be called multiple times: modify copy,0
NOTE: translator returns lists of `n_best` list,0
we can ignore that (i.e. flatten lists) only because,0
we restrict `n_best=1`,0
build back results with empty texts,0
Sorting,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
We will use glob.glob() to find sharded {train|valid}.[0-9]*.pt,0
"when training, so check to avoid tampering with existing pt files",0
or mixing them up.,0
"We save fields in vocab.pt seperately, so make it empty.",0
"For data_type == 'img' or 'audio', currently we don't do",0
preprocess sharding. We only build a monolithic dataset.,0
"But since the interfaces are uniform, it would be not hard",0
to do this should users need this feature.,0
"We save fields in vocab.pt seperately, so make it empty.",0
"Can't save fields, so remove/reconstruct at training time.",0
!/usr/bin/env python,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"propagate exception to parent process, keeping original traceback",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"the vocab object is a list of tuple (name, torchtext.Vocab)",0
we iterate over this list and associate vocabularies based on the name,0
"Add in default model arguments, possibly added since training.",0
"the vocab object is a list of tuple (name, torchtext.Vocab)",0
we iterate over this list and associate vocabularies based on the name,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
"""rnn"" or ""brnn""",0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
Add generator to model (this registers it as parameter of model).,0
!/usr/bin/env python,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
These ensure same initialization in multi gpu mode,0
Load checkpoint if we resume from a previous training.,0
Peek the first dataset to determine the data_type.,0
(All datasets have the same data_type).,0
Load fields generated from preprocess phase.,0
Report src/tgt features.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Do training.,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add_argument('-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Option most relevant to image input,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use TensorboardX for visualization during training,0
Options most relevant to speech,0
Option most relevant to image input,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
Option most relevant to image input,0
MARKDOWN boilerplate,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
**section heading**:,0
# **--argument-one**,0
Basic attributes.,0
Set model in training mode.,0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss in shards for memory efficiency.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
The encoder hidden is  (layers*directions) x batch x dim.,0
"s_len, batch, emb_dim = emb.size()",0
from onmt.utils.misc import aeq,0
Run the forward pass of every layer of the tranformer.,0
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
2) Calculate and scale scores.,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return one attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probibility of not copying: p_{word}(w) * (1 - p(z)),0
Compute unks in align and target for readability,0
Copy probability of tokens in source,0
Set scores for unk to 0 and add eps,0
Get scores for tokens in target,0
Regular prob (no unks and unks that can't be copied),0
Add score for non-unks in target,0
Add score for when word is unk in both align and tgt,0
Forced copy. Add only probability for not-copied tokens,0
Drop padding.,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
Compute Loss as NLL divided by seq length,0
Compute Sequence Lengths,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
"-data option is required, but not used in this test, so dummy.",0
Helper to generate a vocabulary,0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
4 specicials + 2 words (since we pass 2 to merge_vocabs),0
Test image preprocessing,0
Test audio preprocessing,0
Basic attributes.,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
"Set up a separated copy attention layer, if needed.",0
Check,0
tgt.size() returns tgt length and batch,0
END,0
Run the forward pass of the RNN.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: decoder_outputs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Initialize local and return variables.,0
Run the forward pass of the RNN.,0
Check,0
END,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Initialize local and return variables.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Run the forward pass of the copy attention layer.,0
Return result.,0
Init the input feed.,0
Basic attributes.,0
Build the CNN.,0
CNNDecoder has its own attention mechanism.,0
"Set up a separated copy attention layer, if needed.",0
NOTE: memory_lengths is only here for compatibility reasons,0
with onmt.modules.RNNDecoderBase.forward(),0
CHECKS,0
END CHECKS,0
Initialize return variables.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Run the forward pass of the CNNDecoder.,0
Process the result and update the attentions.,0
Update the state.,0
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"Register self.mask as a buffer in TransformerDecoderLayer, so",0
it gets TransformerDecoderLayer's cuda behavior automatically.,0
Basic attributes.,0
Build TransformerDecoder.,0
TransformerDecoder has its own attention mechanism.,0
"Set up a separated copy attention layer, if needed.",0
Initialize return variables.,0
Run the forward pass of the TransformerDecoder.,0
Process the result and update the attentions.,0
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
We need to save a copy of optim.optimizer.state_dict() for setting,0
"the, optimizer state later on in Stage 2 in this method, since",0
the method optim.set_parameters(model.parameters()) will overwrite,0
"optim.optimizer, and with ith the values stored in",0
optim.optimizer.state_dict(),0
Stage 1:,0
Essentially optim.set_parameters (re-)creates and optimizer using,0
model.paramters() as parameters that will be stored in the,0
optim.optimizer.param_groups field of the torch optimizer class.,0
"Importantly, this method does not yet load the optimizer state, as",0
essentially it builds a new optimizer with empty optimizer state and,0
parameters from the model.,0
"Stage 2: In this stage, which is only performed when loading an",0
"optimizer from a checkpoint, we load the saved_optimizer_state_dict",0
"into the re-created optimizer, to set the optim.optimizer.state",0
"field, which was previously empty. For this, we use the optimizer",0
"state saved in the ""saved_optimizer_state_dict"" variable for",0
this purpose.,0
See also: https://github.com/pytorch/pytorch/issues/2830,0
Convert back the state values to cuda type if applicable,0
We want to make sure that indeed we have a non-empty optimizer state,0
when we loaded an existing model. This should be at least the case,0
"for Adam, which saves ""exp_avg"" and ""exp_avg_sq"" state",0
(Exponential moving average of gradient and squared gradient values),0
Decay method used in tensor2tensor.,0
Decay based on start_decay_steps every decay_steps,0
-*- coding: utf-8 -*-,0
"for sparsemax loss, the loss function operates on the raw output",0
"vector, not a probability vector. Hence it's only necessary to",0
apply the first part of the generator here.,0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Log the progress using the number of batches on the x-axis.,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
coding: utf-8,0
Below are helper functions for intra-class use only.,0
-*- coding: utf-8 -*-,0
Hack. Can't pickle defaultdict :(,1
"For all data types, the tgt side corpus is in form of text.",0
Prop src from field to get lower memory using when training with image,0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"All datasets have same num of n_tgt_features,",0
getting the last one is OK.,0
"All datasets have same num of n_src_features,",0
getting the last one is OK.,0
Merge the input and output vocabularies.,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
We have at least one dataset.,0
"We return the len of cur_dataset, otherwise we need to load",0
"all datasets to determine the real len, which loses the benefit",0
of lazy loading.,0
Drop the current dataset for decreasing memory,0
"We clear `fields` when saving, restore when loading.",0
Sort batch by decreasing lengths of sentence required by pytorch.,0
"sort=False means ""Use dataset's sortkey instead of iterator's"".",0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: <bos> w1 ... wN <eos>,0
Tgt: w1 ... wN <eos>,0
Sort the glob output by file name (by increasing indexes).,0
"Only one inputters.*Dataset, simple!",0
-*- coding: utf-8 -*-,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
STFT,0
"The codecs module seems to have bugs with seek()/tell(),",0
so we use io.open().,0
"We have associate iterator, just yields tuples",0
util we run parallel with it.,0
Yield tuples util this shard's size reaches the threshold.,0
-*- coding: utf-8 -*-,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
-*- coding: utf-8 -*-,0
"self.src_vocabs: mutated in dynamic_dict, used in",0
collapse_copy_scores and in Translator.py,0
Each element of an example is a dictionary whose keys represents,0
at minimum the src tokens and their indices and potentially also,0
the src and tgt features and alignment information.,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
"Default to a balanced sort, prioritizing tgt len match.",0
TODO: make this configurable.,1
"All examples have same number of features, so we peek first one",0
to get the num_feats.,0
Chain back the first element - we only want to peek it.,0
Below are helper functions for intra-class use only.,0
Mapping source tokens to indices in the dynamic dict.,0
"The codecs module seems to have bugs with seek()/tell(),",0
so we use io.open().,0
"We have associate iterator, just yields tuples",0
util we run parallel with it.,0
Yield tuples util this shard's size reaches the threshold.,0
This part of check is time consuming on Py2 (but,0
"it is quite fast on Py3, weird!). So we don't bother",0
to check for very line. Instead we chekc every 64,0
lines. Thus we are not dividing exactly per,0
"`shard_size`, but it is not too much difference.",0
All examples must have same number of features.,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
Not yet supported on multi-gpu,0
The score for each translation on the beam.,0
The backpointers at each time-step.,0
The outputs at each time-step.,0
Has EOS topped the beam yet.,0
The attentions (matrix) for each time.,0
Time and k pair for finished.,0
Information for global scoring.,0
Minimum prediction length,0
Apply Penalty at every step,0
force the output to be longer than self.min_length,0
Sum the previous scores.,0
Don't let EOS have children.,0
Block ngram repeats,0
"Last n tokens, n = block_ngram_repeat",0
Skip the blocking if it is in the exclusion list,0
"best_scores_id is flattened beam x word array, so calculate which",0
word and beam each score came from,0
End condition is when top-of-beam is EOS and no global score.,0
Add from beam until we have minimum outputs.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
!/usr/bin/env python,0
use ensemble decoding if more than one model is specified,0
for debugging,0
Statistics,0
Debug attention.,0
TODO: faster code path for beam_size == 1.,1
TODO: support these blacklisted features.,0
Encoder forward.,0
Tile states and memory beam_size times.,0
Give full probability to the first beam on the first step.,0
Structure that holds finished hypotheses.,0
Decoder forward.,0
Generator forward.,0
Multiply probs by the beam probability.,0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Resolve beam origin and true word ids.,0
Map beam_index to batch_index in the flat representation.,0
Append last prediction.,0
End condition is top beam is finished.,0
Save finished hypotheses.,0
Store finished hypotheses for this batch.,0
"If the batch reached the end, save the n_best hypotheses.",0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
Reorder states.,0
(0) Prep each of the components of the search.,0
And helper method for reducing verbosity.,0
Define a list of tokens to exclude from ngram-blocking,0
"exclusion_list = [""<t>"", ""</t>"", "".""]",0
Help functions for working with beams and batches,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
"(3) run the decoder to generate sentences, using beam search.",0
Construct batch x beam_size nxt words.,0
Get all the pending current beam words and arrange for forward.,0
Turn any copied words to UNKs,0
0 is unk,0
Temporary kludge solution to handle changed dim expectation,1
in the decoder,0
Run one step.,0
dec_out: beam x rnn_size,0
(b) Compute a vector of batch x beam word scores.,0
beam x tgt_vocab,0
beam x (tgt_vocab + extra_vocab),0
beam x tgt_vocab,0
(c) Advance each beam.,0
(4) Extract sentences from beam.,0
(1) run the encoder on the src,0
"(2) if a target is specified, compute the 'goldScore'",0
(i.e. log likelihood) of the target under the model,0
Log prob of each word.,0
Rollback pointer to the beginning.,0
!/usr/bin/env python,0
backwards compatibility for confs,0
load can be called multiple times: modify copy,0
NOTE: translator returns lists of `n_best` list,0
we can ignore that (i.e. flatten lists) only because,0
we restrict `n_best=1`,0
build back results with empty texts,0
Sorting,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
We will use glob.glob() to find sharded {train|valid}.[0-9]*.pt,0
"when training, so check to avoid tampering with existing pt files",0
or mixing them up.,0
"We save fields in vocab.pt separately, so make it empty.",0
"We save fields in vocab.pt seperately, so make it empty.",0
Currently we only do preprocess sharding for corpus: data_type=='text'.,0
"For data_type == 'img' or 'audio', currently we don't do",0
preprocess sharding. We only build a monolithic dataset.,0
"But since the interfaces are uniform, it would be not hard",0
to do this should users need this feature.,0
"We save fields in vocab.pt seperately, so make it empty.",0
"Can't save fields, so remove/reconstruct at training time.",0
!/usr/bin/env python,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"propagate exception to parent process, keeping original traceback",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"the vocab object is a list of tuple (name, torchtext.Vocab)",0
we iterate over this list and associate vocabularies based on the name,0
"Add in default model arguments, possibly added since training.",0
"the vocab object is a list of tuple (name, torchtext.Vocab)",0
we iterate over this list and associate vocabularies based on the name,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
"""rnn"" or ""brnn""",0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
Add generator to model (this registers it as parameter of model).,0
!/usr/bin/env python,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
some cudnn methods can be random even after fixing the seed,0
unless you tell it to be deterministic,0
These ensure same initialization in multi gpu mode,0
Load checkpoint if we resume from a previous training.,0
Peek the first dataset to determine the data_type.,0
(All datasets have the same data_type).,0
Load fields generated from preprocess phase.,0
Report src/tgt features.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Do training.,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add_argument('-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Option most relevant to image input,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use TensorboardX for visualization during training,0
Options most relevant to speech,0
Option most relevant to image input,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
Option most relevant to image input,0
MARKDOWN boilerplate,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
**section heading**:,0
# **--argument-one**,0
Basic attributes.,0
Set model in training mode.,0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss in shards for memory efficiency.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
"(batch_size, 1, nfft, t)",0
layer 1,0
"(batch_size, 32, nfft/2, t/2)",0
"(batch_size, 32, nfft/2/2, t/2)",0
layer 2,0
"(batch_size, 32, nfft/2/2, t/2)",0
"s_len, batch, emb_dim = emb.size()",0
from onmt.utils.misc import aeq,0
Run the forward pass of every layer of the tranformer.,0
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
2) Calculate and scale scores.,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return one attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probibility of not copying: p_{word}(w) * (1 - p(z)),0
Compute unks in align and target for readability,0
Copy probability of tokens in source,0
Set scores for unk to 0 and add eps,0
Get scores for tokens in target,0
Regular prob (no unks and unks that can't be copied),0
Add score for non-unks in target,0
Add score for when word is unk in both align and tgt,0
Forced copy. Add only probability for not-copied tokens,0
Drop padding.,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
Compute Loss as NLL divided by seq length,0
Compute Sequence Lengths,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
"-data option is required, but not used in this test, so dummy.",0
Helper to generate a vocabulary,0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16),",0
"('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
4 specicials + 2 words (since we pass 2 to merge_vocabs),0
Test image preprocessing,0
Test audio preprocessing,0
Basic attributes.,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
"Set up a separated copy attention layer, if needed.",0
Check,0
tgt.size() returns tgt length and batch,0
END,0
Run the forward pass of the RNN.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: decoder_outputs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Initialize local and return variables.,0
Run the forward pass of the RNN.,0
Check,0
END,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Initialize local and return variables.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Run the forward pass of the copy attention layer.,0
Return result.,0
Init the input feed.,0
Basic attributes.,0
Build the CNN.,0
CNNDecoder has its own attention mechanism.,0
"Set up a separated copy attention layer, if needed.",0
NOTE: memory_lengths is only here for compatibility reasons,0
with onmt.modules.RNNDecoderBase.forward(),0
CHECKS,0
END CHECKS,0
Initialize return variables.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Run the forward pass of the CNNDecoder.,0
Process the result and update the attentions.,0
Update the state.,0
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"Register self.mask as a buffer in TransformerDecoderLayer, so",0
it gets TransformerDecoderLayer's cuda behavior automatically.,0
Basic attributes.,0
Build TransformerDecoder.,0
TransformerDecoder has its own attention mechanism.,0
"Set up a separated copy attention layer, if needed.",0
Initialize return variables.,0
Run the forward pass of the TransformerDecoder.,0
Process the result and update the attentions.,0
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
We need to save a copy of optim.optimizer.state_dict() for setting,0
"the, optimizer state later on in Stage 2 in this method, since",0
the method optim.set_parameters(model.parameters()) will overwrite,0
"optim.optimizer, and with ith the values stored in",0
optim.optimizer.state_dict(),0
Stage 1:,0
Essentially optim.set_parameters (re-)creates and optimizer using,0
model.paramters() as parameters that will be stored in the,0
optim.optimizer.param_groups field of the torch optimizer class.,0
"Importantly, this method does not yet load the optimizer state, as",0
essentially it builds a new optimizer with empty optimizer state and,0
parameters from the model.,0
"Stage 2: In this stage, which is only performed when loading an",0
"optimizer from a checkpoint, we load the saved_optimizer_state_dict",0
"into the re-created optimizer, to set the optim.optimizer.state",0
"field, which was previously empty. For this, we use the optimizer",0
"state saved in the ""saved_optimizer_state_dict"" variable for",0
this purpose.,0
See also: https://github.com/pytorch/pytorch/issues/2830,0
Convert back the state values to cuda type if applicable,0
We want to make sure that indeed we have a non-empty optimizer state,0
when we loaded an existing model. This should be at least the case,0
"for Adam, which saves ""exp_avg"" and ""exp_avg_sq"" state",0
(Exponential moving average of gradient and squared gradient values),0
Decay method used in tensor2tensor.,0
Decay based on start_decay_steps every decay_steps,0
-*- coding: utf-8 -*-,0
"for sparsemax loss, the loss function operates on the raw output",0
"vector, not a probability vector. Hence it's only necessary to",0
apply the first part of the generator here.,0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Log the progress using the number of batches on the x-axis.,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
coding: utf-8,0
Below are helper functions for intra-class use only.,0
-*- coding: utf-8 -*-,0
Hack. Can't pickle defaultdict :(,1
"For all data types, the tgt side corpus is in form of text.",0
Prop src from field to get lower memory using when training with image,0
Load vocabulary,0
Drop the none-using from memory but keep the last,0
"All datasets have same num of n_tgt_features,",0
getting the last one is OK.,0
"All datasets have same num of n_src_features,",0
getting the last one is OK.,0
Merge the input and output vocabularies.,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
We have at least one dataset.,0
"We return the len of cur_dataset, otherwise we need to load",0
"all datasets to determine the real len, which loses the benefit",0
of lazy loading.,0
Drop the current dataset for decreasing memory,0
"We clear `fields` when saving, restore when loading.",0
Sort batch by decreasing lengths of sentence required by pytorch.,0
"sort=False means ""Use dataset's sortkey instead of iterator's"".",0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: <bos> w1 ... wN <eos>,0
Tgt: w1 ... wN <eos>,0
Sort the glob output by file name (by increasing indexes).,0
"Only one inputters.*Dataset, simple!",0
-*- coding: utf-8 -*-,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
STFT,0
-*- coding: utf-8 -*-,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
-*- coding: utf-8 -*-,0
"self.src_vocabs: mutated in dynamic_dict, used in",0
collapse_copy_scores and in Translator.py,0
Each element of an example is a dictionary whose keys represents,0
at minimum the src tokens and their indices and potentially also,0
the src and tgt features and alignment information.,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
"Default to a balanced sort, prioritizing tgt len match.",0
TODO: make this configurable.,1
"All examples have same number of features, so we peek first one",0
to get the num_feats.,0
Chain back the first element - we only want to peek it.,0
Below are helper functions for intra-class use only.,0
Mapping source tokens to indices in the dynamic dict.,0
"The codecs module seems to have bugs with seek()/tell(),",0
so we use io.open().,0
"We have associate iterator, just yields tuples",0
util we run parallel with it.,0
Yield tuples util this shard's size reaches the threshold.,0
This part of check is time consuming on Py2 (but,0
"it is quite fast on Py3, weird!). So we don't bother",0
to check for very line. Instead we chekc every 64,0
lines. Thus we are not dividing exactly per,0
"`shard_size`, but it is not too much difference.",0
All examples must have same number of features.,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
Not yet supported on multi-gpu,0
The score for each translation on the beam.,0
The backpointers at each time-step.,0
The outputs at each time-step.,0
Has EOS topped the beam yet.,0
The attentions (matrix) for each time.,0
Time and k pair for finished.,0
Information for global scoring.,0
Minimum prediction length,0
Apply Penalty at every step,0
force the output to be longer than self.min_length,0
Sum the previous scores.,0
Don't let EOS have children.,0
Block ngram repeats,0
"Last n tokens, n = block_ngram_repeat",0
Skip the blocking if it is in the exclusion list,0
"best_scores_id is flattened beam x word array, so calculate which",0
word and beam each score came from,0
End condition is when top-of-beam is EOS and no global score.,0
Add from beam until we have minimum outputs.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
!/usr/bin/env python,0
use ensemble decoding if more than one model is specified,0
for debugging,0
Statistics,0
Debug attention.,0
TODO: faster code path for beam_size == 1.,1
TODO: support these blacklisted features.,0
Encoder forward.,0
Tile states and memory beam_size times.,0
Give full probability to the first beam on the first step.,0
Structure that holds finished hypotheses.,0
Decoder forward.,0
Generator forward.,0
Multiply probs by the beam probability.,0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Resolve beam origin and true word ids.,0
Map beam_index to batch_index in the flat representation.,0
Append last prediction.,0
End condition is top beam is finished.,0
Save finished hypotheses.,0
Store finished hypotheses for this batch.,0
"If the batch reached the end, save the n_best hypotheses.",0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
Reorder states.,0
(0) Prep each of the components of the search.,0
And helper method for reducing verbosity.,0
Define a list of tokens to exclude from ngram-blocking,0
"exclusion_list = [""<t>"", ""</t>"", "".""]",0
Help functions for working with beams and batches,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
"(3) run the decoder to generate sentences, using beam search.",0
Construct batch x beam_size nxt words.,0
Get all the pending current beam words and arrange for forward.,0
Turn any copied words to UNKs,0
0 is unk,0
Temporary kludge solution to handle changed dim expectation,1
in the decoder,0
Run one step.,0
dec_out: beam x rnn_size,0
(b) Compute a vector of batch x beam word scores.,0
beam x tgt_vocab,0
beam x (tgt_vocab + extra_vocab),0
beam x tgt_vocab,0
(c) Advance each beam.,0
(4) Extract sentences from beam.,0
(1) run the encoder on the src,0
"(2) if a target is specified, compute the 'goldScore'",0
(i.e. log likelihood) of the target under the model,0
Log prob of each word.,0
Rollback pointer to the beginning.,0
!/usr/bin/env python,0
backwards compatibility for confs,0
load can be called multiple times: modify copy,0
NOTE: translator returns lists of `n_best` list,0
we can ignore that (i.e. flatten lists) only because,0
we restrict `n_best=1`,0
build back results with empty texts,0
Sorting,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
We will use glob.glob() to find sharded {train|valid}.[0-9]*.pt,0
"when training, so check to avoid tampering with existing pt files",0
or mixing them up.,0
"We save fields in vocab.pt separately, so make it empty.",0
Currently we only do preprocess sharding for corpus: data_type=='text'.,0
"For data_type == 'img' or 'audio', currently we don't do",0
preprocess sharding. We only build a monolithic dataset.,0
"But since the interfaces are uniform, it would be not hard",0
to do this should users need this feature.,0
"We save fields in vocab.pt seperately, so make it empty.",0
"Can't save fields, so remove/reconstruct at training time.",0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"the vocab object is a list of tuple (name, torchtext.Vocab)",0
we iterate over this list and associate vocabularies based on the name,0
"Add in default model arguments, possibly added since training.",0
"the vocab object is a list of tuple (name, torchtext.Vocab)",0
we iterate over this list and associate vocabularies based on the name,0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
"""rnn"" or ""brnn""",0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
Add generator to model (this registers it as parameter of model).,0
!/usr/bin/env python,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"propagate exception to parent process, keeping original traceback",0
!/usr/bin/env python,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
These ensure same initialization in multi gpu mode,0
Load checkpoint if we resume from a previous training.,0
Peek the first dataset to determine the data_type.,0
(All datasets have the same data_type).,0
Load fields generated from preprocess phase.,0
Report src/tgt features.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Do training.,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add_argument('-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use TensorboardX for visualization during training,0
Options most relevant to speech,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
MARKDOWN boilerplate,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
**section heading**:,0
# **--argument-one**,0
Basic attributes.,0
Set model in training mode.,0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss in shards for memory efficiency.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
"(batch_size, 1, nfft, t)",0
layer 1,0
"(batch_size, 32, nfft/2, t/2)",0
"(batch_size, 32, nfft/2/2, t/2)",0
layer 2,0
"(batch_size, 32, nfft/2/2, t/2)",0
"s_len, batch, emb_dim = emb.size()",0
from onmt.utils.misc import aeq,0
Run the forward pass of every layer of the tranformer.,0
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
2) Calculate and scale scores.,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return one attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probibility of not copying: p_{word}(w) * (1 - p(z)),0
Compute unks in align and target for readability,0
Copy probability of tokens in source,0
Set scores for unk to 0 and add eps,0
Get scores for tokens in target,0
Regular prob (no unks and unks that can't be copied),0
Add score for non-unks in target,0
Add score for when word is unk in both align and tgt,0
Forced copy. Add only probability for not-copied tokens,0
Drop padding.,0
"We lazily load datasets when there are more than one, so postpone",0
the setting of cur_dataset.,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
Compute Loss as NLL divided by seq length,0
Compute Sequence Lengths,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
"-data option is required, but not used in this test, so dummy.",0
Helper to generate a vocabulary,0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16),",0
"('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
4 specicials + 2 words (since we pass 2 to merge_vocabs),0
Test image preprocessing,0
Test audio preprocessing,0
Basic attributes.,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
"Set up a separated copy attention layer, if needed.",0
Check,0
tgt.size() returns tgt length and batch,0
END,0
Run the forward pass of the RNN.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: decoder_outputs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Initialize local and return variables.,0
Run the forward pass of the RNN.,0
Check,0
END,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Initialize local and return variables.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Run the forward pass of the copy attention layer.,0
Return result.,0
Init the input feed.,0
Basic attributes.,0
Build the CNN.,0
CNNDecoder has its own attention mechanism.,0
"Set up a separated copy attention layer, if needed.",0
NOTE: memory_lengths is only here for compatibility reasons,0
with onmt.modules.RNNDecoderBase.forward(),0
CHECKS,0
END CHECKS,0
Initialize return variables.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Run the forward pass of the CNNDecoder.,0
Process the result and update the attentions.,0
Update the state.,0
Memory_lengths is a single tensor shared between all models.,0
This assumption will not hold if Translator is modified,0
to calculate memory_lengths as something other than the length,0
of the input.,0
"Register self.mask as a buffer in TransformerDecoderLayer, so",0
it gets TransformerDecoderLayer's cuda behavior automatically.,0
Basic attributes.,0
Build TransformerDecoder.,0
TransformerDecoder has its own attention mechanism.,0
"Set up a separated copy attention layer, if needed.",0
Initialize return variables.,0
Run the forward pass of the TransformerDecoder.,0
Process the result and update the attentions.,0
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
We need to save a copy of optim.optimizer.state_dict() for setting,0
"the, optimizer state later on in Stage 2 in this method, since",0
the method optim.set_parameters(model.parameters()) will overwrite,0
"optim.optimizer, and with ith the values stored in",0
optim.optimizer.state_dict(),0
Stage 1:,0
Essentially optim.set_parameters (re-)creates and optimizer using,0
model.paramters() as parameters that will be stored in the,0
optim.optimizer.param_groups field of the torch optimizer class.,0
"Importantly, this method does not yet load the optimizer state, as",0
essentially it builds a new optimizer with empty optimizer state and,0
parameters from the model.,0
"Stage 2: In this stage, which is only performed when loading an",0
"optimizer from a checkpoint, we load the saved_optimizer_state_dict",0
"into the re-created optimizer, to set the optim.optimizer.state",0
"field, which was previously empty. For this, we use the optimizer",0
"state saved in the ""saved_optimizer_state_dict"" variable for",0
this purpose.,0
See also: https://github.com/pytorch/pytorch/issues/2830,0
Convert back the state values to cuda type if applicable,0
We want to make sure that indeed we have a non-empty optimizer state,0
when we loaded an existing model. This should be at least the case,0
"for Adam, which saves ""exp_avg"" and ""exp_avg_sq"" state",0
(Exponential moving average of gradient and squared gradient values),0
Decay method used in tensor2tensor.,0
Decay based on start_decay_steps every decay_steps,0
-*- coding: utf-8 -*-,0
"for sparsemax loss, the loss function operates on the raw output",0
"vector, not a probability vector. Hence it's only necessary to",0
apply the first part of the generator here.,0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Log the progress using the number of batches on the x-axis.,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
coding: utf-8,0
Below are helper functions for intra-class use only.,0
-*- coding: utf-8 -*-,0
Hack. Can't pickle defaultdict :(,1
"For all data types, the tgt side corpus is in form of text.",0
Load vocabulary,0
"All datasets have same num of n_tgt_features,",0
getting the last one is OK.,0
"All datasets have same num of n_src_features,",0
getting the last one is OK.,0
Merge the input and output vocabularies.,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
We have at least one dataset.,0
"We return the len of cur_dataset, otherwise we need to load",0
"all datasets to determine the real len, which loses the benefit",0
of lazy loading.,0
"We clear `fields` when saving, restore when loading.",0
Sort batch by decreasing lengths of sentence required by pytorch.,0
"sort=False means ""Use dataset's sortkey instead of iterator's"".",0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: <bos> w1 ... wN <eos>,0
Tgt: w1 ... wN <eos>,0
device = opt.device_id if opt.gpuid else -1,0
breaking change torchtext 0.3,0
Sort the glob output by file name (by increasing indexes).,0
"Only one inputters.*Dataset, simple!",0
-*- coding: utf-8 -*-,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
STFT,0
-*- coding: utf-8 -*-,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
-*- coding: utf-8 -*-,0
"self.src_vocabs: mutated in dynamic_dict, used in",0
collapse_copy_scores and in Translator.py,0
Each element of an example is a dictionary whose keys represents,0
at minimum the src tokens and their indices and potentially also,0
the src and tgt features and alignment information.,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
"Default to a balanced sort, prioritizing tgt len match.",0
TODO: make this configurable.,1
"All examples have same number of features, so we peek first one",0
to get the num_feats.,0
Chain back the first element - we only want to peek it.,0
Below are helper functions for intra-class use only.,0
Mapping source tokens to indices in the dynamic dict.,0
"The codecs module seems to have bugs with seek()/tell(),",0
so we use io.open().,0
"We have associate iterator, just yields tuples",0
util we run parallel with it.,0
Yield tuples util this shard's size reaches the threshold.,0
This part of check is time consuming on Py2 (but,0
"it is quite fast on Py3, weird!). So we don't bother",0
to check for very line. Instead we chekc every 64,0
lines. Thus we are not dividing exactly per,0
"`shard_size`, but it is not too much difference.",0
All examples must have same number of features.,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
Not yet supported on multi-gpu,0
The score for each translation on the beam.,0
The backpointers at each time-step.,0
The outputs at each time-step.,0
Has EOS topped the beam yet.,0
The attentions (matrix) for each time.,0
Time and k pair for finished.,0
Information for global scoring.,0
Minimum prediction length,0
Apply Penalty at every step,0
force the output to be longer than self.min_length,0
Sum the previous scores.,0
Don't let EOS have children.,0
Block ngram repeats,0
"Last n tokens, n = block_ngram_repeat",0
Skip the blocking if it is in the exclusion list,0
"best_scores_id is flattened beam x word array, so calculate which",0
word and beam each score came from,0
End condition is when top-of-beam is EOS and no global score.,0
Add from beam until we have minimum outputs.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
!/usr/bin/env python,0
use ensemble decoding if more than one model is specified,0
for debugging,0
Statistics,0
Debug attention.,0
TODO: faster code path for beam_size == 1.,1
TODO: support these blacklisted features.,0
Encoder forward.,0
Tile states and memory beam_size times.,0
Give full probability to the first beam on the first step.,0
Decoder forward.,0
Generator forward.,0
Multiply probs by the beam probability.,0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Resolve beam origin and true word ids.,0
Map beam_index to batch_index in the flat representation.,0
End condition is the top beam reached end_token.,0
Save result of finished sentences.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
Select and reorder alive batches.,0
Append last prediction.,0
(0) Prep each of the components of the search.,0
And helper method for reducing verbosity.,0
Define a list of tokens to exclude from ngram-blocking,0
"exclusion_list = [""<t>"", ""</t>"", "".""]",0
Help functions for working with beams and batches,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
"(3) run the decoder to generate sentences, using beam search.",0
Construct batch x beam_size nxt words.,0
Get all the pending current beam words and arrange for forward.,0
Turn any copied words to UNKs,0
0 is unk,0
Temporary kludge solution to handle changed dim expectation,1
in the decoder,0
Run one step.,0
dec_out: beam x rnn_size,0
(b) Compute a vector of batch x beam word scores.,0
beam x tgt_vocab,0
beam x (tgt_vocab + extra_vocab),0
beam x tgt_vocab,0
(c) Advance each beam.,0
(4) Extract sentences from beam.,0
(1) run the encoder on the src,0
"(2) if a target is specified, compute the 'goldScore'",0
(i.e. log likelihood) of the target under the model,0
Log prob of each word.,0
Rollback pointer to the beginning.,0
!/usr/bin/env python,0
backwards compatibility for confs,0
NOTE: translator returns lists of `n_best` list,0
we can ignore that (i.e. flatten lists) only because,0
we restrict `n_best=1`,0
build back results with empty texts,0
Sorting,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
We will use glob.glob() to find sharded {train|valid}.[0-9]*.pt,0
"when training, so check to avoid tampering with existing pt files",0
or mixing them up.,0
"We save fields in vocab.pt separately, so make it empty.",0
Currently we only do preprocess sharding for corpus: data_type=='text'.,0
"For data_type == 'img' or 'audio', currently we don't do",0
preprocess sharding. We only build a monolithic dataset.,0
"But since the interfaces are uniform, it would be not hard",0
to do this should users need this feature.,0
"We save fields in vocab.pt seperately, so make it empty.",0
"Can't save fields, so remove/reconstruct at training time.",0
!/usr/bin/env python,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"the vocab object is a list of tuple (name, torchtext.Vocab)",0
we iterate over this list and associate vocabularies based on the name,0
"Add in default model arguments, possibly added since training.",0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
"""rnn"" or ""brnn""",0
Build encoder.,0
Build decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Build NMTModel(= encoder + decoder).,0
Build Generator.,0
Load the model states from checkpoint or initialize them.,0
Add generator to model (this registers it as parameter of model).,0
!/usr/bin/env python,0
Create a thread to listen for errors in the child processes.,0
Train with multiprocessing.,0
"propagate exception to parent process, keeping original traceback",0
!/usr/bin/env python,0
this one is needed for torchtext random call (shuffled iterator),0
in multi gpu it ensures datasets are read in the same order,0
These ensure same initialization in multi gpu mode,0
Load checkpoint if we resume from a previous training.,0
Peek the first dataset to determine the data_type.,0
(All datasets have the same data_type).,0
Load fields generated from preprocess phase.,0
Report src/tgt features.,0
Build model.,0
Build optimizer.,0
Build model saver,0
Do training.,0
Embedding Options,0
Encoder-Decoder Options,0
"group.add_argument('-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Generator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use TensorboardX for visualization during training,0
Options most relevant to speech,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
MARKDOWN boilerplate,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
**section heading**:,0
# **--argument-one**,0
Basic attributes.,0
Set model in training mode.,0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT: reminder not compatible with accum > 1,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss in shards for memory efficiency.,0
4. Update the parameters and statistics.,0
Multi GPU gradient gather,0
"If truncated, don't backprop fully.",0
"in case of multi step gradient accumulation,",0
update only after accum batches,0
For Flake,0
Initialize the bridge layer,0
"s_len, batch, emb_dim = emb.size()",0
Lengths data is wrapped inside a Tensor.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
"(batch_size, 1, nfft, t)",0
layer 1,0
"(batch_size, 32, nfft/2, t/2)",0
"(batch_size, 32, nfft/2/2, t/2)",0
layer 2,0
"(batch_size, 32, nfft/2/2, t/2)",0
"s_len, batch, emb_dim = emb.size()",0
from onmt.utils.misc import aeq,0
Run the forward pass of every layer of the tranformer.,0
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
This class is mainly used by decoder.py for RNNs but also,0
by the CNN / transformer decoder when copy attention is used,0
CNN has its own attention mechanism ConvMultiStepAttention,0
Transformer has its own MultiHeadedAttention,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax or sparsemax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
clamping necessary because of numerical errors: loss should be lower,0
"bounded by zero, but negative values near zero are possible without",0
the clamp,0
from onmt.utils.misc import aeq,0
CHECKS,0
"batch, k_len, d = key.size()",0
"batch_, k_len_, d_ = value.size()",0
"aeq(batch, batch_)",0
"aeq(k_len, k_len_)",0
"aeq(d, d_)",0
"batch_, q_len, d_ = query.size()",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
"aeq(self.model_dim % 8, 0)",0
if mask is not None:,0
"batch_, q_len_, k_len_ = mask.size()",0
"aeq(batch_, batch)",0
"aeq(k_len_, k_len)",0
aeq(q_len_ == q_len),0
END CHECKS,0
"1) Project key, value, and query.",0
2) Calculate and scale scores.,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
"batch_, q_len_, d_ = output.size()",0
"aeq(q_len, q_len_)",0
"aeq(batch, batch_)",0
"aeq(d, d_)",0
Return one attn,0
At the moment this class is only used by embeddings.Embeddings look-up tables,0
-*- coding: utf-8 -*-,0
checks,0
"batch, channel, height, width = base_target_emb.size()",0
"batch_, channel_, height_, width_ = input_from_dec.size()",0
"enc_batch, enc_channel, enc_height = encoder_out_top.size()",0
"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()",0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018),0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
store roots on diagonal,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probibility of not copying: p_{word}(w) * (1 - p(z)),0
Compute unks in align and target for readability,0
Copy probability of tokens in source,0
Set scores for unk to 0 and add eps,0
Get scores for tokens in target,0
Regular prob (no unks and unks that can't be copied),0
Add score for non-unks in target,0
Add score for when word is unk in both align and tgt,0
Forced copy. Add only probability for not-copied tokens,0
Drop padding.,0
"We lazily load datasets when there are more than one, so postpone",0
the setting of cur_dataset.,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
Compute Loss as NLL divided by seq length,0
Compute Sequence Lengths,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
"-data option is required, but not used in this test, so dummy.",0
Helper to generate a vocabulary,0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16),",0
"('rnn_size', 16)],",0
""""""" Only do SRU test if requirment is safisfied. """"""",0
SRU doesn't support input_feed.,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Remove the generated *pt files.,0
4 specicials + 2 words (since we pass 2 to merge_vocabs),0
Test image preprocessing,0
Test audio preprocessing,0
Basic attributes.,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
"Set up a separated copy attention layer, if needed.",0
Check,0
tgt.size() returns tgt length and batch,0
END,0
Run the forward pass of the RNN.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
NOTE: v0.3 to 0.4: decoder_outputs / attns[*] may not be list,0
(in particular in case of SRU) it was not raising error in 0.3,0
since stack(Variable) was allowed.,0
"In 0.4, SRU returns a tensor that shouldn't be stacke",0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Initialize local and return variables.,0
Run the forward pass of the RNN.,0
Check,0
END,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Initialize local and return variables.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Run the forward pass of the copy attention layer.,0
Return result.,0
Init the input feed.,0
Basic attributes.,0
Build the CNN.,0
CNNDecoder has its own attention mechanism.,0
"Set up a separated copy attention layer, if needed.",0
NOTE: memory_lengths is only here for compatibility reasons,0
with onmt.modules.RNNDecoderBase.forward(),0
CHECKS,0
END CHECKS,0
Initialize return variables.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Run the forward pass of the CNNDecoder.,0
Process the result and update the attentions.,0
Update the state.,0
"Register self.mask as a buffer in TransformerDecoderLayer, so",0
it gets TransformerDecoderLayer's cuda behavior automatically.,0
Basic attributes.,0
Build TransformerDecoder.,0
TransformerDecoder has its own attention mechanism.,0
"Set up a separated copy attention layer, if needed.",0
Initialize return variables.,0
Run the forward pass of the TransformerDecoder.,0
Process the result and update the attentions.,0
"buffer size in bytes, determine equiv. # of elements based on data type",0
copy tensors into buffer_t,0
all-reduce and rescale,0
copy all-reduced buffer back into tensors,0
"tensor is bigger than buffer, all-reduce and rescale directly",0
"buffer is full, all-reduce and replace buffer with grad",0
add tensor to buffer,0
We need to save a copy of optim.optimizer.state_dict() for setting,0
"the, optimizer state later on in Stage 2 in this method, since",0
the method optim.set_parameters(model.parameters()) will overwrite,0
"optim.optimizer, and with ith the values stored in",0
optim.optimizer.state_dict(),0
Stage 1:,0
Essentially optim.set_parameters (re-)creates and optimizer using,0
model.paramters() as parameters that will be stored in the,0
optim.optimizer.param_groups field of the torch optimizer class.,0
"Importantly, this method does not yet load the optimizer state, as",0
essentially it builds a new optimizer with empty optimizer state and,0
parameters from the model.,0
"Stage 2: In this stage, which is only performed when loading an",0
"optimizer from a checkpoint, we load the saved_optimizer_state_dict",0
"into the re-created optimizer, to set the optim.optimizer.state",0
"field, which was previously empty. For this, we use the optimizer",0
"state saved in the ""saved_optimizer_state_dict"" variable for",0
this purpose.,0
See also: https://github.com/pytorch/pytorch/issues/2830,0
Convert back the state values to cuda type if applicable,0
We want to make sure that indeed we have a non-empty optimizer state,0
when we loaded an existing model. This should be at least the case,0
"for Adam, which saves ""exp_avg"" and ""exp_avg_sq"" state",0
(Exponential moving average of gradient and squared gradient values),0
Decay method used in tensor2tensor.,0
Decay based on start_decay_steps every decay_steps,0
-*- coding: utf-8 -*-,0
"for sparsemax loss, the loss function operates on the raw output",0
"vector, not a probability vector. Hence it's only necessary to",0
apply the first part of the generator here.,0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
Log the progress using the number of batches on the x-axis.,0
Get a list of world_size lists with len(stat_list) Statistics objects,0
SRU doesn't support PackedSequence.,0
-*- coding: utf-8 -*-,0
coding: utf-8,0
Below are helper functions for intra-class use only.,0
-*- coding: utf-8 -*-,0
Hack. Can't pickle defaultdict :(,1
"For all data types, the tgt side corpus is in form of text.",0
Load vocabulary,0
"All datasets have same num of n_tgt_features,",0
getting the last one is OK.,0
"All datasets have same num of n_src_features,",0
getting the last one is OK.,0
Merge the input and output vocabularies.,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
We have at least one dataset.,0
"We return the len of cur_dataset, otherwise we need to load",0
"all datasets to determine the real len, which loses the benefit",0
of lazy loading.,0
"We clear `fields` when saving, restore when loading.",0
Sort batch by decreasing lengths of sentence required by pytorch.,0
"sort=False means ""Use dataset's sortkey instead of iterator's"".",0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: <bos> w1 ... wN <eos>,0
Tgt: w1 ... wN <eos>,0
device = opt.device_id if opt.gpuid else -1,0
breaking change torchtext 0.3,0
Sort the glob output by file name (by increasing indexes).,0
"Only one inputters.*Dataset, simple!",0
-*- coding: utf-8 -*-,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
STFT,0
-*- coding: utf-8 -*-,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
-*- coding: utf-8 -*-,0
"self.src_vocabs: mutated in dynamic_dict, used in",0
collapse_copy_scores and in Translator.py,0
Each element of an example is a dictionary whose keys represents,0
at minimum the src tokens and their indices and potentially also,0
the src and tgt features and alignment information.,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
"Default to a balanced sort, prioritizing tgt len match.",0
TODO: make this configurable.,1
"All examples have same number of features, so we peek first one",0
to get the num_feats.,0
Chain back the first element - we only want to peek it.,0
Below are helper functions for intra-class use only.,0
Mapping source tokens to indices in the dynamic dict.,0
"The codecs module seems to have bugs with seek()/tell(),",0
so we use io.open().,0
"We have associate iterator, just yields tuples",0
util we run parallel with it.,0
Yield tuples util this shard's size reaches the threshold.,0
This part of check is time consuming on Py2 (but,0
"it is quite fast on Py3, weird!). So we don't bother",0
to check for very line. Instead we chekc every 64,0
lines. Thus we are not dividing exactly per,0
"`shard_size`, but it is not too much difference.",0
All examples must have same number of features.,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
This sets up device to use.,0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
Not yet supported on multi-gpu,0
The score for each translation on the beam.,0
The backpointers at each time-step.,0
The outputs at each time-step.,0
Has EOS topped the beam yet.,0
The attentions (matrix) for each time.,0
Time and k pair for finished.,0
Information for global scoring.,0
Minimum prediction length,0
Apply Penalty at every step,0
force the output to be longer than self.min_length,0
Sum the previous scores.,0
Don't let EOS have children.,0
Block ngram repeats,0
"Last n tokens, n = block_ngram_repeat",0
Skip the blocking if it is in the exclusion list,0
"best_scores_id is flattened beam x word array, so calculate which",0
word and beam each score came from,0
End condition is when top-of-beam is EOS and no global score.,0
Add from beam until we have minimum outputs.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
!/usr/bin/env python,0
for debugging,0
Statistics,0
Debug attention.,0
TODO: faster code path for beam_size == 1.,1
TODO: support these blacklisted features.,0
Encoder forward.,0
Tile states and memory beam_size times.,0
Give full probability to the first beam on the first step.,0
Decoder forward.,0
Generator forward.,0
Multiply probs by the beam probability.,0
Flatten probs into a list of possibilities.,0
Recover log probs.,0
Resolve beam origin and true word ids.,0
Map beam_index to batch_index in the flat representation.,0
End condition is the top beam reached end_token.,0
Save result of finished sentences.,0
"If all sentences are translated, no need to go further.",0
Remove finished batches for the next step.,0
Select and reorder alive batches.,0
Append last prediction.,0
(0) Prep each of the components of the search.,0
And helper method for reducing verbosity.,0
Define a list of tokens to exclude from ngram-blocking,0
"exclusion_list = [""<t>"", ""</t>"", "".""]",0
Help functions for working with beams and batches,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
"(3) run the decoder to generate sentences, using beam search.",0
Construct batch x beam_size nxt words.,0
Get all the pending current beam words and arrange for forward.,0
Turn any copied words to UNKs,0
0 is unk,0
Temporary kludge solution to handle changed dim expectation,1
in the decoder,0
Run one step.,0
dec_out: beam x rnn_size,0
(b) Compute a vector of batch x beam word scores.,0
beam x tgt_vocab,0
beam x (tgt_vocab + extra_vocab),0
beam x tgt_vocab,0
(c) Advance each beam.,0
(4) Extract sentences from beam.,0
(1) run the encoder on the src,0
"(2) if a target is specified, compute the 'goldScore'",0
(i.e. log likelihood) of the target under the model,0
Log prob of each word.,0
Rollback pointer to the beginning.,0
!/usr/bin/env python,0
NOTE: translator returns lists of `n_best` list,0
we can ignore that (i.e. flatten lists) only because,0
we restrict `n_best=1`,0
build back results with empty texts,0
Sorting,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
We will use glob.glob() to find sharded {train|valid}.[0-9]*.pt,0
"when training, so check to avoid tampering with existing pt files",0
or mixing them up.,0
"We save fields in vocab.pt seperately, so make it empty.",0
Currently we only do preprocess sharding for corpus: data_type=='text'.,0
"For data_type == 'img' or 'audio', currently we don't do",0
preprocess sharding. We only build a monolithic dataset.,0
"But since the interfaces are uniform, it would be not hard",0
to do this should users need this feature.,0
"We save fields in vocab.pt seperately, so make it empty.",0
"Can't save fields, so remove/reconstruct at training time.",0
!/usr/bin/env python,0
onmt.opts.py,0
Set up the Crayon logging server.,0
Log the progress using the number of batches on the x-axis.,0
We have at least one dataset.,0
"We return the len of cur_dataset, otherwise we need to load",0
"all datasets to determine the real len, which loses the benefit",0
of lazy loading.,0
"We clear `fields` when saving, restore when loading.",0
Sort batch by decreasing lengths of sentence required by pytorch.,0
"sort=False means ""Use dataset's sortkey instead of iterator's"".",0
"In token batching scheme, the number of sequences is limited",0
such that the total number of src/tgt tokens (including padding),0
in a batch <= batch_size,0
Maintains the longest src and tgt length in the current batch,0
Reset current longest length at a new batch (count=1),0
Src: <bos> w1 ... wN <eos>,0
Tgt: w1 ... wN <eos>,0
1. Train for one epoch on the training set.,0
2. Validate on the validation set.,0
3. Log to remote server.,0
4. Update the learning rate,0
5. Drop a checkpoint if needed.,0
Sort the glob output by file name (by increasing indexes).,0
"Only one onmt.io.*Dataset, simple!",0
We need to save a copy of optim.optimizer.state_dict() for setting,0
"the, optimizer state later on in Stage 2 in this method, since",0
the method optim.set_parameters(model.parameters()) will overwrite,0
"optim.optimizer, and with ith the values stored in",0
optim.optimizer.state_dict(),0
Stage 1:,0
Essentially optim.set_parameters (re-)creates and optimizer using,0
model.paramters() as parameters that will be stored in the,0
optim.optimizer.param_groups field of the torch optimizer class.,0
"Importantly, this method does not yet load the optimizer state, as",0
essentially it builds a new optimizer with empty optimizer state and,0
parameters from the model.,0
"Stage 2: In this stage, which is only performed when loading an",0
"optimizer from a checkpoint, we load the saved_optimizer_state_dict",0
"into the re-created optimizer, to set the optim.optimizer.state",0
"field, which was previously empty. For this, we use the optimizer",0
"state saved in the ""saved_optimizer_state_dict"" variable for",0
this purpose.,0
See also: https://github.com/pytorch/pytorch/issues/2830,0
Convert back the state values to cuda type if applicable,0
We want to make sure that indeed we have a non-empty optimizer state,0
when we loaded an existing model. This should be at least the case,0
"for Adam, which saves ""exp_avg"" and ""exp_avg_sq"" state",0
(Exponential moving average of gradient and squared gradient values),0
Debugging method for showing the optimizer state,0
Load checkpoint if we resume from a previous training.,0
I don't like reassigning attributes of opt: it's not clear.,0
Peek the fisrt dataset to determine the data_type.,0
(All datasets have the same data_type).,0
Load fields generated from preprocess phase.,0
Report src/tgt features.,0
Build model.,0
Build optimizer.,0
Do training.,0
"If using tensorboard for logging, close the writer after training.",0
illegal_weights_mask = torch.ByteTensor([,0
"[0, 0, 0, 0, 0, 0, 0],",0
"[0, 0, 0, 1, 1, 1, 1],",0
"[0, 0, 0, 0, 0, 1, 1],",0
"[0, 0, 1, 1, 1, 1, 1]])",0
TODO: fix for pytorch 0.3,1
illegal_weights = alignments.masked_select(illegal_weights_mask),0
"self.assertEqual(0.0, illegal_weights.data.sum())",0
"-data option is required, but not used in this test, so dummy.",0
Helper to generate a vocabulary,0
len x batch x nfeat,0
batch x c x h x w,0
batch x 1 x nfft x t,0
Initialize vectors to compare size with,0
Ensure correct sizes and types,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
Make sure that output has the correct size and type,0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16), ('rnn_size', 16)],",0
"[('encoder_type', 'transformer'),",0
"('word_vec_size', 16),",0
"('rnn_size', 16)],",0
SRU doesn't support input_feed.,0
Remove the generated *pt files.,0
4 specicials + 2 words (since we pass 2 to merge_vocabs),0
Test image preprocessing,0
Test audio preprocessing,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
"OpenNMT-py documentation build configuration file, created by",0
sphinx-quickstart on Sun Dec 17 12:07:14 2017.,0
,0
This file is execfile()d with the current directory set to its,0
containing dir.,0
,0
Note that not all possible configuration values are present in this,0
autogenerated file.,0
,0
All configuration values have a default; values that are commented out,0
serve to show the default.,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
import os,0
import sys,0
"sys.path.insert(0, os.path.abspath('.'))",0
-- General configuration ------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
General information about the project.,0
"The version info for the project you're documenting, acts as replacement for",0
"|version| and |release|, also used in various other places throughout the",0
built documents.,0
,0
The short X.Y version.,0
"The full version, including alpha/beta/rc tags.",0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This patterns also effect to html_static_path and html_extra_path,0
The name of the Pygments (syntax highlighting) style to use.,0
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for HTML output ----------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
html_theme = 'sphinx_materialdesign_theme',0
html_theme_path = [sphinx_materialdesign_theme.get_path()],0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
html_theme_options = {},0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
This is required for the alabaster theme,0
refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars,0
-- Options for HTMLHelp output ------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ---------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ---------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output -------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
"the vocab object is a list of tuple (name, torchtext.Vocab)",0
we iterate over this list and associate vocabularies based on the name,0
"Add in default model arguments, possibly added since training.",0
-*- encoding: utf-8 -*-,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
check version information,0
some hacking to deal with duplicates (only consider first instance),0
don't print end-of-word symbols,0
sys.stderr.write('cannot split {0} further.\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
sys.stderr.write('OOV: {0}\n'.format(segment)),0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
!/usr/bin/env python,0
-*- coding: utf-8 -*-,0
Author: Rico Sennrich,0
flake8: noqa,0
This file is retrieved from https://github.com/rsennrich/subword-nmt,0
hack for python2/3 compatibility,1
"find all instances of pair, and update frequency/indices around it",0
find first symbol,0
"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])",0
"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""",0
"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".",0
"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block",0
find new pair,0
"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""",0
"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""",0
"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block",0
data structure of pair frequencies,0
index from pairs to words,0
version 0.2 changes the handling of the end-of-word token ('</w>');,0
version numbering allows bckward compatibility,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
we probably missed the best pair because of pruning; go back to full statistics,0
"threshold is inspired by Zipfian assumption, but should only affect speed",0
python 2/3 compatibility,0
read/write files as UTF-8,0
!/usr/bin/env python,0
Use pytorch version when available.,0
SRU doesn't support PackedSequence.,0
Initialize the bridge layer,0
Lengths data is wrapped inside a Variable.,0
"LSTM has hidden and cell state, other only one",0
Total number of states,0
Build a linear layer for each,0
Basic attributes.,0
Build the RNN.,0
Set up the context gate.,0
Set up the standard attention.,0
"Set up a separated copy attention layer, if needed.",0
Check,0
END,0
Run the forward pass of the RNN.,0
Update the state with the result.,0
Concatenates sequence of tensors along a new dimension.,0
The encoder hidden is  (layers*directions) x batch x dim.,0
We need to convert it to layers x batch x (directions*dim).,0
Initialize local and return variables.,0
Run the forward pass of the RNN.,0
Check,0
END,0
Calculate the attention.,0
Calculate the context gate.,0
Additional args check.,0
END Additional args check.,0
Initialize local and return variables.,0
Input feed concatenates hidden state with,0
input at every time step.,0
TODO: context gate should be employed,1
instead of second RNN transform.,0
Update the coverage attention.,0
Run the forward pass of the copy attention layer.,0
Return result.,0
Not yet supported on multi-gpu,0
Init the input feed.,0
Embedding Options,0
Encoder-Deocder Options,0
"group.add_argument('-residual',   action=""store_true"",",0
"help=""Add residual connections between RNN layers."")",0
Attention options,0
Genenerator and loss options.,0
Data options,0
"Dictionary options, for text corpus",0
"Truncation options, for text corpus",0
Data processing options,0
Options most relevant to speech,0
Model loading/saving options,0
GPU,0
Init options,0
Pretrained word vectors,0
Fixed word vectors,0
Optimization options,0
learning rate,0
Use TensorboardX for visualization during training,0
Options most relevant to speech,0
Options most relevant to summarization.,0
Alpha and Beta values for Google Length + Coverage penalty,0
"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7",0
Options most relevant to speech.,0
MARKDOWN boilerplate,0
Copyright 2016 The Chromium Authors. All rights reserved.,0
Use of this source code is governed by a BSD-style license that can be,0
found in the LICENSE file.,0
**section heading**:,0
# **--argument-one**,0
"When label smoothing is turned on,",0
KL-divergence between q_{smoothed ground truth prob.}(w),0
and p_{prob. computed by model}(w) is minimized.,0
"If label smoothing value is set to zero, the loss",0
is equivalent to NLLLoss or CrossEntropyLoss.,0
All non-true labels are uniformly set to low-confidence.,0
Default: report smoothed ppl.,0
loss_data = -log_likelihood.sum(0),0
non_none: the subdict of the state dictionary where the values,0
are not None.,0
"Now, the iteration:",0
state is a dictionary of sequences of tensor-like but we,0
want a sequence of dictionaries of tensors.,0
"First, unzip the dictionary into a sequence of keys and a",0
sequence of tensor-like sequences.,0
"Now, yield a dictionary for each shard. The keys are always",0
the same. values is a sequence of length #keys where each,0
element is a sequence of length #shards. We want to iterate,0
"over the shards, not over the keys: therefore, the values need",0
to be re-zipped by shard and then each shard can be paired,0
with the keys.,0
Assumed backprop'd,0
We use the default parameters for Adam that are suggested by,0
the original paper https://arxiv.org/pdf/1412.6980.pdf,0
"These values are also used by other established implementations,",0
e.g. https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer,0
https://keras.io/optimizers/,0
Recently there are slightly different values used in the paper,0
"""Attention is all you need""",0
"https://arxiv.org/pdf/1706.03762.pdf, particularly the value beta2=0.98",0
"was used there however, beta2=0.999 is still arguably the more",0
"established value, so we use that here as well",0
Decay method used in tensor2tensor.,0
For flake8 compatibility,0
Basic attributes.,0
Set model in training mode.,0
Dynamic batching,0
Set model in validating mode.,0
F-prop through the model.,0
Compute loss.,0
Update statistics.,0
Set model back to training mode.,0
Truncated BPTT,0
1. Create truncated target.,0
2. F-prop all but generator.,0
3. Compute loss in shards for memory efficiency.,0
4. Update the parameters and statistics.,0
"If truncated, don't backprop fully.",0
"""rnn"" or ""brnn""",0
Make encoder.,0
Make decoder.,0
Share the embedding matrix - preprocess with share_vocab required.,0
src/tgt vocab should be the same if `-share_vocab` is specified.,0
Make NMTModel(= encoder + decoder).,0
Make Generator.,0
Load the model states from checkpoint or initialize them.,0
Add generator to model (this registers it as parameter of model).,0
Make the whole model leverage GPU if indicated to do so.,0
linear transform for 3-d tensor,0
checks,0
flake8: noqa,0
For command-line option parsing,0
"Check pass, set the args.",0
"This SRU version implements its own cuda-level optimization,",0
so it requires that:,0
1. `cupy` and `pynvrtc` python package installed.,0
2. pytorch is built with cuda support.,0
3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.,0
Check 1.,0
Check 2.,0
Check 3.,0
"This cuda() is important, it sets up device to use.",0
-> directions x batch x dim,0
For DEBUG,0
"size = (length, batch, x.size(-1)) \",0
"if x.dim() == 3 else (batch, x.size(-1))",0
grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_(),0
Normal use,0
"An entry check here, will catch on train side and translate side",0
if requirements are not satisfied.,0
RNNDecoderState wraps hidden as a tuple.,0
fh -> (layers*directions) x batch x dim,0
mlp wants it with bias,0
Check input sizes,0
"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)",0
"(batch, t_len, s_len, d)",0
one step input,0
"compute attention scores, as in Luong et al.",0
Softmax to normalize attention weights,0
each context vector c_t is the weighted average,0
over all the source hidden states,0
concatenate,0
Check output sizes,0
Check output sizes,0
Basic attributes.,0
Build the CNN.,0
CNNDecoder has its own attention mechanism.,0
"Set up a separated copy attention layer, if needed.",0
CHECKS,0
END CHECKS,0
Initialize return variables.,0
The output of CNNEncoder.,0
The combination of output of CNNEncoder and source embeddings.,0
Run the forward pass of the CNNDecoder.,0
Process the result and update the attentions.,0
Update the state.,0
"Save a little memory, by doing inplace.",0
CHECKS,0
END CHECKS,0
Make mask.,0
Run the forward pass of every layer of the tranformer.,0
"Register self.mask as a buffer in TransformerDecoderLayer, so",0
it gets TransformerDecoderLayer's cuda behavior automatically.,0
Args Checks,0
"aeq(t_len, t_len_, t_len__, input_len)",0
END Args Checks,0
CHECKS,0
END CHECKS,0
Basic attributes.,0
Build TransformerDecoder.,0
TransformerDecoder has its own attention mechanism.,0
"Set up a separated copy attention layer, if needed.",0
CHECKS,0
END CHECKS,0
Initialize return variables.,0
Run the forward pass of the TransformerDecoder.,0
Process the result and update the attentions.,0
Update the state.,0
utility for retrieving polyak averaged params,0
Update average,0
utility for retrieving polyak averaged params,0
out_features * in_features,0
norm is out_features * 1,0
batch_size * out_features,0
out_features,0
out_features,0
batch_size * out_features,0
"out_channels, in_channels // groups, * kernel_size",0
out_features,0
"in_channels, out_channels, *kernel_size",0
"in_channels, out_channels, *kernel_size",0
"self.out_channels, 1",0
out_features,0
out_features,0
CHECKS,0
Original probabilities.,0
Probability of copying p(z=1) batch.,0
Probibility of not copying: p_{word}(w) * (1 - p(z)),0
Compute unks in align and target for readability,0
Copy probability of tokens in source,0
Set scores for unk to 0 and add eps,0
Get scores for tokens in target,0
Regular prob (no unks and unks that can't be copied),0
Add score for non-unks in target,0
Add score for when word is unk in both align and tgt,0
Forced copy. Add only probability for not-copied tokens,0
Drop padding.,0
"We lazily load datasets when there are more than one, so postpone",0
the setting of cur_dataset.,0
Correct target copy token instead of <unk>,0
tgt[i] = align[i] + len(tgt_vocab),0
for i such that tgt[i] == 0 and align[i] != 0,0
Compute sum of perplexities for stats,0
Compute Loss as NLL divided by seq length,0
Compute Sequence Lengths,0
Compute Total Loss per sequence in batch,0
Divide by length of each sequence and sum,0
store roots on diagonal,0
"We must wrap the self.pe in Variable to compute, not the other",0
way - unwrap emb(i.e. emb.data). Otherwise the computation,0
wouldn't be watched to build the compute graph.,0
Dimensions and padding for constructing the word embedding matrix,0
Dimensions and padding for feature embedding matrices,0
(these have no effect if feat_vocab_sizes is empty),0
The embedding matrix look-up tables. The first look-up table,0
"is for words. Subsequent ones are for features, if any exist.",0
The final output size of word + feature vectors. This can vary,0
from the word vector size if and only if features are defined.,0
This is the attribute you should access if you need to know,0
how big your embeddings are going to be.,0
The sequence of operations that converts the input sequence,0
into a sequence of embeddings. At minimum this consists of,0
looking up the embeddings for each word and feature in the,0
input. Model parameters may require the sequence to contain,0
additional operations as well.,0
For flake8 compatibility.,0
Pass in needed options only when modify function definition.,0
"(batch_size, 1, nfft, t)",0
layer 1,0
"(batch_size, 32, nfft/2, t/2)",0
"(batch_size, 32, nfft/2/2, t/2)",0
layer 2,0
"(batch_size, 32, nfft/2/2, t/2)",0
CHECKS,0
END CHECKS,0
"1) Project key, value, and query.",0
2) Calculate and scale scores.,0
3) Apply attention dropout and compute context vectors.,0
CHECK,0
Return one attn,0
END CHECK,0
Pass in needed options only when modify function definition.,0
"(batch_size, 64, imgH, imgW)",0
layer 1,0
"(batch_size, 64, imgH/2, imgW/2)",0
"(batch_size, 128, imgH/2, imgW/2)",0
layer 2,0
"(batch_size, 128, imgH/2/2, imgW/2/2)",0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer 3,0
batch norm 1,0
"(batch_size, 256, imgH/2/2, imgW/2/2)",0
layer4,0
"(batch_size, 256, imgH/2/2/2, imgW/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2)",0
layer 5,0
batch norm 2,0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)",0
"# (batch_size, 512, H, W)",0
-*- coding: utf-8 -*-,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
-*- coding: utf-8 -*-,0
Hack. Can't pickle defaultdict :(,1
"Build src/tgt examples iterator from corpus files, also extract",0
number of features.,0
"For all data types, the tgt side corpus is in form of text.",0
Load vocabulary,0
"All datasets have same num of n_tgt_features,",0
getting the last one is OK.,0
"All datasets have same num of n_src_features,",0
getting the last one is OK.,0
Merge the input and output vocabularies.,0
`tgt_vocab_size` is ignored when sharing vocabularies,0
-*- coding: utf-8 -*-,0
Below are helper functions for intra-class use only.,0
-*- coding: utf-8 -*-,0
"self.src_vocabs: mutated in dynamic_dict, used in",0
collapse_copy_scores and in Translator.py,0
Each element of an example is a dictionary whose keys represents,0
at minimum the src tokens and their indices and potentially also,0
the src and tgt features and alignment information.,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
"Default to a balanced sort, prioritizing tgt len match.",0
TODO: make this configurable.,1
"All examples have same number of features, so we peek first one",0
to get the num_feats.,0
Chain back the first element - we only want to peek it.,0
Below are helper functions for intra-class use only.,0
Mapping source tokens to indices in the dynamic dict.,0
"The codecs module seems to have bugs with seek()/tell(),",0
so we use io.open().,0
"We have associate iterator, just yields tuples",0
util we run parallel with it.,0
Yield tuples util this shard's size reaches the threshold.,0
This part of check is time consuming on Py2 (but,0
"it is quite fast on Py3, weird!). So we don't bother",0
to check for very line. Instead we chekc every 64,0
lines. Thus we are not dividing exactly per,0
"`shard_size`, but it is not too much difference.",0
We peek the first line and seek back to,0
the beginning of the file.,0
All examples must have same number of features.,0
-*- coding: utf-8 -*-,0
Peek at the first to see which fields are used.,0
"If out_examples is a generator, we need to save the filter_pred",0
"function in serialization too, which would cause a problem when",0
`torch.save()`. Thus we materialize it as a list.,0
STFT,0
NOTE: the translator exept a filepath as parameter,0
therefore we write the data as a temp file.,0
NOTE: If an input contains an line separator \n we split it,0
into subsegments that we translate independantly,0
we then merge the translations together with the same,0
line breaks,0
Creating a new object is faster,0
Sorting,0
"output += (""GOLD SCORE: {:.4f}"".format(self.gold_score))",0
for debugging,0
Statistics,0
Debug attention.,0
(0) Prep each of the components of the search.,0
And helper method for reducing verbosity.,0
Define a list of tokens to exclude from ngram-blocking,0
"exclusion_list = [""<t>"", ""</t>"", "".""]",0
Help functions for working with beams and batches,0
(1) Run the encoder on the src.,0
(2) Repeat src objects `beam_size` times.,0
"(3) run the decoder to generate sentences, using beam search.",0
Construct batch x beam_size nxt words.,0
Get all the pending current beam words and arrange for forward.,0
Turn any copied words to UNKs,0
0 is unk,0
Temporary kludge solution to handle changed dim expectation,1
in the decoder,0
Run one step.,0
dec_out: beam x rnn_size,0
(b) Compute a vector of batch x beam word scores.,0
beam x tgt_vocab,0
beam x (tgt_vocab + extra_vocab),0
beam x tgt_vocab,0
(c) Advance each beam.,0
(4) Extract sentences from beam.,0
(1) run the encoder on the src,0
"(2) if a target is specified, compute the 'goldScore'",0
(i.e. log likelihood) of the target under the model,0
Log prob of each word.,0
The score for each translation on the beam.,0
The backpointers at each time-step.,0
The outputs at each time-step.,0
Has EOS topped the beam yet.,0
The attentions (matrix) for each time.,0
Time and k pair for finished.,0
Information for global scoring.,0
Minimum prediction length,0
Apply Penalty at every step,0
force the output to be longer than self.min_length,0
Sum the previous scores.,0
Don't let EOS have children.,0
Block ngram repeats,0
"Last n tokens, n = block_ngram_repeat",0
Skip the blocking if it is in the exclusion list,0
"best_scores_id is flattened beam x word array, so calculate which",0
word and beam each score came from,0
End condition is when top-of-beam is EOS and no global score.,0
Add from beam until we have minimum outputs.,0
Term will be subtracted from probability,0
Probability will be divided by this,0
