Commit Message,predict
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
Extract all the contents of zip file in current directory,0
use model name as subfolder,0
Lazy import,0
output information,0
Extract all the contents of zip file in current directory,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
load_big_file is a workaround byhttps://github.com/highway11git,1
to load models on some Mac/Windows setups,0
see https://github.com/zalandoresearch/flair/issues/351,0
first determine the distribution of classes in the dataset,0
weight for each sample,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
Build the regular expression pattern dynamically based on the provided symbols,0
This will match any character from the symbols list that doesn't have spaces around it,0
"Add space before and after symbols, where necessary",0
Ensure that we are adding a space only if there isn't one already,0
increment for last token in sentence if not followed by whitespace,0
this is the default init size of a lmdb database for embeddings,0
get db filename from embedding name,0
"In case initialization of cached version failed, just fallback to the original WordEmbeddings",0
SequenceTagger,0
TextClassifier,0
get db filename from embedding name,0
if embedding database already exists,0
"otherwise, push embedding to database",0
if embedding database already exists,0
open the database in read mode,0
we need to set self.k,0
create and load the database in write mode,0
"no idea why, but we need to close and reopen the environment to avoid",0
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot,0
when opening new transaction !,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
set 'add_unk' if the dictionary was created with a version of Flair older than 0.9,0
set 'add_unk' depending on whether <unk> is a key,0
"if one embedding name, directly return it",0
"if multiple embedding names, concatenate them",0
First we remove any existing labels for this PartOfSentence in self.sentence,0
labels also need to be deleted at Sentence object,0
delete labels at object itself,0
The Token is a special _PartOfSentence in that it may be initialized without a Sentence.,0
"therefore, labels get added only to the Sentence if it exists",0
The Token is a special _PartOfSentence in that it may be initialized without a Sentence.,0
"Therefore, labels get set only to the Sentence if it exists",0
"check if the span already exists. If so, return it",0
else make a new span,0
"check if the relation already exists. If so, return it",0
else make a new relation,0
private field for all known spans,0
the tokenizer used for this sentence,0
some sentences represent a document boundary (but most do not),0
internal variables to denote position inside dataset,0
"if text is passed, instantiate sentence with tokens (words)",0
determine token positions and whitespace_after flag,0
the last token has no whitespace after,0
log a warning if the dataset is empty,0
data with zero-width characters cannot be handled,0
set token idx and sentence,0
append token to sentence,0
register token annotations on sentence,0
move sentence embeddings to device,0
also move token embeddings to device,0
clear token embeddings,0
infer whitespace after field,0
"if sentence has no tokens, return empty string",0
"otherwise, return concatenation of tokens with the correct offsets",0
The sentence's start position is not propagated to its tokens.,0
"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces.",0
No character at the corresponding code point: remove it,0
"if no label if specified, return all labels",0
"if the label type exists in the Sentence, return it",0
return empty list if none of the above,0
labels also need to be deleted at all tokens,0
labels also need to be deleted at all known spans,0
remove spans without labels,0
delete labels at object itself,0
set name,0
abort if no data is provided,0
sample test data from train if none is provided,0
sample dev data from train if none is provided,0
set train dev and test data,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
"first, determine the datapoint type by going through dataset until first label is found",0
count all label types per sentence,0
go through all labels of label_type and count values,0
special handling for Token-level annotations. Add all untagged as 'O' label,0
"if an unk threshold is set, UNK all label values below this threshold",0
sample randomly from a label distribution according to the probabilities defined by the noise transition matrix,0
replace the old label with the new one,0
keep track of the old (clean) label using another label type category,0
keep track of how many labels in total are flipped,0
sample randomly from a label distribution according to the probabilities defined by the desired noise share,0
replace the old label with the new one,0
keep track of the old (clean) label using another label type category,0
keep track of how many labels in total are flipped,0
"add a dummy ""O"" to close final prediction",0
return complex list,0
internal variables,0
non-set tags are OUT tags,0
anything that is not OUT is IN,0
does this prediction start a new span?,0
B- and S- always start new spans,0
"if the predicted class changes, I- starts a new span",0
"if the predicted class changes and S- was previous tag, start a new span",0
if an existing span is ended (either by reaching O or starting a new span),0
determine score and value,0
append to result list,0
reset for-loop variables for new span,0
remember previous tag,0
global variable: cache_root,0
Get the device from the environment variable,0
global variable: device,0
"No need for correctness checks, torch is doing it",0
global variable: version,0
global variable: arrow symbol,0
Attach optimizer,0
"convert `metrics` to float, in case it's a zero-dim Tensor",0
if storage mode option 'none' delete everything,0
"if dynamic embedding keys not passed, identify them automatically",0
always delete dynamic embeddings,0
"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)",0
add tokens before the entity,0
add new entity tokens,0
add any remaining tokens to a new chunk,0
optional metric space decoder if prototypes have different length than embedding,0
create initial prototypes for all classes (all initial prototypes are a vector of all 1s),0
"if set, create initial prototypes from normal distribution",0
"if set, use a radius",0
all parameters will be pushed internally to the specified device,0
decode embeddings into prototype space,0
"if unlabeled distance is set, mask out loss to unlabeled class prototype",0
all parameters will be pushed internally to the specified device,0
Reset prototype updates,0
verbalize BIOES labels,0
"if label is not BIOES, use label itself",0
Always include the name of the Model class for which the state dict holds,0
"this seems to just return model name, not a model with that name",0
"write out a ""model card"" if one is set",0
save model,0
"if this class is abstract, go through all inheriting classes and try to fetch and load the model",0
get all non-abstract subclasses,0
"try to fetch the model for each subclass. if fetching is possible, load model and return it",0
"skip any invalid loadings, e.g. not found on HuggingFace hub",0
"if the model cannot be fetched, load as a file",0
try to get model class from state,0
"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses",0
"skip any invalid loadings, e.g. not found on HuggingFace hub",0
"if this class is not abstract, fetch the model and load it",0
"make sure <unk> is contained in gold_label_dictionary, if given",0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
loss calculation,0
variables for printing,0
variables for computing scores,0
remove any previously predicted labels,0
predict for batch,0
get the gold labels,0
add to all_predicted_values,0
make printout lines,0
convert true and predicted values to two span-aligned lists,0
delete excluded labels if exclude_labels is given,0
"if after excluding labels, no label is left, ignore the datapoint",0
write all_predicted_values to out_file if set,0
make the evaluation dictionary,0
check if this is a multi-label problem,0
compute numbers by formatting true and predicted such that Scikit-Learn can use them,0
multi-label problems require a multi-hot vector for each true and predicted label,0
single-label problems can do with a single index for each true and predicted label,0
"now, calculate evaluation numbers",0
there is at least one gold label or one prediction (default),0
compute accuracy separately as it is not always in classification_report (e.g. when micro avg exists),0
"if there is only one label, then ""micro avg"" = ""macro avg""",0
"The ""micro avg"" appears only in the classification report if no prediction is possible.",0
"Otherwise, it is identical to the ""macro avg"". In this case, we add it to the report.",0
"Create and populate score object for logging with all evaluation values, plus the loss",0
issue error and default all evaluation numbers to 0.,0
check if there is a label mismatch,0
print info,0
set the embeddings,0
initialize the label dictionary,0
initialize the decoder,0
set up multi-label logic,0
init dropouts,0
loss weights and loss function,0
Initialize the weight tensor,0
set up gradient reversal if so specified,0
embed sentences,0
get a tensor of data points,0
do dropout,0
make a forward pass to produce embedded data points and labels,0
get the data points for which to predict labels,0
get their gold labels as a tensor,0
pass data points through network to get encoded data point tensor,0
"decode, passing label tensor if needed, such as for prototype updates",0
an optional masking step (no masking in most cases),0
calculate the loss,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
filter data points in batch,0
stop if all sentences are empty,0
pass data points through network and decode,0
if anything could possibly be predicted,0
remove previously predicted labels of this type,0
filter data points that have labels outside of dictionary,0
add DefaultClassifier arguments,0
add variables of DefaultClassifier,0
Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23,0
Get projected 1st dimension,0
Compute bilinear form,0
Arcosh,0
Project the input data to n+1 dimensions,0
"The first dimension, is recomputed in the distance module",0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for",0
"train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
plot i,0
save plots,0
save plots,0
plt.show(),0
save plot,0
auto-spawn on GPU if available,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
TODO: not saving lines yet,1
TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.,1
- MaskedRelationClassifier ?,0
This depends if this relation classification architecture should replace or offer as an alternative.,0
Set label type and prepare label dictionary,0
Initialize super default classifier,0
Add the special tokens from the encoding strategy,0
"Auto-spawn on GPU, if available",0
Only use entities labelled with the specified labels for each label type,0
Only use entities above the specified threshold,0
Use a dictionary to find gold relation annotations for a given entity pair,0
Yield head and tail entity pairs from the cross product of all entities,0
Remove identity relation entity pairs,0
Remove entity pairs with labels that do not match any,0
of the specified relations in `self.entity_pair_labels`,0
"Obtain gold label, if existing",0
Preserve context around the entities. Always include their in-between context.,0
Some sanity checks,0
Sanity check: Do not create a labeled span if one entity contains the other,0
Pre-compute non-leading head and tail tokens for entity masking,0
We can not use the plaintext of the head/tail span in the sentence as the mask/marker,0
since there may be multiple occurrences of the same entity mentioned in the sentence.,0
"Therefore, we use the span's position in the sentence.",0
Filter cases in which the distance between the two entities is too large,0
Remove excess tokens left and right of entity pair to make encoded sentence shorter,0
Create masked sentence,0
Add gold relation annotation as sentence label,0
"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,",0
"during prediction, the forward pass does not need any knowledge about the entities in the sentence.",0
"If we sample missing splits, the encoded sentences that correspond to the same original sentences",0
"may get distributed into different splits. For training purposes, this is always undesired.",0
Ensure that all sentences are encoded properly,0
Deal with the case where all sentences are encoded sentences,0
"mypy does not infer the type of ""sentences"" restricted by the if statement",0
Deal with the case where all sentences are standard (non-encoded) sentences,0
"For each encoded sentence, transfer its prediction onto the original relation",0
check if there is a label mismatch,0
"if the gold label is O and is correctly predicted as no label, do not print out as this clutters",0
the output file with trivial predictions,0
auto-spawn on GPU if available,0
pad strings with whitespaces to longest sentence,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
"""document_delimiter"" property may be missing in some older pre-trained models",0
serialize the language models and the constructor arguments (but nothing else),0
special handling for deserializing language models,0
re-initialize language model with constructor arguments,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
Transform input data into TARS format,0
"if there are no labels, return a random sample as negatives",0
"otherwise, go through all labels",0
make sure the probabilities always sum up to 1,0
get and embed all labels by making a Sentence object that contains only the label text,0
get each label embedding and scale between 0 and 1,0
compute similarity matrix,0
"the higher the similarity, the greater the chance that a label is",0
sampled as negative example,0
make label dictionary if no Dictionary object is passed,0
prepare dictionary of tags (without B- I- prefixes and without UNK),0
check if candidate_label_set is empty,0
make list if only one candidate label is passed,0
create label dictionary,0
note current task,0
create a temporary task,0
make zero shot predictions,0
switch to the pre-existing task,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
make a tars sentence where all labels are O by default,0
init new TARS classifier,0
set all task information,0
progress bar for verbosity,0
stop if all sentences are empty,0
always remove tags first,0
go through each sentence in the batch,0
always remove tags first,0
get the span and its label,0
determine whether tokens in this span already have a label,0
only add if all tokens have no label,0
make and add a corresponding predicted span,0
set indices so that no token can be tagged twice,0
clearing token embeddings to save memory,0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
get the serialized embeddings,0
remap state dict for models serialized with Flair <= 0.11.3,0
init new TARS classifier,0
set all task information,0
with torch.no_grad():,0
progress bar for verbosity,0
stop if all sentences are empty,0
always remove tags first,0
go through each sentence in the batch,0
always remove tags first,0
add all labels that according to TARS match the text and are above threshold,0
do not add labels below confidence threshold,0
only use label with the highest confidence if enforcing single-label predictions,0
add the label with the highest score even if below the threshold if force label is activated.,0
remove previously added labels and only add the best label,0
clearing token embeddings to save memory,0
set separator to concatenate three sentences,0
auto-spawn on GPU if available,0
set separator to concatenate two sentences,0
auto-spawn on GPU if available,0
"If the concatenated version of the text pair does not exist yet, create it",0
pooling operation to get embeddings for entites,0
set embeddings,0
set relation and entity label types,0
"whether to use gold entity pairs, and whether to filter entity pairs by type",0
filter entity pairs according to their tags if set,0
whether to encode characters and whether to use attention (attention can only be used if chars are encoded),0
character dictionary for decoding and encoding,0
make sure <unk> is in dictionary for handling of unknown characters,0
add special symbols to dictionary if necessary and save respective indices,0
---- ENCODER ----,0
encoder character embeddings,0
encoder pre-trained embeddings,0
encoder RNN,0
additional encoder linear layer if bidirectional encoding,0
---- DECODER ----,0
decoder: linear layers to transform vectors to and from alphabet_size,0
when using attention we concatenate attention outcome and decoder hidden states,0
decoder RNN,0
loss and softmax,0
self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction,0
add additional columns for special symbols if necessary,0
initialize with dummy symbols,0
encode inputs,0
get labels (we assume each token has a lemma label),0
get char indices for labels of sentence,0
"(batch_size, max_sequence_length) batch_size = #words in sentence,",0
max_sequence_length = length of longest label of sentence + 1,0
get char embeddings,0
"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size",0
take decoder input and initial hidden and pass through RNN,0
"if all encoder outputs are provided, use attention",0
take convex combinations of encoder hidden states as new output using the computed attention coefficients,0
"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)",0
get all tokens,0
encode input characters by sending them through RNN,0
get one-hots for characters and add special symbols / padding,0
determine length of each token,0
embed sentences,0
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)",0
variable to store initial hidden states for decoder,0
encode input characters by sending them through RNN,0
test packing and padding,0
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder,0
concatenate the final hidden states of the encoder. These will be projected to hidden_size of,0
decoder later with self.emb_to_hidden,0
mask out vectors that correspond to a dummy symbol (TODO: check attention masking),1
use token embedding as initial hidden state for decoder,0
concatenate everything together and project to appropriate size for decoder,0
variable to store initial hidden states for decoder,0
encode input characters by sending them through RNN,0
note that we do not need to fill up with dummy symbols since we process each token seperately,0
embed character one-hots,0
send through encoder RNN (produces initial hidden for decoder),0
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder,0
project 2*hidden_size to hidden_size,0
concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder,0
later with self.emb_to_hidden,0
use token embedding as initial hidden state for decoder,0
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)",0
concatenate everything together and project to appropriate size for decoder,0
"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)",0
"create target vector (batch_size, max_label_seq_length + 1)",0
filter empty sentences,0
max length of the predicted sequences,0
for printing,0
stop if all sentences are empty,0
remove previously predicted labels of this type,0
create list of tokens in batch,0
encode inputs,0
"create input for first pass (batch_size, 1, input_size), first letter is special character <S>",0
sequence length is always set to one in prediction,0
option 1: greedy decoding,0
predictions,0
decode next character,0
pick top beam size many outputs with highest probabilities,0
option 2: beam search,0
out_probs = self.softmax(output_vectors).squeeze(1),0
make sure no dummy symbol <> or start symbol <S> is predicted,0
pick top beam size many outputs with highest probabilities,0
"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1",0
"leading_indices and probabilities have size (batch_size, beam_size)",0
keep scores of beam_size many hypothesis for each token in the batch,0
stack all leading indices of all hypothesis and corresponding hidden states in two tensors,0
save sequences so far,0
keep track of how many hypothesis were completed for each token,0
"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)",0
decode with log softmax,0
make sure no dummy symbol <> or start symbol <S> is predicted,0
"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside",0
"if the sequence is already ended, do not record as candidate",0
index of token in in list tokens_in_batch,0
print(token_number),0
hypothesis score,0
TODO: remove token if number of completed hypothesis exceeds given value,1
set score of corresponding entry to -inf so it will not be expanded,0
get leading_indices for next expansion,0
find highest scoring hypothesis among beam_size*beam_size possible ones for each token,0
take beam_size many copies of scores vector and add scores of possible new extensions,0
"size (beam_size*batch_size, beam_size)",0
print(hypothesis_scores),0
"reshape to vector of size (batch_size, beam_size*beam_size),",0
each row contains beam_size*beam_size scores of the new possible hypothesis,0
print(hypothesis_scores_per_token),0
"choose beam_size best for each token - size (batch_size, beam_size)",0
out of indices_per_token we now need to recompute the original indices of the hypothesis in,0
a list of length beam_size*batch_size,0
"where the first three inidices belong to the first token, the next three to the second token,",0
and so on,0
with these indices we can compute the tensors for the next iteration,0
expand sequences with corresponding index,0
add log-probabilities to the scores,0
save new leading indices,0
save corresponding hidden states,0
it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations,0
in that case we append one of the final seuqences without end symbol to the final_candidates,0
get best final hypothesis for each token,0
get characters from index sequences and add predicted label to token,0
"Overwrites evaluate of parent class to remove the ""by class"" printout",0
set separator to concatenate two sentences,0
init dropouts,0
auto-spawn on GPU if available,0
make a forward pass to produce embedded data points and labels,0
get their gold labels as a tensor,0
pass data points through network to get encoded data point tensor,0
decode,0
calculate the loss,0
get a tensor of data points,0
do dropout,0
"If the concatenated version of the text pair does not exist yet, create it",0
add Model arguments,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
"if the classifier predicts BIO/BIOES span labels, the internal label dictionary must be computed",0
fields in case this is a span-prediction problem,0
the label type,0
all parameters will be pushed internally to the specified device,0
special handling during training if this is a span prediction problem,0
internal variables,0
non-set tags are OUT tags,0
anything that is not OUT is IN,0
does this prediction start a new span?,0
B- and S- always start new spans,0
"if the predicted class changes, I- starts a new span",0
"if the predicted class changes and S- was previous tag, start a new span",0
if an existing span is ended (either by reaching O or starting a new span),0
reset for-loop variables for new span,0
remember previous tag,0
"if there is a span at end of sentence, add it",0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
print labels in CoNLL format,0
internal candidate lists of generator,0
load Zelda candidates if so passed,0
create candidate lists,0
"if lower casing is enabled, create candidate lists of lower cased versions",0
create a new dictionary for lower cased mentions,0
go through each mention and its candidates,0
"check if backoff mention already seen. If so, add candidates. Else, create new entry.",0
set lowercased version as map,0
"only use span label type if there are predictions, otherwise search for output label type (training labels)",0
remap state dict for models serialized with Flair <= 0.11.3,0
get the candidates,0
"during training, add the gold value as candidate",0
----- Create the internal tag dictionary -----,0
span-labels need special encoding (BIO or BIOES),0
the big question is whether the label dictionary should contain an UNK or not,0
"without UNK, we cannot evaluate on data that contains labels not seen in test",0
"with UNK, the model learns less well if there are no UNK examples",0
is this a span prediction problem?,0
----- Embeddings -----,0
----- Initial loss weights parameters -----,0
----- RNN specific parameters -----,0
----- Conditional Random Field parameters -----,0
"Previously trained models have been trained without an explicit CRF, thus it is required to check",0
whether we are loading a model from state dict in order to skip or add START and STOP token,0
----- Dropout parameters -----,0
dropouts,0
remove word dropout if there is no contact over the sequence dimension.,0
----- Model layers -----,0
----- RNN layer -----,0
"If shared RNN provided, else create one for model",0
Whether to train initial hidden state,0
final linear map to tag space,0
"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss",0
"if using CRF, we also require a CRF and a Viterbi decoder",0
"if there are no sentences, there is no loss",0
forward pass to get scores,0
calculate loss given scores and labels,0
make a zero-padded tensor for the whole sentence,0
linear map to tag space,0
"Depending on whether we are using CRF or a linear layer, scores is either:",0
"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF",0
"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer",0
spans need to be encoded as token-level predictions,0
all others are regular labels for each token,0
make sure it's a list,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
get features from forward propagation,0
remove previously predicted labels of this type,0
"if return_loss, get loss value",0
make predictions,0
add predictions to Sentence,0
BIOES-labels need to be converted to spans,0
"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)",0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
core Flair models on Huggingface ModelHub,0
"Large NER models,",0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Language-specific NER models,0
Language-specific POS models,0
Historic German,0
English NER models,0
English SRL models,0
Danish models,0
German models,0
Arabic models,0
French models,0
Dutch models,0
Malayalam models,0
Portuguese models,0
Biomedical models,0
check if model name is a valid local file,0
"check if model key is remapped to HF key - if so, print out information",0
get mapped name,0
"if not, check if model key is remapped to direct download location. If so, download model",0
"for all other cases (not local file or special download location), use HF model hub",0
## Demo: How to use in Flair,0
load tagger,0
make example sentence,0
predict NER tags,0
print sentence,0
print predicted NER spans,0
iterate over entities and print,0
Lazy import,0
Save model weight,0
Determine if model card already exists,0
Generate and save model card,0
Upload files,0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
print labels in CoNLL format,0
Dense + sparse retrieval,0
fetched from original repo to avoid download,0
"just in case we add: fuzzy search, Levenstein, ...",0
"for now we always fall back to SapBERT,",0
but we should train our own models at some point,0
NOTE: Avoid emtpy string if mentions are just punctutations (e.g. `-` or `(`),0
NOTE: Avoid emtpy string if mentions are just punctuations (e.g. `-` or `(`),0
Ab3P works on sentence-level and not on a single entity mention / name,0
- so we just apply the wrapped text pre-processing here (if configured),0
NOTE: ensure correct similarity metric for pretrained model,0
empty cuda cache if device is a cuda device,0
"Sanity conversion: if flair.device was set as a string, convert to torch.device",0
NOTE: This is a hacky workaround for the fact that,1
the `label_type`s in `Classifier.load('hunflair)` are,0
"'diseases', 'genes', 'species', 'chemical' instead of 'ner'.",0
We warn users once they need to update SequenceTagger model,0
See: https://github.com/flairNLP/flair/pull/3387,0
make sure sentences is a list of sentences,0
Make sure entity label types are represented as dict,0
Collect all entities based on entity type labels configuration,0
Preprocess entity mentions,0
Retrieve top-k concept / entity candidates,0
Add a label annotation for each candidate,0
load model by entity_type,0
check if we have a hybrid pre-trained model,0
the multi task model has several labels,0
Add metrics so they will be available to _publish_eval_result.,0
biomedical models,0
entity linker,0
print(match),0
print(span),0
auto-spawn on GPU if available,0
remap state dict for models serialized with Flair <= 0.11.3,0
English sentiment models,0
Communicative Functions Model,0
"If we sample missing splits, the encoded sentences that correspond to the same original sentences",0
"may get distributed into different splits. For training purposes, this is always undesired.",0
Prepend the task description prompt to the sentence text,0
Make sure it's a list,0
Reconstruct all annotations from the original sentence (necessary for learning classifiers),0
If all sentences are not augmented -> augment them,0
"mypy does not infer the type of ""sentences"" restricted by the if statement",0
"mypy does not infer the type of ""sentences"" restricted by code above",0
Compute prediction label type,0
make sure it's a list,0
"If all sentences are already augmented (i.e. compatible with this class), just forward the sentences",0
"mypy does not infer the type of ""sentences"" restricted by the if statement",0
Remove existing labels,0
Augment sentences - copy all annotation of the given tag type,0
Predict on augmented sentence and store it in an internal annotation layer / label,0
Append predicted labels to the original sentences,0
check if model name is a valid local file,0
check if model name is a pre-configured hf model,0
"scores_at_targets[range(features.shape[0]), lengths.values -1]",0
Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices,0
"Initially, get scores from <start> tag to all other tags",0
"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp",0
"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep",0
Create a tensor to hold accumulated sequence scores at each current tag,0
Create a tensor to hold back-pointers,0
"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag",0
"Let pads be the <end> tag index, since that was the last tag in the decoded sequence",0
"We add scores at current timestep to scores accumulated up to previous timestep, and",0
choose the previous timestep that corresponds to the max. accumulated score for each current timestep,0
"If sentence is over, add transition to STOP-tag",0
Decode/trace best path backwards,0
Sanity check,0
remove start-tag and backscore to stop-tag,0
Max + Softmax to get confidence score for predicted label and append label to each token,0
"Transitions are used in the following way: transitions[to, from].",0
"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag",0
to START-tag and from STOP-tag to any other tag to -10000.,0
"if necessary, make batch_steps",0
break up the batch into slices of size,0
mini_batch_chunk_size,0
"if training also uses dev/train data, include in training set",0
evaluation and monitoring,0
sampling and shuffling,0
evaluation and monitoring,0
when and what to save,0
logging parameters,0
acceleration,0
plugins,0
activate annealing plugin,0
call self.train_custom with all parameters (minus the ones specific to the AnnealingPlugin),0
training parameters,0
evaluation and monitoring,0
sampling and shuffling,0
evaluation and monitoring,0
when and what to save,0
logging parameters,0
acceleration,0
plugins,0
annealing logic,0
training parameters,0
evaluation and monitoring,0
sampling and shuffling,0
evaluation and monitoring,0
when and what to save,0
logging parameters,0
acceleration,0
plugins,0
training parameters,0
evaluation and monitoring,0
sampling and shuffling,0
evaluation and monitoring,0
when and what to save,0
logging parameters,0
acceleration,0
plugins,0
Create output folder,0
=== START BLOCK: ACTIVATE PLUGINS === #,0
We first activate all optional plugins. These take care of optional functionality such as various,0
logging techniques and checkpointing,0
log file plugin,0
loss file plugin,0
plugin for writing weights,0
plugin for checkpointing,0
=== END BLOCK: ACTIVATE PLUGINS === #,0
derive parameters the function was called with (or defaults),0
initialize model card with these parameters,0
Prepare training data and get dataset size,0
"determine what splits (train, dev, test) to evaluate",0
determine how to determine best model and whether to save it,0
instantiate the optimizer,0
initialize sampler if provided,0
init with default values if only class is provided,0
set dataset to sample from,0
configure special behavior to use multiple GPUs,0
Guard against each process initializing corpus differently due to e.g. different random seeds,0
this field stores the names of all dynamic embeddings in the model (determined after first forward pass),0
Sanity checks,0
"Sanity conversion: if flair.device was set as a string, convert to torch.device",0
-- AmpPlugin -> wraps with AMP,0
-- AnnealingPlugin -> initialize schedulers (requires instantiated optimizer),0
At any point you can hit Ctrl + C to break out of training early.,0
"- SchedulerPlugin -> load state for anneal_with_restarts, batch_growth_annealing, logic for early stopping",0
- LossFilePlugin -> get the current epoch for loss file logging,0
"if shuffle_first_epoch==False, the first epoch is not shuffled",0
log infos on training progress every `log_modulo` batches,0
process mini-batches,0
zero the gradients on the model and optimizer,0
forward and backward for batch,0
forward pass,0
We need to __call__ ddp_model() because this triggers hooks that sync gradients.,0
But that calls forward rather than forward_loss. So we patch forward to redirect,0
to forward_loss. Then undo the patch in case forward_loss itself calls forward.,0
identify dynamic embeddings (always deleted) on first sentence,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
do the optimizer step,0
DDP averages across processes but we want the sum,0
- SchedulerPlugin -> do the scheduler step if one-cycle or linear decay,0
- WeightExtractorPlugin -> extracts weights,0
- CheckpointPlugin -> executes save_model_each_k_epochs,0
- SchedulerPlugin -> log bad epochs,0
Determine if this is the best model or if we need to anneal,0
log results,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
use DEV split to determine if this is the best model so far,0
"if not using DEV score, determine best model using train loss",0
- LossFilePlugin -> somehow prints all relevant metrics,0
- AnnealPlugin -> scheduler step,0
- SWAPlugin -> restores SGD weights from SWA,0
"if we do not use dev data for model selection, save final model",0
TensorboardLogger -> closes writer,0
test best model if test data is present,0
get and return the final test score of best model,0
MetricHistoryPlugin -> stores the loss history in return_values,0
"Store return values, as they will be erased by reset_training_attributes",0
get a random sample of training sentences,0
create a model card for this model with Flair and PyTorch version,0
record Transformers version if library is loaded,0
remember the training parameters,0
special rule for Path variables to make sure models can be deserialized on other OS,0
classes are only serialized as names,0
"TextDataset returns a list. valid and test are only one file,",0
so return the first element,0
cast string to Path,0
error message if the validation dataset is too small,0
Shuffle training files randomly after serially iterating,0
through corpus one,0
"iterate through training data, starting at",0
self.split (for checkpointing),0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
Backward,0
`clip_grad_norm` helps prevent the exploding gradient,0
problem in RNNs / LSTMs.,0
We detach the hidden state from how it was,0
previously produced.,0
"If we didn't, the model would try backpropagating",0
all the way to start of the dataset.,0
explicitly remove loss to clear up memory,0
#########################################################,0
Save the model if the validation loss is the best we've,0
seen so far.,0
#########################################################,0
print info,0
#########################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
"no need to check for MetricName, as __add__ of other would be called in this case",0
"This flag tracks, whether an event is currently being processed (otherwise it is added to the queue)",0
instantiate plugin,0
"Reset the flag, since an exception event might be dispatched",0
"If there is no **kw argument in the callback, check if any of the passed kw args is not accepted by",0
the callback,0
go through all attributes,0
get attribute hook events (may raise an AttributeError),0
register function as a hook,0
"Decorator was used with parentheses, but no args",0
Decorator was used with args (strings specifiying the events),0
Decorator was used without args,0
path to store the model,0
special annealing modes,0
determine the min learning rate,0
"minimize training loss if training with dev data, else maximize dev score",0
instantiate the scheduler,0
stop training if learning rate becomes too small,0
reload last best model if annealing with restarts is enabled,0
calculate warmup steps,0
skip if no optimization has happened.,0
saves the model with full vocab as checkpoints etc were created with reduced vocab.,0
TODO: check if metric is in tracked metrics,1
prepare loss logging file and set up header,0
set up all metrics to collect,0
set up headers,0
name: HEADER,0
Add all potentially relevant metrics. If a metric is not published,0
"after the first epoch (when the header is written), the column is",0
removed at that point.,0
initialize the first log line,0
record is a list of scalars,0
output log file,0
remove columns where no value was found on the first epoch (could be != 1 if training was resumed),0
make headers on epoch 1,0
write header,0
adjust alert level,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn,0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
"legacy pickle-like saving for image embeddings, as implementation details are not obvious",0
"legacy pickle-like loading for image embeddings, as implementation details are not obvious",0
"<cls> token initially set to 1/D, so it attends to all image features equally",0
add positional encodings,0
reshape the pixels into the sequence,0
layer norm after convolution and positional encodings,0
add <cls> token,0
"transformer requires input in the shape [h*w+1, b, d]",0
the output is an embedding of <cls> token,0
this parameter is fixed,0
optional fine-tuning on top of embedding layer,0
"if only one sentence is passed, convert to list of sentence",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push through RNN,0
after-RNN dropout,0
extract embeddings from RNN,0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
serialize the language models and the constructor arguments (but nothing else),0
re-initialize language model with constructor arguments,0
special handling for deserializing language models,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
IMPORTANT: add embeddings as torch modules,0
iterate over sentences,0
"if its a forward LM, take last state",0
"convert to plain strings, embedded in a list for the encode function",0
CNN,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push CNN,0
after-CNN dropout,0
extract embeddings from CNN,0
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency",0
"if only one sentence is passed, convert to list of sentence",0
Expose base classses,0
Expose document embedding classes,0
Expose image embedding classes,0
Expose legacy embedding classes,0
Expose token embedding classes,0
in some cases we need to insert zero vectors for tokens without embedding.,0
sum embeddings for each token,0
calculate the mean of subtokens,0
Create a mask for valid tokens based on token_lengths,0
padding,0
remove special markup,0
check if special tokens exist to circumvent error message,0
iterate over subtokens and reconstruct tokens,0
remove special markup,0
check if reconstructed token is special begin token ([CLS] or similar),0
some BERT tokenizers somehow omit words - in such cases skip to next token,0
"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token.",0
if tokens are unaccounted for,0
check if all tokens were matched to subtokens,0
The layoutlm tokenizer doesn't handle ocr themselves,0
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial",0
"cannot run `.encode` if ocr boxes are required, assume",0
"transformers returns the ""added_tokens.json"" even if it doesn't create it",0
"transformers returns the ""added_tokens.json"" even if it doesn't create it",0
in case of doubt: token embedding has higher priority than document embedding,0
random check some tokens to save performance.,0
Models such as FNet do not have an attention_mask,0
set language IDs for XLM-style transformers,0
"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have",0
"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids.",0
set context if not set already,0
flair specific pre-tokenization,0
fields to store left and right context,0
expand context only if context_length is set,0
"if context_dropout is set, randomly deactivate left context during training",0
"if context_dropout is set, randomly deactivate right context during training",0
"if use_context_separator is set, add a [FLERT] token",0
return expanded sentence and context length information,0
"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first.",0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
do not print transformer warnings as these are confusing in this case,0
load tokenizer and transformer model,0
load tokenizer from inmemory zip-file,0
if model is quantized by BitsAndBytes this will fail,0
add adapters for finetuning,0
peft_config: PeftConfig,0
model name,0
embedding parameters,0
send mini-token through to check how many layers the model has,0
return length,0
"If we use a context separator, add a new special token",0
"most models have an initial BOS token, except for XLNet, T5 and GPT2",0
"when initializing, embeddings are in eval mode by default",0
in case of doubt: token embedding has higher priority than document embedding,0
in case of doubt: token embedding has higher priority than document embedding,0
legacy TransformerDocumentEmbedding,0
legacy TransformerTokenEmbedding,0
legacy Flair <= 0.12,0
legacy Flair <= 0.7,0
legacy TransformerTokenEmbedding,0
Legacy TransformerDocumentEmbedding,0
legacy TransformerTokenEmbedding,0
legacy TransformerDocumentEmbedding,0
some models like the tars model somehow lost this information.,0
copy values from new embedding,0
do not switch the attention implementation upon reload.,0
those parameters are only from the super class and will be recreated in the constructor.,0
cls first pooling can be done without recreating sentence hidden states,0
make the tuple a tensor; makes working with it easier.,0
"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention",0
only use layers that will be outputted,0
this parameter is fixed,0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
make compatible with serialized models,0
gensim version 4,0
gensim version 3,0
"if no embedding is set, the vocab and embedding length is required",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNINOS embeddings,0
pubmed embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
"this is required to force the module on the cpu,",0
"if a parent module is put to gpu, the _apply is called to each sub_module",0
self.to(..) actually sets the device properly,0
this ignores the get_cached_vec method when loading older versions,0
it is needed for compatibility reasons,0
gensim version 4,0
gensim version 3,0
"when loading the old versions from pickle, the embeddings might not be added as pytorch module.",0
"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might",0
lead to issues while loading (trying to load weights that weren't stored as python weights and therefore,0
not finding them),0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
multilingual models,0
English models,0
Arabic,0
Bulgarian,0
Czech,0
Danish,0
German,0
Spanish,0
Basque,0
Persian,0
Finnish,0
French,0
Hebrew,0
Hindi,0
Croatian,0
Indonesian,0
Italian,0
Japanese,0
Malayalam,0
Dutch,0
Norwegian,0
Polish,0
Portuguese,0
Pubmed,0
Slovenian,0
Swedish,0
Tamil,0
Spanish clinical,0
CLEF HIPE Shared task,0
Amharic,0
Ukrainian,0
load model if in pretrained model map,0
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir),0
CLEF HIPE models are lowercased,0
embeddings are static if we don't do finetuning,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout",0
gradients are enable if fine-tuning is enabled,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
offset mode that extracts at whitespace after last character,0
offset mode that extracts at last character,0
make compatible with old models,0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
check token.text is empty or not,0
set aggregation operation,0
add embeddings after updating,0
model architecture,0
model architecture,0
"""pl"",",0
download if necessary,0
load the model,0
"this is required to force the module on the cpu,",0
"if a parent module is put to gpu, the _apply is called to each sub_module",0
self.to(..) actually sets the device properly,0
"when loading the old versions from pickle, the embeddings might not be added as pytorch module.",0
"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might",0
lead to issues while loading (trying to load weights that weren't stored as python weights and therefore,0
not finding them),0
old embeddings do not have a torch-embedding and therefore do not store the weights in the saved torch state_dict,0
"however they are already initialized rightfully, so we just set the state dict from our current state dict",0
GLOVE embeddings,0
no need to recreate as NILCEmbeddings,0
read in test file if exists,0
read in dev file if exists,0
"find train, dev and test files if not specified",0
Add tags for each annotated span,0
Remove leading and trailing whitespaces from annotated spans,0
Search start and end token index for current span,0
If end index is not found set to last token,0
Throw error if indices are not valid,0
Add metadatas for sentence,0
Currently all Jsonl Datasets are stored in Memory,0
get train data,0
read in test file if exists,0
read in dev file if exists,0
"find train, dev and test files if not specified",0
special key for space after,0
special key for feature columns,0
special key for dependency head id,0
"store either Sentence objects in memory, or only file offsets",0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
determine encoding of text file,0
identify which columns are spans and which are word-level,0
now load all sentences,0
skip first line if to selected,0
option 1: keep Sentence objects in memory,0
pointer to previous,0
parse next sentence,0
quit if last sentence reached,0
skip banned sentences,0
set previous and next sentence for context,0
append parsed sentence to list in memory,0
option 2: keep source data in memory,0
"read lines for next sentence, but don't parse",0
quit if last sentence reached,0
append raw lines for each sentence,0
we make a distinction between word-level tags and span-level tags,0
read first sentence to determine which columns are span-labels,0
skip first line if to selected,0
check the first 5 sentences,0
go through all annotations and identify word- and span-level annotations,0
- if a column has at least one BIES we know it's a Span label,0
"- if a column has at least one tag that is not BIOES, we know it's a Token label",0
- problem cases are columns for which we see only O - in this case we default to Span,0
skip assigned columns,0
the space after key is always word-levels,0
"if at least one token has a BIES, we know it's a span label",0
"if at least one token has a label other than BIOES, we know it's a token label",0
all remaining columns that are not word-level are span-level,0
for column in self.word_level_tag_columns:,0
"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")",0
"if sentence ends, break",0
parse comments if possible,0
"otherwise, this line is a token. parse and add to sentence",0
check if this sentence is a document boundary,0
add span labels,0
discard tags from tokens that are not added to the sentence,0
parse relations if they are set,0
head and tail span indices are 1-indexed and end index is inclusive,0
parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas',0
"to set the metadata ""domain"" to ""de-orcas""",0
get fields from line,0
get head_id if exists (only in dependency parses),0
initialize token,0
go through all columns,0
'feats' and 'misc' column should be split into different fields,0
special handling for whitespace after,0
add each other feature as label-value pair,0
get the task name (e.g. 'ner'),0
get the label value,0
add label,0
remap regular tag names,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
set sentence context using partials TODO: pointer to dataset is really inefficient,1
use all domains,0
iter over all domains / sources and create target files,0
The conll representation of coref spans allows spans to,0
"overlap. If spans end or begin at the same word, they are",0
"separated by a ""|"".",0
The span begins at this word.,0
The span begins and ends at this word (single word span).,0
"The span is starting, so we record the index of the word.",0
"The span for this id is ending, but didn't start at this word.",0
Retrieve the start index from the document state and,0
add the span to the clusters for this id.,0
strip all bracketing information to,0
get the actual propbank label.,0
Entering into a span for a particular semantic role label.,0
We append the label and set the current span for this annotation.,0
"If there's no '(' token, but the current_span_label is not None,",0
then we are inside a span.,0
We're outside a span.,0
"Exiting a span, so we reset the current span label for this annotation.",0
The words in the sentence.,0
The pos tags of the words in the sentence.,0
the pieces of the parse tree.,0
The lemmatised form of the words in the sentence which,0
have SRL or word sense information.,0
The FrameNet ID of the predicate.,0
"The sense of the word, if available.",0
"The current speaker, if available.",0
"Cluster id -> List of (start_index, end_index) spans.",0
Cluster id -> List of start_indices which are open for this id.,0
Replace brackets in text and pos tags,0
with a different token for parse trees.,0
only keep ')' if there are nested brackets with nothing in them.,0
There are some bad annotations in the CONLL data.,0
"They contain no information, so to make this explicit,",0
we just set the parse piece to be None which will result,0
in the overall parse tree being None.,0
"If this is the first word in the sentence, create",0
empty lists to collect the NER and SRL BIO labels.,0
"We can't do this upfront, because we don't know how many",0
"components we are collecting, as a sentence can have",0
variable numbers of SRL frames.,0
Create variables representing the current label for each label,0
sequence we are collecting.,0
"If any annotation marks this word as a verb predicate,",0
we need to record its index. This also has the side effect,0
of ordering the verbal predicates by their location in the,0
"sentence, automatically aligning them with the annotations.",0
"this would not be reached if parse_pieces contained None, hence the cast",0
Non-empty line. Collect the annotation.,0
Collect any stragglers or files which might not,0
have the '#end document' format for the end of the file.,0
this dataset name,0
check if data there,0
column format,0
this dataset name,0
check if data there,0
column format,0
this dataset name,0
download data if necessary,0
download files if not present locally,0
we need to slightly modify the original files by adding some new lines after document separators,0
column format,0
this dataset name,0
download data if necessary,0
Set the base path for the dataset,0
Define column format,0
Define dataset name,0
Define data folder path,0
"Check if the train data file exists, otherwise download and prepare the dataset",0
Download and prepare the dataset,0
Initialize the parent class with the specified parameters,0
"Check if the line is a change, delete or add command (like 17721c17703,17705 or 5728d5727)",0
Append the previous change block to the changes list,0
Start a new change block,0
"Capture original lines (those marked with ""<"")",0
"Capture new lines (those marked with "">"")",0
Append the last change block to the changes list,0
Apply each change in reverse order (important to avoid index shift issues),0
"Determine the type of the change: `c` for change, `d` for delete, `a` for add",0
"Example command: 17721c17703,17705",0
Example command: 5728d5727,0
"Example command: 1000a1001,1002",0
Write the modified content to the output file,0
Strip whitespace to check if the line is empty,0
Write the first token followed by a newline if the line is not empty,0
Write an empty line if the line is empty,0
Strip the leading '[TOKEN]\t' from the annotation,0
Create a temporary directory,0
Check the contents of the temporary directory,0
Extract only the tokens from the original CoNLL03 files,0
Apply the downloaded patch files to apply our token modifications (e.g. line breaks),0
Merge the updated token files with the CleanCoNLL annotations,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
Remove CoNLL-U meta information in the last column,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
entity_mapping,0
this dataset name,0
download data if necessary,0
data validation,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download files if not present locallys,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
# download zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
download data if necessary,0
"unzip the downloaded repo and merge the train, dev and test datasets",0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
check if data there,0
create folder,0
download dataset,0
column format,0
this dataset name,0
download and parse data if necessary,0
create train test dev if not exist,0
column format,0
this dataset name,0
If the extracted corpus file is not yet present in dir,0
download zip if necessary,0
"extracted corpus is not present , so unpacking it.",0
column format,0
this dataset name,0
download zip,0
unpacking the zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)",0
download files if not present locally,0
we need to modify the original files by adding new lines after after the end of each sentence,0
if only one language is given,0
column format,0
this dataset name,0
"use all languages if explicitly set to ""all""",0
download data if necessary,0
initialize comlumncorpus and add it to list,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
this list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
get google drive id from list,0
download from google drive,0
unzip,0
transform data into required format,0
"the processed dataset has the additional ending ""_new""",0
remove the unprocessed dataset,0
initialize comlumncorpus and add it to list,0
if no languages are given as argument all languages used in XTREME will be loaded,0
if only one language is given,0
column format,0
this dataset name,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
This list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
download from HU Server,0
unzip,0
transform data into required format,0
initialize comlumncorpus and add it to list,0
if only one language is given,0
column format,0
this dataset name,0
download data if necessary,0
initialize comlumncorpus and add it to list,0
download data if necessary,0
unpack and write out in CoNLL column-like format,0
column format,0
this dataset name,0
download data if necessary,0
data is not in IOB2 format. Thus we transform it to IOB2,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
column format,0
this dataset name,0
rename according to train - test - dev - convention,0
column format,0
this dataset name,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
Add missing newline after header,0
Workaround for empty tokens,1
"Add ""real"" document marker",0
Dataset split mapping,0
v2.0 only adds new language and splits for AJMC dataset,0
Special document marker for sample splits in AJMC dataset,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
"No test data is available, so do not shrink dev data for shared task preparation!",0
create dataset files from index and train/test splits,0
news date is usually in 3rd or 4th sentence of each article,0
"generate NoiseBench dataset variants, given CleanCoNLL, noisy label files and index file",0
"os.makedirs(os.path.join('data','noisebench'), exist_ok=True)",0
copy test set,0
if only one language is given,0
column format,0
this dataset name,0
"use all languages if explicitly set to ""all""",0
download data if necessary,0
initialize comlumncorpus and add it to list,0
Get original version,0
Add sentence boundary marker,0
"Only allowed classes in course setting are: PER, LOC, ORG and MISC.",0
"All other NEs are normalized to O, except EVENT and WOA are normalized to MISC (cf. Table 3 of paper).",0
this dataset name,0
one name can map to multiple concepts,0
NOTE: EntityLinkingDictionary are lazy-loaded from a preprocessed file.,0
Use this class to load into memory all candidates,0
"if identifier == ""MESH:D013749"":",0
# This MeSH ID was used by MeSH when this chemical was part of the MeSH controlled vocabulary.,0
continue,0
parse line,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
paths to train and test splits,0
init corpus,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
iterate over all html files,0
"get rid of html syntax, we only need the text",0
between all documents we write a separator symbol,0
skip empty strings,0
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)",0
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention",0
sentence splitting and tokenization,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out-file in column format,0
"in case something goes wrong, delete the dataset and raise error",0
this dataset name,0
download and parse data if necessary,0
from qwikidata.linked_data_interface import get_entity_dict_from_api,0
generate qid wikiname dictionaries,0
merge dictionaries,0
ignore first line,0
commented and empty lines,0
read all Q-IDs,0
ignore first line,0
request,0
this dataset name,0
we use the wikiids in the data instead of directly utilizing the wikipedia urls.,0
like this we can quickly check if the corresponding page exists,0
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi,0
delete unprocessed file,0
collect all wikiids,0
create the dictionary,0
request,0
this dataset name,0
names of raw text documents,0
open output_file,0
iterate through all documents,0
split sentences and tokenize,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out file,0
annotation from one annotator or two agreeing annotators,0
this dataset name,0
download and parse data if necessary,0
this dataset name,0
download and parse data if necessary,0
First parse the post titles,0
Keep track of how many and which entity mentions does a given post title have,0
Check if the current post title has an entity link and parse accordingly,0
Post titles with entity mentions (if any) are handled via this function,0
Then parse the comments,0
"Iterate over the comments.tsv file, until the end is reached",0
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched.",0
Each comment thread is handled as one 'document'.,0
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.,0
This if-condition is needed to handle this problem.,0
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure",0
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above",0
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle.",0
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,",0
and not just single letters into single rows.,0
If there are annotated entity mentions for given post title or a comment thread,0
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence",0
Write the token with a corresponding tag to file,0
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed",0
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag",0
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized,0
"incorrectly, in order to keep the desired format (empty line as a sentence separator).",0
"Thrown when the second check above happens, but the last token of a sentence is reached.",0
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below.",0
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS",0
Check if further annotations belong to the current post title or comment thread as well,0
Stop when the end of an annotation file is reached,0
Check if further annotations belong to the current sentence as well,0
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)",0
Docstart,0
if there is more than one word in the chunk we write each in a separate line,0
print(chunks),0
empty line after each sentence,0
convert the file to CoNLL,0
this dataset name,0
"check if data there, if not, download the data",0
create folder,0
download data,0
transform data into column format if necessary,0
if no filenames are specified we use all the data,0
"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled",0
also we remove 'raganato_ALL' from filenames in case its in the list,0
generate the test file,0
make column file and save to data_folder,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
TODO: Adapt this following: https://github.com/flairNLP/flair/pull/3146,1
+1 assumes the title and abstract will be joined by a space.,0
"We need a unique identifier for this entity, so build it from the document id and entity id",0
The user can provide a callable that returns the database name.,0
some entities are not linked and,0
some entities are linked to multiple normalized ids,0
passages must not overlap and spans must cover the entire document,0
entities,0
parse db ids,0
Some of the entities have a off-by-one error. Correct these annotations!,0
"passage offsets/lengths do not connect, recalculate them for this schema.",0
this dataset name,0
download data if necessary,0
if True:,0
write CoNLL-U Plus header,0
"Some special cases (e.g., missing spaces before entity marker)",0
necessary if text should be whitespace tokenizeable,0
Handle case where tail may occur before the head,0
this dataset name,0
write CoNLL-U Plus header,0
this dataset name,0
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors,1
download data if necessary,0
write CoNLL-U Plus header,0
The span has ended.,0
We are entering a new span; reset indices,0
and active tag to new span.,0
We're inside a span.,0
Last token might have been a part of a valid span.,0
this dataset name,0
write CoNLL-U Plus header,0
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):",0
"with zip_file.open(source_file_path, mode=""r"") as source_file:",0
target_file_path = Path(data_folder) / target_filename,0
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:",0
# write CoNLL-U Plus header,0
"target_file.write(""# global.columns = id form ner\n"")",0
for example in json.load(source_file):,0
token_list = self._tacred_example_to_token_list(example),0
target_file.write(token_list.serialize()),0
check if first tag row is already occupied,0
"if first tag row is occupied, use second tag row",0
hardcoded mapping TODO: perhaps find nicer solution,1
remap regular tag names,0
else skip to position in file where sentence begins,0
set sentence context using partials TODO: pointer to dataset is really inefficient,1
read in dev file if exists,0
read in test file if exists,0
the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44,0
"find train, dev and test files if not specified",0
use test_file to create test split if available,0
use dev_file to create test split if available,0
"if data point contains black-listed label, do not use",0
first check if valid sentence,0
"if so, add to indices",0
"find train, dev and test files if not specified",0
variables,0
different handling of in_memory data than streaming data,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
test if format is OK,0
test if at least one label given,0
make sentence from text (and filter for length),0
"if a pair column is defined, make a sentence pair object",0
noinspection PyDefaultArgument,0
dataset name includes the split size,0
default dataset folder is the cache root,0
download data if necessary,0
download each of the 28 splits,0
create dataset directory if necessary,0
download senteval datasets if necessary und unzip,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
download data from same source as in huggingface's implementations,0
read label order,0
"Original labels are [1, 2, 3, 4] -> ['World', 'Sports', 'Business', 'Sci/Tech']",0
"Re-map to [0, 1, 2, 3].",0
this dataset name,0
download data if necessary,0
handle labels file,0
handle data file,0
Create flair compatible labels,0
"by default, map point score to POSITIVE / NEGATIVE values",0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file from CSV,0
create test.txt file from CSV,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create train dev and test files in fasttext format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
convert to FastText format,0
download data if necessary,0
"if data is not downloaded yet, download it",0
get the zip file,0
move original .tsv files to another folder,0
create train and dev splits in fasttext format,0
create eval_dataset file with no labels,0
download zip archive,0
unpack file in datasets directory (zip archive contains a directory named SST-2),0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download datasets if necessary,0
create dataset directory if necessary,0
create correctly formated txt files,0
multiple labels are possible,0
this dataset name,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
create a separate directory for different tasks,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
check if dataset is supported,0
set file names,0
set file names,0
download and unzip in file structure if necessary,0
instantiate corpus,0
"find train, dev and test files if not specified",0
"create DataPairDataset for train, test and dev file, if they are given",0
stop if file does not exist,0
create a DataPair object from strings,0
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings",0
"find train, dev, and test files if not specified",0
"create DataTripleDataset for train, test, and dev files, if they are given",0
stop if the file does not exist,0
create a DataTriple object from strings,0
"if in_memory is True we return a DataTriple, otherwise we create one from the lists of strings",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"reorder dev datasets to have same columns as in train set: 8, 9, and 11",0
dev sets include 5 different annotations but we will only keep the gold label,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get test and dev sets,0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data not downloaded yet, download it",0
get the zip file,0
"the downloaded files have json format, we transform them to tsv",0
Function to transform JSON file to tsv for Recognizing Textual Entailment Data,0
remove json file,0
Uses dynamic programming approach to calculate maximum independent set in interval graph,0
with sum of all entity lengths as secondary key,0
calculate offset without current text,0
because we stick all passages of a document together,0
TODO For split entities we also annotate everything inbetween which might be a bad idea?,1
Try to fix incorrect annotations,0
print(,0
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}""",0
),0
Ignore empty lines or relation annotations,0
FIX annotation of whitespaces (necessary for PDR),0
Add task description for multi-task learning,0
One token may contain multiple entities -> deque all of them,0
column format,0
this dataset name,0
Create tokenization-dependent CONLL files. This is necessary to prevent,0
from caching issues (e.g. loading the same corpus with different sentence splitters),0
column format,0
this dataset name,0
column format,0
this dataset name,0
Edge case: last token starts a new entity,0
Last document in file,0
column format,0
this dataset name,0
column format,0
this dataset name,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
Edge case: last token starts a new entity,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
column format,0
this dataset name,0
Read texts,0
Read annotations,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
We need to apply a patch to correct the original training file,0
Articles title,0
Article abstract,0
Entity annotations,0
column format,0
this dataset name,0
Edge case: last token starts a new entity,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
Incomplete article,0
Invalid XML syntax,0
column format,0
this dataset name,0
column format,0
this dataset name,0
if len(mid) != 3:,0
continue,0
Try to fix entity offsets,0
column format,0
this dataset name,0
There is still one illegal annotation in the file ..,0
column format,0
this dataset name,0
"Abstract first, title second to prevent issues with sentence splitting",0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
column format,0
this dataset name,0
"Filter for specific entity types, by default no entities will be filtered",0
Get original HUNER splits to retrieve a list of all document ids contained in V2,0
train and dev split of V2 will be train in V4,0
test split of V2 will be dev in V4,0
New documents in V4 will become test documents,0
column format,0
this dataset name,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
build dataset name and full huggingface reference name,0
Download data if necessary,0
"Some datasets in BigBio only have train or test splits, not both",0
"If only test split, assign it to train split",0
"If only train split, sample other from it (sample_missing_splits=True)",0
Not every dataset has a dev / validation set!,0
Perform type mapping if necessary,0
return None,0
TODO: Add entity type mapping for all remaining bigbio datasets not in HunFlair?,1
"""simple_chemical"": ""chemical"",  # BioNLP ST 2013 CG",0
"""cancer"": ""disease"",  # BioNLP ST 2013 CG",0
"""gene_or_gene_product"": ""gene"",  # BioNLP ST 2013 CG",0
"""gene"": ""gene"",  # NLM Gene",0
"""chemical"": ""chemical"",  # NLM Chem",0
"""cellline"": ""cell_line"",  # Cell Finder",0
"""species"": ""species"",  # Cell Finder",0
"""protein"": ""gene"",  # BioID",0
"Collect all texts of the document, each passage will be",0
a text in our internal format,0
Sort passages by start offset,0
Transform all entity annotations into internal format,0
Find the passage of the entity (necessary for offset adaption),0
Adapt entity offsets according to passage offsets,0
FIXME: This is just for debugging purposes,1
passage_text = id_to_text[passage_id],0
doc_text = passage_text[entity_offset[0] : entity_offset[1]],0
"mention_text = entity[""text""][0]",0
if doc_text != mention_text:,0
"print(f""Annotation error ({document['document_id']}) - Doc: {doc_text} vs. Mention: {mention_text}"")",0
Get element in the middle,0
Is the mention with the passage offsets?,0
"If element is smaller than mid, then it can only",0
be present in left subarray,0
Else the element can only be present in right subarray,0
TODO whether cell or cell line is the correct tag,1
TODO whether cell or cell line is the correct tag,1
Special case for ProGene: We need to use the split_0_train and split_0_test splits,0
as they are currently provided in BigBio,0
cache Feidegger config file,0
cache Feidegger images,0
replace image URL with local cached file,0
append Sentence-Image data point,0
cast to list if necessary,0
cast to list if necessary,0
"first, check if pymongo is installed",0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
Expose base classses,0
Expose all biomedical data sets used for the evaluation of BioBERT,0
-,0
-,0
-,0
-,0
Expose all biomedical data sets using the HUNER splits,0
Expose all biomedical data sets,0
Expose all document classification datasets,0
word sense disambiguation,0
Expose all entity linking datasets,0
Expose all relation extraction datasets,0
universal proposition banks,0
keyphrase detection datasets,0
other NER datasets,0
standard NER datasets,0
Expose all sequence labeling datasets,0
Expose all text-image datasets,0
Expose all text-text datasets,0
Expose all treebanks,0
"find train, dev and test files if not specified",0
get train data,0
get test data,0
get dev data,0
option 1: read only sentence boundaries as offset positions,0
option 2: keep everything in memory,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
current token ID,0
handling for the awful UD multiword format,0
end of sentence,0
comments or ellipsis,0
if token is a multi-word,0
normal single-word tokens,0
"if we don't split multiwords, skip over component words",0
add token,0
add morphological tags,0
derive whitespace logic for multiwords,0
"if multi-word equals component tokens, there should be no whitespace",0
go through all tokens in subword and set whitespace_after information,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
this dataset name,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"finally, print model card for information",0
Note: Multi-GPU can affect corpus loading,0
This code will run multiple times -- each GPU gets its own process and each process runs this code. We need to,0
"ensure that the corpus has the same elements and order on all processes, despite sampling. We do that by using",0
the same seed on all processes.,0
Note: Multi-GPU can affect choice of batch size.,0
"In order to compare batch updates fairly between single and multi-GPU training, we should:",0
1) Step the optimizer after the same number of examples to achieve com,0
2) Process the same number of examples in each forward pass,0
"e.g. Suppose your machine has 2 GPUs. If multi_gpu=False, the first gpu will process 32 examples, then the",0
"first gpu will process another 32 examples, then the optimizer will step. If multi_gpu=True, each gpu will",0
"process 32 examples at the same time, then the optimizer will step.",0
noqa: INP001,0
-- Project information -----------------------------------------------------,0
"The full version, including alpha/beta/rc tags",0
use smv_current_version as the git url,0
-- General configuration ---------------------------------------------------,0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
Napoleon settings,0
Whitelist pattern for tags (set to None to ignore all tags),0
Whitelist pattern for branches (set to None to ignore all branches),0
Whitelist pattern for remotes (set to None to use local branches only),0
Pattern for released versions,0
Format for versioned output directories inside the build directory,0
Determines whether remote or local git branches/tags are preferred if their output dirs conflict,0
test corpus,0
create a TARS classifier,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
test corpus,0
create a TARS classifier,0
switch to a new task (TARS can do multiple tasks so you must define one),0
initialize the text classifier trainer,0
start the training,0
"With end symbol, without start symbol, padding in front",0
"Without end symbol, with start symbol, padding in back",0
"Without end symbol, without start symbol, padding in front",0
initialize trainer,0
initialize trainer,0
initialize trainer,0
clean up directory,0
clean up directory,0
example sentence,0
set 4 labels for 2 tokens ('love' is tagged twice),0
check if there are three POS labels with correct text and values,0
check if there are is one SENTIMENT label with correct text and values,0
check if all tokens are correctly labeled,0
remove the pos label from the last word,0
there should be 2 POS labels left,0
now remove all pos tags,0
set 3 labels for 2 spans (HU is tagged twice),0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
"now delete the NER tags of ""Humboldt-Universitt zu Berlin""",0
should be only one NER label left,0
and only one NER span,0
set 3 labels for 2 spans (HU is tagged twice with different tags),0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
"now delete the NER tags of ""Humboldt-Universitt zu Berlin""",0
should be only one NER label left,0
and only one NER span,0
but there is also one orgtype span and label,0
and only one NER span,0
let's add the NER tag back,0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
now remove all NER tags,0
set 3 labels for 2 spans (HU is tagged twice with different tags),0
create two relation label,0
there should be two relation labels,0
there should be one syntactic labels,0
"there should be two relations, one with two and one with one label",0
example sentence,0
add another topic label,0
example sentence,0
has sentiment value,0
has 4 part of speech tags,0
has 1 NER tag,0
should be in total 6 labels,0
example sentence,0
add two NER labels,0
get the four labels,0
check that only two of the respective data points are equal,0
make a sentence and some right context,0
TODO: is this desirable? Or should two sentences with same text be considered same objects?,1
Initializing a Sentence this way assumes that there is a space after each token,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
now exclude -DOCSTART- sentences,0
now load whole documents as sentences,0
ban each boundary but set each sentence to be independent,0
"get training, test and dev data",0
"get training, test and dev data",0
load column dataset with one entry,0
load column dataset with two entries,0
load column dataset with three entries,0
"get training, test and dev data",0
"get training, test and dev data",0
check if Token labels are correct,0
"get training, test and dev data",0
check if Token labels for frames are correct,0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
"assert [token.get_tag(""head"").value for token in sent1.tokens] == [",0
"""2"",",0
"""0"",",0
"""4"",",0
"""2"",",0
"""2"",",0
"""2"",",0
],0
This test only covers basic universal dependencies datasets.,0
"For example, multi-word tokens or the ""deps"" column sentence annotations are not supported yet.",0
"Here, we use the default token annotation fields.",0
This test covers the complete HIPE 2022 dataset.,0
https://github.com/hipe-eval/HIPE-2022-data,0
"Includes variant with document separator, and all versions of the dataset.",0
"We have manually checked, that these numbers are correct:",0
"+1 offset, because of missing EOS marker at EOD",0
Test data for v2.1 release,0
This test covers the complete ICDAR Europeana corpus:,0
https://github.com/stefan-it/historic-domain-adaptation-icdar,0
"This test covers the complete MasakhaNER dataset, including support for v1 and v2.",0
This test covers the NERMuD dataset. Official stats can be found here:,0
https://github.com/dhfbk/KIND/tree/main/evalita-2023,0
Number of instances per dataset split are taken from https://huggingface.co/datasets/elenanereiss/german-ler,0
This test covers the complete MasakhaPOS dataset.,0
"See MobIE paper (https://aclanthology.org/2021.konvens-1.22/), table 2",0
--- Embeddings that are shared by both models --- #,0
--- Task 1: Sentiment Analysis (5-class) --- #,0
Define corpus and model,0
-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #,0
Define corpus and model,0
-- Define mapping (which tagger should train on which model) -- #,0
-- Create model trainer and train -- #,0
NOTE: Avoid emtpy string if mentions are just punctutations (e.g. `-` or `(`),0
clean up file,0
get features from forward propagation,0
reverse sort all sequences by their length,0
remove previously predicted labels of this type,0
no need for label_dict,0
"pretrained_model = ""tars-ner""  # disabled due to too much space requirements.",0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
Intel ----founded_by---> Gordon Moore,0
Intel ----founded_by---> Robert Noyce,0
"Ground truth is a set of tuples of (<Sentence Text>, <Relation Label Values>)",0
Check sentence masking and relation label annotation on,0
"training, validation and test dataset (in this test the splits are the same)",0
"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google .""",0
"Entity pair permutations of: ""Microsoft was founded by Bill Gates .""",0
"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 .""",0
"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany .""",0
"Entity pair permutations of: ""The German - American computer scientist Joseph Weizenbaum ( 8 January 1923 - 5 March 2008 ) was born in Berlin .""",0
This sentence is only included if we transform the corpus with cross augmentation,0
"""The German - American computer scientist Joseph Weizenbaum ( 8 January 1923 - 5 March 2008 ) was born in Berlin .""",0
"""The German - American computer scientist Joseph Weizenbaum ( 8 January 1923 - 5 March 2008 ) was born in Berlin .""",0
Simulate training epoch,0
Simulate training batch,0
"pretrained_model = ""tars-base""  # disabled due to too much space requirements.",0
Ensure this is an example that predicts no classes in multilabel,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
ensure that the prepared tensors is what we expect,0
use a SequenceTagger to save and reload the embedding in the manner it is supposed to work,0
previous and next sentence as context,0
test expansion for sentence without context,0
test expansion for with previous and next as context,0
test expansion if first sentence is document boundary,0
test expansion if we don't use context,0
"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher.",0
dummy model with embeddings,0
save the dummy and load it again,0
check that context_length and use_context_separator is the same for both,0
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
Extract all the contents of zip file in current directory,0
use model name as subfolder,0
Lazy import,0
output information,0
Extract all the contents of zip file in current directory,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
load_big_file is a workaround byhttps://github.com/highway11git,1
to load models on some Mac/Windows setups,0
see https://github.com/zalandoresearch/flair/issues/351,0
first determine the distribution of classes in the dataset,0
weight for each sample,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
increment for last token in sentence if not followed by whitespace,0
this is the default init size of a lmdb database for embeddings,0
get db filename from embedding name,0
"In case initialization of cached version failed, just fallback to the original WordEmbeddings",0
SequenceTagger,0
TextClassifier,0
get db filename from embedding name,0
if embedding database already exists,0
"otherwise, push embedding to database",0
if embedding database already exists,0
open the database in read mode,0
we need to set self.k,0
create and load the database in write mode,0
"no idea why, but we need to close and reopen the environment to avoid",0
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot,0
when opening new transaction !,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
set 'add_unk' if the dictionary was created with a version of Flair older than 0.9,0
set 'add_unk' depending on whether <unk> is a key,0
"if one embedding name, directly return it",0
"if multiple embedding names, concatenate them",0
First we remove any existing labels for this PartOfSentence in self.sentence,0
labels also need to be deleted at Sentence object,0
delete labels at object itself,0
The Token is a special _PartOfSentence in that it may be initialized without a Sentence.,0
"therefore, labels get added only to the Sentence if it exists",0
The Token is a special _PartOfSentence in that it may be initialized without a Sentence.,0
"Therefore, labels get set only to the Sentence if it exists",0
"check if the span already exists. If so, return it",0
else make a new span,0
"check if the relation already exists. If so, return it",0
else make a new relation,0
private field for all known spans,0
the tokenizer used for this sentence,0
some sentences represent a document boundary (but most do not),0
internal variables to denote position inside dataset,0
"if text is passed, instantiate sentence with tokens (words)",0
determine token positions and whitespace_after flag,0
the last token has no whitespace after,0
log a warning if the dataset is empty,0
data with zero-width characters cannot be handled,0
set token idx and sentence,0
append token to sentence,0
register token annotations on sentence,0
move sentence embeddings to device,0
also move token embeddings to device,0
clear token embeddings,0
infer whitespace after field,0
"if sentence has no tokens, return empty string",0
"otherwise, return concatenation of tokens with the correct offsets",0
The sentence's start position is not propagated to its tokens.,0
"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces.",0
No character at the corresponding code point: remove it,0
"if no label if specified, return all labels",0
"if the label type exists in the Sentence, return it",0
return empty list if none of the above,0
labels also need to be deleted at all tokens,0
labels also need to be deleted at all known spans,0
remove spans without labels,0
delete labels at object itself,0
set name,0
abort if no data is provided,0
sample test data from train if none is provided,0
sample dev data from train if none is provided,0
set train dev and test data,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
"first, determine the datapoint type by going through dataset until first label is found",0
count all label types per sentence,0
go through all labels of label_type and count values,0
special handling for Token-level annotations. Add all untagged as 'O' label,0
"if an unk threshold is set, UNK all label values below this threshold",0
sample randomly from a label distribution according to the probabilities defined by the noise transition matrix,0
replace the old label with the new one,0
keep track of the old (clean) label using another label type category,0
keep track of how many labels in total are flipped,0
sample randomly from a label distribution according to the probabilities defined by the desired noise share,0
replace the old label with the new one,0
keep track of the old (clean) label using another label type category,0
keep track of how many labels in total are flipped,0
"add a dummy ""O"" to close final prediction",0
return complex list,0
internal variables,0
non-set tags are OUT tags,0
anything that is not OUT is IN,0
does this prediction start a new span?,0
B- and S- always start new spans,0
"if the predicted class changes, I- starts a new span",0
"if the predicted class changes and S- was previous tag, start a new span",0
if an existing span is ended (either by reaching O or starting a new span),0
determine score and value,0
append to result list,0
reset for-loop variables for new span,0
remember previous tag,0
global variable: cache_root,0
Get the device from the environment variable,0
global variable: device,0
"No need for correctness checks, torch is doing it",0
global variable: version,0
global variable: arrow symbol,0
dummy return to fulfill trainer.train() needs,0
print(vec),0
Attach optimizer,0
"convert `metrics` to float, in case it's a zero-dim Tensor",0
if memory mode option 'none' delete everything,0
"if dynamic embedding keys not passed, identify them automatically",0
always delete dynamic embeddings,0
"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)",0
optional metric space decoder if prototypes have different length than embedding,0
create initial prototypes for all classes (all initial prototypes are a vector of all 1s),0
"if set, create initial prototypes from normal distribution",0
"if set, use a radius",0
all parameters will be pushed internally to the specified device,0
decode embeddings into prototype space,0
"if unlabeled distance is set, mask out loss to unlabeled class prototype",0
verbalize BIOES labels,0
"if label is not BIOES, use label itself",0
Always include the name of the Model class for which the state dict holds,0
"this seems to just return model name, not a model with that name",0
"write out a ""model card"" if one is set",0
save model,0
"if this class is abstract, go through all inheriting classes and try to fetch and load the model",0
get all non-abstract subclasses,0
"try to fetch the model for each subclass. if fetching is possible, load model and return it",0
"skip any invalid loadings, e.g. not found on HuggingFace hub",0
"if the model cannot be fetched, load as a file",0
try to get model class from state,0
"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses",0
"skip any invalid loadings, e.g. not found on HuggingFace hub",0
"if this class is not abstract, fetch the model and load it",0
"make sure <unk> is contained in gold_label_dictionary, if given",0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
loss calculation,0
variables for printing,0
variables for computing scores,0
remove any previously predicted labels,0
predict for batch,0
get the gold labels,0
add to all_predicted_values,0
make printout lines,0
convert true and predicted values to two span-aligned lists,0
delete excluded labels if exclude_labels is given,0
"if after excluding labels, no label is left, ignore the datapoint",0
write all_predicted_values to out_file if set,0
make the evaluation dictionary,0
check if this is a multi-label problem,0
compute numbers by formatting true and predicted such that Scikit-Learn can use them,0
multi-label problems require a multi-hot vector for each true and predicted label,0
single-label problems can do with a single index for each true and predicted label,0
"now, calculate evaluation numbers",0
there is at least one gold label or one prediction (default),0
compute accuracy separately as it is not always in classification_report (e.g. when micro avg exists),0
"if there is only one label, then ""micro avg"" = ""macro avg""",0
"The ""micro avg"" appears only in the classification report if no prediction is possible.",0
"Otherwise, it is identical to the ""macro avg"". In this case, we add it to the report.",0
"Create and populate score object for logging with all evaluation values, plus the loss",0
issue error and default all evaluation numbers to 0.,0
check if there is a label mismatch,0
print info,0
set the embeddings,0
initialize the label dictionary,0
initialize the decoder,0
set up multi-label logic,0
init dropouts,0
loss weights and loss function,0
Initialize the weight tensor,0
set up gradient reversal if so specified,0
embed sentences,0
get a tensor of data points,0
do dropout,0
make a forward pass to produce embedded data points and labels,0
get the data points for which to predict labels,0
get their gold labels as a tensor,0
pass data points through network to get encoded data point tensor,0
decode,0
an optional masking step (no masking in most cases),0
calculate the loss,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
filter data points in batch,0
stop if all sentences are empty,0
pass data points through network and decode,0
if anything could possibly be predicted,0
remove previously predicted labels of this type,0
filter data points that have labels outside of dictionary,0
add DefaultClassifier arguments,0
add variables of DefaultClassifier,0
Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23,0
Get projected 1st dimension,0
Compute bilinear form,0
Arcosh,0
Project the input data to n+1 dimensions,0
"The first dimension, is recomputed in the distance module",0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for",0
"train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
plot i,0
save plots,0
save plots,0
plt.show(),0
save plot,0
auto-spawn on GPU if available,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
TODO: not saving lines yet,1
TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.,1
- MaskedRelationClassifier ?,0
This depends if this relation classification architecture should replace or offer as an alternative.,0
Set label type and prepare label dictionary,0
Initialize super default classifier,0
Add the special tokens from the encoding strategy,0
"Auto-spawn on GPU, if available",0
Only use entities labelled with the specified labels for each label type,0
Only use entities above the specified threshold,0
Use a dictionary to find gold relation annotations for a given entity pair,0
Yield head and tail entity pairs from the cross product of all entities,0
Remove identity relation entity pairs,0
Remove entity pairs with labels that do not match any,0
of the specified relations in `self.entity_pair_labels`,0
"Obtain gold label, if existing",0
Some sanity checks,0
Pre-compute non-leading head and tail tokens for entity masking,0
We can not use the plaintext of the head/tail span in the sentence as the mask/marker,0
since there may be multiple occurrences of the same entity mentioned in the sentence.,0
"Therefore, we use the span's position in the sentence.",0
Create masked sentence,0
Add gold relation annotation as sentence label,0
"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,",0
"during prediction, the forward pass does not need any knowledge about the entities in the sentence.",0
"If we sample missing splits, the encoded sentences that correspond to the same original sentences",0
"may get distributed into different splits. For training purposes, this is always undesired.",0
Ensure that all sentences are encoded properly,0
Deal with the case where all sentences are encoded sentences,0
"mypy does not infer the type of ""sentences"" restricted by the if statement",0
Deal with the case where all sentences are standard (non-encoded) sentences,0
"For each encoded sentence, transfer its prediction onto the original relation",0
auto-spawn on GPU if available,0
pad strings with whitespaces to longest sentence,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
"""document_delimiter"" property may be missing in some older pre-trained models",0
serialize the language models and the constructor arguments (but nothing else),0
special handling for deserializing language models,0
re-initialize language model with constructor arguments,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
Transform input data into TARS format,0
"if there are no labels, return a random sample as negatives",0
"otherwise, go through all labels",0
make sure the probabilities always sum up to 1,0
get and embed all labels by making a Sentence object that contains only the label text,0
get each label embedding and scale between 0 and 1,0
compute similarity matrix,0
"the higher the similarity, the greater the chance that a label is",0
sampled as negative example,0
make label dictionary if no Dictionary object is passed,0
prepare dictionary of tags (without B- I- prefixes and without UNK),0
check if candidate_label_set is empty,0
make list if only one candidate label is passed,0
create label dictionary,0
note current task,0
create a temporary task,0
make zero shot predictions,0
switch to the pre-existing task,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
make a tars sentence where all labels are O by default,0
init new TARS classifier,0
set all task information,0
progress bar for verbosity,0
stop if all sentences are empty,0
always remove tags first,0
go through each sentence in the batch,0
always remove tags first,0
get the span and its label,0
determine whether tokens in this span already have a label,0
only add if all tokens have no label,0
make and add a corresponding predicted span,0
set indices so that no token can be tagged twice,0
clearing token embeddings to save memory,0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
get the serialized embeddings,0
remap state dict for models serialized with Flair <= 0.11.3,0
init new TARS classifier,0
set all task information,0
with torch.no_grad():,0
progress bar for verbosity,0
stop if all sentences are empty,0
always remove tags first,0
go through each sentence in the batch,0
always remove tags first,0
add all labels that according to TARS match the text and are above threshold,0
do not add labels below confidence threshold,0
only use label with the highest confidence if enforcing single-label predictions,0
add the label with the highest score even if below the threshold if force label is activated.,0
remove previously added labels and only add the best label,0
clearing token embeddings to save memory,0
set separator to concatenate three sentences,0
auto-spawn on GPU if available,0
set separator to concatenate two sentences,0
auto-spawn on GPU if available,0
"If the concatenated version of the text pair does not exist yet, create it",0
pooling operation to get embeddings for entites,0
set embeddings,0
set relation and entity label types,0
"whether to use gold entity pairs, and whether to filter entity pairs by type",0
filter entity pairs according to their tags if set,0
whether to encode characters and whether to use attention (attention can only be used if chars are encoded),0
character dictionary for decoding and encoding,0
make sure <unk> is in dictionary for handling of unknown characters,0
add special symbols to dictionary if necessary and save respective indices,0
---- ENCODER ----,0
encoder character embeddings,0
encoder pre-trained embeddings,0
encoder RNN,0
additional encoder linear layer if bidirectional encoding,0
---- DECODER ----,0
decoder: linear layers to transform vectors to and from alphabet_size,0
when using attention we concatenate attention outcome and decoder hidden states,0
decoder RNN,0
loss and softmax,0
self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction,0
add additional columns for special symbols if necessary,0
initialize with dummy symbols,0
encode inputs,0
get labels (we assume each token has a lemma label),0
get char indices for labels of sentence,0
"(batch_size, max_sequence_length) batch_size = #words in sentence,",0
max_sequence_length = length of longest label of sentence + 1,0
get char embeddings,0
"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size",0
take decoder input and initial hidden and pass through RNN,0
"if all encoder outputs are provided, use attention",0
take convex combinations of encoder hidden states as new output using the computed attention coefficients,0
"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)",0
get all tokens,0
encode input characters by sending them through RNN,0
get one-hots for characters and add special symbols / padding,0
determine length of each token,0
embed sentences,0
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)",0
variable to store initial hidden states for decoder,0
encode input characters by sending them through RNN,0
test packing and padding,0
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder,0
concatenate the final hidden states of the encoder. These will be projected to hidden_size of,0
decoder later with self.emb_to_hidden,0
mask out vectors that correspond to a dummy symbol (TODO: check attention masking),1
use token embedding as initial hidden state for decoder,0
concatenate everything together and project to appropriate size for decoder,0
variable to store initial hidden states for decoder,0
encode input characters by sending them through RNN,0
note that we do not need to fill up with dummy symbols since we process each token seperately,0
embed character one-hots,0
send through encoder RNN (produces initial hidden for decoder),0
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder,0
project 2*hidden_size to hidden_size,0
concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder,0
later with self.emb_to_hidden,0
use token embedding as initial hidden state for decoder,0
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)",0
concatenate everything together and project to appropriate size for decoder,0
"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)",0
"create target vector (batch_size, max_label_seq_length + 1)",0
filter empty sentences,0
max length of the predicted sequences,0
for printing,0
stop if all sentences are empty,0
remove previously predicted labels of this type,0
create list of tokens in batch,0
encode inputs,0
"create input for first pass (batch_size, 1, input_size), first letter is special character <S>",0
sequence length is always set to one in prediction,0
option 1: greedy decoding,0
predictions,0
decode next character,0
pick top beam size many outputs with highest probabilities,0
option 2: beam search,0
out_probs = self.softmax(output_vectors).squeeze(1),0
make sure no dummy symbol <> or start symbol <S> is predicted,0
pick top beam size many outputs with highest probabilities,0
"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1",0
"leading_indices and probabilities have size (batch_size, beam_size)",0
keep scores of beam_size many hypothesis for each token in the batch,0
stack all leading indices of all hypothesis and corresponding hidden states in two tensors,0
save sequences so far,0
keep track of how many hypothesis were completed for each token,0
"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)",0
decode with log softmax,0
make sure no dummy symbol <> or start symbol <S> is predicted,0
"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside",0
"if the sequence is already ended, do not record as candidate",0
index of token in in list tokens_in_batch,0
print(token_number),0
hypothesis score,0
TODO: remove token if number of completed hypothesis exceeds given value,1
set score of corresponding entry to -inf so it will not be expanded,0
get leading_indices for next expansion,0
find highest scoring hypothesis among beam_size*beam_size possible ones for each token,0
take beam_size many copies of scores vector and add scores of possible new extensions,0
"size (beam_size*batch_size, beam_size)",0
print(hypothesis_scores),0
"reshape to vector of size (batch_size, beam_size*beam_size),",0
each row contains beam_size*beam_size scores of the new possible hypothesis,0
print(hypothesis_scores_per_token),0
"choose beam_size best for each token - size (batch_size, beam_size)",0
out of indices_per_token we now need to recompute the original indices of the hypothesis in,0
a list of length beam_size*batch_size,0
"where the first three inidices belong to the first token, the next three to the second token,",0
and so on,0
with these indices we can compute the tensors for the next iteration,0
expand sequences with corresponding index,0
add log-probabilities to the scores,0
save new leading indices,0
save corresponding hidden states,0
it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations,0
in that case we append one of the final seuqences without end symbol to the final_candidates,0
get best final hypothesis for each token,0
get characters from index sequences and add predicted label to token,0
"Overwrites evaluate of parent class to remove the ""by class"" printout",0
set separator to concatenate two sentences,0
init dropouts,0
auto-spawn on GPU if available,0
make a forward pass to produce embedded data points and labels,0
get their gold labels as a tensor,0
pass data points through network to get encoded data point tensor,0
decode,0
calculate the loss,0
get a tensor of data points,0
do dropout,0
"If the concatenated version of the text pair does not exist yet, create it",0
add Model arguments,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
"if the classifier predicts BIO/BIOES span labels, the internal label dictionary must be computed",0
fields in case this is a span-prediction problem,0
the label type,0
all parameters will be pushed internally to the specified device,0
special handling during training if this is a span prediction problem,0
internal variables,0
non-set tags are OUT tags,0
anything that is not OUT is IN,0
does this prediction start a new span?,0
B- and S- always start new spans,0
"if the predicted class changes, I- starts a new span",0
"if the predicted class changes and S- was previous tag, start a new span",0
if an existing span is ended (either by reaching O or starting a new span),0
reset for-loop variables for new span,0
remember previous tag,0
"if there is a span at end of sentence, add it",0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
print labels in CoNLL format,0
internal candidate lists of generator,0
load Zelda candidates if so passed,0
create candidate lists,0
"if lower casing is enabled, create candidate lists of lower cased versions",0
create a new dictionary for lower cased mentions,0
go through each mention and its candidates,0
"check if backoff mention already seen. If so, add candidates. Else, create new entry.",0
set lowercased version as map,0
"only use span label type if there are predictions, otherwise search for output label type (training labels)",0
remap state dict for models serialized with Flair <= 0.11.3,0
get the candidates,0
"during training, add the gold value as candidate",0
----- Create the internal tag dictionary -----,0
span-labels need special encoding (BIO or BIOES),0
the big question is whether the label dictionary should contain an UNK or not,0
"without UNK, we cannot evaluate on data that contains labels not seen in test",0
"with UNK, the model learns less well if there are no UNK examples",0
is this a span prediction problem?,0
----- Embeddings -----,0
----- Initial loss weights parameters -----,0
----- RNN specific parameters -----,0
----- Conditional Random Field parameters -----,0
"Previously trained models have been trained without an explicit CRF, thus it is required to check",0
whether we are loading a model from state dict in order to skip or add START and STOP token,0
----- Dropout parameters -----,0
dropouts,0
remove word dropout if there is no contact over the sequence dimension.,0
----- Model layers -----,0
----- RNN layer -----,0
"If shared RNN provided, else create one for model",0
Whether to train initial hidden state,0
final linear map to tag space,0
"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss",0
"if using CRF, we also require a CRF and a Viterbi decoder",0
"if there are no sentences, there is no loss",0
forward pass to get scores,0
calculate loss given scores and labels,0
make a zero-padded tensor for the whole sentence,0
linear map to tag space,0
"Depending on whether we are using CRF or a linear layer, scores is either:",0
"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF",0
"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer",0
spans need to be encoded as token-level predictions,0
all others are regular labels for each token,0
make sure it's a list,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
get features from forward propagation,0
remove previously predicted labels of this type,0
"if return_loss, get loss value",0
make predictions,0
add predictions to Sentence,0
BIOES-labels need to be converted to spans,0
"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)",0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
core Flair models on Huggingface ModelHub,0
"Large NER models,",0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Language-specific NER models,0
Language-specific POS models,0
Historic German,0
English NER models,0
English SRL models,0
Danish models,0
German models,0
Arabic models,0
French models,0
Dutch models,0
Malayalam models,0
Portuguese models,0
Biomedical models,0
check if model name is a valid local file,0
"check if model key is remapped to HF key - if so, print out information",0
get mapped name,0
"if not, check if model key is remapped to direct download location. If so, download model",0
"for all other cases (not local file or special download location), use HF model hub",0
## Demo: How to use in Flair,0
load tagger,0
make example sentence,0
predict NER tags,0
print sentence,0
print predicted NER spans,0
iterate over entities and print,0
Lazy import,0
Save model weight,0
Determine if model card already exists,0
Generate and save model card,0
Upload files,0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
print labels in CoNLL format,0
Dense + sparse retrieval,0
fetched from original repo to avoid download,0
"just in case we add: fuzzy search, Levenstein, ...",0
"for now we always fall back to SapBERT,",0
but we should train our own models at some point,0
NOTE: Avoid emtpy string if mentions are just punctutations (e.g. `-` or `(`),0
NOTE: Avoid emtpy string if mentions are just punctuations (e.g. `-` or `(`),0
Ab3P works on sentence-level and not on a single entity mention / name,0
- so we just apply the wrapped text pre-processing here (if configured),0
NOTE: ensure correct similarity metric for pretrained model,0
empty cuda cache if device is a cuda device,0
"Sanity conversion: if flair.device was set as a string, convert to torch.device",0
NOTE: This is a hacky workaround for the fact that,1
the `label_type`s in `Classifier.load('hunflair)` are,0
"'diseases', 'genes', 'species', 'chemical' instead of 'ner'.",0
We warn users once they need to update SequenceTagger model,0
See: https://github.com/flairNLP/flair/pull/3387,0
make sure sentences is a list of sentences,0
Make sure entity label types are represented as dict,0
Collect all entities based on entity type labels configuration,0
Preprocess entity mentions,0
Retrieve top-k concept / entity candidates,0
Add a label annotation for each candidate,0
load model by entity_type,0
check if we have a hybrid pre-trained model,0
the multi task model has several labels,0
biomedical models,0
entity linker,0
auto-spawn on GPU if available,0
remap state dict for models serialized with Flair <= 0.11.3,0
English sentiment models,0
Communicative Functions Model,0
"If we sample missing splits, the encoded sentences that correspond to the same original sentences",0
"may get distributed into different splits. For training purposes, this is always undesired.",0
Prepend the task description prompt to the sentence text,0
Make sure it's a list,0
Reconstruct all annotations from the original sentence (necessary for learning classifiers),0
If all sentences are not augmented -> augment them,0
"mypy does not infer the type of ""sentences"" restricted by the if statement",0
"mypy does not infer the type of ""sentences"" restricted by code above",0
Compute prediction label type,0
make sure it's a list,0
"If all sentences are already augmented (i.e. compatible with this class), just forward the sentences",0
"mypy does not infer the type of ""sentences"" restricted by the if statement",0
Remove existing labels,0
Augment sentences - copy all annotation of the given tag type,0
Predict on augmented sentence and store it in an internal annotation layer / label,0
Append predicted labels to the original sentences,0
check if model name is a valid local file,0
check if model name is a pre-configured hf model,0
"scores_at_targets[range(features.shape[0]), lengths.values -1]",0
Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices,0
"Initially, get scores from <start> tag to all other tags",0
"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp",0
"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep",0
Create a tensor to hold accumulated sequence scores at each current tag,0
Create a tensor to hold back-pointers,0
"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag",0
"Let pads be the <end> tag index, since that was the last tag in the decoded sequence",0
"We add scores at current timestep to scores accumulated up to previous timestep, and",0
choose the previous timestep that corresponds to the max. accumulated score for each current timestep,0
"If sentence is over, add transition to STOP-tag",0
Decode/trace best path backwards,0
Sanity check,0
remove start-tag and backscore to stop-tag,0
Max + Softmax to get confidence score for predicted label and append label to each token,0
"Transitions are used in the following way: transitions[to, from].",0
"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag",0
to START-tag and from STOP-tag to any other tag to -10000.,0
"if necessary, make batch_steps",0
break up the batch into slices of size,0
mini_batch_chunk_size,0
"if training also uses dev/train data, include in training set",0
evaluation and monitoring,0
sampling and shuffling,0
evaluation and monitoring,0
when and what to save,0
logging parameters,0
acceleration,0
plugins,0
activate annealing plugin,0
call self.train_custom with all parameters (minus the ones specific to the AnnealingPlugin),0
training parameters,0
evaluation and monitoring,0
sampling and shuffling,0
evaluation and monitoring,0
when and what to save,0
logging parameters,0
acceleration,0
plugins,0
annealing logic,0
training parameters,0
evaluation and monitoring,0
sampling and shuffling,0
evaluation and monitoring,0
when and what to save,0
logging parameters,0
acceleration,0
plugins,0
training parameters,0
evaluation and monitoring,0
sampling and shuffling,0
evaluation and monitoring,0
when and what to save,0
logging parameters,0
acceleration,0
plugins,0
Create output folder,0
=== START BLOCK: ACTIVATE PLUGINS === #,0
We first activate all optional plugins. These take care of optional functionality such as various,0
logging techniques and checkpointing,0
log file plugin,0
loss file plugin,0
plugin for writing weights,0
plugin for checkpointing,0
=== END BLOCK: ACTIVATE PLUGINS === #,0
derive parameters the function was called with (or defaults),0
initialize model card with these parameters,0
Prepare training data and get dataset size,0
"determine what splits (train, dev, test) to evaluate",0
determine how to determine best model and whether to save it,0
instantiate the optimizer,0
initialize sampler if provided,0
init with default values if only class is provided,0
set dataset to sample from,0
configure special behavior to use multiple GPUs,0
Guard against each process initializing corpus differently due to e.g. different random seeds,0
this field stores the names of all dynamic embeddings in the model (determined after first forward pass),0
Sanity checks,0
"Sanity conversion: if flair.device was set as a string, convert to torch.device",0
-- AmpPlugin -> wraps with AMP,0
-- AnnealingPlugin -> initialize schedulers (requires instantiated optimizer),0
At any point you can hit Ctrl + C to break out of training early.,0
"- SchedulerPlugin -> load state for anneal_with_restarts, batch_growth_annealing, logic for early stopping",0
- LossFilePlugin -> get the current epoch for loss file logging,0
"if shuffle_first_epoch==False, the first epoch is not shuffled",0
log infos on training progress every `log_modulo` batches,0
process mini-batches,0
zero the gradients on the model and optimizer,0
forward and backward for batch,0
forward pass,0
We need to __call__ ddp_model() because this triggers hooks that sync gradients.,0
But that calls forward rather than forward_loss. So we patch forward to redirect,0
to forward_loss. Then undo the patch in case forward_loss itself calls forward.,0
identify dynamic embeddings (always deleted) on first sentence,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
do the optimizer step,0
DDP averages across processes but we want the sum,0
- SchedulerPlugin -> do the scheduler step if one-cycle or linear decay,0
- WeightExtractorPlugin -> extracts weights,0
- CheckpointPlugin -> executes save_model_each_k_epochs,0
- SchedulerPlugin -> log bad epochs,0
Determine if this is the best model or if we need to anneal,0
log results,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
use DEV split to determine if this is the best model so far,0
"if not using DEV score, determine best model using train loss",0
- LossFilePlugin -> somehow prints all relevant metrics,0
- AnnealPlugin -> scheduler step,0
- SWAPlugin -> restores SGD weights from SWA,0
"if we do not use dev data for model selection, save final model",0
TensorboardLogger -> closes writer,0
test best model if test data is present,0
get and return the final test score of best model,0
MetricHistoryPlugin -> stores the loss history in return_values,0
"Store return values, as they will be erased by reset_training_attributes",0
get a random sample of training sentences,0
create a model card for this model with Flair and PyTorch version,0
record Transformers version if library is loaded,0
remember all parameters used in train() call,0
"TextDataset returns a list. valid and test are only one file,",0
so return the first element,0
cast string to Path,0
error message if the validation dataset is too small,0
Shuffle training files randomly after serially iterating,0
through corpus one,0
"iterate through training data, starting at",0
self.split (for checkpointing),0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
Backward,0
`clip_grad_norm` helps prevent the exploding gradient,0
problem in RNNs / LSTMs.,0
We detach the hidden state from how it was,0
previously produced.,0
"If we didn't, the model would try backpropagating",0
all the way to start of the dataset.,0
explicitly remove loss to clear up memory,0
#########################################################,0
Save the model if the validation loss is the best we've,0
seen so far.,0
#########################################################,0
print info,0
#########################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
"no need to check for MetricName, as __add__ of other would be called in this case",0
"This flag tracks, whether an event is currently being processed (otherwise it is added to the queue)",0
instantiate plugin,0
"Reset the flag, since an exception event might be dispatched",0
"If there is no **kw argument in the callback, check if any of the passed kw args is not accepted by",0
the callback,0
go through all attributes,0
get attribute hook events (may raise an AttributeError),0
register function as a hook,0
"Decorator was used with parentheses, but no args",0
Decorator was used with args (strings specifiying the events),0
Decorator was used without args,0
path to store the model,0
special annealing modes,0
determine the min learning rate,0
"minimize training loss if training with dev data, else maximize dev score",0
instantiate the scheduler,0
stop training if learning rate becomes too small,0
reload last best model if annealing with restarts is enabled,0
calculate warmup steps,0
skip if no optimization has happened.,0
saves the model with full vocab as checkpoints etc were created with reduced vocab.,0
TODO: check if metric is in tracked metrics,1
prepare loss logging file and set up header,0
set up all metrics to collect,0
set up headers,0
name: HEADER,0
Add all potentially relevant metrics. If a metric is not published,0
"after the first epoch (when the header is written), the column is",0
removed at that point.,0
initialize the first log line,0
record is a list of scalars,0
output log file,0
remove columns where no value was found on the first epoch (could be != 1 if training was resumed),0
make headers on epoch 1,0
write header,0
adjust alert level,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn,0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
"legacy pickle-like saving for image embeddings, as implementation details are not obvious",0
"legacy pickle-like loading for image embeddings, as implementation details are not obvious",0
"<cls> token initially set to 1/D, so it attends to all image features equally",0
add positional encodings,0
reshape the pixels into the sequence,0
layer norm after convolution and positional encodings,0
add <cls> token,0
"transformer requires input in the shape [h*w+1, b, d]",0
the output is an embedding of <cls> token,0
this parameter is fixed,0
optional fine-tuning on top of embedding layer,0
"if only one sentence is passed, convert to list of sentence",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push through RNN,0
after-RNN dropout,0
extract embeddings from RNN,0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
serialize the language models and the constructor arguments (but nothing else),0
re-initialize language model with constructor arguments,0
special handling for deserializing language models,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
IMPORTANT: add embeddings as torch modules,0
iterate over sentences,0
"if its a forward LM, take last state",0
"convert to plain strings, embedded in a list for the encode function",0
CNN,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push CNN,0
after-CNN dropout,0
extract embeddings from CNN,0
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency",0
"if only one sentence is passed, convert to list of sentence",0
Expose base classses,0
Expose document embedding classes,0
Expose image embedding classes,0
Expose legacy embedding classes,0
Expose token embedding classes,0
in some cases we need to insert zero vectors for tokens without embedding.,0
sum embeddings for each token,0
calculate the mean of subtokens,0
Create a mask for valid tokens based on token_lengths,0
padding,0
remove special markup,0
check if special tokens exist to circumvent error message,0
iterate over subtokens and reconstruct tokens,0
remove special markup,0
check if reconstructed token is special begin token ([CLS] or similar),0
some BERT tokenizers somehow omit words - in such cases skip to next token,0
"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token.",0
if tokens are unaccounted for,0
check if all tokens were matched to subtokens,0
The layoutlm tokenizer doesn't handle ocr themselves,0
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial",0
"cannot run `.encode` if ocr boxes are required, assume",0
"transformers returns the ""added_tokens.json"" even if it doesn't create it",0
"transformers returns the ""added_tokens.json"" even if it doesn't create it",0
in case of doubt: token embedding has higher priority than document embedding,0
random check some tokens to save performance.,0
Models such as FNet do not have an attention_mask,0
set language IDs for XLM-style transformers,0
"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have",0
"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids.",0
set context if not set already,0
flair specific pre-tokenization,0
fields to store left and right context,0
expand context only if context_length is set,0
"if context_dropout is set, randomly deactivate left context during training",0
"if context_dropout is set, randomly deactivate right context during training",0
"if use_context_separator is set, add a [FLERT] token",0
return expanded sentence and context length information,0
"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first.",0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
do not print transformer warnings as these are confusing in this case,0
load tokenizer and transformer model,0
load tokenizer from inmemory zip-file,0
if model is quantized by BitsAndBytes this will fail,0
add adapters for finetuning,0
peft_config: PeftConfig,0
model name,0
embedding parameters,0
send mini-token through to check how many layers the model has,0
return length,0
"If we use a context separator, add a new special token",0
"most models have an initial BOS token, except for XLNet, T5 and GPT2",0
"when initializing, embeddings are in eval mode by default",0
in case of doubt: token embedding has higher priority than document embedding,0
in case of doubt: token embedding has higher priority than document embedding,0
legacy TransformerDocumentEmbedding,0
legacy TransformerTokenEmbedding,0
legacy Flair <= 0.12,0
legacy Flair <= 0.7,0
legacy TransformerTokenEmbedding,0
Legacy TransformerDocumentEmbedding,0
legacy TransformerTokenEmbedding,0
legacy TransformerDocumentEmbedding,0
some models like the tars model somehow lost this information.,0
copy values from new embedding,0
do not switch the attention implementation upon reload.,0
those parameters are only from the super class and will be recreated in the constructor.,0
cls first pooling can be done without recreating sentence hidden states,0
make the tuple a tensor; makes working with it easier.,0
"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention",0
only use layers that will be outputted,0
this parameter is fixed,0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
make compatible with serialized models,0
gensim version 4,0
gensim version 3,0
"if no embedding is set, the vocab and embedding length is required",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNINOS embeddings,0
pubmed embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
"this is required to force the module on the cpu,",0
"if a parent module is put to gpu, the _apply is called to each sub_module",0
self.to(..) actually sets the device properly,0
this ignores the get_cached_vec method when loading older versions,0
it is needed for compatibility reasons,0
gensim version 4,0
gensim version 3,0
"when loading the old versions from pickle, the embeddings might not be added as pytorch module.",0
"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might",0
lead to issues while loading (trying to load weights that weren't stored as python weights and therefore,0
not finding them),0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
multilingual models,0
English models,0
Arabic,0
Bulgarian,0
Czech,0
Danish,0
German,0
Spanish,0
Basque,0
Persian,0
Finnish,0
French,0
Hebrew,0
Hindi,0
Croatian,0
Indonesian,0
Italian,0
Japanese,0
Malayalam,0
Dutch,0
Norwegian,0
Polish,0
Portuguese,0
Pubmed,0
Slovenian,0
Swedish,0
Tamil,0
Spanish clinical,0
CLEF HIPE Shared task,0
Amharic,0
Ukrainian,0
load model if in pretrained model map,0
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir),0
CLEF HIPE models are lowercased,0
embeddings are static if we don't do finetuning,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout",0
gradients are enable if fine-tuning is enabled,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
offset mode that extracts at whitespace after last character,0
offset mode that extracts at last character,0
make compatible with old models,0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
check token.text is empty or not,0
set aggregation operation,0
add embeddings after updating,0
model architecture,0
model architecture,0
"""pl"",",0
download if necessary,0
load the model,0
"this is required to force the module on the cpu,",0
"if a parent module is put to gpu, the _apply is called to each sub_module",0
self.to(..) actually sets the device properly,0
"when loading the old versions from pickle, the embeddings might not be added as pytorch module.",0
"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might",0
lead to issues while loading (trying to load weights that weren't stored as python weights and therefore,0
not finding them),0
old embeddings do not have a torch-embedding and therefore do not store the weights in the saved torch state_dict,0
"however they are already initialized rightfully, so we just set the state dict from our current state dict",0
GLOVE embeddings,0
no need to recreate as NILCEmbeddings,0
read in test file if exists,0
read in dev file if exists,0
"find train, dev and test files if not specified",0
Add tags for each annotated span,0
Remove leading and trailing whitespaces from annotated spans,0
Search start and end token index for current span,0
If end index is not found set to last token,0
Throw error if indices are not valid,0
Add metadatas for sentence,0
Currently all Jsonl Datasets are stored in Memory,0
get train data,0
read in test file if exists,0
read in dev file if exists,0
"find train, dev and test files if not specified",0
special key for space after,0
special key for feature columns,0
special key for dependency head id,0
"store either Sentence objects in memory, or only file offsets",0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
determine encoding of text file,0
identify which columns are spans and which are word-level,0
now load all sentences,0
skip first line if to selected,0
option 1: keep Sentence objects in memory,0
pointer to previous,0
parse next sentence,0
quit if last sentence reached,0
skip banned sentences,0
set previous and next sentence for context,0
append parsed sentence to list in memory,0
option 2: keep source data in memory,0
"read lines for next sentence, but don't parse",0
quit if last sentence reached,0
append raw lines for each sentence,0
we make a distinction between word-level tags and span-level tags,0
read first sentence to determine which columns are span-labels,0
skip first line if to selected,0
check the first 5 sentences,0
go through all annotations and identify word- and span-level annotations,0
- if a column has at least one BIES we know it's a Span label,0
"- if a column has at least one tag that is not BIOES, we know it's a Token label",0
- problem cases are columns for which we see only O - in this case we default to Span,0
skip assigned columns,0
the space after key is always word-levels,0
"if at least one token has a BIES, we know it's a span label",0
"if at least one token has a label other than BIOES, we know it's a token label",0
all remaining columns that are not word-level are span-level,0
for column in self.word_level_tag_columns:,0
"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")",0
"if sentence ends, break",0
parse comments if possible,0
"otherwise, this line is a token. parse and add to sentence",0
check if this sentence is a document boundary,0
add span labels,0
discard tags from tokens that are not added to the sentence,0
parse relations if they are set,0
head and tail span indices are 1-indexed and end index is inclusive,0
parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas',0
"to set the metadata ""domain"" to ""de-orcas""",0
get fields from line,0
get head_id if exists (only in dependency parses),0
initialize token,0
go through all columns,0
'feats' and 'misc' column should be split into different fields,0
special handling for whitespace after,0
add each other feature as label-value pair,0
get the task name (e.g. 'ner'),0
get the label value,0
add label,0
remap regular tag names,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
set sentence context using partials TODO: pointer to dataset is really inefficient,1
use all domains,0
iter over all domains / sources and create target files,0
The conll representation of coref spans allows spans to,0
"overlap. If spans end or begin at the same word, they are",0
"separated by a ""|"".",0
The span begins at this word.,0
The span begins and ends at this word (single word span).,0
"The span is starting, so we record the index of the word.",0
"The span for this id is ending, but didn't start at this word.",0
Retrieve the start index from the document state and,0
add the span to the clusters for this id.,0
strip all bracketing information to,0
get the actual propbank label.,0
Entering into a span for a particular semantic role label.,0
We append the label and set the current span for this annotation.,0
"If there's no '(' token, but the current_span_label is not None,",0
then we are inside a span.,0
We're outside a span.,0
"Exiting a span, so we reset the current span label for this annotation.",0
The words in the sentence.,0
The pos tags of the words in the sentence.,0
the pieces of the parse tree.,0
The lemmatised form of the words in the sentence which,0
have SRL or word sense information.,0
The FrameNet ID of the predicate.,0
"The sense of the word, if available.",0
"The current speaker, if available.",0
"Cluster id -> List of (start_index, end_index) spans.",0
Cluster id -> List of start_indices which are open for this id.,0
Replace brackets in text and pos tags,0
with a different token for parse trees.,0
only keep ')' if there are nested brackets with nothing in them.,0
There are some bad annotations in the CONLL data.,0
"They contain no information, so to make this explicit,",0
we just set the parse piece to be None which will result,0
in the overall parse tree being None.,0
"If this is the first word in the sentence, create",0
empty lists to collect the NER and SRL BIO labels.,0
"We can't do this upfront, because we don't know how many",0
"components we are collecting, as a sentence can have",0
variable numbers of SRL frames.,0
Create variables representing the current label for each label,0
sequence we are collecting.,0
"If any annotation marks this word as a verb predicate,",0
we need to record its index. This also has the side effect,0
of ordering the verbal predicates by their location in the,0
"sentence, automatically aligning them with the annotations.",0
"this would not be reached if parse_pieces contained None, hence the cast",0
Non-empty line. Collect the annotation.,0
Collect any stragglers or files which might not,0
have the '#end document' format for the end of the file.,0
this dataset name,0
check if data there,0
column format,0
this dataset name,0
check if data there,0
column format,0
this dataset name,0
download data if necessary,0
download files if not present locally,0
we need to slightly modify the original files by adding some new lines after document separators,0
column format,0
this dataset name,0
download data if necessary,0
Set the base path for the dataset,0
Define column format,0
Define dataset name,0
Define data folder path,0
"Check if the train data file exists, otherwise download and prepare the dataset",0
Download and prepare the dataset,0
Initialize the parent class with the specified parameters,0
"Check if the line is a change, delete or add command (like 17721c17703,17705 or 5728d5727)",0
Append the previous change block to the changes list,0
Start a new change block,0
"Capture original lines (those marked with ""<"")",0
"Capture new lines (those marked with "">"")",0
Append the last change block to the changes list,0
Apply each change in reverse order (important to avoid index shift issues),0
"Determine the type of the change: `c` for change, `d` for delete, `a` for add",0
"Example command: 17721c17703,17705",0
Example command: 5728d5727,0
"Example command: 1000a1001,1002",0
Write the modified content to the output file,0
Strip whitespace to check if the line is empty,0
Write the first token followed by a newline if the line is not empty,0
Write an empty line if the line is empty,0
Strip the leading '[TOKEN]\t' from the annotation,0
Create a temporary directory,0
Check the contents of the temporary directory,0
Extract only the tokens from the original CoNLL03 files,0
Apply the downloaded patch files to apply our token modifications (e.g. line breaks),0
Merge the updated token files with the CleanCoNLL annotations,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
Remove CoNLL-U meta information in the last column,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
entity_mapping,0
this dataset name,0
download data if necessary,0
data validation,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download files if not present locallys,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
# download zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
download data if necessary,0
"unzip the downloaded repo and merge the train, dev and test datasets",0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
check if data there,0
create folder,0
download dataset,0
column format,0
this dataset name,0
download and parse data if necessary,0
create train test dev if not exist,0
column format,0
this dataset name,0
If the extracted corpus file is not yet present in dir,0
download zip if necessary,0
"extracted corpus is not present , so unpacking it.",0
column format,0
this dataset name,0
download zip,0
unpacking the zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)",0
download files if not present locally,0
we need to modify the original files by adding new lines after after the end of each sentence,0
if only one language is given,0
column format,0
this dataset name,0
"use all languages if explicitly set to ""all""",0
download data if necessary,0
initialize comlumncorpus and add it to list,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
this list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
get google drive id from list,0
download from google drive,0
unzip,0
transform data into required format,0
"the processed dataset has the additional ending ""_new""",0
remove the unprocessed dataset,0
initialize comlumncorpus and add it to list,0
if no languages are given as argument all languages used in XTREME will be loaded,0
if only one language is given,0
column format,0
this dataset name,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
This list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
download from HU Server,0
unzip,0
transform data into required format,0
initialize comlumncorpus and add it to list,0
if only one language is given,0
column format,0
this dataset name,0
download data if necessary,0
initialize comlumncorpus and add it to list,0
download data if necessary,0
unpack and write out in CoNLL column-like format,0
column format,0
this dataset name,0
download data if necessary,0
data is not in IOB2 format. Thus we transform it to IOB2,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
column format,0
this dataset name,0
rename according to train - test - dev - convention,0
column format,0
this dataset name,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
Add missing newline after header,0
Workaround for empty tokens,1
"Add ""real"" document marker",0
Dataset split mapping,0
v2.0 only adds new language and splits for AJMC dataset,0
Special document marker for sample splits in AJMC dataset,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
create dataset files from index and train/test splits,0
news date is usually in 3rd or 4th sentence of each article,0
"generate NoiseBench dataset variants, given CleanCoNLL, noisy label files and index file",0
"os.makedirs(os.path.join('data','noisebench'), exist_ok=True)",0
copy test set,0
if only one language is given,0
column format,0
this dataset name,0
"use all languages if explicitly set to ""all""",0
download data if necessary,0
initialize comlumncorpus and add it to list,0
this dataset name,0
one name can map to multiple concepts,0
NOTE: EntityLinkingDictionary are lazy-loaded from a preprocessed file.,0
Use this class to load into memory all candidates,0
"if identifier == ""MESH:D013749"":",0
# This MeSH ID was used by MeSH when this chemical was part of the MeSH controlled vocabulary.,0
continue,0
parse line,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
paths to train and test splits,0
init corpus,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
iterate over all html files,0
"get rid of html syntax, we only need the text",0
between all documents we write a separator symbol,0
skip empty strings,0
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)",0
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention",0
sentence splitting and tokenization,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out-file in column format,0
"in case something goes wrong, delete the dataset and raise error",0
this dataset name,0
download and parse data if necessary,0
from qwikidata.linked_data_interface import get_entity_dict_from_api,0
generate qid wikiname dictionaries,0
merge dictionaries,0
ignore first line,0
commented and empty lines,0
read all Q-IDs,0
ignore first line,0
request,0
this dataset name,0
we use the wikiids in the data instead of directly utilizing the wikipedia urls.,0
like this we can quickly check if the corresponding page exists,0
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi,0
delete unprocessed file,0
collect all wikiids,0
create the dictionary,0
request,0
this dataset name,0
names of raw text documents,0
open output_file,0
iterate through all documents,0
split sentences and tokenize,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out file,0
annotation from one annotator or two agreeing annotators,0
this dataset name,0
download and parse data if necessary,0
this dataset name,0
download and parse data if necessary,0
First parse the post titles,0
Keep track of how many and which entity mentions does a given post title have,0
Check if the current post title has an entity link and parse accordingly,0
Post titles with entity mentions (if any) are handled via this function,0
Then parse the comments,0
"Iterate over the comments.tsv file, until the end is reached",0
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched.",0
Each comment thread is handled as one 'document'.,0
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.,0
This if-condition is needed to handle this problem.,0
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure",0
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above",0
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle.",0
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,",0
and not just single letters into single rows.,0
If there are annotated entity mentions for given post title or a comment thread,0
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence",0
Write the token with a corresponding tag to file,0
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed",0
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag",0
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized,0
"incorrectly, in order to keep the desired format (empty line as a sentence separator).",0
"Thrown when the second check above happens, but the last token of a sentence is reached.",0
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below.",0
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS",0
Check if further annotations belong to the current post title or comment thread as well,0
Stop when the end of an annotation file is reached,0
Check if further annotations belong to the current sentence as well,0
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)",0
Docstart,0
if there is more than one word in the chunk we write each in a separate line,0
print(chunks),0
empty line after each sentence,0
convert the file to CoNLL,0
this dataset name,0
"check if data there, if not, download the data",0
create folder,0
download data,0
transform data into column format if necessary,0
if no filenames are specified we use all the data,0
"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled",0
also we remove 'raganato_ALL' from filenames in case its in the list,0
generate the test file,0
make column file and save to data_folder,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
TODO: Adapt this following: https://github.com/flairNLP/flair/pull/3146,1
+1 assumes the title and abstract will be joined by a space.,0
"We need a unique identifier for this entity, so build it from the document id and entity id",0
The user can provide a callable that returns the database name.,0
some entities are not linked and,0
some entities are linked to multiple normalized ids,0
passages must not overlap and spans must cover the entire document,0
entities,0
parse db ids,0
Some of the entities have a off-by-one error. Correct these annotations!,0
"passage offsets/lengths do not connect, recalculate them for this schema.",0
this dataset name,0
download data if necessary,0
if True:,0
write CoNLL-U Plus header,0
"Some special cases (e.g., missing spaces before entity marker)",0
necessary if text should be whitespace tokenizeable,0
Handle case where tail may occur before the head,0
this dataset name,0
write CoNLL-U Plus header,0
this dataset name,0
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors,1
download data if necessary,0
write CoNLL-U Plus header,0
The span has ended.,0
We are entering a new span; reset indices,0
and active tag to new span.,0
We're inside a span.,0
Last token might have been a part of a valid span.,0
this dataset name,0
write CoNLL-U Plus header,0
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):",0
"with zip_file.open(source_file_path, mode=""r"") as source_file:",0
target_file_path = Path(data_folder) / target_filename,0
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:",0
# write CoNLL-U Plus header,0
"target_file.write(""# global.columns = id form ner\n"")",0
for example in json.load(source_file):,0
token_list = self._tacred_example_to_token_list(example),0
target_file.write(token_list.serialize()),0
check if first tag row is already occupied,0
"if first tag row is occupied, use second tag row",0
hardcoded mapping TODO: perhaps find nicer solution,1
remap regular tag names,0
else skip to position in file where sentence begins,0
set sentence context using partials TODO: pointer to dataset is really inefficient,1
read in dev file if exists,0
read in test file if exists,0
the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44,0
"find train, dev and test files if not specified",0
use test_file to create test split if available,0
use dev_file to create test split if available,0
"if data point contains black-listed label, do not use",0
first check if valid sentence,0
"if so, add to indices",0
"find train, dev and test files if not specified",0
variables,0
different handling of in_memory data than streaming data,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
test if format is OK,0
test if at least one label given,0
make sentence from text (and filter for length),0
"if a pair column is defined, make a sentence pair object",0
noinspection PyDefaultArgument,0
dataset name includes the split size,0
default dataset folder is the cache root,0
download data if necessary,0
download each of the 28 splits,0
create dataset directory if necessary,0
download senteval datasets if necessary und unzip,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
download data from same source as in huggingface's implementations,0
read label order,0
"Original labels are [1, 2, 3, 4] -> ['World', 'Sports', 'Business', 'Sci/Tech']",0
"Re-map to [0, 1, 2, 3].",0
this dataset name,0
download data if necessary,0
handle labels file,0
handle data file,0
Create flair compatible labels,0
"by default, map point score to POSITIVE / NEGATIVE values",0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file from CSV,0
create test.txt file from CSV,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create train dev and test files in fasttext format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
convert to FastText format,0
download data if necessary,0
"if data is not downloaded yet, download it",0
get the zip file,0
move original .tsv files to another folder,0
create train and dev splits in fasttext format,0
create eval_dataset file with no labels,0
download zip archive,0
unpack file in datasets directory (zip archive contains a directory named SST-2),0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download datasets if necessary,0
create dataset directory if necessary,0
create correctly formated txt files,0
multiple labels are possible,0
this dataset name,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
create a separate directory for different tasks,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
check if dataset is supported,0
set file names,0
set file names,0
download and unzip in file structure if necessary,0
instantiate corpus,0
"find train, dev and test files if not specified",0
"create DataPairDataset for train, test and dev file, if they are given",0
stop if file does not exist,0
create a DataPair object from strings,0
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings",0
"find train, dev, and test files if not specified",0
"create DataTripleDataset for train, test, and dev files, if they are given",0
stop if the file does not exist,0
create a DataTriple object from strings,0
"if in_memory is True we return a DataTriple, otherwise we create one from the lists of strings",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"reorder dev datasets to have same columns as in train set: 8, 9, and 11",0
dev sets include 5 different annotations but we will only keep the gold label,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get test and dev sets,0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data not downloaded yet, download it",0
get the zip file,0
"the downloaded files have json format, we transform them to tsv",0
Function to transform JSON file to tsv for Recognizing Textual Entailment Data,0
remove json file,0
Uses dynamic programming approach to calculate maximum independent set in interval graph,0
with sum of all entity lengths as secondary key,0
calculate offset without current text,0
because we stick all passages of a document together,0
TODO For split entities we also annotate everything inbetween which might be a bad idea?,1
Try to fix incorrect annotations,0
print(,0
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}""",0
),0
Ignore empty lines or relation annotations,0
FIX annotation of whitespaces (necessary for PDR),0
Add task description for multi-task learning,0
One token may contain multiple entities -> deque all of them,0
column format,0
this dataset name,0
Create tokenization-dependent CONLL files. This is necessary to prevent,0
from caching issues (e.g. loading the same corpus with different sentence splitters),0
column format,0
this dataset name,0
column format,0
this dataset name,0
Edge case: last token starts a new entity,0
Last document in file,0
column format,0
this dataset name,0
column format,0
this dataset name,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
Edge case: last token starts a new entity,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
column format,0
this dataset name,0
Read texts,0
Read annotations,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
We need to apply a patch to correct the original training file,0
Articles title,0
Article abstract,0
Entity annotations,0
column format,0
this dataset name,0
Edge case: last token starts a new entity,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
Incomplete article,0
Invalid XML syntax,0
column format,0
this dataset name,0
column format,0
this dataset name,0
if len(mid) != 3:,0
continue,0
Try to fix entity offsets,0
column format,0
this dataset name,0
There is still one illegal annotation in the file ..,0
column format,0
this dataset name,0
"Abstract first, title second to prevent issues with sentence splitting",0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
column format,0
this dataset name,0
"Filter for specific entity types, by default no entities will be filtered",0
Get original HUNER splits to retrieve a list of all document ids contained in V2,0
train and dev split of V2 will be train in V4,0
test split of V2 will be dev in V4,0
New documents in V4 will become test documents,0
column format,0
this dataset name,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
build dataset name and full huggingface reference name,0
Download data if necessary,0
"Some datasets in BigBio only have train or test splits, not both",0
"If only test split, assign it to train split",0
"If only train split, sample other from it (sample_missing_splits=True)",0
Not every dataset has a dev / validation set!,0
Perform type mapping if necessary,0
return None,0
TODO: Add entity type mapping for all remaining bigbio datasets not in HunFlair?,1
"""simple_chemical"": ""chemical"",  # BioNLP ST 2013 CG",0
"""cancer"": ""disease"",  # BioNLP ST 2013 CG",0
"""gene_or_gene_product"": ""gene"",  # BioNLP ST 2013 CG",0
"""gene"": ""gene"",  # NLM Gene",0
"""chemical"": ""chemical"",  # NLM Chem",0
"""cellline"": ""cell_line"",  # Cell Finder",0
"""species"": ""species"",  # Cell Finder",0
"""protein"": ""gene"",  # BioID",0
"Collect all texts of the document, each passage will be",0
a text in our internal format,0
Sort passages by start offset,0
Transform all entity annotations into internal format,0
Find the passage of the entity (necessary for offset adaption),0
Adapt entity offsets according to passage offsets,0
FIXME: This is just for debugging purposes,1
passage_text = id_to_text[passage_id],0
doc_text = passage_text[entity_offset[0] : entity_offset[1]],0
"mention_text = entity[""text""][0]",0
if doc_text != mention_text:,0
"print(f""Annotation error ({document['document_id']}) - Doc: {doc_text} vs. Mention: {mention_text}"")",0
Get element in the middle,0
Is the mention with the passage offsets?,0
"If element is smaller than mid, then it can only",0
be present in left subarray,0
Else the element can only be present in right subarray,0
TODO whether cell or cell line is the correct tag,1
TODO whether cell or cell line is the correct tag,1
Special case for ProGene: We need to use the split_0_train and split_0_test splits,0
as they are currently provided in BigBio,0
cache Feidegger config file,0
cache Feidegger images,0
replace image URL with local cached file,0
append Sentence-Image data point,0
cast to list if necessary,0
cast to list if necessary,0
"first, check if pymongo is installed",0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
Expose base classses,0
Expose all biomedical data sets used for the evaluation of BioBERT,0
-,0
-,0
-,0
-,0
Expose all biomedical data sets using the HUNER splits,0
Expose all biomedical data sets,0
Expose all document classification datasets,0
word sense disambiguation,0
Expose all entity linking datasets,0
Expose all relation extraction datasets,0
universal proposition banks,0
keyphrase detection datasets,0
other NER datasets,0
standard NER datasets,0
Expose all sequence labeling datasets,0
Expose all text-image datasets,0
Expose all text-text datasets,0
Expose all treebanks,0
"find train, dev and test files if not specified",0
get train data,0
get test data,0
get dev data,0
option 1: read only sentence boundaries as offset positions,0
option 2: keep everything in memory,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
current token ID,0
handling for the awful UD multiword format,0
end of sentence,0
comments or ellipsis,0
if token is a multi-word,0
normal single-word tokens,0
"if we don't split multiwords, skip over component words",0
add token,0
add morphological tags,0
derive whitespace logic for multiwords,0
"if multi-word equals component tokens, there should be no whitespace",0
go through all tokens in subword and set whitespace_after information,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
this dataset name,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"finally, print model card for information",0
Note: Multi-GPU can affect corpus loading,0
This code will run multiple times -- each GPU gets its own process and each process runs this code. We need to,0
"ensure that the corpus has the same elements and order on all processes, despite sampling. We do that by using",0
the same seed on all processes.,0
Note: Multi-GPU can affect choice of batch size.,0
"In order to compare batch updates fairly between single and multi-GPU training, we should:",0
1) Step the optimizer after the same number of examples to achieve com,0
2) Process the same number of examples in each forward pass,0
"e.g. Suppose your machine has 2 GPUs. If multi_gpu=False, the first gpu will process 32 examples, then the",0
"first gpu will process another 32 examples, then the optimizer will step. If multi_gpu=True, each gpu will",0
"process 32 examples at the same time, then the optimizer will step.",0
noqa: INP001,0
-- Project information -----------------------------------------------------,0
"The full version, including alpha/beta/rc tags",0
use smv_current_version as the git url,0
-- General configuration ---------------------------------------------------,0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
Napoleon settings,0
Whitelist pattern for tags (set to None to ignore all tags),0
Whitelist pattern for branches (set to None to ignore all branches),0
Whitelist pattern for remotes (set to None to use local branches only),0
Pattern for released versions,0
Format for versioned output directories inside the build directory,0
Determines whether remote or local git branches/tags are preferred if their output dirs conflict,0
test corpus,0
create a TARS classifier,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
test corpus,0
create a TARS classifier,0
switch to a new task (TARS can do multiple tasks so you must define one),0
initialize the text classifier trainer,0
start the training,0
"With end symbol, without start symbol, padding in front",0
"Without end symbol, with start symbol, padding in back",0
"Without end symbol, without start symbol, padding in front",0
initialize trainer,0
initialize trainer,0
initialize trainer,0
clean up directory,0
clean up directory,0
example sentence,0
set 4 labels for 2 tokens ('love' is tagged twice),0
check if there are three POS labels with correct text and values,0
check if there are is one SENTIMENT label with correct text and values,0
check if all tokens are correctly labeled,0
remove the pos label from the last word,0
there should be 2 POS labels left,0
now remove all pos tags,0
set 3 labels for 2 spans (HU is tagged twice),0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
"now delete the NER tags of ""Humboldt-Universitt zu Berlin""",0
should be only one NER label left,0
and only one NER span,0
set 3 labels for 2 spans (HU is tagged twice with different tags),0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
"now delete the NER tags of ""Humboldt-Universitt zu Berlin""",0
should be only one NER label left,0
and only one NER span,0
but there is also one orgtype span and label,0
and only one NER span,0
let's add the NER tag back,0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
now remove all NER tags,0
set 3 labels for 2 spans (HU is tagged twice with different tags),0
create two relation label,0
there should be two relation labels,0
there should be one syntactic labels,0
"there should be two relations, one with two and one with one label",0
example sentence,0
add another topic label,0
example sentence,0
has sentiment value,0
has 4 part of speech tags,0
has 1 NER tag,0
should be in total 6 labels,0
example sentence,0
add two NER labels,0
get the four labels,0
check that only two of the respective data points are equal,0
make a sentence and some right context,0
TODO: is this desirable? Or should two sentences with same text be considered same objects?,1
Initializing a Sentence this way assumes that there is a space after each token,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
load column dataset with one entry,0
load column dataset with two entries,0
load column dataset with three entries,0
"get training, test and dev data",0
"get training, test and dev data",0
check if Token labels are correct,0
"get training, test and dev data",0
check if Token labels for frames are correct,0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
"assert [token.get_tag(""head"").value for token in sent1.tokens] == [",0
"""2"",",0
"""0"",",0
"""4"",",0
"""2"",",0
"""2"",",0
"""2"",",0
],0
This test only covers basic universal dependencies datasets.,0
"For example, multi-word tokens or the ""deps"" column sentence annotations are not supported yet.",0
"Here, we use the default token annotation fields.",0
This test covers the complete HIPE 2022 dataset.,0
https://github.com/hipe-eval/HIPE-2022-data,0
"Includes variant with document separator, and all versions of the dataset.",0
"We have manually checked, that these numbers are correct:",0
"+1 offset, because of missing EOS marker at EOD",0
Test data for v2.1 release,0
This test covers the complete ICDAR Europeana corpus:,0
https://github.com/stefan-it/historic-domain-adaptation-icdar,0
"This test covers the complete MasakhaNER dataset, including support for v1 and v2.",0
This test covers the NERMuD dataset. Official stats can be found here:,0
https://github.com/dhfbk/KIND/tree/main/evalita-2023,0
Number of instances per dataset split are taken from https://huggingface.co/datasets/elenanereiss/german-ler,0
This test covers the complete MasakhaPOS dataset.,0
"See MobIE paper (https://aclanthology.org/2021.konvens-1.22/), table 2",0
--- Embeddings that are shared by both models --- #,0
--- Task 1: Sentiment Analysis (5-class) --- #,0
Define corpus and model,0
-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #,0
Define corpus and model,0
-- Define mapping (which tagger should train on which model) -- #,0
-- Create model trainer and train -- #,0
NOTE: Avoid emtpy string if mentions are just punctutations (e.g. `-` or `(`),0
clean up file,0
get features from forward propagation,0
reverse sort all sequences by their length,0
remove previously predicted labels of this type,0
no need for label_dict,0
"pretrained_model = ""tars-ner""  # disabled due to too much space requirements.",0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
Intel ----founded_by---> Gordon Moore,0
Intel ----founded_by---> Robert Noyce,0
"Ground truth is a set of tuples of (<Sentence Text>, <Relation Label Values>)",0
Check sentence masking and relation label annotation on,0
"training, validation and test dataset (in this test the splits are the same)",0
"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google .""",0
"Entity pair permutations of: ""Microsoft was founded by Bill Gates .""",0
"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 .""",0
"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany.""",0
This sentence is only included if we transform the corpus with cross augmentation,0
"pretrained_model = ""tars-base""  # disabled due to too much space requirements.",0
Ensure this is an example that predicts no classes in multilabel,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
ensure that the prepared tensors is what we expect,0
use a SequenceTagger to save and reload the embedding in the manner it is supposed to work,0
previous and next sentence as context,0
test expansion for sentence without context,0
test expansion for with previous and next as context,0
test expansion if first sentence is document boundary,0
test expansion if we don't use context,0
"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher.",0
dummy model with embeddings,0
save the dummy and load it again,0
check that context_length and use_context_separator is the same for both,0
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
Extract all the contents of zip file in current directory,0
use model name as subfolder,0
Lazy import,0
output information,0
Extract all the contents of zip file in current directory,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
load_big_file is a workaround byhttps://github.com/highway11git,1
to load models on some Mac/Windows setups,0
see https://github.com/zalandoresearch/flair/issues/351,0
first determine the distribution of classes in the dataset,0
weight for each sample,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
increment for last token in sentence if not followed by whitespace,0
this is the default init size of a lmdb database for embeddings,0
get db filename from embedding name,0
"In case initialization of cached version failed, just fallback to the original WordEmbeddings",0
SequenceTagger,0
TextClassifier,0
get db filename from embedding name,0
if embedding database already exists,0
"otherwise, push embedding to database",0
if embedding database already exists,0
open the database in read mode,0
we need to set self.k,0
create and load the database in write mode,0
"no idea why, but we need to close and reopen the environment to avoid",0
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot,0
when opening new transaction !,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
set 'add_unk' if the dictionary was created with a version of Flair older than 0.9,0
set 'add_unk' depending on whether <unk> is a key,0
"if one embedding name, directly return it",0
"if multiple embedding names, concatenate them",0
First we remove any existing labels for this PartOfSentence in self.sentence,0
labels also need to be deleted at Sentence object,0
delete labels at object itself,0
The Token is a special _PartOfSentence in that it may be initialized without a Sentence.,0
"therefore, labels get added only to the Sentence if it exists",0
The Token is a special _PartOfSentence in that it may be initialized without a Sentence.,0
"Therefore, labels get set only to the Sentence if it exists",0
"check if the span already exists. If so, return it",0
else make a new span,0
"check if the relation already exists. If so, return it",0
else make a new relation,0
private field for all known spans,0
the tokenizer used for this sentence,0
some sentences represent a document boundary (but most do not),0
internal variables to denote position inside dataset,0
"if text is passed, instantiate sentence with tokens (words)",0
determine token positions and whitespace_after flag,0
the last token has no whitespace after,0
log a warning if the dataset is empty,0
data with zero-width characters cannot be handled,0
set token idx and sentence,0
append token to sentence,0
register token annotations on sentence,0
move sentence embeddings to device,0
also move token embeddings to device,0
clear token embeddings,0
infer whitespace after field,0
"if sentence has no tokens, return empty string",0
"otherwise, return concatenation of tokens with the correct offsets",0
The sentence's start position is not propagated to its tokens.,0
"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces.",0
No character at the corresponding code point: remove it,0
"if no label if specified, return all labels",0
"if the label type exists in the Sentence, return it",0
return empty list if none of the above,0
labels also need to be deleted at all tokens,0
labels also need to be deleted at all known spans,0
remove spans without labels,0
delete labels at object itself,0
set name,0
abort if no data is provided,0
sample test data from train if none is provided,0
sample dev data from train if none is provided,0
set train dev and test data,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
"first, determine the datapoint type by going through dataset until first label is found",0
count all label types per sentence,0
go through all labels of label_type and count values,0
special handling for Token-level annotations. Add all untagged as 'O' label,0
"if an unk threshold is set, UNK all label values below this threshold",0
sample randomly from a label distribution according to the probabilities defined by the noise transition matrix,0
replace the old label with the new one,0
keep track of the old (clean) label using another label type category,0
keep track of how many labels in total are flipped,0
sample randomly from a label distribution according to the probabilities defined by the desired noise share,0
replace the old label with the new one,0
keep track of the old (clean) label using another label type category,0
keep track of how many labels in total are flipped,0
"add a dummy ""O"" to close final prediction",0
return complex list,0
internal variables,0
non-set tags are OUT tags,0
anything that is not OUT is IN,0
does this prediction start a new span?,0
B- and S- always start new spans,0
"if the predicted class changes, I- starts a new span",0
"if the predicted class changes and S- was previous tag, start a new span",0
if an existing span is ended (either by reaching O or starting a new span),0
determine score and value,0
append to result list,0
reset for-loop variables for new span,0
remember previous tag,0
global variable: cache_root,0
Get the device from the environment variable,0
global variable: device,0
"No need for correctness checks, torch is doing it",0
global variable: version,0
global variable: arrow symbol,0
dummy return to fulfill trainer.train() needs,0
print(vec),0
Attach optimizer,0
"convert `metrics` to float, in case it's a zero-dim Tensor",0
if memory mode option 'none' delete everything,0
"if dynamic embedding keys not passed, identify them automatically",0
always delete dynamic embeddings,0
"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)",0
optional metric space decoder if prototypes have different length than embedding,0
create initial prototypes for all classes (all initial prototypes are a vector of all 1s),0
"if set, create initial prototypes from normal distribution",0
"if set, use a radius",0
all parameters will be pushed internally to the specified device,0
decode embeddings into prototype space,0
"if unlabeled distance is set, mask out loss to unlabeled class prototype",0
verbalize BIOES labels,0
"if label is not BIOES, use label itself",0
Always include the name of the Model class for which the state dict holds,0
"write out a ""model card"" if one is set",0
save model,0
"if this class is abstract, go through all inheriting classes and try to fetch and load the model",0
get all non-abstract subclasses,0
"try to fetch the model for each subclass. if fetching is possible, load model and return it",0
"skip any invalid loadings, e.g. not found on huggingface hub",0
"if the model cannot be fetched, load as a file",0
try to get model class from state,0
"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses",0
"if str(model_cls) == ""<class 'flair.models.pairwise_classification_model.TextPairClassifier'>"": continue",0
"skip any invalid loadings, e.g. not found on huggingface hub",0
"if this class is not abstract, fetch the model and load it",0
"make sure <unk> is contained in gold_label_dictionary, if given",0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
loss calculation,0
variables for printing,0
variables for computing scores,0
remove any previously predicted labels,0
predict for batch,0
get the gold labels,0
add to all_predicted_values,0
make printout lines,0
convert true and predicted values to two span-aligned lists,0
delete exluded labels if exclude_labels is given,0
"if after excluding labels, no label is left, ignore the datapoint",0
write all_predicted_values to out_file if set,0
make the evaluation dictionary,0
check if this is a multi-label problem,0
compute numbers by formatting true and predicted such that Scikit-Learn can use them,0
multi-label problems require a multi-hot vector for each true and predicted label,0
single-label problems can do with a single index for each true and predicted label,0
"now, calculate evaluation numbers",0
there is at least one gold label or one prediction (default),0
compute accuracy separately as it is not always in classification_report (e.. when micro avg exists),0
"if there is only one label, then ""micro avg"" = ""macro avg""",0
"The ""micro avg"" appears only in the classification report if no prediction is possible.",0
"Otherwise, it is identical to the ""macro avg"". In this case, we add it to the report.",0
"Create and populate score object for logging with all evaluation values, plus the loss",0
issue error and default all evaluation numbers to 0.,0
check if there is a label mismatch,0
print info,0
set the embeddings,0
initialize the label dictionary,0
initialize the decoder,0
set up multi-label logic,0
init dropouts,0
loss weights and loss function,0
Initialize the weight tensor,0
set up gradient reversal if so specified,0
embed sentences,0
get a tensor of data points,0
do dropout,0
make a forward pass to produce embedded data points and labels,0
get the data points for which to predict labels,0
get their gold labels as a tensor,0
pass data points through network to get encoded data point tensor,0
decode,0
an optional masking step (no masking in most cases),0
calculate the loss,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
filter data points in batch,0
stop if all sentences are empty,0
pass data points through network and decode,0
if anything could possibly be predicted,0
remove previously predicted labels of this type,0
filter data points that have labels outside of dictionary,0
add DefaultClassifier arguments,0
add variables of DefaultClassifier,0
Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23,0
Get projected 1st dimension,0
Compute bilinear form,0
Arcosh,0
Project the input data to n+1 dimensions,0
"The first dimension, is recomputed in the distance module",0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for",0
"train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
plot i,0
save plots,0
save plots,0
plt.show(),0
save plot,0
auto-spawn on GPU if available,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
TODO: not saving lines yet,1
TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.,1
- MaskedRelationClassifier ?,0
This depends if this relation classification architecture should replace or offer as an alternative.,0
Set label type and prepare label dictionary,0
Initialize super default classifier,0
Add the special tokens from the encoding strategy,0
"Auto-spawn on GPU, if available",0
Only use entities labelled with the specified labels for each label type,0
Only use entities above the specified threshold,0
Use a dictionary to find gold relation annotations for a given entity pair,0
Yield head and tail entity pairs from the cross product of all entities,0
Remove identity relation entity pairs,0
Remove entity pairs with labels that do not match any,0
of the specified relations in `self.entity_pair_labels`,0
"Obtain gold label, if existing",0
Some sanity checks,0
Pre-compute non-leading head and tail tokens for entity masking,0
We can not use the plaintext of the head/tail span in the sentence as the mask/marker,0
since there may be multiple occurrences of the same entity mentioned in the sentence.,0
"Therefore, we use the span's position in the sentence.",0
Create masked sentence,0
Add gold relation annotation as sentence label,0
"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,",0
"during prediction, the forward pass does not need any knowledge about the entities in the sentence.",0
"If we sample missing splits, the encoded sentences that correspond to the same original sentences",0
"may get distributed into different splits. For training purposes, this is always undesired.",0
Ensure that all sentences are encoded properly,0
Deal with the case where all sentences are encoded sentences,0
"mypy does not infer the type of ""sentences"" restricted by the if statement",0
Deal with the case where all sentences are standard (non-encoded) sentences,0
"For each encoded sentence, transfer its prediction onto the original relation",0
auto-spawn on GPU if available,0
pad strings with whitespaces to longest sentence,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
"""document_delimiter"" property may be missing in some older pre-trained models",0
serialize the language models and the constructor arguments (but nothing else),0
special handling for deserializing language models,0
re-initialize language model with constructor arguments,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
Transform input data into TARS format,0
"if there are no labels, return a random sample as negatives",0
"otherwise, go through all labels",0
make sure the probabilities always sum up to 1,0
get and embed all labels by making a Sentence object that contains only the label text,0
get each label embedding and scale between 0 and 1,0
compute similarity matrix,0
"the higher the similarity, the greater the chance that a label is",0
sampled as negative example,0
make label dictionary if no Dictionary object is passed,0
prepare dictionary of tags (without B- I- prefixes and without UNK),0
check if candidate_label_set is empty,0
make list if only one candidate label is passed,0
create label dictionary,0
note current task,0
create a temporary task,0
make zero shot predictions,0
switch to the pre-existing task,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
make a tars sentence where all labels are O by default,0
init new TARS classifier,0
set all task information,0
progress bar for verbosity,0
stop if all sentences are empty,0
always remove tags first,0
go through each sentence in the batch,0
always remove tags first,0
get the span and its label,0
determine whether tokens in this span already have a label,0
only add if all tokens have no label,0
make and add a corresponding predicted span,0
set indices so that no token can be tagged twice,0
clearing token embeddings to save memory,0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
get the serialized embeddings,0
remap state dict for models serialized with Flair <= 0.11.3,0
init new TARS classifier,0
set all task information,0
with torch.no_grad():,0
progress bar for verbosity,0
stop if all sentences are empty,0
always remove tags first,0
go through each sentence in the batch,0
always remove tags first,0
add all labels that according to TARS match the text and are above threshold,0
do not add labels below confidence threshold,0
only use label with the highest confidence if enforcing single-label predictions,0
add the label with the highest score even if below the threshold if force label is activated.,0
remove previously added labels and only add the best label,0
clearing token embeddings to save memory,0
set separator to concatenate three sentences,0
auto-spawn on GPU if available,0
set separator to concatenate two sentences,0
auto-spawn on GPU if available,0
"If the concatenated version of the text pair does not exist yet, create it",0
pooling operation to get embeddings for entites,0
set embeddings,0
set relation and entity label types,0
"whether to use gold entity pairs, and whether to filter entity pairs by type",0
filter entity pairs according to their tags if set,0
whether to encode characters and whether to use attention (attention can only be used if chars are encoded),0
character dictionary for decoding and encoding,0
make sure <unk> is in dictionary for handling of unknown characters,0
add special symbols to dictionary if necessary and save respective indices,0
---- ENCODER ----,0
encoder character embeddings,0
encoder pre-trained embeddings,0
encoder RNN,0
additional encoder linear layer if bidirectional encoding,0
---- DECODER ----,0
decoder: linear layers to transform vectors to and from alphabet_size,0
when using attention we concatenate attention outcome and decoder hidden states,0
decoder RNN,0
loss and softmax,0
self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction,0
add additional columns for special symbols if necessary,0
initialize with dummy symbols,0
encode inputs,0
get labels (we assume each token has a lemma label),0
get char indices for labels of sentence,0
"(batch_size, max_sequence_length) batch_size = #words in sentence,",0
max_sequence_length = length of longest label of sentence + 1,0
get char embeddings,0
"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size",0
take decoder input and initial hidden and pass through RNN,0
"if all encoder outputs are provided, use attention",0
take convex combinations of encoder hidden states as new output using the computed attention coefficients,0
"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)",0
get all tokens,0
encode input characters by sending them through RNN,0
get one-hots for characters and add special symbols / padding,0
determine length of each token,0
embed sentences,0
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)",0
variable to store initial hidden states for decoder,0
encode input characters by sending them through RNN,0
test packing and padding,0
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder,0
concatenate the final hidden states of the encoder. These will be projected to hidden_size of,0
decoder later with self.emb_to_hidden,0
mask out vectors that correspond to a dummy symbol (TODO: check attention masking),1
use token embedding as initial hidden state for decoder,0
concatenate everything together and project to appropriate size for decoder,0
variable to store initial hidden states for decoder,0
encode input characters by sending them through RNN,0
note that we do not need to fill up with dummy symbols since we process each token seperately,0
embed character one-hots,0
send through encoder RNN (produces initial hidden for decoder),0
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder,0
project 2*hidden_size to hidden_size,0
concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder,0
later with self.emb_to_hidden,0
use token embedding as initial hidden state for decoder,0
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)",0
concatenate everything together and project to appropriate size for decoder,0
"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)",0
"create target vector (batch_size, max_label_seq_length + 1)",0
filter empty sentences,0
max length of the predicted sequences,0
for printing,0
stop if all sentences are empty,0
remove previously predicted labels of this type,0
create list of tokens in batch,0
encode inputs,0
"create input for first pass (batch_size, 1, input_size), first letter is special character <S>",0
sequence length is always set to one in prediction,0
option 1: greedy decoding,0
predictions,0
decode next character,0
pick top beam size many outputs with highest probabilities,0
option 2: beam search,0
out_probs = self.softmax(output_vectors).squeeze(1),0
make sure no dummy symbol <> or start symbol <S> is predicted,0
pick top beam size many outputs with highest probabilities,0
"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1",0
"leading_indices and probabilities have size (batch_size, beam_size)",0
keep scores of beam_size many hypothesis for each token in the batch,0
stack all leading indices of all hypothesis and corresponding hidden states in two tensors,0
save sequences so far,0
keep track of how many hypothesis were completed for each token,0
"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)",0
decode with log softmax,0
make sure no dummy symbol <> or start symbol <S> is predicted,0
"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside",0
"if the sequence is already ended, do not record as candidate",0
index of token in in list tokens_in_batch,0
print(token_number),0
hypothesis score,0
TODO: remove token if number of completed hypothesis exceeds given value,1
set score of corresponding entry to -inf so it will not be expanded,0
get leading_indices for next expansion,0
find highest scoring hypothesis among beam_size*beam_size possible ones for each token,0
take beam_size many copies of scores vector and add scores of possible new extensions,0
"size (beam_size*batch_size, beam_size)",0
print(hypothesis_scores),0
"reshape to vector of size (batch_size, beam_size*beam_size),",0
each row contains beam_size*beam_size scores of the new possible hypothesis,0
print(hypothesis_scores_per_token),0
"choose beam_size best for each token - size (batch_size, beam_size)",0
out of indices_per_token we now need to recompute the original indices of the hypothesis in,0
a list of length beam_size*batch_size,0
"where the first three inidices belong to the first token, the next three to the second token,",0
and so on,0
with these indices we can compute the tensors for the next iteration,0
expand sequences with corresponding index,0
add log-probabilities to the scores,0
save new leading indices,0
save corresponding hidden states,0
it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations,0
in that case we append one of the final seuqences without end symbol to the final_candidates,0
get best final hypothesis for each token,0
get characters from index sequences and add predicted label to token,0
"Overwrites evaluate of parent class to remove the ""by class"" printout",0
set separator to concatenate two sentences,0
init dropouts,0
auto-spawn on GPU if available,0
make a forward pass to produce embedded data points and labels,0
get their gold labels as a tensor,0
pass data points through network to get encoded data point tensor,0
decode,0
calculate the loss,0
get a tensor of data points,0
do dropout,0
"If the concatenated version of the text pair does not exist yet, create it",0
add DefaultClassifier arguments,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
"if the classifier predicts BIO/BIOES span labels, the internal label dictionary must be computed",0
fields in case this is a span-prediction problem,0
the label type,0
all parameters will be pushed internally to the specified device,0
special handling during training if this is a span prediction problem,0
internal variables,0
non-set tags are OUT tags,0
anything that is not OUT is IN,0
does this prediction start a new span?,0
B- and S- always start new spans,0
"if the predicted class changes, I- starts a new span",0
"if the predicted class changes and S- was previous tag, start a new span",0
if an existing span is ended (either by reaching O or starting a new span),0
reset for-loop variables for new span,0
remember previous tag,0
"if there is a span at end of sentence, add it",0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
print labels in CoNLL format,0
internal candidate lists of generator,0
load Zelda candidates if so passed,0
create candidate lists,0
"if lower casing is enabled, create candidate lists of lower cased versions",0
create a new dictionary for lower cased mentions,0
go through each mention and its candidates,0
"check if backoff mention already seen. If so, add candidates. Else, create new entry.",0
set lowercased version as map,0
"only use span label type if there are predictions, otherwise search for output label type (training labels)",0
remap state dict for models serialized with Flair <= 0.11.3,0
get the candidates,0
"during training, add the gold value as candidate",0
----- Create the internal tag dictionary -----,0
span-labels need special encoding (BIO or BIOES),0
the big question is whether the label dictionary should contain an UNK or not,0
"without UNK, we cannot evaluate on data that contains labels not seen in test",0
"with UNK, the model learns less well if there are no UNK examples",0
is this a span prediction problem?,0
----- Embeddings -----,0
----- Initial loss weights parameters -----,0
----- RNN specific parameters -----,0
----- Conditional Random Field parameters -----,0
"Previously trained models have been trained without an explicit CRF, thus it is required to check",0
whether we are loading a model from state dict in order to skip or add START and STOP token,0
----- Dropout parameters -----,0
dropouts,0
remove word dropout if there is no contact over the sequence dimension.,0
----- Model layers -----,0
----- RNN layer -----,0
"If shared RNN provided, else create one for model",0
Whether to train initial hidden state,0
final linear map to tag space,0
"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss",0
"if using CRF, we also require a CRF and a Viterbi decoder",0
"if there are no sentences, there is no loss",0
forward pass to get scores,0
calculate loss given scores and labels,0
make a zero-padded tensor for the whole sentence,0
linear map to tag space,0
"Depending on whether we are using CRF or a linear layer, scores is either:",0
"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF",0
"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer",0
spans need to be encoded as token-level predictions,0
all others are regular labels for each token,0
make sure it's a list,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
get features from forward propagation,0
remove previously predicted labels of this type,0
"if return_loss, get loss value",0
make predictions,0
add predictions to Sentence,0
BIOES-labels need to be converted to spans,0
"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)",0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
core Flair models on Huggingface ModelHub,0
"Large NER models,",0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Language-specific NER models,0
Language-specific POS models,0
Historic German,0
English NER models,0
English SRL models,0
Danish models,0
German models,0
Arabic models,0
French models,0
Dutch models,0
Malayalam models,0
Portuguese models,0
Biomedical models,0
check if model name is a valid local file,0
"check if model key is remapped to HF key - if so, print out information",0
get mapped name,0
"if not, check if model key is remapped to direct download location. If so, download model",0
"for all other cases (not local file or special download location), use HF model hub",0
## Demo: How to use in Flair,0
load tagger,0
make example sentence,0
predict NER tags,0
print sentence,0
print predicted NER spans,0
iterate over entities and print,0
Lazy import,0
Save model weight,0
Determine if model card already exists,0
Generate and save model card,0
Upload files,0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
print labels in CoNLL format,0
Dense + sparse retrieval,0
fetched from original repo to avoid download,0
"just in case we add: fuzzy search, Levenstein, ...",0
"for now we always fall back to SapBERT,",0
but we should train our own models at some point,0
NOTE: Avoid emtpy string if mentions are just punctutations (e.g. `-` or `(`),0
NOTE: Avoid emtpy string if mentions are just punctuations (e.g. `-` or `(`),0
Ab3P works on sentence-level and not on a single entity mention / name,0
- so we just apply the wrapped text pre-processing here (if configured),0
NOTE: ensure correct similarity metric for pretrained model,0
empty cuda cache if device is a cuda device,0
"Sanity conversion: if flair.device was set as a string, convert to torch.device",0
NOTE: This is a hacky workaround for the fact that,1
the `label_type`s in `Classifier.load('hunflair)` are,0
"'diseases', 'genes', 'species', 'chemical' instead of 'ner'.",0
We warn users once they need to update SequenceTagger model,0
See: https://github.com/flairNLP/flair/pull/3387,0
make sure sentences is a list of sentences,0
Make sure entity label types are represented as dict,0
Collect all entities based on entity type labels configuration,0
Preprocess entity mentions,0
Retrieve top-k concept / entity candidates,0
Add a label annotation for each candidate,0
load model by entity_type,0
check if we have a hybrid pre-trained model,0
the multi task model has several labels,0
biomedical models,0
entity linker,0
auto-spawn on GPU if available,0
remap state dict for models serialized with Flair <= 0.11.3,0
English sentiment models,0
Communicative Functions Model,0
"If we sample missing splits, the encoded sentences that correspond to the same original sentences",0
"may get distributed into different splits. For training purposes, this is always undesired.",0
Prepend the task description prompt to the sentence text,0
Make sure it's a list,0
Reconstruct all annotations from the original sentence (necessary for learning classifiers),0
If all sentences are not augmented -> augment them,0
"mypy does not infer the type of ""sentences"" restricted by the if statement",0
"mypy does not infer the type of ""sentences"" restricted by code above",0
Compute prediction label type,0
make sure it's a list,0
"If all sentences are already augmented (i.e. compatible with this class), just forward the sentences",0
"mypy does not infer the type of ""sentences"" restricted by the if statement",0
Remove existing labels,0
Augment sentences - copy all annotation of the given tag type,0
Predict on augmented sentence and store it in an internal annotation layer / label,0
Append predicted labels to the original sentences,0
check if model name is a valid local file,0
check if model name is a pre-configured hf model,0
"scores_at_targets[range(features.shape[0]), lengths.values -1]",0
Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices,0
"Initially, get scores from <start> tag to all other tags",0
"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp",0
"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep",0
Create a tensor to hold accumulated sequence scores at each current tag,0
Create a tensor to hold back-pointers,0
"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag",0
"Let pads be the <end> tag index, since that was the last tag in the decoded sequence",0
"We add scores at current timestep to scores accumulated up to previous timestep, and",0
choose the previous timestep that corresponds to the max. accumulated score for each current timestep,0
"If sentence is over, add transition to STOP-tag",0
Decode/trace best path backwards,0
Sanity check,0
remove start-tag and backscore to stop-tag,0
Max + Softmax to get confidence score for predicted label and append label to each token,0
"Transitions are used in the following way: transitions[to, from].",0
"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag",0
to START-tag and from STOP-tag to any other tag to -10000.,0
"if necessary, make batch_steps",0
break up the batch into slices of size,0
mini_batch_chunk_size,0
"if training also uses dev/train data, include in training set",0
evaluation and monitoring,0
sampling and shuffling,0
evaluation and monitoring,0
when and what to save,0
logging parameters,0
plugins,0
activate annealing plugin,0
call self.train_custom with all parameters (minus the ones specific to the AnnealingPlugin),0
training parameters,0
evaluation and monitoring,0
sampling and shuffling,0
evaluation and monitoring,0
when and what to save,0
logging parameters,0
amp,0
plugins,0
annealing logic,0
training parameters,0
evaluation and monitoring,0
sampling and shuffling,0
evaluation and monitoring,0
when and what to save,0
logging parameters,0
amp,0
plugins,0
training parameters,0
evaluation and monitoring,0
sampling and shuffling,0
evaluation and monitoring,0
when and what to save,0
logging parameters,0
amp,0
plugins,0
Create output folder,0
=== START BLOCK: ACTIVATE PLUGINS === #,0
We first activate all optional plugins. These take care of optional functionality such as various,0
logging techniques and checkpointing,0
log file plugin,0
loss file plugin,0
plugin for writing weights,0
plugin for checkpointing,0
=== END BLOCK: ACTIVATE PLUGINS === #,0
derive parameters the function was called with (or defaults),0
initialize model card with these parameters,0
Prepare training data and get dataset size,0
"determine what splits (train, dev, test) to evaluate",0
determine how to determine best model and whether to save it,0
instantiate the optimizer,0
initialize sampler if provided,0
init with default values if only class is provided,0
set dataset to sample from,0
this field stores the names of all dynamic embeddings in the model (determined after first forward pass),0
Sanity checks,0
"Sanity conversion: if flair.device was set as a string, convert to torch.device",0
-- AmpPlugin -> wraps with AMP,0
-- AnnealingPlugin -> initialize schedulers (requires instantiated optimizer),0
At any point you can hit Ctrl + C to break out of training early.,0
"- SchedulerPlugin -> load state for anneal_with_restarts, batch_growth_annealing, logic for early stopping",0
- LossFilePlugin -> get the current epoch for loss file logging,0
"if shuffle_first_epoch==False, the first epoch is not shuffled",0
log infos on training progress every `log_modulo` batches,0
process mini-batches,0
zero the gradients on the model and optimizer,0
forward and backward for batch,0
forward pass,0
identify dynamic embeddings (always deleted) on first sentence,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
do the optimizer step,0
- SchedulerPlugin -> do the scheduler step if one-cycle or linear decay,0
- WeightExtractorPlugin -> extracts weights,0
- CheckpointPlugin -> executes save_model_each_k_epochs,0
- SchedulerPlugin -> log bad epochs,0
Determine if this is the best model or if we need to anneal,0
log results,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
use DEV split to determine if this is the best model so far,0
"if not using DEV score, determine best model using train loss",0
- LossFilePlugin -> somehow prints all relevant metrics,0
- AnnealPlugin -> scheduler step,0
- SWAPlugin -> restores SGD weights from SWA,0
"if we do not use dev data for model selection, save final model",0
TensorboardLogger -> closes writer,0
test best model if test data is present,0
get and return the final test score of best model,0
MetricHistoryPlugin -> stores the loss history in return_values,0
"Store return values, as they will be erased by reset_training_attributes",0
get a random sample of training sentences,0
create a model card for this model with Flair and PyTorch version,0
record Transformers version if library is loaded,0
remember all parameters used in train() call,0
"TextDataset returns a list. valid and test are only one file,",0
so return the first element,0
cast string to Path,0
error message if the validation dataset is too small,0
Shuffle training files randomly after serially iterating,0
through corpus one,0
"iterate through training data, starting at",0
self.split (for checkpointing),0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
Backward,0
`clip_grad_norm` helps prevent the exploding gradient,0
problem in RNNs / LSTMs.,0
We detach the hidden state from how it was,0
previously produced.,0
"If we didn't, the model would try backpropagating",0
all the way to start of the dataset.,0
explicitly remove loss to clear up memory,0
#########################################################,0
Save the model if the validation loss is the best we've,0
seen so far.,0
#########################################################,0
print info,0
#########################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
"no need to check for MetricName, as __add__ of other would be called in this case",0
"This flag tracks, whether an event is currently being processed (otherwise it is added to the queue)",0
instantiate plugin,0
"Reset the flag, since an exception event might be dispatched",0
"If there is no **kw argument in the callback, check if any of the passed kw args is not accepted by",0
the callback,0
go through all attributes,0
get attribute hook events (may raise an AttributeError),0
register function as a hook,0
"Decorator was used with parentheses, but no args",0
Decorator was used with args (strings specifiying the events),0
Decorator was used without args,0
path to store the model,0
special annealing modes,0
determine the min learning rate,0
"minimize training loss if training with dev data, else maximize dev score",0
instantiate the scheduler,0
stop training if learning rate becomes too small,0
reload last best model if annealing with restarts is enabled,0
calculate warmup steps,0
skip if no optimization has happened.,0
saves the model with full vocab as checkpoints etc were created with reduced vocab.,0
TODO: check if metric is in tracked metrics,1
prepare loss logging file and set up header,0
set up all metrics to collect,0
set up headers,0
name: HEADER,0
Add all potentially relevant metrics. If a metric is not published,0
"after the first epoch (when the header is written), the column is",0
removed at that point.,0
initialize the first log line,0
record is a list of scalars,0
output log file,0
remove columns where no value was found on the first epoch (could be != 1 if training was resumed),0
make headers on epoch 1,0
write header,0
adjust alert level,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn,0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
"legacy pickle-like saving for image embeddings, as implementation details are not obvious",0
"legacy pickle-like loading for image embeddings, as implementation details are not obvious",0
"<cls> token initially set to 1/D, so it attends to all image features equally",0
add positional encodings,0
reshape the pixels into the sequence,0
layer norm after convolution and positional encodings,0
add <cls> token,0
"transformer requires input in the shape [h*w+1, b, d]",0
the output is an embedding of <cls> token,0
this parameter is fixed,0
optional fine-tuning on top of embedding layer,0
"if only one sentence is passed, convert to list of sentence",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push through RNN,0
after-RNN dropout,0
extract embeddings from RNN,0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
serialize the language models and the constructor arguments (but nothing else),0
re-initialize language model with constructor arguments,0
special handling for deserializing language models,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
IMPORTANT: add embeddings as torch modules,0
iterate over sentences,0
"if its a forward LM, take last state",0
"convert to plain strings, embedded in a list for the encode function",0
CNN,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push CNN,0
after-CNN dropout,0
extract embeddings from CNN,0
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency",0
"if only one sentence is passed, convert to list of sentence",0
Expose base classses,0
Expose document embedding classes,0
Expose image embedding classes,0
Expose legacy embedding classes,0
Expose token embedding classes,0
in some cases we need to insert zero vectors for tokens without embedding.,0
padding,0
remove special markup,0
check if special tokens exist to circumvent error message,0
iterate over subtokens and reconstruct tokens,0
remove special markup,0
check if reconstructed token is special begin token ([CLS] or similar),0
some BERT tokenizers somehow omit words - in such cases skip to next token,0
"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token.",0
if tokens are unaccounted for,0
check if all tokens were matched to subtokens,0
The layoutlm tokenizer doesn't handle ocr themselves,0
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial",0
"cannot run `.encode` if ocr boxes are required, assume",0
"transformers returns the ""added_tokens.json"" even if it doesn't create it",0
"transformers returns the ""added_tokens.json"" even if it doesn't create it",0
in case of doubt: token embedding has higher priority than document embedding,0
random check some tokens to save performance.,0
Models such as FNet do not have an attention_mask,0
set language IDs for XLM-style transformers,0
"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have",0
"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids.",0
set context if not set already,0
flair specific pre-tokenization,0
fields to store left and right context,0
expand context only if context_length is set,0
"if context_dropout is set, randomly deactivate left context during training",0
"if context_dropout is set, randomly deactivate right context during training",0
"if use_context_separator is set, add a [FLERT] token",0
return expanded sentence and context length information,0
"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first.",0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
do not print transformer warnings as these are confusing in this case,0
load tokenizer and transformer model,0
load tokenizer from inmemory zip-file,0
if model is quantized by BitsAndBytes this will fail,0
add adapters for finetuning,0
peft_config: PeftConfig,0
model name,0
embedding parameters,0
send mini-token through to check how many layers the model has,0
return length,0
"If we use a context separator, add a new special token",0
"most models have an initial BOS token, except for XLNet, T5 and GPT2",0
"when initializing, embeddings are in eval mode by default",0
in case of doubt: token embedding has higher priority than document embedding,0
in case of doubt: token embedding has higher priority than document embedding,0
legacy TransformerDocumentEmbedding,0
legacy TransformerTokenEmbedding,0
legacy Flair <= 0.12,0
legacy Flair <= 0.7,0
legacy TransformerTokenEmbedding,0
Legacy TransformerDocumentEmbedding,0
legacy TransformerTokenEmbedding,0
legacy TransformerDocumentEmbedding,0
some models like the tars model somehow lost this information.,0
copy values from new embedding,0
those parameters are only from the super class and will be recreated in the constructor.,0
cls first pooling can be done without recreating sentence hidden states,0
make the tuple a tensor; makes working with it easier.,0
"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention",0
only use layers that will be outputted,0
this parameter is fixed,0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
make compatible with serialized models,0
gensim version 4,0
gensim version 3,0
"if no embedding is set, the vocab and embedding length is required",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNINOS embeddings,0
pubmed embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
"this is required to force the module on the cpu,",0
"if a parent module is put to gpu, the _apply is called to each sub_module",0
self.to(..) actually sets the device properly,0
this ignores the get_cached_vec method when loading older versions,0
it is needed for compatibility reasons,0
gensim version 4,0
gensim version 3,0
"when loading the old versions from pickle, the embeddings might not be added as pytorch module.",0
"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might",0
lead to issues while loading (trying to load weights that weren't stored as python weights and therefore,0
not finding them),0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
multilingual models,0
English models,0
Arabic,0
Bulgarian,0
Czech,0
Danish,0
German,0
Spanish,0
Basque,0
Persian,0
Finnish,0
French,0
Hebrew,0
Hindi,0
Croatian,0
Indonesian,0
Italian,0
Japanese,0
Malayalam,0
Dutch,0
Norwegian,0
Polish,0
Portuguese,0
Pubmed,0
Slovenian,0
Swedish,0
Tamil,0
Spanish clinical,0
CLEF HIPE Shared task,0
Amharic,0
Ukrainian,0
load model if in pretrained model map,0
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir),0
CLEF HIPE models are lowercased,0
embeddings are static if we don't do finetuning,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout",0
gradients are enable if fine-tuning is enabled,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
offset mode that extracts at whitespace after last character,0
offset mode that extracts at last character,0
make compatible with old models,0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
check token.text is empty or not,0
set aggregation operation,0
add embeddings after updating,0
model architecture,0
model architecture,0
"""pl"",",0
download if necessary,0
load the model,0
"this is required to force the module on the cpu,",0
"if a parent module is put to gpu, the _apply is called to each sub_module",0
self.to(..) actually sets the device properly,0
"when loading the old versions from pickle, the embeddings might not be added as pytorch module.",0
"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might",0
lead to issues while loading (trying to load weights that weren't stored as python weights and therefore,0
not finding them),0
old embeddings do not have a torch-embedding and therefore do not store the weights in the saved torch state_dict,0
"however they are already initialized rightfully, so we just set the state dict from our current state dict",0
GLOVE embeddings,0
no need to recreate as NILCEmbeddings,0
read in test file if exists,0
read in dev file if exists,0
"find train, dev and test files if not specified",0
Add tags for each annotated span,0
Remove leading and trailing whitespaces from annotated spans,0
Search start and end token index for current span,0
If end index is not found set to last token,0
Throw error if indices are not valid,0
Add metadatas for sentence,0
Currently all Jsonl Datasets are stored in Memory,0
get train data,0
read in test file if exists,0
read in dev file if exists,0
"find train, dev and test files if not specified",0
special key for space after,0
special key for feature columns,0
special key for dependency head id,0
"store either Sentence objects in memory, or only file offsets",0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
determine encoding of text file,0
identify which columns are spans and which are word-level,0
now load all sentences,0
skip first line if to selected,0
option 1: keep Sentence objects in memory,0
pointer to previous,0
parse next sentence,0
quit if last sentence reached,0
skip banned sentences,0
set previous and next sentence for context,0
append parsed sentence to list in memory,0
option 2: keep source data in memory,0
"read lines for next sentence, but don't parse",0
quit if last sentence reached,0
append raw lines for each sentence,0
we make a distinction between word-level tags and span-level tags,0
read first sentence to determine which columns are span-labels,0
skip first line if to selected,0
check the first 5 sentences,0
go through all annotations and identify word- and span-level annotations,0
- if a column has at least one BIES we know it's a Span label,0
"- if a column has at least one tag that is not BIOES, we know it's a Token label",0
- problem cases are columns for which we see only O - in this case we default to Span,0
skip assigned columns,0
the space after key is always word-levels,0
"if at least one token has a BIES, we know it's a span label",0
"if at least one token has a label other than BIOES, we know it's a token label",0
all remaining columns that are not word-level are span-level,0
for column in self.word_level_tag_columns:,0
"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")",0
"if sentence ends, break",0
parse comments if possible,0
"otherwise, this line is a token. parse and add to sentence",0
check if this sentence is a document boundary,0
add span labels,0
discard tags from tokens that are not added to the sentence,0
parse relations if they are set,0
head and tail span indices are 1-indexed and end index is inclusive,0
parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas',0
"to set the metadata ""domain"" to ""de-orcas""",0
get fields from line,0
get head_id if exists (only in dependency parses),0
initialize token,0
go through all columns,0
'feats' and 'misc' column should be split into different fields,0
special handling for whitespace after,0
add each other feature as label-value pair,0
get the task name (e.g. 'ner'),0
get the label value,0
add label,0
remap regular tag names,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
set sentence context using partials TODO: pointer to dataset is really inefficient,1
use all domains,0
iter over all domains / sources and create target files,0
The conll representation of coref spans allows spans to,0
"overlap. If spans end or begin at the same word, they are",0
"separated by a ""|"".",0
The span begins at this word.,0
The span begins and ends at this word (single word span).,0
"The span is starting, so we record the index of the word.",0
"The span for this id is ending, but didn't start at this word.",0
Retrieve the start index from the document state and,0
add the span to the clusters for this id.,0
strip all bracketing information to,0
get the actual propbank label.,0
Entering into a span for a particular semantic role label.,0
We append the label and set the current span for this annotation.,0
"If there's no '(' token, but the current_span_label is not None,",0
then we are inside a span.,0
We're outside a span.,0
"Exiting a span, so we reset the current span label for this annotation.",0
The words in the sentence.,0
The pos tags of the words in the sentence.,0
the pieces of the parse tree.,0
The lemmatised form of the words in the sentence which,0
have SRL or word sense information.,0
The FrameNet ID of the predicate.,0
"The sense of the word, if available.",0
"The current speaker, if available.",0
"Cluster id -> List of (start_index, end_index) spans.",0
Cluster id -> List of start_indices which are open for this id.,0
Replace brackets in text and pos tags,0
with a different token for parse trees.,0
only keep ')' if there are nested brackets with nothing in them.,0
There are some bad annotations in the CONLL data.,0
"They contain no information, so to make this explicit,",0
we just set the parse piece to be None which will result,0
in the overall parse tree being None.,0
"If this is the first word in the sentence, create",0
empty lists to collect the NER and SRL BIO labels.,0
"We can't do this upfront, because we don't know how many",0
"components we are collecting, as a sentence can have",0
variable numbers of SRL frames.,0
Create variables representing the current label for each label,0
sequence we are collecting.,0
"If any annotation marks this word as a verb predicate,",0
we need to record its index. This also has the side effect,0
of ordering the verbal predicates by their location in the,0
"sentence, automatically aligning them with the annotations.",0
"this would not be reached if parse_pieces contained None, hence the cast",0
Non-empty line. Collect the annotation.,0
Collect any stragglers or files which might not,0
have the '#end document' format for the end of the file.,0
this dataset name,0
check if data there,0
column format,0
this dataset name,0
check if data there,0
column format,0
this dataset name,0
download data if necessary,0
download files if not present locally,0
we need to slightly modify the original files by adding some new lines after document separators,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
Remove CoNLL-U meta information in the last column,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
entity_mapping,0
this dataset name,0
download data if necessary,0
data validation,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download files if not present locallys,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
# download zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
download data if necessary,0
"unzip the downloaded repo and merge the train, dev and test datasets",0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
check if data there,0
create folder,0
download dataset,0
column format,0
this dataset name,0
download and parse data if necessary,0
create train test dev if not exist,0
column format,0
this dataset name,0
If the extracted corpus file is not yet present in dir,0
download zip if necessary,0
"extracted corpus is not present , so unpacking it.",0
column format,0
this dataset name,0
download zip,0
unpacking the zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)",0
download files if not present locally,0
we need to modify the original files by adding new lines after after the end of each sentence,0
if only one language is given,0
column format,0
this dataset name,0
"use all languages if explicitly set to ""all""",0
download data if necessary,0
initialize comlumncorpus and add it to list,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
this list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
get google drive id from list,0
download from google drive,0
unzip,0
"tar.extractall(language_folder,members=[tar.getmember(file_name)])",0
transform data into required format,0
"the processed dataset has the additional ending ""_new""",0
remove the unprocessed dataset,0
initialize comlumncorpus and add it to list,0
if no languages are given as argument all languages used in XTREME will be loaded,0
if only one language is given,0
column format,0
this dataset name,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
This list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
download from HU Server,0
unzip,0
transform data into required format,0
initialize comlumncorpus and add it to list,0
if only one language is given,0
column format,0
this dataset name,0
download data if necessary,0
initialize comlumncorpus and add it to list,0
download data if necessary,0
unpack and write out in CoNLL column-like format,0
column format,0
this dataset name,0
download data if necessary,0
data is not in IOB2 format. Thus we transform it to IOB2,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
column format,0
this dataset name,0
rename according to train - test - dev - convention,0
column format,0
this dataset name,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
Add missing newline after header,0
Workaround for empty tokens,1
"Add ""real"" document marker",0
Dataset split mapping,0
v2.0 only adds new language and splits for AJMC dataset,0
Special document marker for sample splits in AJMC dataset,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
if only one language is given,0
column format,0
this dataset name,0
"use all languages if explicitly set to ""all""",0
download data if necessary,0
initialize comlumncorpus and add it to list,0
this dataset name,0
one name can map to multiple concepts,0
NOTE: EntityLinkingDictionary are lazy-loaded from a preprocessed file.,0
Use this class to load into memory all candidates,0
"if identifier == ""MESH:D013749"":",0
# This MeSH ID was used by MeSH when this chemical was part of the MeSH controlled vocabulary.,0
continue,0
parse line,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
paths to train and test splits,0
init corpus,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
iterate over all html files,0
"get rid of html syntax, we only need the text",0
between all documents we write a separator symbol,0
skip empty strings,0
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)",0
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention",0
sentence splitting and tokenization,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out-file in column format,0
"in case something goes wrong, delete the dataset and raise error",0
this dataset name,0
download and parse data if necessary,0
from qwikidata.linked_data_interface import get_entity_dict_from_api,0
generate qid wikiname dictionaries,0
merge dictionaries,0
ignore first line,0
commented and empty lines,0
read all Q-IDs,0
ignore first line,0
request,0
this dataset name,0
we use the wikiids in the data instead of directly utilizing the wikipedia urls.,0
like this we can quickly check if the corresponding page exists,0
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi,0
delete unprocessed file,0
collect all wikiids,0
create the dictionary,0
request,0
this dataset name,0
names of raw text documents,0
open output_file,0
iterate through all documents,0
split sentences and tokenize,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out file,0
annotation from one annotator or two agreeing annotators,0
this dataset name,0
download and parse data if necessary,0
this dataset name,0
download and parse data if necessary,0
First parse the post titles,0
Keep track of how many and which entity mentions does a given post title have,0
Check if the current post title has an entity link and parse accordingly,0
Post titles with entity mentions (if any) are handled via this function,0
Then parse the comments,0
"Iterate over the comments.tsv file, until the end is reached",0
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched.",0
Each comment thread is handled as one 'document'.,0
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.,0
This if-condition is needed to handle this problem.,0
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure",0
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above",0
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle.",0
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,",0
and not just single letters into single rows.,0
If there are annotated entity mentions for given post title or a comment thread,0
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence",0
Write the token with a corresponding tag to file,0
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed",0
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag",0
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized,0
"incorrectly, in order to keep the desired format (empty line as a sentence separator).",0
"Thrown when the second check above happens, but the last token of a sentence is reached.",0
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below.",0
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS",0
Check if further annotations belong to the current post title or comment thread as well,0
Stop when the end of an annotation file is reached,0
Check if further annotations belong to the current sentence as well,0
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)",0
Docstart,0
if there is more than one word in the chunk we write each in a separate line,0
print(chunks),0
empty line after each sentence,0
convert the file to CoNLL,0
this dataset name,0
"check if data there, if not, download the data",0
create folder,0
download data,0
transform data into column format if necessary,0
if no filenames are specified we use all the data,0
"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled",0
also we remove 'raganato_ALL' from filenames in case its in the list,0
generate the test file,0
make column file and save to data_folder,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
TODO: Adapt this following: https://github.com/flairNLP/flair/pull/3146,1
+1 assumes the title and abstract will be joined by a space.,0
"We need a unique identifier for this entity, so build it from the document id and entity id",0
The user can provide a callable that returns the database name.,0
some entities are not linked and,0
some entities are linked to multiple normalized ids,0
passages must not overlap and spans must cover the entire document,0
entities,0
parse db ids,0
Some of the entities have a off-by-one error. Correct these annotations!,0
"passage offsets/lengths do not connect, recalculate them for this schema.",0
this dataset name,0
download data if necessary,0
if True:,0
write CoNLL-U Plus header,0
"Some special cases (e.g., missing spaces before entity marker)",0
necessary if text should be whitespace tokenizeable,0
Handle case where tail may occur before the head,0
this dataset name,0
write CoNLL-U Plus header,0
this dataset name,0
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors,1
download data if necessary,0
write CoNLL-U Plus header,0
The span has ended.,0
We are entering a new span; reset indices,0
and active tag to new span.,0
We're inside a span.,0
Last token might have been a part of a valid span.,0
this dataset name,0
write CoNLL-U Plus header,0
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):",0
"with zip_file.open(source_file_path, mode=""r"") as source_file:",0
target_file_path = Path(data_folder) / target_filename,0
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:",0
# write CoNLL-U Plus header,0
"target_file.write(""# global.columns = id form ner\n"")",0
for example in json.load(source_file):,0
token_list = self._tacred_example_to_token_list(example),0
target_file.write(token_list.serialize()),0
check if first tag row is already occupied,0
"if first tag row is occupied, use second tag row",0
hardcoded mapping TODO: perhaps find nicer solution,1
remap regular tag names,0
else skip to position in file where sentence begins,0
set sentence context using partials TODO: pointer to dataset is really inefficient,1
read in dev file if exists,0
read in test file if exists,0
the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44,0
"find train, dev and test files if not specified",0
use test_file to create test split if available,0
use dev_file to create test split if available,0
"if data point contains black-listed label, do not use",0
first check if valid sentence,0
"if so, add to indices",0
"find train, dev and test files if not specified",0
variables,0
different handling of in_memory data than streaming data,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
test if format is OK,0
test if at least one label given,0
make sentence from text (and filter for length),0
"if a pair column is defined, make a sentence pair object",0
noinspection PyDefaultArgument,0
dataset name includes the split size,0
default dataset folder is the cache root,0
download data if necessary,0
download each of the 28 splits,0
create dataset directory if necessary,0
download senteval datasets if necessary und unzip,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
download data from same source as in huggingface's implementations,0
read label order,0
"Original labels are [1, 2, 3, 4] -> ['World', 'Sports', 'Business', 'Sci/Tech']",0
"Re-map to [0, 1, 2, 3].",0
this dataset name,0
download data if necessary,0
handle labels file,0
handle data file,0
Create flair compatible labels,0
"by default, map point score to POSITIVE / NEGATIVE values",0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file from CSV,0
create test.txt file from CSV,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create train dev and test files in fasttext format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
convert to FastText format,0
download data if necessary,0
"if data is not downloaded yet, download it",0
get the zip file,0
move original .tsv files to another folder,0
create train and dev splits in fasttext format,0
create eval_dataset file with no labels,0
download zip archive,0
unpack file in datasets directory (zip archive contains a directory named SST-2),0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download datasets if necessary,0
create dataset directory if necessary,0
create correctly formated txt files,0
multiple labels are possible,0
this dataset name,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
create a separate directory for different tasks,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
check if dataset is supported,0
set file names,0
set file names,0
download and unzip in file structure if necessary,0
instantiate corpus,0
"find train, dev and test files if not specified",0
"create DataPairDataset for train, test and dev file, if they are given",0
stop if file does not exist,0
create a DataPair object from strings,0
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings",0
"find train, dev, and test files if not specified",0
"create DataTripleDataset for train, test, and dev files, if they are given",0
stop if the file does not exist,0
create a DataTriple object from strings,0
"if in_memory is True we return a DataTriple, otherwise we create one from the lists of strings",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"reorder dev datasets to have same columns as in train set: 8, 9, and 11",0
dev sets include 5 different annotations but we will only keep the gold label,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get test and dev sets,0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data not downloaded yet, download it",0
get the zip file,0
"the downloaded files have json format, we transform them to tsv",0
Function to transform JSON file to tsv for Recognizing Textual Entailment Data,0
remove json file,0
Uses dynamic programming approach to calculate maximum independent set in interval graph,0
with sum of all entity lengths as secondary key,0
calculate offset without current text,0
because we stick all passages of a document together,0
TODO For split entities we also annotate everything inbetween which might be a bad idea?,1
Try to fix incorrect annotations,0
print(,0
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}""",0
),0
Ignore empty lines or relation annotations,0
FIX annotation of whitespaces (necessary for PDR),0
Add task description for multi-task learning,0
One token may contain multiple entities -> deque all of them,0
column format,0
this dataset name,0
Create tokenization-dependent CONLL files. This is necessary to prevent,0
from caching issues (e.g. loading the same corpus with different sentence splitters),0
column format,0
this dataset name,0
column format,0
this dataset name,0
Edge case: last token starts a new entity,0
Last document in file,0
column format,0
this dataset name,0
column format,0
this dataset name,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
Edge case: last token starts a new entity,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
column format,0
this dataset name,0
Read texts,0
Read annotations,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
We need to apply a patch to correct the original training file,0
Articles title,0
Article abstract,0
Entity annotations,0
column format,0
this dataset name,0
Edge case: last token starts a new entity,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
Incomplete article,0
Invalid XML syntax,0
column format,0
this dataset name,0
column format,0
this dataset name,0
if len(mid) != 3:,0
continue,0
Try to fix entity offsets,0
column format,0
this dataset name,0
There is still one illegal annotation in the file ..,0
column format,0
this dataset name,0
"Abstract first, title second to prevent issues with sentence splitting",0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
column format,0
this dataset name,0
"Filter for specific entity types, by default no entities will be filtered",0
Get original HUNER splits to retrieve a list of all document ids contained in V2,0
train and dev split of V2 will be train in V4,0
test split of V2 will be dev in V4,0
New documents in V4 will become test documents,0
column format,0
this dataset name,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
build dataset name and full huggingface reference name,0
Download data if necessary,0
"Some datasets in BigBio only have train or test splits, not both",0
"If only test split, assign it to train split",0
"If only train split, sample other from it (sample_missing_splits=True)",0
Not every dataset has a dev / validation set!,0
Perform type mapping if necessary,0
return None,0
TODO: Add entity type mapping for all remaining bigbio datasets not in HunFlair?,1
"""simple_chemical"": ""chemical"",  # BioNLP ST 2013 CG",0
"""cancer"": ""disease"",  # BioNLP ST 2013 CG",0
"""gene_or_gene_product"": ""gene"",  # BioNLP ST 2013 CG",0
"""gene"": ""gene"",  # NLM Gene",0
"""chemical"": ""chemical"",  # NLM Chem",0
"""cellline"": ""cell_line"",  # Cell Finder",0
"""species"": ""species"",  # Cell Finder",0
"""protein"": ""gene"",  # BioID",0
"Collect all texts of the document, each passage will be",0
a text in our internal format,0
Sort passages by start offset,0
Transform all entity annotations into internal format,0
Find the passage of the entity (necessary for offset adaption),0
Adapt entity offsets according to passage offsets,0
FIXME: This is just for debugging purposes,1
passage_text = id_to_text[passage_id],0
doc_text = passage_text[entity_offset[0] : entity_offset[1]],0
"mention_text = entity[""text""][0]",0
if doc_text != mention_text:,0
"print(f""Annotation error ({document['document_id']}) - Doc: {doc_text} vs. Mention: {mention_text}"")",0
Get element in the middle,0
Is the mention with the passage offsets?,0
"If element is smaller than mid, then it can only",0
be present in left subarray,0
Else the element can only be present in right subarray,0
TODO whether cell or cell line is the correct tag,1
TODO whether cell or cell line is the correct tag,1
Special case for ProGene: We need to use the split_0_train and split_0_test splits,0
as they are currently provided in BigBio,0
cache Feidegger config file,0
cache Feidegger images,0
replace image URL with local cached file,0
append Sentence-Image data point,0
cast to list if necessary,0
cast to list if necessary,0
"first, check if pymongo is installed",0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
Expose base classses,0
Expose all biomedical data sets used for the evaluation of BioBERT,0
-,0
-,0
-,0
-,0
Expose all biomedical data sets using the HUNER splits,0
Expose all biomedical data sets,0
Expose all document classification datasets,0
word sense disambiguation,0
Expose all entity linking datasets,0
Expose all relation extraction datasets,0
universal proposition banks,0
keyphrase detection datasets,0
other NER datasets,0
standard NER datasets,0
Expose all sequence labeling datasets,0
Expose all text-image datasets,0
Expose all text-text datasets,0
Expose all treebanks,0
"find train, dev and test files if not specified",0
get train data,0
get test data,0
get dev data,0
option 1: read only sentence boundaries as offset positions,0
option 2: keep everything in memory,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
current token ID,0
handling for the awful UD multiword format,0
end of sentence,0
comments or ellipsis,0
if token is a multi-word,0
normal single-word tokens,0
"if we don't split multiwords, skip over component words",0
add token,0
add morphological tags,0
derive whitespace logic for multiwords,0
"if multi-word equals component tokens, there should be no whitespace",0
go through all tokens in subword and set whitespace_after information,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
this dataset name,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"finally, print model card for information",0
noqa: INP001,0
-- Project information -----------------------------------------------------,0
"The full version, including alpha/beta/rc tags",0
use smv_current_version as the git url,0
-- General configuration ---------------------------------------------------,0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
Napoleon settings,0
Whitelist pattern for tags (set to None to ignore all tags),0
Whitelist pattern for branches (set to None to ignore all branches),0
Whitelist pattern for remotes (set to None to use local branches only),0
Pattern for released versions,0
Format for versioned output directories inside the build directory,0
Determines whether remote or local git branches/tags are preferred if their output dirs conflict,0
test corpus,0
create a TARS classifier,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
test corpus,0
create a TARS classifier,0
switch to a new task (TARS can do multiple tasks so you must define one),0
initialize the text classifier trainer,0
start the training,0
"With end symbol, without start symbol, padding in front",0
"Without end symbol, with start symbol, padding in back",0
"Without end symbol, without start symbol, padding in front",0
initialize trainer,0
initialize trainer,0
initialize trainer,0
increment for last token in sentence if not followed by whitespace,0
clean up directory,0
clean up directory,0
example sentence,0
set 4 labels for 2 tokens ('love' is tagged twice),0
check if there are three POS labels with correct text and values,0
check if there are is one SENTIMENT label with correct text and values,0
check if all tokens are correctly labeled,0
remove the pos label from the last word,0
there should be 2 POS labels left,0
now remove all pos tags,0
set 3 labels for 2 spans (HU is tagged twice),0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
"now delete the NER tags of ""Humboldt-Universitt zu Berlin""",0
should be only one NER label left,0
and only one NER span,0
set 3 labels for 2 spans (HU is tagged twice with different tags),0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
"now delete the NER tags of ""Humboldt-Universitt zu Berlin""",0
should be only one NER label left,0
and only one NER span,0
but there is also one orgtype span and label,0
and only one NER span,0
let's add the NER tag back,0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
now remove all NER tags,0
set 3 labels for 2 spans (HU is tagged twice with different tags),0
create two relation label,0
there should be two relation labels,0
there should be one syntactic labels,0
"there should be two relations, one with two and one with one label",0
example sentence,0
add another topic label,0
example sentence,0
has sentiment value,0
has 4 part of speech tags,0
has 1 NER tag,0
should be in total 6 labels,0
example sentence,0
add two NER labels,0
get the four labels,0
check that only two of the respective data points are equal,0
make a sentence and some right context,0
TODO: is this desirable? Or should two sentences with same text be considered same objects?,1
Initializing a Sentence this way assumes that there is a space after each token,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
load column dataset with one entry,0
load column dataset with two entries,0
load column dataset with three entries,0
"get training, test and dev data",0
"get training, test and dev data",0
check if Token labels are correct,0
"get training, test and dev data",0
check if Token labels for frames are correct,0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
"assert [token.get_tag(""head"").value for token in sent1.tokens] == [",0
"""2"",",0
"""0"",",0
"""4"",",0
"""2"",",0
"""2"",",0
"""2"",",0
],0
This test only covers basic universal dependencies datasets.,0
"For example, multi-word tokens or the ""deps"" column sentence annotations are not supported yet.",0
"Here, we use the default token annotation fields.",0
This test covers the complete HIPE 2022 dataset.,0
https://github.com/hipe-eval/HIPE-2022-data,0
"Includes variant with document separator, and all versions of the dataset.",0
"We have manually checked, that these numbers are correct:",0
"+1 offset, because of missing EOS marker at EOD",0
Test data for v2.1 release,0
This test covers the complete ICDAR Europeana corpus:,0
https://github.com/stefan-it/historic-domain-adaptation-icdar,0
"This test covers the complete MasakhaNER dataset, including support for v1 and v2.",0
This test covers the NERMuD dataset. Official stats can be found here:,0
https://github.com/dhfbk/KIND/tree/main/evalita-2023,0
Number of instances per dataset split are taken from https://huggingface.co/datasets/elenanereiss/german-ler,0
This test covers the complete MasakhaPOS dataset.,0
"See MobIE paper (https://aclanthology.org/2021.konvens-1.22/), table 2",0
--- Embeddings that are shared by both models --- #,0
--- Task 1: Sentiment Analysis (5-class) --- #,0
Define corpus and model,0
-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #,0
Define corpus and model,0
-- Define mapping (which tagger should train on which model) -- #,0
-- Create model trainer and train -- #,0
NOTE: Avoid emtpy string if mentions are just punctutations (e.g. `-` or `(`),0
clean up file,0
no need for label_dict,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
Intel ----founded_by---> Gordon Moore,0
Intel ----founded_by---> Robert Noyce,0
"Ground truth is a set of tuples of (<Sentence Text>, <Relation Label Values>)",0
Check sentence masking and relation label annotation on,0
"training, validation and test dataset (in this test the splits are the same)",0
"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google .""",0
"Entity pair permutations of: ""Microsoft was founded by Bill Gates .""",0
"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 .""",0
"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany.""",0
This sentence is only included if we transform the corpus with cross augmentation,0
Ensure this is an example that predicts no classes in multilabel,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
ensure that the prepared tensors is what we expect,0
use a SequenceTagger to save and reload the embedding in the manner it is supposed to work,0
previous and next sentence as context,0
test expansion for sentence without context,0
test expansion for with previous and next as context,0
test expansion if first sentence is document boundary,0
test expansion if we don't use context,0
"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher.",0
dummy model with embeddings,0
save the dummy and load it again,0
check that context_length and use_context_separator is the same for both,0
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
Extract all the contents of zip file in current directory,0
Extract all the contents of zip file in current directory,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
load_big_file is a workaround byhttps://github.com/highway11git,1
to load models on some Mac/Windows setups,0
see https://github.com/zalandoresearch/flair/issues/351,0
first determine the distribution of classes in the dataset,0
weight for each sample,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
increment for last token in sentence if not followed by whitespace,0
this is the default init size of a lmdb database for embeddings,0
get db filename from embedding name,0
"In case initialization of cached version failed, just fallback to the original WordEmbeddings",0
SequenceTagger,0
TextClassifier,0
get db filename from embedding name,0
if embedding database already exists,0
"otherwise, push embedding to database",0
if embedding database already exists,0
open the database in read mode,0
we need to set self.k,0
create and load the database in write mode,0
"no idea why, but we need to close and reopen the environment to avoid",0
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot,0
when opening new transaction !,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
set 'add_unk' if the dictionary was created with a version of Flair older than 0.9,0
set 'add_unk' depending on whether <unk> is a key,0
"if one embedding name, directly return it",0
"if multiple embedding names, concatenate them",0
First we remove any existing labels for this PartOfSentence in self.sentence,0
labels also need to be deleted at Sentence object,0
delete labels at object itself,0
The Token is a special _PartOfSentence in that it may be initialized without a Sentence.,0
"therefore, labels get added only to the Sentence if it exists",0
The Token is a special _PartOfSentence in that it may be initialized without a Sentence.,0
"Therefore, labels get set only to the Sentence if it exists",0
"check if the span already exists. If so, return it",0
else make a new span,0
"check if the relation already exists. If so, return it",0
else make a new relation,0
private field for all known spans,0
the tokenizer used for this sentence,0
some sentences represent a document boundary (but most do not),0
internal variables to denote position inside dataset,0
"if text is passed, instantiate sentence with tokens (words)",0
determine token positions and whitespace_after flag,0
the last token has no whitespace after,0
log a warning if the dataset is empty,0
data with zero-width characters cannot be handled,0
set token idx and sentence,0
append token to sentence,0
register token annotations on sentence,0
move sentence embeddings to device,0
also move token embeddings to device,0
clear token embeddings,0
infer whitespace after field,0
"if sentence has no tokens, return empty string",0
"otherwise, return concatenation of tokens with the correct offsets",0
The sentence's start position is not propagated to its tokens.,0
"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces.",0
No character at the corresponding code point: remove it,0
"if no label if specified, return all labels",0
"if the label type exists in the Sentence, return it",0
return empty list if none of the above,0
labels also need to be deleted at all tokens,0
labels also need to be deleted at all known spans,0
remove spans without labels,0
delete labels at object itself,0
set name,0
abort if no data is provided,0
sample test data from train if none is provided,0
sample dev data from train if none is provided,0
set train dev and test data,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
"first, determine the datapoint type by going through dataset until first label is found",0
count all label types per sentence,0
go through all labels of label_type and count values,0
special handling for Token-level annotations. Add all untagged as 'O' label,0
"if an unk threshold is set, UNK all label values below this threshold",0
sample randomly from a label distribution according to the probabilities defined by the noise transition matrix,0
replace the old label with the new one,0
keep track of the old (clean) label using another label type category,0
keep track of how many labels in total are flipped,0
sample randomly from a label distribution according to the probabilities defined by the desired noise share,0
replace the old label with the new one,0
keep track of the old (clean) label using another label type category,0
keep track of how many labels in total are flipped,0
"add a dummy ""O"" to close final prediction",0
return complex list,0
internal variables,0
non-set tags are OUT tags,0
anything that is not OUT is IN,0
does this prediction start a new span?,0
B- and S- always start new spans,0
"if the predicted class changes, I- starts a new span",0
"if the predicted class changes and S- was previous tag, start a new span",0
if an existing span is ended (either by reaching O or starting a new span),0
determine score and value,0
append to result list,0
reset for-loop variables for new span,0
remember previous tag,0
global variable: cache_root,0
global variable: device,0
"No need for correctness checks, torch is doing it",0
global variable: version,0
global variable: arrow symbol,0
dummy return to fulfill trainer.train() needs,0
print(vec),0
Attach optimizer,0
"convert `metrics` to float, in case it's a zero-dim Tensor",0
if memory mode option 'none' delete everything,0
"if dynamic embedding keys not passed, identify them automatically",0
always delete dynamic embeddings,0
"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)",0
optional metric space decoder if prototypes have different length than embedding,0
create initial prototypes for all classes (all initial prototypes are a vector of all 1s),0
"if set, create initial prototypes from normal distribution",0
"if set, use a radius",0
all parameters will be pushed internally to the specified device,0
decode embeddings into prototype space,0
"if unlabeled distance is set, mask out loss to unlabeled class prototype",0
verbalize BIOES labels,0
"if label is not BIOES, use label itself",0
Always include the name of the Model class for which the state dict holds,0
"write out a ""model card"" if one is set",0
save model,0
"if this class is abstract, go through all inheriting classes and try to fetch and load the model",0
get all non-abstract subclasses,0
"try to fetch the model for each subclass. if fetching is possible, load model and return it",0
"skip any invalid loadings, e.g. not found on huggingface hub",0
"if the model cannot be fetched, load as a file",0
try to get model class from state,0
"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses",0
"if str(model_cls) == ""<class 'flair.models.pairwise_classification_model.TextPairClassifier'>"": continue",0
"skip any invalid loadings, e.g. not found on huggingface hub",0
"if this class is not abstract, fetch the model and load it",0
"make sure <unk> is contained in gold_label_dictionary, if given",0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
loss calculation,0
variables for printing,0
variables for computing scores,0
remove any previously predicted labels,0
predict for batch,0
get the gold labels,0
add to all_predicted_values,0
make printout lines,0
convert true and predicted values to two span-aligned lists,0
delete exluded labels if exclude_labels is given,0
"if after excluding labels, no label is left, ignore the datapoint",0
write all_predicted_values to out_file if set,0
make the evaluation dictionary,0
check if this is a multi-label problem,0
compute numbers by formatting true and predicted such that Scikit-Learn can use them,0
multi-label problems require a multi-hot vector for each true and predicted label,0
single-label problems can do with a single index for each true and predicted label,0
"now, calculate evaluation numbers",0
there is at least one gold label or one prediction (default),0
compute accuracy separately as it is not always in classification_report (e.. when micro avg exists),0
"if there is only one label, then ""micro avg"" = ""macro avg""",0
"The ""micro avg"" appears only in the classification report if no prediction is possible.",0
"Otherwise, it is identical to the ""macro avg"". In this case, we add it to the report.",0
"Create and populate score object for logging with all evaluation values, plus the loss",0
issue error and default all evaluation numbers to 0.,0
check if there is a label mismatch,0
print info,0
set the embeddings,0
initialize the label dictionary,0
initialize the decoder,0
set up multi-label logic,0
init dropouts,0
loss weights and loss function,0
Initialize the weight tensor,0
set up gradient reversal if so specified,0
embed sentences,0
get a tensor of data points,0
do dropout,0
make a forward pass to produce embedded data points and labels,0
get the data points for which to predict labels,0
get their gold labels as a tensor,0
pass data points through network to get encoded data point tensor,0
decode,0
an optional masking step (no masking in most cases),0
calculate the loss,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
filter data points in batch,0
stop if all sentences are empty,0
pass data points through network and decode,0
if anything could possibly be predicted,0
remove previously predicted labels of this type,0
filter data points that have labels outside of dictionary,0
add DefaultClassifier arguments,0
add variables of DefaultClassifier,0
Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23,0
Get projected 1st dimension,0
Compute bilinear form,0
Arcosh,0
Project the input data to n+1 dimensions,0
"The first dimension, is recomputed in the distance module",0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for",0
"train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
plot i,0
save plots,0
save plots,0
plt.show(),0
save plot,0
auto-spawn on GPU if available,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
TODO: not saving lines yet,1
TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.,1
- MaskedRelationClassifier ?,0
This depends if this relation classification architecture should replace or offer as an alternative.,0
Set label type and prepare label dictionary,0
Initialize super default classifier,0
Add the special tokens from the encoding strategy,0
"Auto-spawn on GPU, if available",0
Only use entities labelled with the specified labels for each label type,0
Only use entities above the specified threshold,0
Use a dictionary to find gold relation annotations for a given entity pair,0
Yield head and tail entity pairs from the cross product of all entities,0
Remove identity relation entity pairs,0
Remove entity pairs with labels that do not match any,0
of the specified relations in `self.entity_pair_labels`,0
"Obtain gold label, if existing",0
Some sanity checks,0
Pre-compute non-leading head and tail tokens for entity masking,0
We can not use the plaintext of the head/tail span in the sentence as the mask/marker,0
since there may be multiple occurrences of the same entity mentioned in the sentence.,0
"Therefore, we use the span's position in the sentence.",0
Create masked sentence,0
Add gold relation annotation as sentence label,0
"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,",0
"during prediction, the forward pass does not need any knowledge about the entities in the sentence.",0
"If we sample missing splits, the encoded sentences that correspond to the same original sentences",0
"may get distributed into different splits. For training purposes, this is always undesired.",0
Ensure that all sentences are encoded properly,0
Deal with the case where all sentences are encoded sentences,0
"mypy does not infer the type of ""sentences"" restricted by the if statement",0
Deal with the case where all sentences are standard (non-encoded) sentences,0
"For each encoded sentence, transfer its prediction onto the original relation",0
auto-spawn on GPU if available,0
pad strings with whitespaces to longest sentence,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
"""document_delimiter"" property may be missing in some older pre-trained models",0
serialize the language models and the constructor arguments (but nothing else),0
special handling for deserializing language models,0
re-initialize language model with constructor arguments,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
Transform input data into TARS format,0
"if there are no labels, return a random sample as negatives",0
"otherwise, go through all labels",0
make sure the probabilities always sum up to 1,0
get and embed all labels by making a Sentence object that contains only the label text,0
get each label embedding and scale between 0 and 1,0
compute similarity matrix,0
"the higher the similarity, the greater the chance that a label is",0
sampled as negative example,0
make label dictionary if no Dictionary object is passed,0
prepare dictionary of tags (without B- I- prefixes and without UNK),0
check if candidate_label_set is empty,0
make list if only one candidate label is passed,0
create label dictionary,0
note current task,0
create a temporary task,0
make zero shot predictions,0
switch to the pre-existing task,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
make a tars sentence where all labels are O by default,0
init new TARS classifier,0
set all task information,0
progress bar for verbosity,0
stop if all sentences are empty,0
always remove tags first,0
go through each sentence in the batch,0
always remove tags first,0
get the span and its label,0
determine whether tokens in this span already have a label,0
only add if all tokens have no label,0
make and add a corresponding predicted span,0
set indices so that no token can be tagged twice,0
clearing token embeddings to save memory,0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
get the serialized embeddings,0
remap state dict for models serialized with Flair <= 0.11.3,0
init new TARS classifier,0
set all task information,0
with torch.no_grad():,0
progress bar for verbosity,0
stop if all sentences are empty,0
always remove tags first,0
go through each sentence in the batch,0
always remove tags first,0
add all labels that according to TARS match the text and are above threshold,0
do not add labels below confidence threshold,0
only use label with the highest confidence if enforcing single-label predictions,0
add the label with the highest score even if below the threshold if force label is activated.,0
remove previously added labels and only add the best label,0
clearing token embeddings to save memory,0
set separator to concatenate two sentences,0
auto-spawn on GPU if available,0
pooling operation to get embeddings for entites,0
set embeddings,0
set relation and entity label types,0
"whether to use gold entity pairs, and whether to filter entity pairs by type",0
filter entity pairs according to their tags if set,0
whether to encode characters and whether to use attention (attention can only be used if chars are encoded),0
character dictionary for decoding and encoding,0
make sure <unk> is in dictionary for handling of unknown characters,0
add special symbols to dictionary if necessary and save respective indices,0
---- ENCODER ----,0
encoder character embeddings,0
encoder pre-trained embeddings,0
encoder RNN,0
additional encoder linear layer if bidirectional encoding,0
---- DECODER ----,0
decoder: linear layers to transform vectors to and from alphabet_size,0
when using attention we concatenate attention outcome and decoder hidden states,0
decoder RNN,0
loss and softmax,0
self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction,0
add additional columns for special symbols if necessary,0
initialize with dummy symbols,0
encode inputs,0
get labels (we assume each token has a lemma label),0
get char indices for labels of sentence,0
"(batch_size, max_sequence_length) batch_size = #words in sentence,",0
max_sequence_length = length of longest label of sentence + 1,0
get char embeddings,0
"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size",0
take decoder input and initial hidden and pass through RNN,0
"if all encoder outputs are provided, use attention",0
take convex combinations of encoder hidden states as new output using the computed attention coefficients,0
"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)",0
get all tokens,0
encode input characters by sending them through RNN,0
get one-hots for characters and add special symbols / padding,0
determine length of each token,0
embed sentences,0
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)",0
variable to store initial hidden states for decoder,0
encode input characters by sending them through RNN,0
test packing and padding,0
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder,0
concatenate the final hidden states of the encoder. These will be projected to hidden_size of,0
decoder later with self.emb_to_hidden,0
mask out vectors that correspond to a dummy symbol (TODO: check attention masking),1
use token embedding as initial hidden state for decoder,0
concatenate everything together and project to appropriate size for decoder,0
variable to store initial hidden states for decoder,0
encode input characters by sending them through RNN,0
note that we do not need to fill up with dummy symbols since we process each token seperately,0
embed character one-hots,0
send through encoder RNN (produces initial hidden for decoder),0
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder,0
project 2*hidden_size to hidden_size,0
concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder,0
later with self.emb_to_hidden,0
use token embedding as initial hidden state for decoder,0
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)",0
concatenate everything together and project to appropriate size for decoder,0
"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)",0
"create target vector (batch_size, max_label_seq_length + 1)",0
filter empty sentences,0
max length of the predicted sequences,0
for printing,0
stop if all sentences are empty,0
remove previously predicted labels of this type,0
create list of tokens in batch,0
encode inputs,0
"create input for first pass (batch_size, 1, input_size), first letter is special character <S>",0
sequence length is always set to one in prediction,0
option 1: greedy decoding,0
predictions,0
decode next character,0
pick top beam size many outputs with highest probabilities,0
option 2: beam search,0
out_probs = self.softmax(output_vectors).squeeze(1),0
make sure no dummy symbol <> or start symbol <S> is predicted,0
pick top beam size many outputs with highest probabilities,0
"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1",0
"leading_indices and probabilities have size (batch_size, beam_size)",0
keep scores of beam_size many hypothesis for each token in the batch,0
stack all leading indices of all hypothesis and corresponding hidden states in two tensors,0
save sequences so far,0
keep track of how many hypothesis were completed for each token,0
"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)",0
decode with log softmax,0
make sure no dummy symbol <> or start symbol <S> is predicted,0
"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside",0
"if the sequence is already ended, do not record as candidate",0
index of token in in list tokens_in_batch,0
print(token_number),0
hypothesis score,0
TODO: remove token if number of completed hypothesis exceeds given value,1
set score of corresponding entry to -inf so it will not be expanded,0
get leading_indices for next expansion,0
find highest scoring hypothesis among beam_size*beam_size possible ones for each token,0
take beam_size many copies of scores vector and add scores of possible new extensions,0
"size (beam_size*batch_size, beam_size)",0
print(hypothesis_scores),0
"reshape to vector of size (batch_size, beam_size*beam_size),",0
each row contains beam_size*beam_size scores of the new possible hypothesis,0
print(hypothesis_scores_per_token),0
"choose beam_size best for each token - size (batch_size, beam_size)",0
out of indices_per_token we now need to recompute the original indices of the hypothesis in,0
a list of length beam_size*batch_size,0
"where the first three inidices belong to the first token, the next three to the second token,",0
and so on,0
with these indices we can compute the tensors for the next iteration,0
expand sequences with corresponding index,0
add log-probabilities to the scores,0
save new leading indices,0
save corresponding hidden states,0
it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations,0
in that case we append one of the final seuqences without end symbol to the final_candidates,0
get best final hypothesis for each token,0
get characters from index sequences and add predicted label to token,0
"Overwrites evaluate of parent class to remove the ""by class"" printout",0
set separator to concatenate two sentences,0
init dropouts,0
auto-spawn on GPU if available,0
make a forward pass to produce embedded data points and labels,0
get their gold labels as a tensor,0
pass data points through network to get encoded data point tensor,0
decode,0
calculate the loss,0
get a tensor of data points,0
do dropout,0
add DefaultClassifier arguments,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
"if the classifier predicts BIO/BIOES span labels, the internal label dictionary must be computed",0
fields in case this is a span-prediction problem,0
the label type,0
all parameters will be pushed internally to the specified device,0
special handling during training if this is a span prediction problem,0
internal variables,0
non-set tags are OUT tags,0
anything that is not OUT is IN,0
does this prediction start a new span?,0
B- and S- always start new spans,0
"if the predicted class changes, I- starts a new span",0
"if the predicted class changes and S- was previous tag, start a new span",0
if an existing span is ended (either by reaching O or starting a new span),0
reset for-loop variables for new span,0
remember previous tag,0
"if there is a span at end of sentence, add it",0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
print labels in CoNLL format,0
internal candidate lists of generator,0
load Zelda candidates if so passed,0
create candidate lists,0
"if lower casing is enabled, create candidate lists of lower cased versions",0
create a new dictionary for lower cased mentions,0
go through each mention and its candidates,0
"check if backoff mention already seen. If so, add candidates. Else, create new entry.",0
set lowercased version as map,0
remap state dict for models serialized with Flair <= 0.11.3,0
get the candidates,0
"during training, add the gold value as candidate",0
----- Create the internal tag dictionary -----,0
span-labels need special encoding (BIO or BIOES),0
the big question is whether the label dictionary should contain an UNK or not,0
"without UNK, we cannot evaluate on data that contains labels not seen in test",0
"with UNK, the model learns less well if there are no UNK examples",0
is this a span prediction problem?,0
----- Embeddings -----,0
----- Initial loss weights parameters -----,0
----- RNN specific parameters -----,0
----- Conditional Random Field parameters -----,0
"Previously trained models have been trained without an explicit CRF, thus it is required to check",0
whether we are loading a model from state dict in order to skip or add START and STOP token,0
----- Dropout parameters -----,0
dropouts,0
remove word dropout if there is no contact over the sequence dimension.,0
----- Model layers -----,0
----- RNN layer -----,0
"If shared RNN provided, else create one for model",0
Whether to train initial hidden state,0
final linear map to tag space,0
"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss",0
"if using CRF, we also require a CRF and a Viterbi decoder",0
"if there are no sentences, there is no loss",0
forward pass to get scores,0
calculate loss given scores and labels,0
make a zero-padded tensor for the whole sentence,0
linear map to tag space,0
"Depending on whether we are using CRF or a linear layer, scores is either:",0
"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF",0
"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer",0
spans need to be encoded as token-level predictions,0
all others are regular labels for each token,0
make sure it's a list,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
get features from forward propagation,0
remove previously predicted labels of this type,0
"if return_loss, get loss value",0
make predictions,0
add predictions to Sentence,0
BIOES-labels need to be converted to spans,0
"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)",0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
core Flair models on Huggingface ModelHub,0
"Large NER models,",0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Language-specific NER models,0
Language-specific POS models,0
English NER models,0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Danish models,0
German models,0
French models,0
Dutch models,0
Malayalam models,0
Portuguese models,0
Keyphase models,0
Biomedical models,0
check if model name is a valid local file,0
"check if model key is remapped to HF key - if so, print out information",0
get mapped name,0
use mapped name instead,0
"if not, check if model key is remapped to direct download location. If so, download model",0
special handling for the taggers by the @redewiegergabe project (TODO: move to model hub),1
"for all other cases (not local file or special download location), use HF model hub",0
"if not a local file, get from model hub",0
use model name as subfolder,0
Lazy import,0
output information,0
## Demo: How to use in Flair,0
load tagger,0
make example sentence,0
predict NER tags,0
print sentence,0
print predicted NER spans,0
iterate over entities and print,0
Lazy import,0
Save model weight,0
Determine if model card already exists,0
Generate and save model card,0
Upload files,0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
print labels in CoNLL format,0
the multi task model has several labels,0
biomedical models,0
entity linker,0
auto-spawn on GPU if available,0
remap state dict for models serialized with Flair <= 0.11.3,0
English sentiment models,0
Communicative Functions Model,0
"scores_at_targets[range(features.shape[0]), lengths.values -1]",0
Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices,0
"Initially, get scores from <start> tag to all other tags",0
"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp",0
"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep",0
Create a tensor to hold accumulated sequence scores at each current tag,0
Create a tensor to hold back-pointers,0
"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag",0
"Let pads be the <end> tag index, since that was the last tag in the decoded sequence",0
"We add scores at current timestep to scores accumulated up to previous timestep, and",0
choose the previous timestep that corresponds to the max. accumulated score for each current timestep,0
"If sentence is over, add transition to STOP-tag",0
Decode/trace best path backwards,0
Sanity check,0
remove start-tag and backscore to stop-tag,0
Max + Softmax to get confidence score for predicted label and append label to each token,0
"Transitions are used in the following way: transitions[to, from].",0
"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag",0
to START-tag and from STOP-tag to any other tag to -10000.,0
"if necessary, make batch_steps",0
break up the batch into slices of size,0
mini_batch_chunk_size,0
"if training also uses dev/train data, include in training set",0
evaluation and monitoring,0
sampling and shuffling,0
evaluation and monitoring,0
when and what to save,0
logging parameters,0
plugins,0
activate annealing plugin,0
call self.train_custom with all parameters (minus the ones specific to the AnnealingPlugin),0
training parameters,0
evaluation and monitoring,0
sampling and shuffling,0
evaluation and monitoring,0
when and what to save,0
logging parameters,0
amp,0
plugins,0
annealing logic,0
training parameters,0
evaluation and monitoring,0
sampling and shuffling,0
evaluation and monitoring,0
when and what to save,0
logging parameters,0
amp,0
plugins,0
training parameters,0
evaluation and monitoring,0
sampling and shuffling,0
evaluation and monitoring,0
when and what to save,0
logging parameters,0
amp,0
plugins,0
Create output folder,0
=== START BLOCK: ACTIVATE PLUGINS === #,0
We first activate all optional plugins. These take care of optional functionality such as various,0
logging techniques and checkpointing,0
log file plugin,0
loss file plugin,0
plugin for writing weights,0
plugin for checkpointing,0
=== END BLOCK: ACTIVATE PLUGINS === #,0
derive parameters the function was called with (or defaults),0
initialize model card with these parameters,0
Prepare training data and get dataset size,0
"determine what splits (train, dev, test) to evaluate",0
determine how to determine best model and whether to save it,0
instantiate the optimizer,0
initialize sampler if provided,0
init with default values if only class is provided,0
set dataset to sample from,0
this field stores the names of all dynamic embeddings in the model (determined after first forward pass),0
Sanity checks,0
"Sanity conversion: if flair.device was set as a string, convert to torch.device",0
-- AmpPlugin -> wraps with AMP,0
-- AnnealingPlugin -> initialize schedulers (requires instantiated optimizer),0
At any point you can hit Ctrl + C to break out of training early.,0
"- SchedulerPlugin -> load state for anneal_with_restarts, batch_growth_annealing, logic for early stopping",0
- LossFilePlugin -> get the current epoch for loss file logging,0
"if shuffle_first_epoch==False, the first epoch is not shuffled",0
log infos on training progress every `log_modulo` batches,0
process mini-batches,0
zero the gradients on the model and optimizer,0
forward and backward for batch,0
forward pass,0
identify dynamic embeddings (always deleted) on first sentence,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
do the optimizer step,0
- SchedulerPlugin -> do the scheduler step if one-cycle or linear decay,0
- WeightExtractorPlugin -> extracts weights,0
- CheckpointPlugin -> executes save_model_each_k_epochs,0
- SchedulerPlugin -> log bad epochs,0
Determine if this is the best model or if we need to anneal,0
log results,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
use DEV split to determine if this is the best model so far,0
"if not using DEV score, determine best model using train loss",0
- LossFilePlugin -> somehow prints all relevant metrics,0
- AnnealPlugin -> scheduler step,0
- SWAPlugin -> restores SGD weights from SWA,0
"if we do not use dev data for model selection, save final model",0
TensorboardLogger -> closes writer,0
test best model if test data is present,0
get and return the final test score of best model,0
MetricHistoryPlugin -> stores the loss history in return_values,0
"Store return values, as they will be erased by reset_training_attributes",0
get a random sample of training sentences,0
create a model card for this model with Flair and PyTorch version,0
record Transformers version if library is loaded,0
remember all parameters used in train() call,0
"TextDataset returns a list. valid and test are only one file,",0
so return the first element,0
cast string to Path,0
error message if the validation dataset is too small,0
Shuffle training files randomly after serially iterating,0
through corpus one,0
"iterate through training data, starting at",0
self.split (for checkpointing),0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
Backward,0
`clip_grad_norm` helps prevent the exploding gradient,0
problem in RNNs / LSTMs.,0
We detach the hidden state from how it was,0
previously produced.,0
"If we didn't, the model would try backpropagating",0
all the way to start of the dataset.,0
explicitly remove loss to clear up memory,0
#########################################################,0
Save the model if the validation loss is the best we've,0
seen so far.,0
#########################################################,0
print info,0
#########################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
"no need to check for MetricName, as __add__ of other would be called in this case",0
"This flag tracks, whether an event is currently being processed (otherwise it is added to the queue)",0
instantiate plugin,0
"Reset the flag, since an exception event might be dispatched",0
"If there is no **kw argument in the callback, check if any of the passed kw args is not accepted by",0
the callback,0
go through all attributes,0
get attribute hook events (may raise an AttributeError),0
register function as a hook,0
"Decorator was used with parentheses, but no args",0
Decorator was used with args (strings specifiying the events),0
Decorator was used without args,0
path to store the model,0
special annealing modes,0
determine the min learning rate,0
"minimize training loss if training with dev data, else maximize dev score",0
instantiate the scheduler,0
stop training if learning rate becomes too small,0
reload last best model if annealing with restarts is enabled,0
calculate warmup steps,0
skip if no optimization has happened.,0
saves the model with full vocab as checkpoints etc were created with reduced vocab.,0
TODO: check if metric is in tracked metrics,1
prepare loss logging file and set up header,0
set up all metrics to collect,0
set up headers,0
name: HEADER,0
Add all potentially relevant metrics. If a metric is not published,0
"after the first epoch (when the header is written), the column is",0
removed at that point.,0
initialize the first log line,0
record is a list of scalars,0
output log file,0
remove columns where no value was found on the first epoch (could be != 1 if training was resumed),0
make headers on epoch 1,0
write header,0
adjust alert level,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn,0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
"legacy pickle-like saving for image embeddings, as implementation details are not obvious",0
"legacy pickle-like loading for image embeddings, as implementation details are not obvious",0
"<cls> token initially set to 1/D, so it attends to all image features equally",0
add positional encodings,0
reshape the pixels into the sequence,0
layer norm after convolution and positional encodings,0
add <cls> token,0
"transformer requires input in the shape [h*w+1, b, d]",0
the output is an embedding of <cls> token,0
this parameter is fixed,0
optional fine-tuning on top of embedding layer,0
"if only one sentence is passed, convert to list of sentence",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push through RNN,0
after-RNN dropout,0
extract embeddings from RNN,0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
serialize the language models and the constructor arguments (but nothing else),0
re-initialize language model with constructor arguments,0
special handling for deserializing language models,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
IMPORTANT: add embeddings as torch modules,0
iterate over sentences,0
"if its a forward LM, take last state",0
"convert to plain strings, embedded in a list for the encode function",0
CNN,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push CNN,0
after-CNN dropout,0
extract embeddings from CNN,0
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency",0
"if only one sentence is passed, convert to list of sentence",0
Expose base classses,0
Expose document embedding classes,0
Expose image embedding classes,0
Expose legacy embedding classes,0
Expose token embedding classes,0
in some cases we need to insert zero vectors for tokens without embedding.,0
padding,0
remove special markup,0
check if special tokens exist to circumvent error message,0
iterate over subtokens and reconstruct tokens,0
remove special markup,0
check if reconstructed token is special begin token ([CLS] or similar),0
some BERT tokenizers somehow omit words - in such cases skip to next token,0
"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token.",0
if tokens are unaccounted for,0
check if all tokens were matched to subtokens,0
The layoutlm tokenizer doesn't handle ocr themselves,0
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial",0
"cannot run `.encode` if ocr boxes are required, assume",0
"transformers returns the ""added_tokens.json"" even if it doesn't create it",0
"transformers returns the ""added_tokens.json"" even if it doesn't create it",0
in case of doubt: token embedding has higher priority than document embedding,0
random check some tokens to save performance.,0
Models such as FNet do not have an attention_mask,0
set language IDs for XLM-style transformers,0
"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have",0
"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids.",0
set context if not set already,0
flair specific pre-tokenization,0
fields to store left and right context,0
expand context only if context_length is set,0
"if context_dropout is set, randomly deactivate left context during training",0
"if context_dropout is set, randomly deactivate right context during training",0
"if use_context_separator is set, add a [FLERT] token",0
return expanded sentence and context length information,0
"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first.",0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
do not print transformer warnings as these are confusing in this case,0
load tokenizer and transformer model,0
load tokenizer from inmemory zip-file,0
model name,0
embedding parameters,0
send mini-token through to check how many layers the model has,0
return length,0
"If we use a context separator, add a new special token",0
"most models have an initial BOS token, except for XLNet, T5 and GPT2",0
"when initializing, embeddings are in eval mode by default",0
in case of doubt: token embedding has higher priority than document embedding,0
in case of doubt: token embedding has higher priority than document embedding,0
legacy TransformerDocumentEmbedding,0
legacy TransformerTokenEmbedding,0
legacy Flair <= 0.12,0
legacy Flair <= 0.7,0
legacy TransformerTokenEmbedding,0
Legacy TransformerDocumentEmbedding,0
legacy TransformerTokenEmbedding,0
legacy TransformerDocumentEmbedding,0
some models like the tars model somehow lost this information.,0
copy values from new embedding,0
those parameters are only from the super class and will be recreated in the constructor.,0
cls first pooling can be done without recreating sentence hidden states,0
make the tuple a tensor; makes working with it easier.,0
"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention",0
only use layers that will be outputted,0
this parameter is fixed,0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
make compatible with serialized models,0
gensim version 4,0
gensim version 3,0
"if no embedding is set, the vocab and embedding length is requried",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNINOS embeddings,0
pubmed embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
"this is required to force the module on the cpu,",0
"if a parent module is put to gpu, the _apply is called to each sub_module",0
self.to(..) actually sets the device properly,0
this ignores the get_cached_vec method when loading older versions,0
it is needed for compatibility reasons,0
gensim version 4,0
gensim version 3,0
"when loading the old versions from pickle, the embeddings might not be added as pytorch module.",0
"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might",0
lead to issues while loading (trying to load weights that weren't stored as python weights and therefore,0
not finding them),0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
multilingual models,0
English models,0
Arabic,0
Bulgarian,0
Czech,0
Danish,0
German,0
Spanish,0
Basque,0
Persian,0
Finnish,0
French,0
Hebrew,0
Hindi,0
Croatian,0
Indonesian,0
Italian,0
Japanese,0
Malayalam,0
Dutch,0
Norwegian,0
Polish,0
Portuguese,0
Pubmed,0
Slovenian,0
Swedish,0
Tamil,0
Spanish clinical,0
CLEF HIPE Shared task,0
Amharic,0
Ukrainian,0
load model if in pretrained model map,0
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir),0
CLEF HIPE models are lowercased,0
embeddings are static if we don't do finetuning,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout",0
gradients are enable if fine-tuning is enabled,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
offset mode that extracts at whitespace after last character,0
offset mode that extracts at last character,0
make compatible with old models,0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
check token.text is empty or not,0
set aggregation operation,0
add embeddings after updating,0
model architecture,0
model architecture,0
"""pl"",",0
download if necessary,0
load the model,0
"TODO: keep for backwards compatibility, but remove in future",1
save the sentence piece model as binary file (not as path which may change),0
write out the binary sentence piece model into the expected directory,0
"if the model was saved as binary and it is not found on disk, write to appropriate path",0
"otherwise, use normal process and potentially trigger another download",0
"once the modes if there, load it with sentence piece",0
empty words get no embedding,0
all other words get embedded,0
GLOVE embeddings,0
no need to recreate as NILCEmbeddings,0
read in test file if exists,0
read in dev file if exists,0
"find train, dev and test files if not specified",0
Add tags for each annotated span,0
Remove leading and trailing whitespaces from annotated spans,0
Search start and end token index for current span,0
If end index is not found set to last token,0
Throw error if indices are not valid,0
Add metadatas for sentence,0
Currently all Jsonl Datasets are stored in Memory,0
get train data,0
read in test file if exists,0
read in dev file if exists,0
"find train, dev and test files if not specified",0
special key for space after,0
special key for feature columns,0
special key for dependency head id,0
"store either Sentence objects in memory, or only file offsets",0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
determine encoding of text file,0
identify which columns are spans and which are word-level,0
now load all sentences,0
skip first line if to selected,0
option 1: keep Sentence objects in memory,0
pointer to previous,0
parse next sentence,0
quit if last sentence reached,0
skip banned sentences,0
set previous and next sentence for context,0
append parsed sentence to list in memory,0
option 2: keep source data in memory,0
"read lines for next sentence, but don't parse",0
quit if last sentence reached,0
append raw lines for each sentence,0
we make a distinction between word-level tags and span-level tags,0
read first sentence to determine which columns are span-labels,0
skip first line if to selected,0
check the first 5 sentences,0
go through all annotations and identify word- and span-level annotations,0
- if a column has at least one BIES we know it's a Span label,0
"- if a column has at least one tag that is not BIOES, we know it's a Token label",0
- problem cases are columns for which we see only O - in this case we default to Span,0
skip assigned columns,0
the space after key is always word-levels,0
"if at least one token has a BIES, we know it's a span label",0
"if at least one token has a label other than BIOES, we know it's a token label",0
all remaining columns that are not word-level are span-level,0
for column in self.word_level_tag_columns:,0
"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")",0
"if sentence ends, break",0
parse comments if possible,0
"otherwise, this line is a token. parse and add to sentence",0
check if this sentence is a document boundary,0
add span labels,0
discard tags from tokens that are not added to the sentence,0
parse relations if they are set,0
head and tail span indices are 1-indexed and end index is inclusive,0
parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas',0
"to set the metadata ""domain"" to ""de-orcas""",0
get fields from line,0
get head_id if exists (only in dependency parses),0
initialize token,0
go through all columns,0
'feats' and 'misc' column should be split into different fields,0
special handling for whitespace after,0
add each other feature as label-value pair,0
get the task name (e.g. 'ner'),0
get the label value,0
add label,0
remap regular tag names,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
set sentence context using partials TODO: pointer to dataset is really inefficient,1
use all domains,0
iter over all domains / sources and create target files,0
The conll representation of coref spans allows spans to,0
"overlap. If spans end or begin at the same word, they are",0
"separated by a ""|"".",0
The span begins at this word.,0
The span begins and ends at this word (single word span).,0
"The span is starting, so we record the index of the word.",0
"The span for this id is ending, but didn't start at this word.",0
Retrieve the start index from the document state and,0
add the span to the clusters for this id.,0
strip all bracketing information to,0
get the actual propbank label.,0
Entering into a span for a particular semantic role label.,0
We append the label and set the current span for this annotation.,0
"If there's no '(' token, but the current_span_label is not None,",0
then we are inside a span.,0
We're outside a span.,0
"Exiting a span, so we reset the current span label for this annotation.",0
The words in the sentence.,0
The pos tags of the words in the sentence.,0
the pieces of the parse tree.,0
The lemmatised form of the words in the sentence which,0
have SRL or word sense information.,0
The FrameNet ID of the predicate.,0
"The sense of the word, if available.",0
"The current speaker, if available.",0
"Cluster id -> List of (start_index, end_index) spans.",0
Cluster id -> List of start_indices which are open for this id.,0
Replace brackets in text and pos tags,0
with a different token for parse trees.,0
only keep ')' if there are nested brackets with nothing in them.,0
There are some bad annotations in the CONLL data.,0
"They contain no information, so to make this explicit,",0
we just set the parse piece to be None which will result,0
in the overall parse tree being None.,0
"If this is the first word in the sentence, create",0
empty lists to collect the NER and SRL BIO labels.,0
"We can't do this upfront, because we don't know how many",0
"components we are collecting, as a sentence can have",0
variable numbers of SRL frames.,0
Create variables representing the current label for each label,0
sequence we are collecting.,0
"If any annotation marks this word as a verb predicate,",0
we need to record its index. This also has the side effect,0
of ordering the verbal predicates by their location in the,0
"sentence, automatically aligning them with the annotations.",0
"this would not be reached if parse_pieces contained None, hence the cast",0
Non-empty line. Collect the annotation.,0
Collect any stragglers or files which might not,0
have the '#end document' format for the end of the file.,0
this dataset name,0
check if data there,0
column format,0
this dataset name,0
check if data there,0
column format,0
this dataset name,0
download data if necessary,0
download files if not present locally,0
we need to slightly modify the original files by adding some new lines after document separators,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
Remove CoNLL-U meta information in the last column,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
entity_mapping,0
this dataset name,0
download data if necessary,0
data validation,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download files if not present locallys,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
# download zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
download data if necessary,0
"unzip the downloaded repo and merge the train, dev and test datasets",0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
check if data there,0
create folder,0
download dataset,0
column format,0
this dataset name,0
download and parse data if necessary,0
create train test dev if not exist,0
column format,0
this dataset name,0
If the extracted corpus file is not yet present in dir,0
download zip if necessary,0
"extracted corpus is not present , so unpacking it.",0
column format,0
this dataset name,0
download zip,0
unpacking the zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)",0
download files if not present locally,0
we need to modify the original files by adding new lines after after the end of each sentence,0
if only one language is given,0
column format,0
this dataset name,0
"use all languages if explicitly set to ""all""",0
download data if necessary,0
initialize comlumncorpus and add it to list,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
this list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
get google drive id from list,0
download from google drive,0
unzip,0
"tar.extractall(language_folder,members=[tar.getmember(file_name)])",0
transform data into required format,0
"the processed dataset has the additional ending ""_new""",0
remove the unprocessed dataset,0
initialize comlumncorpus and add it to list,0
if no languages are given as argument all languages used in XTREME will be loaded,0
if only one language is given,0
column format,0
this dataset name,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
This list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
download from HU Server,0
unzip,0
transform data into required format,0
initialize comlumncorpus and add it to list,0
if only one language is given,0
column format,0
this dataset name,0
download data if necessary,0
initialize comlumncorpus and add it to list,0
download data if necessary,0
unpack and write out in CoNLL column-like format,0
column format,0
this dataset name,0
download data if necessary,0
data is not in IOB2 format. Thus we transform it to IOB2,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
column format,0
this dataset name,0
rename according to train - test - dev - convention,0
column format,0
this dataset name,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
Add missing newline after header,0
Workaround for empty tokens,1
"Add ""real"" document marker",0
Dataset split mapping,0
v2.0 only adds new language and splits for AJMC dataset,0
Special document marker for sample splits in AJMC dataset,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
if only one language is given,0
column format,0
this dataset name,0
"use all languages if explicitly set to ""all""",0
download data if necessary,0
initialize comlumncorpus and add it to list,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
paths to train and test splits,0
init corpus,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
iterate over all html files,0
"get rid of html syntax, we only need the text",0
between all documents we write a separator symbol,0
skip empty strings,0
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)",0
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention",0
sentence splitting and tokenization,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out-file in column format,0
"in case something goes wrong, delete the dataset and raise error",0
this dataset name,0
download and parse data if necessary,0
from qwikidata.linked_data_interface import get_entity_dict_from_api,0
generate qid wikiname dictionaries,0
merge dictionaries,0
ignore first line,0
commented and empty lines,0
read all Q-IDs,0
ignore first line,0
request,0
this dataset name,0
we use the wikiids in the data instead of directly utilizing the wikipedia urls.,0
like this we can quickly check if the corresponding page exists,0
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi,0
delete unprocessed file,0
collect all wikiids,0
create the dictionary,0
request,0
this dataset name,0
names of raw text documents,0
open output_file,0
iterate through all documents,0
split sentences and tokenize,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out file,0
annotation from one annotator or two agreeing annotators,0
this dataset name,0
download and parse data if necessary,0
this dataset name,0
download and parse data if necessary,0
First parse the post titles,0
Keep track of how many and which entity mentions does a given post title have,0
Check if the current post title has an entity link and parse accordingly,0
Post titles with entity mentions (if any) are handled via this function,0
Then parse the comments,0
"Iterate over the comments.tsv file, until the end is reached",0
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched.",0
Each comment thread is handled as one 'document'.,0
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.,0
This if-condition is needed to handle this problem.,0
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure",0
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above",0
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle.",0
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,",0
and not just single letters into single rows.,0
If there are annotated entity mentions for given post title or a comment thread,0
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence",0
Write the token with a corresponding tag to file,0
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed",0
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag",0
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized,0
"incorrectly, in order to keep the desired format (empty line as a sentence separator).",0
"Thrown when the second check above happens, but the last token of a sentence is reached.",0
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below.",0
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS",0
Check if further annotations belong to the current post title or comment thread as well,0
Stop when the end of an annotation file is reached,0
Check if further annotations belong to the current sentence as well,0
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)",0
Docstart,0
if there is more than one word in the chunk we write each in a separate line,0
print(chunks),0
empty line after each sentence,0
convert the file to CoNLL,0
this dataset name,0
"check if data there, if not, download the data",0
create folder,0
download data,0
transform data into column format if necessary,0
if no filenames are specified we use all the data,0
"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled",0
also we remove 'raganato_ALL' from filenames in case its in the list,0
generate the test file,0
make column file and save to data_folder,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
this dataset name,0
download data if necessary,0
if True:,0
write CoNLL-U Plus header,0
"Some special cases (e.g., missing spaces before entity marker)",0
necessary if text should be whitespace tokenizeable,0
Handle case where tail may occur before the head,0
this dataset name,0
write CoNLL-U Plus header,0
this dataset name,0
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors,1
download data if necessary,0
write CoNLL-U Plus header,0
The span has ended.,0
We are entering a new span; reset indices,0
and active tag to new span.,0
We're inside a span.,0
Last token might have been a part of a valid span.,0
this dataset name,0
write CoNLL-U Plus header,0
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):",0
"with zip_file.open(source_file_path, mode=""r"") as source_file:",0
target_file_path = Path(data_folder) / target_filename,0
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:",0
# write CoNLL-U Plus header,0
"target_file.write(""# global.columns = id form ner\n"")",0
for example in json.load(source_file):,0
token_list = self._tacred_example_to_token_list(example),0
target_file.write(token_list.serialize()),0
check if first tag row is already occupied,0
"if first tag row is occupied, use second tag row",0
hardcoded mapping TODO: perhaps find nicer solution,1
remap regular tag names,0
else skip to position in file where sentence begins,0
set sentence context using partials TODO: pointer to dataset is really inefficient,1
read in dev file if exists,0
read in test file if exists,0
the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44,0
"find train, dev and test files if not specified",0
use test_file to create test split if available,0
use dev_file to create test split if available,0
"if data point contains black-listed label, do not use",0
first check if valid sentence,0
"if so, add to indices",0
"find train, dev and test files if not specified",0
variables,0
different handling of in_memory data than streaming data,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
test if format is OK,0
test if at least one label given,0
make sentence from text (and filter for length),0
"if a pair column is defined, make a sentence pair object",0
noinspection PyDefaultArgument,0
dataset name includes the split size,0
default dataset folder is the cache root,0
download data if necessary,0
download each of the 28 splits,0
create dataset directory if necessary,0
download senteval datasets if necessary und unzip,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
download data from same source as in huggingface's implementations,0
read label order,0
"Original labels are [1, 2, 3, 4] -> ['World', 'Sports', 'Business', 'Sci/Tech']",0
"Re-map to [0, 1, 2, 3].",0
this dataset name,0
download data if necessary,0
handle labels file,0
handle data file,0
Create flair compatible labels,0
"by default, map point score to POSITIVE / NEGATIVE values",0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file from CSV,0
create test.txt file from CSV,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create train dev and test files in fasttext format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
convert to FastText format,0
download data if necessary,0
"if data is not downloaded yet, download it",0
get the zip file,0
move original .tsv files to another folder,0
create train and dev splits in fasttext format,0
create eval_dataset file with no labels,0
download zip archive,0
unpack file in datasets directory (zip archive contains a directory named SST-2),0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download datasets if necessary,0
create dataset directory if necessary,0
create correctly formated txt files,0
multiple labels are possible,0
this dataset name,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
create a separate directory for different tasks,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
check if dataset is supported,0
set file names,0
set file names,0
download and unzip in file structure if necessary,0
instantiate corpus,0
"find train, dev and test files if not specified",0
"create DataPairDataset for train, test and dev file, if they are given",0
stop if file does not exist,0
create a DataPair object from strings,0
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"reorder dev datasets to have same columns as in train set: 8, 9, and 11",0
dev sets include 5 different annotations but we will only keep the gold label,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get test and dev sets,0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data not downloaded yet, download it",0
get the zip file,0
"the downloaded files have json format, we transform them to tsv",0
Function to transform JSON file to tsv for Recognizing Textual Entailment Data,0
remove json file,0
Uses dynamic programming approach to calculate maximum independent set in interval graph,0
with sum of all entity lengths as secondary key,0
calculate offset without current text,0
because we stick all passages of a document together,0
TODO For split entities we also annotate everything inbetween which might be a bad idea?,1
Try to fix incorrect annotations,0
print(,0
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}""",0
),0
Ignore empty lines or relation annotations,0
FIX annotation of whitespaces (necessary for PDR),0
One token may contain multiple entities -> deque all of them,0
column format,0
this dataset name,0
Create tokenization-dependent CONLL files. This is necessary to prevent,0
from caching issues (e.g. loading the same corpus with different sentence splitters),0
column format,0
this dataset name,0
column format,0
this dataset name,0
Edge case: last token starts a new entity,0
Last document in file,0
column format,0
this dataset name,0
column format,0
this dataset name,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
Edge case: last token starts a new entity,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
column format,0
this dataset name,0
Read texts,0
Read annotations,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
We need to apply a patch to correct the original training file,0
Articles title,0
Article abstract,0
Entity annotations,0
column format,0
this dataset name,0
Edge case: last token starts a new entity,0
Map all entities to chemicals,0
Map all entities to disease,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
Incomplete article,0
Invalid XML syntax,0
column format,0
this dataset name,0
column format,0
this dataset name,0
if len(mid) != 3:,0
continue,0
Try to fix entity offsets,0
column format,0
this dataset name,0
There is still one illegal annotation in the file ..,0
column format,0
this dataset name,0
"Abstract first, title second to prevent issues with sentence splitting",0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
column format,0
this dataset name,0
"Filter for specific entity types, by default no entities will be filtered",0
Get original HUNER splits to retrieve a list of all document ids contained in V2,0
train and dev split of V2 will be train in V4,0
test split of V2 will be dev in V4,0
New documents in V4 will become test documents,0
column format,0
this dataset name,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
build dataset name and full huggingface reference name,0
Download data if necessary,0
"Some datasets in BigBio only have train or test splits, not both",0
"If only test split, assign it to train split",0
"If only train split, sample other from it (sample_missing_splits=True)",0
Not every dataset has a dev / validation set!,0
Perform type mapping if necessary,0
"Collect all texts of the document, each passage will be",0
a text in our internal format,0
Sort passages by start offset,0
Transform all entity annotations into internal format,0
Find the passage of the entity (necessary for offset adaption),0
Adapt entity offsets according to passage offsets,0
FIXME: This is just for debugging purposes,1
passage_text = id_to_text[passage_id],0
doc_text = passage_text[entity_offset[0] : entity_offset[1]],0
"mention_text = entity[""text""][0]",0
if doc_text != mention_text:,0
"print(f""Annotation error ({document['document_id']}) - Doc: {doc_text} vs. Mention: {mention_text}"")",0
Check base case,0
Get element in the middle,0
Is the mention with the passage offsets?,0
"If element is smaller than mid, then it can only",0
be present in left subarray,0
Else the element can only be present in right subarray,0
Special case for ProGene: We need to use the split_0_train and split_0_test splits,0
as they are currently provided in BigBio,0
cache Feidegger config file,0
cache Feidegger images,0
replace image URL with local cached file,0
append Sentence-Image data point,0
cast to list if necessary,0
cast to list if necessary,0
"first, check if pymongo is installed",0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
Expose base classses,0
Expose all biomedical data sets used for the evaluation of BioBERT,0
-,0
-,0
-,0
-,0
Expose all biomedical data sets using the HUNER splits,0
Expose all biomedical data sets,0
Expose all document classification datasets,0
word sense disambiguation,0
Expose all entity linking datasets,0
Expose all relation extraction datasets,0
universal proposition banks,0
keyphrase detection datasets,0
other NER datasets,0
standard NER datasets,0
Expose all sequence labeling datasets,0
Expose all text-image datasets,0
Expose all text-text datasets,0
Expose all treebanks,0
"find train, dev and test files if not specified",0
get train data,0
get test data,0
get dev data,0
option 1: read only sentence boundaries as offset positions,0
option 2: keep everything in memory,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
current token ID,0
handling for the awful UD multiword format,0
end of sentence,0
comments or ellipsis,0
if token is a multi-word,0
normal single-word tokens,0
"if we don't split multiwords, skip over component words",0
add token,0
add morphological tags,0
derive whitespace logic for multiwords,0
print(token),0
print(current_multiword_last_token),0
print(current_multiword_first_token),0
"if multi-word equals component tokens, there should be no whitespace",0
go through all tokens in subword and set whitespace_after information,0
print(i),0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
this dataset name,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"finally, print model card for information",0
noqa: INP001,0
-- Project information -----------------------------------------------------,0
"The full version, including alpha/beta/rc tags",0
use smv_current_version as the git url,0
-- General configuration ---------------------------------------------------,0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
Napoleon settings,0
Whitelist pattern for tags (set to None to ignore all tags),0
Whitelist pattern for branches (set to None to ignore all branches),0
Whitelist pattern for remotes (set to None to use local branches only),0
Pattern for released versions,0
Format for versioned output directories inside the build directory,0
Determines whether remote or local git branches/tags are preferred if their output dirs conflict,0
test corpus,0
create a TARS classifier,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
test corpus,0
create a TARS classifier,0
switch to a new task (TARS can do multiple tasks so you must define one),0
initialize the text classifier trainer,0
start the training,0
"With end symbol, without start symbol, padding in front",0
"Without end symbol, with start symbol, padding in back",0
"Without end symbol, without start symbol, padding in front",0
initialize trainer,0
initialize trainer,0
initialize trainer,0
increment for last token in sentence if not followed by whitespace,0
clean up directory,0
clean up directory,0
example sentence,0
set 4 labels for 2 tokens ('love' is tagged twice),0
check if there are three POS labels with correct text and values,0
check if there are is one SENTIMENT label with correct text and values,0
check if all tokens are correctly labeled,0
remove the pos label from the last word,0
there should be 2 POS labels left,0
now remove all pos tags,0
set 3 labels for 2 spans (HU is tagged twice),0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
"now delete the NER tags of ""Humboldt-Universitt zu Berlin""",0
should be only one NER label left,0
and only one NER span,0
set 3 labels for 2 spans (HU is tagged twice with different tags),0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
"now delete the NER tags of ""Humboldt-Universitt zu Berlin""",0
should be only one NER label left,0
and only one NER span,0
but there is also one orgtype span and label,0
and only one NER span,0
let's add the NER tag back,0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
now remove all NER tags,0
set 3 labels for 2 spans (HU is tagged twice with different tags),0
create two relation label,0
there should be two relation labels,0
there should be one syntactic labels,0
"there should be two relations, one with two and one with one label",0
example sentence,0
add another topic label,0
example sentence,0
has sentiment value,0
has 4 part of speech tags,0
has 1 NER tag,0
should be in total 6 labels,0
example sentence,0
add two NER labels,0
get the four labels,0
check that only two of the respective data points are equal,0
make a sentence and some right context,0
TODO: is this desirable? Or should two sentences with same text be considered same objects?,1
Initializing a Sentence this way assumes that there is a space after each token,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
load column dataset with one entry,0
load column dataset with two entries,0
load column dataset with three entries,0
"get training, test and dev data",0
"get training, test and dev data",0
check if Token labels are correct,0
"get training, test and dev data",0
check if Token labels for frames are correct,0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
"assert [token.get_tag(""head"").value for token in sent1.tokens] == [",0
"""2"",",0
"""0"",",0
"""4"",",0
"""2"",",0
"""2"",",0
"""2"",",0
],0
This test only covers basic universal dependencies datasets.,0
"For example, multi-word tokens or the ""deps"" column sentence annotations are not supported yet.",0
"Here, we use the default token annotation fields.",0
This test covers the complete HIPE 2022 dataset.,0
https://github.com/hipe-eval/HIPE-2022-data,0
"Includes variant with document separator, and all versions of the dataset.",0
"We have manually checked, that these numbers are correct:",0
"+1 offset, because of missing EOS marker at EOD",0
Test data for v2.1 release,0
This test covers the complete ICDAR Europeana corpus:,0
https://github.com/stefan-it/historic-domain-adaptation-icdar,0
"This test covers the complete MasakhaNER dataset, including support for v1 and v2.",0
This test covers the NERMuD dataset. Official stats can be found here:,0
https://github.com/dhfbk/KIND/tree/main/evalita-2023,0
Number of instances per dataset split are taken from https://huggingface.co/datasets/elenanereiss/german-ler,0
This test covers the complete MasakhaPOS dataset.,0
"See MobIE paper (https://aclanthology.org/2021.konvens-1.22/), table 2",0
--- Embeddings that are shared by both models --- #,0
--- Task 1: Sentiment Analysis (5-class) --- #,0
Define corpus and model,0
-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #,0
Define corpus and model,0
-- Define mapping (which tagger should train on which model) -- #,0
-- Create model trainer and train -- #,0
clean up file,0
no need for label_dict,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
Intel ----founded_by---> Gordon Moore,0
Intel ----founded_by---> Robert Noyce,0
"Ground truth is a set of tuples of (<Sentence Text>, <Relation Label Values>)",0
Check sentence masking and relation label annotation on,0
"training, validation and test dataset (in this test the splits are the same)",0
"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google .""",0
"Entity pair permutations of: ""Microsoft was founded by Bill Gates .""",0
"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 .""",0
"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany.""",0
This sentence is only included if we transform the corpus with cross augmentation,0
Ensure this is an example that predicts no classes in multilabel,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
ensure that the prepared tensors is what we expect,0
use a SequenceTagger to save and reload the embedding in the manner it is supposed to work,0
previous and next sentence as context,0
test expansion for sentence without context,0
test expansion for with previous and next as context,0
test expansion if first sentence is document boundary,0
test expansion if we don't use context,0
"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher.",0
dummy model with embeddings,0
save the dummy and load it again,0
check that context_length and use_context_separator is the same for both,0
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
Extract all the contents of zip file in current directory,0
Extract all the contents of zip file in current directory,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
load_big_file is a workaround byhttps://github.com/highway11git,1
to load models on some Mac/Windows setups,0
see https://github.com/zalandoresearch/flair/issues/351,0
first determine the distribution of classes in the dataset,0
weight for each sample,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
increment for last token in sentence if not followed by whitespace,0
this is the default init size of a lmdb database for embeddings,0
get db filename from embedding name,0
"In case initialization of cached version failed, just fallback to the original WordEmbeddings",0
SequenceTagger,0
TextClassifier,0
get db filename from embedding name,0
if embedding database already exists,0
"otherwise, push embedding to database",0
if embedding database already exists,0
open the database in read mode,0
we need to set self.k,0
create and load the database in write mode,0
"no idea why, but we need to close and reopen the environment to avoid",0
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot,0
when opening new transaction !,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
set 'add_unk' if the dictionary was created with a version of Flair older than 0.9,0
set 'add_unk' depending on whether <unk> is a key,0
"if one embedding name, directly return it",0
"if multiple embedding names, concatenate them",0
First we remove any existing labels for this PartOfSentence in self.sentence,0
labels also need to be deleted at Sentence object,0
delete labels at object itself,0
The Token is a special _PartOfSentence in that it may be initialized without a Sentence.,0
"therefore, labels get added only to the Sentence if it exists",0
The Token is a special _PartOfSentence in that it may be initialized without a Sentence.,0
"Therefore, labels get set only to the Sentence if it exists",0
"check if the span already exists. If so, return it",0
else make a new span,0
"check if the relation already exists. If so, return it",0
else make a new relation,0
private field for all known spans,0
the tokenizer used for this sentence,0
some sentences represent a document boundary (but most do not),0
internal variables to denote position inside dataset,0
"if text is passed, instantiate sentence with tokens (words)",0
determine token positions and whitespace_after flag,0
the last token has no whitespace after,0
log a warning if the dataset is empty,0
data with zero-width characters cannot be handled,0
set token idx and sentence,0
append token to sentence,0
register token annotations on sentence,0
move sentence embeddings to device,0
also move token embeddings to device,0
clear token embeddings,0
infer whitespace after field,0
"if sentence has no tokens, return empty string",0
"otherwise, return concatenation of tokens with the correct offsets",0
The sentence's start position is not propagated to its tokens.,0
"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces.",0
No character at the corresponding code point: remove it,0
"if no label if specified, return all labels",0
"if the label type exists in the Sentence, return it",0
return empty list if none of the above,0
labels also need to be deleted at all tokens,0
labels also need to be deleted at all known spans,0
remove spans without labels,0
delete labels at object itself,0
set name,0
abort if no data is provided,0
sample test data from train if none is provided,0
sample dev data from train if none is provided,0
set train dev and test data,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
"first, determine the datapoint type by going through dataset until first label is found",0
count all label types per sentence,0
go through all labels of label_type and count values,0
special handling for Token-level annotations. Add all untagged as 'O' label,0
"if an unk threshold is set, UNK all label values below this threshold",0
sample randomly from a label distribution according to the probabilities defined by the noise transition matrix,0
replace the old label with the new one,0
keep track of the old (clean) label using another label type category,0
keep track of how many labels in total are flipped,0
sample randomly from a label distribution according to the probabilities defined by the desired noise share,0
replace the old label with the new one,0
keep track of the old (clean) label using another label type category,0
keep track of how many labels in total are flipped,0
"add a dummy ""O"" to close final prediction",0
return complex list,0
internal variables,0
non-set tags are OUT tags,0
anything that is not OUT is IN,0
does this prediction start a new span?,0
B- and S- always start new spans,0
"if the predicted class changes, I- starts a new span",0
"if the predicted class changes and S- was previous tag, start a new span",0
if an existing span is ended (either by reaching O or starting a new span),0
determine score and value,0
append to result list,0
reset for-loop variables for new span,0
remember previous tag,0
global variable: cache_root,0
global variable: device,0
"No need for correctness checks, torch is doing it",0
global variable: version,0
global variable: arrow symbol,0
dummy return to fulfill trainer.train() needs,0
print(vec),0
Attach optimizer,0
"convert `metrics` to float, in case it's a zero-dim Tensor",0
if memory mode option 'none' delete everything,0
"if dynamic embedding keys not passed, identify them automatically",0
always delete dynamic embeddings,0
"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)",0
optional metric space decoder if prototypes have different length than embedding,0
create initial prototypes for all classes (all initial prototypes are a vector of all 1s),0
"if set, create initial prototypes from normal distribution",0
"if set, use a radius",0
all parameters will be pushed internally to the specified device,0
decode embeddings into prototype space,0
"if unlabeled distance is set, mask out loss to unlabeled class prototype",0
verbalize BIOES labels,0
"if label is not BIOES, use label itself",0
Always include the name of the Model class for which the state dict holds,0
"write out a ""model card"" if one is set",0
save model,0
"if this class is abstract, go through all inheriting classes and try to fetch and load the model",0
get all non-abstract subclasses,0
"try to fetch the model for each subclass. if fetching is possible, load model and return it",0
"skip any invalid loadings, e.g. not found on huggingface hub",0
"if the model cannot be fetched, load as a file",0
try to get model class from state,0
"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses",0
"if str(model_cls) == ""<class 'flair.models.pairwise_classification_model.TextPairClassifier'>"": continue",0
"skip any invalid loadings, e.g. not found on huggingface hub",0
"if this class is not abstract, fetch the model and load it",0
"make sure <unk> is contained in gold_label_dictionary, if given",0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
loss calculation,0
variables for printing,0
variables for computing scores,0
remove any previously predicted labels,0
predict for batch,0
get the gold labels,0
add to all_predicted_values,0
make printout lines,0
convert true and predicted values to two span-aligned lists,0
delete exluded labels if exclude_labels is given,0
"if after excluding labels, no label is left, ignore the datapoint",0
write all_predicted_values to out_file if set,0
make the evaluation dictionary,0
check if this is a multi-label problem,0
compute numbers by formatting true and predicted such that Scikit-Learn can use them,0
multi-label problems require a multi-hot vector for each true and predicted label,0
single-label problems can do with a single index for each true and predicted label,0
"now, calculate evaluation numbers",0
there is at least one gold label or one prediction (default),0
"if there is only one label, then ""micro avg"" = ""macro avg""",0
"micro average is only computed if zero-label exists (for instance ""O"")",0
if no zero-label exists (such as in POS tagging) micro average is equal to accuracy,0
same for the main score,0
issue error and default all evaluation numbers to 0.,0
check if there is a label mismatch,0
print info,0
set the embeddings,0
initialize the label dictionary,0
initialize the decoder,0
set up multi-label logic,0
init dropouts,0
loss weights and loss function,0
Initialize the weight tensor,0
set up gradient reversal if so specified,0
embed sentences,0
get a tensor of data points,0
do dropout,0
make a forward pass to produce embedded data points and labels,0
get the data points for which to predict labels,0
get their gold labels as a tensor,0
pass data points through network to get encoded data point tensor,0
decode,0
an optional masking step (no masking in most cases),0
calculate the loss,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
filter data points in batch,0
stop if all sentences are empty,0
pass data points through network and decode,0
if anything could possibly be predicted,0
remove previously predicted labels of this type,0
filter data points that have labels outside of dictionary,0
add DefaultClassifier arguments,0
add variables of DefaultClassifier,0
Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23,0
Get projected 1st dimension,0
Compute bilinear form,0
Arcosh,0
Project the input data to n+1 dimensions,0
"The first dimension, is recomputed in the distance module",0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for",0
"train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
plot i,0
save plots,0
save plots,0
plt.show(),0
save plot,0
auto-spawn on GPU if available,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
TODO: not saving lines yet,1
TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.,1
- MaskedRelationClassifier ?,0
This depends if this relation classification architecture should replace or offer as an alternative.,0
Set label type and prepare label dictionary,0
Initialize super default classifier,0
Add the special tokens from the encoding strategy,0
"Auto-spawn on GPU, if available",0
Only use entities labelled with the specified labels for each label type,0
Only use entities above the specified threshold,0
Use a dictionary to find gold relation annotations for a given entity pair,0
Yield head and tail entity pairs from the cross product of all entities,0
Remove identity relation entity pairs,0
Remove entity pairs with labels that do not match any,0
of the specified relations in `self.entity_pair_labels`,0
"Obtain gold label, if existing",0
Some sanity checks,0
Pre-compute non-leading head and tail tokens for entity masking,0
We can not use the plaintext of the head/tail span in the sentence as the mask/marker,0
since there may be multiple occurrences of the same entity mentioned in the sentence.,0
"Therefore, we use the span's position in the sentence.",0
Create masked sentence,0
Add gold relation annotation as sentence label,0
"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,",0
"during prediction, the forward pass does not need any knowledge about the entities in the sentence.",0
"If we sample missing splits, the encoded sentences that correspond to the same original sentences",0
"may get distributed into different splits. For training purposes, this is always undesired.",0
Ensure that all sentences are encoded properly,0
Deal with the case where all sentences are encoded sentences,0
"mypy does not infer the type of ""sentences"" restricted by the if statement",0
Deal with the case where all sentences are standard (non-encoded) sentences,0
"For each encoded sentence, transfer its prediction onto the original relation",0
auto-spawn on GPU if available,0
pad strings with whitespaces to longest sentence,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
"""document_delimiter"" property may be missing in some older pre-trained models",0
serialize the language models and the constructor arguments (but nothing else),0
special handling for deserializing language models,0
re-initialize language model with constructor arguments,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
Transform input data into TARS format,0
"if there are no labels, return a random sample as negatives",0
"otherwise, go through all labels",0
make sure the probabilities always sum up to 1,0
get and embed all labels by making a Sentence object that contains only the label text,0
get each label embedding and scale between 0 and 1,0
compute similarity matrix,0
"the higher the similarity, the greater the chance that a label is",0
sampled as negative example,0
make label dictionary if no Dictionary object is passed,0
prepare dictionary of tags (without B- I- prefixes and without UNK),0
check if candidate_label_set is empty,0
make list if only one candidate label is passed,0
create label dictionary,0
note current task,0
create a temporary task,0
make zero shot predictions,0
switch to the pre-existing task,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
make a tars sentence where all labels are O by default,0
init new TARS classifier,0
set all task information,0
progress bar for verbosity,0
stop if all sentences are empty,0
always remove tags first,0
go through each sentence in the batch,0
always remove tags first,0
get the span and its label,0
determine whether tokens in this span already have a label,0
only add if all tokens have no label,0
make and add a corresponding predicted span,0
set indices so that no token can be tagged twice,0
clearing token embeddings to save memory,0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
get the serialized embeddings,0
remap state dict for models serialized with Flair <= 0.11.3,0
init new TARS classifier,0
set all task information,0
with torch.no_grad():,0
progress bar for verbosity,0
stop if all sentences are empty,0
always remove tags first,0
go through each sentence in the batch,0
always remove tags first,0
add all labels that according to TARS match the text and are above threshold,0
do not add labels below confidence threshold,0
only use label with the highest confidence if enforcing single-label predictions,0
add the label with the highest score even if below the threshold if force label is activated.,0
remove previously added labels and only add the best label,0
clearing token embeddings to save memory,0
set separator to concatenate two sentences,0
auto-spawn on GPU if available,0
pooling operation to get embeddings for entites,0
set embeddings,0
set relation and entity label types,0
"whether to use gold entity pairs, and whether to filter entity pairs by type",0
filter entity pairs according to their tags if set,0
whether to encode characters and whether to use attention (attention can only be used if chars are encoded),0
character dictionary for decoding and encoding,0
make sure <unk> is in dictionary for handling of unknown characters,0
add special symbols to dictionary if necessary and save respective indices,0
---- ENCODER ----,0
encoder character embeddings,0
encoder pre-trained embeddings,0
encoder RNN,0
additional encoder linear layer if bidirectional encoding,0
---- DECODER ----,0
decoder: linear layers to transform vectors to and from alphabet_size,0
when using attention we concatenate attention outcome and decoder hidden states,0
decoder RNN,0
loss and softmax,0
self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction,0
add additional columns for special symbols if necessary,0
initialize with dummy symbols,0
encode inputs,0
get labels (we assume each token has a lemma label),0
get char indices for labels of sentence,0
"(batch_size, max_sequence_length) batch_size = #words in sentence,",0
max_sequence_length = length of longest label of sentence + 1,0
get char embeddings,0
"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size",0
take decoder input and initial hidden and pass through RNN,0
"if all encoder outputs are provided, use attention",0
take convex combinations of encoder hidden states as new output using the computed attention coefficients,0
"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)",0
get all tokens,0
encode input characters by sending them through RNN,0
get one-hots for characters and add special symbols / padding,0
determine length of each token,0
embed sentences,0
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)",0
variable to store initial hidden states for decoder,0
encode input characters by sending them through RNN,0
test packing and padding,0
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder,0
concatenate the final hidden states of the encoder. These will be projected to hidden_size of,0
decoder later with self.emb_to_hidden,0
mask out vectors that correspond to a dummy symbol (TODO: check attention masking),1
use token embedding as initial hidden state for decoder,0
concatenate everything together and project to appropriate size for decoder,0
variable to store initial hidden states for decoder,0
encode input characters by sending them through RNN,0
note that we do not need to fill up with dummy symbols since we process each token seperately,0
embed character one-hots,0
send through encoder RNN (produces initial hidden for decoder),0
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder,0
project 2*hidden_size to hidden_size,0
concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder,0
later with self.emb_to_hidden,0
use token embedding as initial hidden state for decoder,0
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)",0
concatenate everything together and project to appropriate size for decoder,0
"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)",0
"create target vector (batch_size, max_label_seq_length + 1)",0
filter empty sentences,0
max length of the predicted sequences,0
for printing,0
stop if all sentences are empty,0
remove previously predicted labels of this type,0
create list of tokens in batch,0
encode inputs,0
"create input for first pass (batch_size, 1, input_size), first letter is special character <S>",0
sequence length is always set to one in prediction,0
option 1: greedy decoding,0
predictions,0
decode next character,0
pick top beam size many outputs with highest probabilities,0
option 2: beam search,0
out_probs = self.softmax(output_vectors).squeeze(1),0
make sure no dummy symbol <> or start symbol <S> is predicted,0
pick top beam size many outputs with highest probabilities,0
"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1",0
"leading_indices and probabilities have size (batch_size, beam_size)",0
keep scores of beam_size many hypothesis for each token in the batch,0
stack all leading indices of all hypothesis and corresponding hidden states in two tensors,0
save sequences so far,0
keep track of how many hypothesis were completed for each token,0
"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)",0
decode with log softmax,0
make sure no dummy symbol <> or start symbol <S> is predicted,0
"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside",0
"if the sequence is already ended, do not record as candidate",0
index of token in in list tokens_in_batch,0
print(token_number),0
hypothesis score,0
TODO: remove token if number of completed hypothesis exceeds given value,1
set score of corresponding entry to -inf so it will not be expanded,0
get leading_indices for next expansion,0
find highest scoring hypothesis among beam_size*beam_size possible ones for each token,0
take beam_size many copies of scores vector and add scores of possible new extensions,0
"size (beam_size*batch_size, beam_size)",0
print(hypothesis_scores),0
"reshape to vector of size (batch_size, beam_size*beam_size),",0
each row contains beam_size*beam_size scores of the new possible hypothesis,0
print(hypothesis_scores_per_token),0
"choose beam_size best for each token - size (batch_size, beam_size)",0
out of indices_per_token we now need to recompute the original indices of the hypothesis in,0
a list of length beam_size*batch_size,0
"where the first three inidices belong to the first token, the next three to the second token,",0
and so on,0
with these indices we can compute the tensors for the next iteration,0
expand sequences with corresponding index,0
add log-probabilities to the scores,0
save new leading indices,0
save corresponding hidden states,0
it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations,0
in that case we append one of the final seuqences without end symbol to the final_candidates,0
get best final hypothesis for each token,0
get characters from index sequences and add predicted label to token,0
"Overwrites evaluate of parent class to remove the ""by class"" printout",0
set separator to concatenate two sentences,0
init dropouts,0
auto-spawn on GPU if available,0
make a forward pass to produce embedded data points and labels,0
get their gold labels as a tensor,0
pass data points through network to get encoded data point tensor,0
decode,0
calculate the loss,0
get a tensor of data points,0
do dropout,0
add DefaultClassifier arguments,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
"if the classifier predicts BIO/BIOES span labels, the internal label dictionary must be computed",0
fields in case this is a span-prediction problem,0
the label type,0
all parameters will be pushed internally to the specified device,0
special handling during training if this is a span prediction problem,0
internal variables,0
non-set tags are OUT tags,0
anything that is not OUT is IN,0
does this prediction start a new span?,0
B- and S- always start new spans,0
"if the predicted class changes, I- starts a new span",0
"if the predicted class changes and S- was previous tag, start a new span",0
if an existing span is ended (either by reaching O or starting a new span),0
reset for-loop variables for new span,0
remember previous tag,0
"if there is a span at end of sentence, add it",0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
print labels in CoNLL format,0
internal candidate lists of generator,0
load Zelda candidates if so passed,0
create candidate lists,0
"if lower casing is enabled, create candidate lists of lower cased versions",0
create a new dictionary for lower cased mentions,0
go through each mention and its candidates,0
"check if backoff mention already seen. If so, add candidates. Else, create new entry.",0
set lowercased version as map,0
remap state dict for models serialized with Flair <= 0.11.3,0
get the candidates,0
"during training, add the gold value as candidate",0
----- Create the internal tag dictionary -----,0
span-labels need special encoding (BIO or BIOES),0
the big question is whether the label dictionary should contain an UNK or not,0
"without UNK, we cannot evaluate on data that contains labels not seen in test",0
"with UNK, the model learns less well if there are no UNK examples",0
is this a span prediction problem?,0
----- Embeddings -----,0
----- Initial loss weights parameters -----,0
----- RNN specific parameters -----,0
----- Conditional Random Field parameters -----,0
"Previously trained models have been trained without an explicit CRF, thus it is required to check",0
whether we are loading a model from state dict in order to skip or add START and STOP token,0
----- Dropout parameters -----,0
dropouts,0
remove word dropout if there is no contact over the sequence dimension.,0
----- Model layers -----,0
----- RNN layer -----,0
"If shared RNN provided, else create one for model",0
Whether to train initial hidden state,0
final linear map to tag space,0
"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss",0
"if using CRF, we also require a CRF and a Viterbi decoder",0
"if there are no sentences, there is no loss",0
forward pass to get scores,0
calculate loss given scores and labels,0
make a zero-padded tensor for the whole sentence,0
linear map to tag space,0
"Depending on whether we are using CRF or a linear layer, scores is either:",0
"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF",0
"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer",0
spans need to be encoded as token-level predictions,0
all others are regular labels for each token,0
make sure it's a list,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
get features from forward propagation,0
remove previously predicted labels of this type,0
"if return_loss, get loss value",0
make predictions,0
add predictions to Sentence,0
BIOES-labels need to be converted to spans,0
"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)",0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
core Flair models on Huggingface ModelHub,0
"Large NER models,",0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Language-specific NER models,0
Language-specific POS models,0
English NER models,0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Danish models,0
German models,0
French models,0
Dutch models,0
Malayalam models,0
Portuguese models,0
Keyphase models,0
Biomedical models,0
check if model name is a valid local file,0
"check if model key is remapped to HF key - if so, print out information",0
get mapped name,0
use mapped name instead,0
"if not, check if model key is remapped to direct download location. If so, download model",0
special handling for the taggers by the @redewiegergabe project (TODO: move to model hub),1
"for all other cases (not local file or special download location), use HF model hub",0
"if not a local file, get from model hub",0
use model name as subfolder,0
Lazy import,0
output information,0
## Demo: How to use in Flair,0
load tagger,0
make example sentence,0
predict NER tags,0
print sentence,0
print predicted NER spans,0
iterate over entities and print,0
Lazy import,0
Save model weight,0
Determine if model card already exists,0
Generate and save model card,0
Upload files,0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
print labels in CoNLL format,0
the multi task model has several labels,0
biomedical models,0
entity linker,0
auto-spawn on GPU if available,0
remap state dict for models serialized with Flair <= 0.11.3,0
English sentiment models,0
Communicative Functions Model,0
"scores_at_targets[range(features.shape[0]), lengths.values -1]",0
Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices,0
"Initially, get scores from <start> tag to all other tags",0
"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp",0
"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep",0
Create a tensor to hold accumulated sequence scores at each current tag,0
Create a tensor to hold back-pointers,0
"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag",0
"Let pads be the <end> tag index, since that was the last tag in the decoded sequence",0
"We add scores at current timestep to scores accumulated up to previous timestep, and",0
choose the previous timestep that corresponds to the max. accumulated score for each current timestep,0
"If sentence is over, add transition to STOP-tag",0
Decode/trace best path backwards,0
Sanity check,0
remove start-tag and backscore to stop-tag,0
Max + Softmax to get confidence score for predicted label and append label to each token,0
"Transitions are used in the following way: transitions[to, from].",0
"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag",0
to START-tag and from STOP-tag to any other tag to -10000.,0
"if necessary, make batch_steps",0
break up the batch into slices of size,0
mini_batch_chunk_size,0
"if training also uses dev/train data, include in training set",0
evaluation and monitoring,0
sampling and shuffling,0
evaluation and monitoring,0
when and what to save,0
logging parameters,0
plugins,0
activate annealing plugin,0
call self.train_custom with all parameters (minus the ones specific to the AnnealingPlugin),0
training parameters,0
evaluation and monitoring,0
sampling and shuffling,0
evaluation and monitoring,0
when and what to save,0
logging parameters,0
amp,0
plugins,0
annealing logic,0
training parameters,0
evaluation and monitoring,0
sampling and shuffling,0
evaluation and monitoring,0
when and what to save,0
logging parameters,0
amp,0
plugins,0
training parameters,0
evaluation and monitoring,0
sampling and shuffling,0
evaluation and monitoring,0
when and what to save,0
logging parameters,0
amp,0
plugins,0
Create output folder,0
=== START BLOCK: ACTIVATE PLUGINS === #,0
We first activate all optional plugins. These take care of optional functionality such as various,0
logging techniques and checkpointing,0
log file plugin,0
loss file plugin,0
plugin for writing weights,0
plugin for checkpointing,0
=== END BLOCK: ACTIVATE PLUGINS === #,0
derive parameters the function was called with (or defaults),0
initialize model card with these parameters,0
Prepare training data and get dataset size,0
"determine what splits (train, dev, test) to evaluate",0
determine how to determine best model and whether to save it,0
instantiate the optimizer,0
initialize sampler if provided,0
init with default values if only class is provided,0
set dataset to sample from,0
this field stores the names of all dynamic embeddings in the model (determined after first forward pass),0
Sanity checks,0
"Sanity conversion: if flair.device was set as a string, convert to torch.device",0
-- AmpPlugin -> wraps with AMP,0
-- AnnealingPlugin -> initialize schedulers (requires instantiated optimizer),0
At any point you can hit Ctrl + C to break out of training early.,0
"- SchedulerPlugin -> load state for anneal_with_restarts, batch_growth_annealing, logic for early stopping",0
- LossFilePlugin -> get the current epoch for loss file logging,0
"if shuffle_first_epoch==False, the first epoch is not shuffled",0
log infos on training progress every `log_modulo` batches,0
process mini-batches,0
zero the gradients on the model and optimizer,0
forward and backward for batch,0
forward pass,0
identify dynamic embeddings (always deleted) on first sentence,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
do the optimizer step,0
- SchedulerPlugin -> do the scheduler step if one-cycle or linear decay,0
- WeightExtractorPlugin -> extracts weights,0
- CheckpointPlugin -> executes save_model_each_k_epochs,0
- SchedulerPlugin -> log bad epochs,0
Determine if this is the best model or if we need to anneal,0
log results,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
use DEV split to determine if this is the best model so far,0
"if not using DEV score, determine best model using train loss",0
- LossFilePlugin -> somehow prints all relevant metrics,0
- AnnealPlugin -> scheduler step,0
- SWAPlugin -> restores SGD weights from SWA,0
"if we do not use dev data for model selection, save final model",0
TensorboardLogger -> closes writer,0
test best model if test data is present,0
get and return the final test score of best model,0
MetricHistoryPlugin -> stores the loss history in return_values,0
"Store return values, as they will be erased by reset_training_attributes",0
get a random sample of training sentences,0
create a model card for this model with Flair and PyTorch version,0
record Transformers version if library is loaded,0
remember all parameters used in train() call,0
"TextDataset returns a list. valid and test are only one file,",0
so return the first element,0
cast string to Path,0
error message if the validation dataset is too small,0
Shuffle training files randomly after serially iterating,0
through corpus one,0
"iterate through training data, starting at",0
self.split (for checkpointing),0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
Backward,0
`clip_grad_norm` helps prevent the exploding gradient,0
problem in RNNs / LSTMs.,0
We detach the hidden state from how it was,0
previously produced.,0
"If we didn't, the model would try backpropagating",0
all the way to start of the dataset.,0
explicitly remove loss to clear up memory,0
#########################################################,0
Save the model if the validation loss is the best we've,0
seen so far.,0
#########################################################,0
print info,0
#########################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
"no need to check for MetricName, as __add__ of other would be called in this case",0
"This flag tracks, whether an event is currently being processed (otherwise it is added to the queue)",0
instantiate plugin,0
"Reset the flag, since an exception event might be dispatched",0
"If there is no **kw argument in the callback, check if any of the passed kw args is not accepted by",0
the callback,0
go through all attributes,0
get attribute hook events (may raise an AttributeError),0
register function as a hook,0
"Decorator was used with parentheses, but no args",0
Decorator was used with args (strings specifiying the events),0
Decorator was used without args,0
path to store the model,0
special annealing modes,0
determine the min learning rate,0
"minimize training loss if training with dev data, else maximize dev score",0
instantiate the scheduler,0
stop training if learning rate becomes too small,0
reload last best model if annealing with restarts is enabled,0
calculate warmup steps,0
skip if no optimization has happened.,0
saves the model with full vocab as checkpoints etc were created with reduced vocab.,0
TODO: check if metric is in tracked metrics,1
prepare loss logging file and set up header,0
set up all metrics to collect,0
set up headers,0
name: HEADER,0
Add all potentially relevant metrics. If a metric is not published,0
"after the first epoch (when the header is written), the column is",0
removed at that point.,0
initialize the first log line,0
record is a list of scalars,0
output log file,0
remove columns where no value was found on the first epoch (could be != 1 if training was resumed),0
make headers on epoch 1,0
write header,0
adjust alert level,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn,0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
"legacy pickle-like saving for image embeddings, as implementation details are not obvious",0
"legacy pickle-like loading for image embeddings, as implementation details are not obvious",0
"<cls> token initially set to 1/D, so it attends to all image features equally",0
add positional encodings,0
reshape the pixels into the sequence,0
layer norm after convolution and positional encodings,0
add <cls> token,0
"transformer requires input in the shape [h*w+1, b, d]",0
the output is an embedding of <cls> token,0
this parameter is fixed,0
optional fine-tuning on top of embedding layer,0
"if only one sentence is passed, convert to list of sentence",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push through RNN,0
after-RNN dropout,0
extract embeddings from RNN,0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
serialize the language models and the constructor arguments (but nothing else),0
re-initialize language model with constructor arguments,0
special handling for deserializing language models,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
IMPORTANT: add embeddings as torch modules,0
iterate over sentences,0
"if its a forward LM, take last state",0
"convert to plain strings, embedded in a list for the encode function",0
CNN,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push CNN,0
after-CNN dropout,0
extract embeddings from CNN,0
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency",0
"if only one sentence is passed, convert to list of sentence",0
Expose base classses,0
Expose document embedding classes,0
Expose image embedding classes,0
Expose legacy embedding classes,0
Expose token embedding classes,0
in some cases we need to insert zero vectors for tokens without embedding.,0
padding,0
remove special markup,0
check if special tokens exist to circumvent error message,0
iterate over subtokens and reconstruct tokens,0
remove special markup,0
check if reconstructed token is special begin token ([CLS] or similar),0
some BERT tokenizers somehow omit words - in such cases skip to next token,0
"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token.",0
if tokens are unaccounted for,0
check if all tokens were matched to subtokens,0
The layoutlm tokenizer doesn't handle ocr themselves,0
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial",0
"cannot run `.encode` if ocr boxes are required, assume",0
"transformers returns the ""added_tokens.json"" even if it doesn't create it",0
"transformers returns the ""added_tokens.json"" even if it doesn't create it",0
in case of doubt: token embedding has higher priority than document embedding,0
random check some tokens to save performance.,0
Models such as FNet do not have an attention_mask,0
set language IDs for XLM-style transformers,0
"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have",0
"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids.",0
set context if not set already,0
flair specific pre-tokenization,0
fields to store left and right context,0
expand context only if context_length is set,0
"if context_dropout is set, randomly deactivate left context during training",0
"if context_dropout is set, randomly deactivate right context during training",0
"if use_context_separator is set, add a [FLERT] token",0
return expanded sentence and context length information,0
"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first.",0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
do not print transformer warnings as these are confusing in this case,0
load tokenizer and transformer model,0
load tokenizer from inmemory zip-file,0
model name,0
embedding parameters,0
send mini-token through to check how many layers the model has,0
return length,0
"If we use a context separator, add a new special token",0
"most models have an initial BOS token, except for XLNet, T5 and GPT2",0
"when initializing, embeddings are in eval mode by default",0
in case of doubt: token embedding has higher priority than document embedding,0
in case of doubt: token embedding has higher priority than document embedding,0
legacy TransformerDocumentEmbedding,0
legacy TransformerTokenEmbedding,0
legacy Flair <= 0.12,0
legacy Flair <= 0.7,0
legacy TransformerTokenEmbedding,0
Legacy TransformerDocumentEmbedding,0
legacy TransformerTokenEmbedding,0
legacy TransformerDocumentEmbedding,0
some models like the tars model somehow lost this information.,0
copy values from new embedding,0
those parameters are only from the super class and will be recreated in the constructor.,0
cls first pooling can be done without recreating sentence hidden states,0
make the tuple a tensor; makes working with it easier.,0
"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention",0
only use layers that will be outputted,0
this parameter is fixed,0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
make compatible with serialized models,0
gensim version 4,0
gensim version 3,0
"if no embedding is set, the vocab and embedding length is requried",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNINOS embeddings,0
pubmed embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
"this is required to force the module on the cpu,",0
"if a parent module is put to gpu, the _apply is called to each sub_module",0
self.to(..) actually sets the device properly,0
this ignores the get_cached_vec method when loading older versions,0
it is needed for compatibility reasons,0
gensim version 4,0
gensim version 3,0
"when loading the old versions from pickle, the embeddings might not be added as pytorch module.",0
"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might",0
lead to issues while loading (trying to load weights that weren't stored as python weights and therefore,0
not finding them),0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
multilingual models,0
English models,0
Arabic,0
Bulgarian,0
Czech,0
Danish,0
German,0
Spanish,0
Basque,0
Persian,0
Finnish,0
French,0
Hebrew,0
Hindi,0
Croatian,0
Indonesian,0
Italian,0
Japanese,0
Malayalam,0
Dutch,0
Norwegian,0
Polish,0
Portuguese,0
Pubmed,0
Slovenian,0
Swedish,0
Tamil,0
Spanish clinical,0
CLEF HIPE Shared task,0
Amharic,0
Ukrainian,0
load model if in pretrained model map,0
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir),0
CLEF HIPE models are lowercased,0
embeddings are static if we don't do finetuning,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout",0
gradients are enable if fine-tuning is enabled,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
offset mode that extracts at whitespace after last character,0
offset mode that extracts at last character,0
make compatible with old models,0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
check token.text is empty or not,0
set aggregation operation,0
add embeddings after updating,0
model architecture,0
model architecture,0
"""pl"",",0
download if necessary,0
load the model,0
"TODO: keep for backwards compatibility, but remove in future",1
save the sentence piece model as binary file (not as path which may change),0
write out the binary sentence piece model into the expected directory,0
"if the model was saved as binary and it is not found on disk, write to appropriate path",0
"otherwise, use normal process and potentially trigger another download",0
"once the modes if there, load it with sentence piece",0
empty words get no embedding,0
all other words get embedded,0
GLOVE embeddings,0
no need to recreate as NILCEmbeddings,0
read in test file if exists,0
read in dev file if exists,0
"find train, dev and test files if not specified",0
Add tags for each annotated span,0
Remove leading and trailing whitespaces from annotated spans,0
Search start and end token index for current span,0
If end index is not found set to last token,0
Throw error if indices are not valid,0
Add metadatas for sentence,0
Currently all Jsonl Datasets are stored in Memory,0
get train data,0
read in test file if exists,0
read in dev file if exists,0
"find train, dev and test files if not specified",0
special key for space after,0
special key for feature columns,0
special key for dependency head id,0
"store either Sentence objects in memory, or only file offsets",0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
determine encoding of text file,0
identify which columns are spans and which are word-level,0
now load all sentences,0
skip first line if to selected,0
option 1: keep Sentence objects in memory,0
pointer to previous,0
parse next sentence,0
quit if last sentence reached,0
skip banned sentences,0
set previous and next sentence for context,0
append parsed sentence to list in memory,0
option 2: keep source data in memory,0
"read lines for next sentence, but don't parse",0
quit if last sentence reached,0
append raw lines for each sentence,0
we make a distinction between word-level tags and span-level tags,0
read first sentence to determine which columns are span-labels,0
skip first line if to selected,0
check the first 5 sentences,0
go through all annotations and identify word- and span-level annotations,0
- if a column has at least one BIES we know it's a Span label,0
"- if a column has at least one tag that is not BIOES, we know it's a Token label",0
- problem cases are columns for which we see only O - in this case we default to Span,0
skip assigned columns,0
the space after key is always word-levels,0
"if at least one token has a BIES, we know it's a span label",0
"if at least one token has a label other than BIOES, we know it's a token label",0
all remaining columns that are not word-level are span-level,0
for column in self.word_level_tag_columns:,0
"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")",0
"if sentence ends, break",0
parse comments if possible,0
"otherwise, this line is a token. parse and add to sentence",0
check if this sentence is a document boundary,0
add span labels,0
discard tags from tokens that are not added to the sentence,0
parse relations if they are set,0
head and tail span indices are 1-indexed and end index is inclusive,0
parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas',0
"to set the metadata ""domain"" to ""de-orcas""",0
get fields from line,0
get head_id if exists (only in dependency parses),0
initialize token,0
go through all columns,0
'feats' and 'misc' column should be split into different fields,0
special handling for whitespace after,0
add each other feature as label-value pair,0
get the task name (e.g. 'ner'),0
get the label value,0
add label,0
remap regular tag names,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
set sentence context using partials TODO: pointer to dataset is really inefficient,1
use all domains,0
iter over all domains / sources and create target files,0
The conll representation of coref spans allows spans to,0
"overlap. If spans end or begin at the same word, they are",0
"separated by a ""|"".",0
The span begins at this word.,0
The span begins and ends at this word (single word span).,0
"The span is starting, so we record the index of the word.",0
"The span for this id is ending, but didn't start at this word.",0
Retrieve the start index from the document state and,0
add the span to the clusters for this id.,0
strip all bracketing information to,0
get the actual propbank label.,0
Entering into a span for a particular semantic role label.,0
We append the label and set the current span for this annotation.,0
"If there's no '(' token, but the current_span_label is not None,",0
then we are inside a span.,0
We're outside a span.,0
"Exiting a span, so we reset the current span label for this annotation.",0
The words in the sentence.,0
The pos tags of the words in the sentence.,0
the pieces of the parse tree.,0
The lemmatised form of the words in the sentence which,0
have SRL or word sense information.,0
The FrameNet ID of the predicate.,0
"The sense of the word, if available.",0
"The current speaker, if available.",0
"Cluster id -> List of (start_index, end_index) spans.",0
Cluster id -> List of start_indices which are open for this id.,0
Replace brackets in text and pos tags,0
with a different token for parse trees.,0
only keep ')' if there are nested brackets with nothing in them.,0
There are some bad annotations in the CONLL data.,0
"They contain no information, so to make this explicit,",0
we just set the parse piece to be None which will result,0
in the overall parse tree being None.,0
"If this is the first word in the sentence, create",0
empty lists to collect the NER and SRL BIO labels.,0
"We can't do this upfront, because we don't know how many",0
"components we are collecting, as a sentence can have",0
variable numbers of SRL frames.,0
Create variables representing the current label for each label,0
sequence we are collecting.,0
"If any annotation marks this word as a verb predicate,",0
we need to record its index. This also has the side effect,0
of ordering the verbal predicates by their location in the,0
"sentence, automatically aligning them with the annotations.",0
"this would not be reached if parse_pieces contained None, hence the cast",0
Non-empty line. Collect the annotation.,0
Collect any stragglers or files which might not,0
have the '#end document' format for the end of the file.,0
this dataset name,0
check if data there,0
column format,0
this dataset name,0
check if data there,0
column format,0
this dataset name,0
download data if necessary,0
download files if not present locally,0
we need to slightly modify the original files by adding some new lines after document separators,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
Remove CoNLL-U meta information in the last column,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
entity_mapping,0
this dataset name,0
download data if necessary,0
data validation,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download files if not present locallys,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
# download zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
download data if necessary,0
"unzip the downloaded repo and merge the train, dev and test datasets",0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
check if data there,0
create folder,0
download dataset,0
column format,0
this dataset name,0
download and parse data if necessary,0
create train test dev if not exist,0
column format,0
this dataset name,0
If the extracted corpus file is not yet present in dir,0
download zip if necessary,0
"extracted corpus is not present , so unpacking it.",0
column format,0
this dataset name,0
download zip,0
unpacking the zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)",0
download files if not present locally,0
we need to modify the original files by adding new lines after after the end of each sentence,0
if only one language is given,0
column format,0
this dataset name,0
"use all languages if explicitly set to ""all""",0
download data if necessary,0
initialize comlumncorpus and add it to list,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
this list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
get google drive id from list,0
download from google drive,0
unzip,0
"tar.extractall(language_folder,members=[tar.getmember(file_name)])",0
transform data into required format,0
"the processed dataset has the additional ending ""_new""",0
remove the unprocessed dataset,0
initialize comlumncorpus and add it to list,0
if no languages are given as argument all languages used in XTREME will be loaded,0
if only one language is given,0
column format,0
this dataset name,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
This list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
download from HU Server,0
unzip,0
transform data into required format,0
initialize comlumncorpus and add it to list,0
if only one language is given,0
column format,0
this dataset name,0
download data if necessary,0
initialize comlumncorpus and add it to list,0
download data if necessary,0
unpack and write out in CoNLL column-like format,0
column format,0
this dataset name,0
download data if necessary,0
data is not in IOB2 format. Thus we transform it to IOB2,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
column format,0
this dataset name,0
rename according to train - test - dev - convention,0
column format,0
this dataset name,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
Add missing newline after header,0
Workaround for empty tokens,1
"Add ""real"" document marker",0
Dataset split mapping,0
v2.0 only adds new language and splits for AJMC dataset,0
Special document marker for sample splits in AJMC dataset,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
if only one language is given,0
column format,0
this dataset name,0
"use all languages if explicitly set to ""all""",0
download data if necessary,0
initialize comlumncorpus and add it to list,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
paths to train and test splits,0
init corpus,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
iterate over all html files,0
"get rid of html syntax, we only need the text",0
between all documents we write a separator symbol,0
skip empty strings,0
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)",0
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention",0
sentence splitting and tokenization,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out-file in column format,0
"in case something goes wrong, delete the dataset and raise error",0
this dataset name,0
download and parse data if necessary,0
from qwikidata.linked_data_interface import get_entity_dict_from_api,0
generate qid wikiname dictionaries,0
merge dictionaries,0
ignore first line,0
commented and empty lines,0
read all Q-IDs,0
ignore first line,0
request,0
this dataset name,0
we use the wikiids in the data instead of directly utilizing the wikipedia urls.,0
like this we can quickly check if the corresponding page exists,0
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi,0
delete unprocessed file,0
collect all wikiids,0
create the dictionary,0
request,0
this dataset name,0
names of raw text documents,0
open output_file,0
iterate through all documents,0
split sentences and tokenize,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out file,0
annotation from one annotator or two agreeing annotators,0
this dataset name,0
download and parse data if necessary,0
this dataset name,0
download and parse data if necessary,0
First parse the post titles,0
Keep track of how many and which entity mentions does a given post title have,0
Check if the current post title has an entity link and parse accordingly,0
Post titles with entity mentions (if any) are handled via this function,0
Then parse the comments,0
"Iterate over the comments.tsv file, until the end is reached",0
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched.",0
Each comment thread is handled as one 'document'.,0
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.,0
This if-condition is needed to handle this problem.,0
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure",0
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above",0
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle.",0
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,",0
and not just single letters into single rows.,0
If there are annotated entity mentions for given post title or a comment thread,0
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence",0
Write the token with a corresponding tag to file,0
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed",0
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag",0
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized,0
"incorrectly, in order to keep the desired format (empty line as a sentence separator).",0
"Thrown when the second check above happens, but the last token of a sentence is reached.",0
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below.",0
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS",0
Check if further annotations belong to the current post title or comment thread as well,0
Stop when the end of an annotation file is reached,0
Check if further annotations belong to the current sentence as well,0
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)",0
Docstart,0
if there is more than one word in the chunk we write each in a separate line,0
print(chunks),0
empty line after each sentence,0
convert the file to CoNLL,0
this dataset name,0
"check if data there, if not, download the data",0
create folder,0
download data,0
transform data into column format if necessary,0
if no filenames are specified we use all the data,0
"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled",0
also we remove 'raganato_ALL' from filenames in case its in the list,0
generate the test file,0
make column file and save to data_folder,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
this dataset name,0
download data if necessary,0
if True:,0
write CoNLL-U Plus header,0
"Some special cases (e.g., missing spaces before entity marker)",0
necessary if text should be whitespace tokenizeable,0
Handle case where tail may occur before the head,0
this dataset name,0
write CoNLL-U Plus header,0
this dataset name,0
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors,1
download data if necessary,0
write CoNLL-U Plus header,0
The span has ended.,0
We are entering a new span; reset indices,0
and active tag to new span.,0
We're inside a span.,0
Last token might have been a part of a valid span.,0
this dataset name,0
write CoNLL-U Plus header,0
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):",0
"with zip_file.open(source_file_path, mode=""r"") as source_file:",0
target_file_path = Path(data_folder) / target_filename,0
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:",0
# write CoNLL-U Plus header,0
"target_file.write(""# global.columns = id form ner\n"")",0
for example in json.load(source_file):,0
token_list = self._tacred_example_to_token_list(example),0
target_file.write(token_list.serialize()),0
check if first tag row is already occupied,0
"if first tag row is occupied, use second tag row",0
hardcoded mapping TODO: perhaps find nicer solution,1
remap regular tag names,0
else skip to position in file where sentence begins,0
set sentence context using partials TODO: pointer to dataset is really inefficient,1
read in dev file if exists,0
read in test file if exists,0
the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44,0
"find train, dev and test files if not specified",0
use test_file to create test split if available,0
use dev_file to create test split if available,0
"if data point contains black-listed label, do not use",0
first check if valid sentence,0
"if so, add to indices",0
"find train, dev and test files if not specified",0
variables,0
different handling of in_memory data than streaming data,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
test if format is OK,0
test if at least one label given,0
make sentence from text (and filter for length),0
"if a pair column is defined, make a sentence pair object",0
noinspection PyDefaultArgument,0
dataset name includes the split size,0
default dataset folder is the cache root,0
download data if necessary,0
download each of the 28 splits,0
create dataset directory if necessary,0
download senteval datasets if necessary und unzip,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
handle labels file,0
handle data file,0
Create flair compatible labels,0
"by default, map point score to POSITIVE / NEGATIVE values",0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file from CSV,0
create test.txt file from CSV,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create train dev and test files in fasttext format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
convert to FastText format,0
download data if necessary,0
"if data is not downloaded yet, download it",0
get the zip file,0
move original .tsv files to another folder,0
create train and dev splits in fasttext format,0
create eval_dataset file with no labels,0
download zip archive,0
unpack file in datasets directory (zip archive contains a directory named SST-2),0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download datasets if necessary,0
create dataset directory if necessary,0
create correctly formated txt files,0
multiple labels are possible,0
this dataset name,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
create a separate directory for different tasks,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
check if dataset is supported,0
set file names,0
set file names,0
download and unzip in file structure if necessary,0
instantiate corpus,0
"find train, dev and test files if not specified",0
"create DataPairDataset for train, test and dev file, if they are given",0
stop if file does not exist,0
create a DataPair object from strings,0
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"reorder dev datasets to have same columns as in train set: 8, 9, and 11",0
dev sets include 5 different annotations but we will only keep the gold label,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get test and dev sets,0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data not downloaded yet, download it",0
get the zip file,0
"the downloaded files have json format, we transform them to tsv",0
Function to transform JSON file to tsv for Recognizing Textual Entailment Data,0
remove json file,0
Uses dynamic programming approach to calculate maximum independent set in interval graph,0
with sum of all entity lengths as secondary key,0
calculate offset without current text,0
because we stick all passages of a document together,0
TODO For split entities we also annotate everything inbetween which might be a bad idea?,1
Try to fix incorrect annotations,0
print(,0
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}""",0
),0
Ignore empty lines or relation annotations,0
FIX annotation of whitespaces (necessary for PDR),0
One token may contain multiple entities -> deque all of them,0
column format,0
this dataset name,0
Create tokenization-dependent CONLL files. This is necessary to prevent,0
from caching issues (e.g. loading the same corpus with different sentence splitters),0
column format,0
this dataset name,0
column format,0
this dataset name,0
Edge case: last token starts a new entity,0
Last document in file,0
column format,0
this dataset name,0
column format,0
this dataset name,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
Edge case: last token starts a new entity,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
column format,0
this dataset name,0
Read texts,0
Read annotations,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
We need to apply a patch to correct the original training file,0
Articles title,0
Article abstract,0
Entity annotations,0
column format,0
this dataset name,0
Edge case: last token starts a new entity,0
Map all entities to chemicals,0
Map all entities to disease,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
Incomplete article,0
Invalid XML syntax,0
column format,0
this dataset name,0
column format,0
this dataset name,0
if len(mid) != 3:,0
continue,0
Try to fix entity offsets,0
column format,0
this dataset name,0
There is still one illegal annotation in the file ..,0
column format,0
this dataset name,0
"Abstract first, title second to prevent issues with sentence splitting",0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
column format,0
this dataset name,0
"Filter for specific entity types, by default no entities will be filtered",0
Get original HUNER splits to retrieve a list of all document ids contained in V2,0
train and dev split of V2 will be train in V4,0
test split of V2 will be dev in V4,0
New documents in V4 will become test documents,0
column format,0
this dataset name,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
build dataset name and full huggingface reference name,0
Download data if necessary,0
"Some datasets in BigBio only have train or test splits, not both",0
"If only test split, assign it to train split",0
"If only train split, sample other from it (sample_missing_splits=True)",0
Not every dataset has a dev / validation set!,0
Perform type mapping if necessary,0
"Collect all texts of the document, each passage will be",0
a text in our internal format,0
Sort passages by start offset,0
Transform all entity annotations into internal format,0
Find the passage of the entity (necessary for offset adaption),0
Adapt entity offsets according to passage offsets,0
FIXME: This is just for debugging purposes,1
passage_text = id_to_text[passage_id],0
doc_text = passage_text[entity_offset[0] : entity_offset[1]],0
"mention_text = entity[""text""][0]",0
if doc_text != mention_text:,0
"print(f""Annotation error ({document['document_id']}) - Doc: {doc_text} vs. Mention: {mention_text}"")",0
Check base case,0
Get element in the middle,0
Is the mention with the passage offsets?,0
"If element is smaller than mid, then it can only",0
be present in left subarray,0
Else the element can only be present in right subarray,0
Special case for ProGene: We need to use the split_0_train and split_0_test splits,0
as they are currently provided in BigBio,0
cache Feidegger config file,0
cache Feidegger images,0
replace image URL with local cached file,0
append Sentence-Image data point,0
cast to list if necessary,0
cast to list if necessary,0
"first, check if pymongo is installed",0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
Expose base classses,0
Expose all biomedical data sets used for the evaluation of BioBERT,0
-,0
-,0
-,0
-,0
Expose all biomedical data sets using the HUNER splits,0
Expose all biomedical data sets,0
Expose all document classification datasets,0
word sense disambiguation,0
Expose all entity linking datasets,0
Expose all relation extraction datasets,0
universal proposition banks,0
keyphrase detection datasets,0
other NER datasets,0
standard NER datasets,0
Expose all sequence labeling datasets,0
Expose all text-image datasets,0
Expose all text-text datasets,0
Expose all treebanks,0
"find train, dev and test files if not specified",0
get train data,0
get test data,0
get dev data,0
option 1: read only sentence boundaries as offset positions,0
option 2: keep everything in memory,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
current token ID,0
handling for the awful UD multiword format,0
end of sentence,0
comments or ellipsis,0
if token is a multi-word,0
normal single-word tokens,0
"if we don't split multiwords, skip over component words",0
add token,0
add morphological tags,0
derive whitespace logic for multiwords,0
print(token),0
print(current_multiword_last_token),0
print(current_multiword_first_token),0
"if multi-word equals component tokens, there should be no whitespace",0
go through all tokens in subword and set whitespace_after information,0
print(i),0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
this dataset name,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"finally, print model card for information",0
noqa: INP001,0
-- Project information -----------------------------------------------------,0
"The full version, including alpha/beta/rc tags",0
use smv_current_version as the git url,0
-- General configuration ---------------------------------------------------,0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
Napoleon settings,0
Whitelist pattern for tags (set to None to ignore all tags),0
Whitelist pattern for branches (set to None to ignore all branches),0
Whitelist pattern for remotes (set to None to use local branches only),0
Pattern for released versions,0
Format for versioned output directories inside the build directory,0
Determines whether remote or local git branches/tags are preferred if their output dirs conflict,0
test corpus,0
create a TARS classifier,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
test corpus,0
create a TARS classifier,0
switch to a new task (TARS can do multiple tasks so you must define one),0
initialize the text classifier trainer,0
start the training,0
"With end symbol, without start symbol, padding in front",0
"Without end symbol, with start symbol, padding in back",0
"Without end symbol, without start symbol, padding in front",0
initialize trainer,0
initialize trainer,0
initialize trainer,0
increment for last token in sentence if not followed by whitespace,0
clean up directory,0
clean up directory,0
example sentence,0
set 4 labels for 2 tokens ('love' is tagged twice),0
check if there are three POS labels with correct text and values,0
check if there are is one SENTIMENT label with correct text and values,0
check if all tokens are correctly labeled,0
remove the pos label from the last word,0
there should be 2 POS labels left,0
now remove all pos tags,0
set 3 labels for 2 spans (HU is tagged twice),0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
"now delete the NER tags of ""Humboldt-Universitt zu Berlin""",0
should be only one NER label left,0
and only one NER span,0
set 3 labels for 2 spans (HU is tagged twice with different tags),0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
"now delete the NER tags of ""Humboldt-Universitt zu Berlin""",0
should be only one NER label left,0
and only one NER span,0
but there is also one orgtype span and label,0
and only one NER span,0
let's add the NER tag back,0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
now remove all NER tags,0
set 3 labels for 2 spans (HU is tagged twice with different tags),0
create two relation label,0
there should be two relation labels,0
there should be one syntactic labels,0
"there should be two relations, one with two and one with one label",0
example sentence,0
add another topic label,0
example sentence,0
has sentiment value,0
has 4 part of speech tags,0
has 1 NER tag,0
should be in total 6 labels,0
example sentence,0
add two NER labels,0
get the four labels,0
check that only two of the respective data points are equal,0
make a sentence and some right context,0
TODO: is this desirable? Or should two sentences with same text be considered same objects?,1
Initializing a Sentence this way assumes that there is a space after each token,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
load column dataset with one entry,0
load column dataset with two entries,0
load column dataset with three entries,0
"get training, test and dev data",0
"get training, test and dev data",0
check if Token labels are correct,0
"get training, test and dev data",0
check if Token labels for frames are correct,0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
"assert [token.get_tag(""head"").value for token in sent1.tokens] == [",0
"""2"",",0
"""0"",",0
"""4"",",0
"""2"",",0
"""2"",",0
"""2"",",0
],0
This test only covers basic universal dependencies datasets.,0
"For example, multi-word tokens or the ""deps"" column sentence annotations are not supported yet.",0
"Here, we use the default token annotation fields.",0
This test covers the complete HIPE 2022 dataset.,0
https://github.com/hipe-eval/HIPE-2022-data,0
"Includes variant with document separator, and all versions of the dataset.",0
"We have manually checked, that these numbers are correct:",0
"+1 offset, because of missing EOS marker at EOD",0
Test data for v2.1 release,0
This test covers the complete ICDAR Europeana corpus:,0
https://github.com/stefan-it/historic-domain-adaptation-icdar,0
"This test covers the complete MasakhaNER dataset, including support for v1 and v2.",0
This test covers the NERMuD dataset. Official stats can be found here:,0
https://github.com/dhfbk/KIND/tree/main/evalita-2023,0
Number of instances per dataset split are taken from https://huggingface.co/datasets/elenanereiss/german-ler,0
This test covers the complete MasakhaPOS dataset.,0
"See MobIE paper (https://aclanthology.org/2021.konvens-1.22/), table 2",0
--- Embeddings that are shared by both models --- #,0
--- Task 1: Sentiment Analysis (5-class) --- #,0
Define corpus and model,0
-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #,0
Define corpus and model,0
-- Define mapping (which tagger should train on which model) -- #,0
-- Create model trainer and train -- #,0
clean up file,0
no need for label_dict,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
Intel ----founded_by---> Gordon Moore,0
Intel ----founded_by---> Robert Noyce,0
"Ground truth is a set of tuples of (<Sentence Text>, <Relation Label Values>)",0
Check sentence masking and relation label annotation on,0
"training, validation and test dataset (in this test the splits are the same)",0
"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google .""",0
"Entity pair permutations of: ""Microsoft was founded by Bill Gates .""",0
"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 .""",0
"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany.""",0
This sentence is only included if we transform the corpus with cross augmentation,0
Ensure this is an example that predicts no classes in multilabel,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
ensure that the prepared tensors is what we expect,0
use a SequenceTagger to save and reload the embedding in the manner it is supposed to work,0
previous and next sentence as context,0
test expansion for sentence without context,0
test expansion for with previous and next as context,0
test expansion if first sentence is document boundary,0
test expansion if we don't use context,0
"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher.",0
dummy model with embeddings,0
save the dummy and load it again,0
check that context_length and use_context_separator is the same for both,0
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
Extract all the contents of zip file in current directory,0
Extract all the contents of zip file in current directory,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
load_big_file is a workaround byhttps://github.com/highway11git,1
to load models on some Mac/Windows setups,0
see https://github.com/zalandoresearch/flair/issues/351,0
first determine the distribution of classes in the dataset,0
weight for each sample,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
increment for last token in sentence if not followed by whitespace,0
this is the default init size of a lmdb database for embeddings,0
get db filename from embedding name,0
"In case initialization of cached version failed, just fallback to the original WordEmbeddings",0
SequenceTagger,0
TextClassifier,0
get db filename from embedding name,0
if embedding database already exists,0
"otherwise, push embedding to database",0
if embedding database already exists,0
open the database in read mode,0
we need to set self.k,0
create and load the database in write mode,0
"no idea why, but we need to close and reopen the environment to avoid",0
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot,0
when opening new transaction !,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
set 'add_unk' if the dictionary was created with a version of Flair older than 0.9,0
set 'add_unk' depending on whether <unk> is a key,0
"if one embedding name, directly return it",0
"if multiple embedding names, concatenate them",0
First we remove any existing labels for this PartOfSentence in self.sentence,0
labels also need to be deleted at Sentence object,0
delete labels at object itself,0
"check if the span already exists. If so, return it",0
else make a new span,0
"check if the relation already exists. If so, return it",0
else make a new relation,0
private field for all known spans,0
the tokenizer used for this sentence,0
some sentences represent a document boundary (but most do not),0
internal variables to denote position inside dataset,0
"if text is passed, instantiate sentence with tokens (words)",0
determine token positions and whitespace_after flag,0
the last token has no whitespace after,0
log a warning if the dataset is empty,0
data with zero-width characters cannot be handled,0
set token idx and sentence,0
append token to sentence,0
register token annotations on sentence,0
move sentence embeddings to device,0
also move token embeddings to device,0
clear token embeddings,0
infer whitespace after field,0
"if sentence has no tokens, return empty string",0
"otherwise, return concatenation of tokens with the correct offsets",0
The sentence's start position is not propagated to its tokens.,0
"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces.",0
No character at the corresponding code point: remove it,0
"if no label if specified, return all labels",0
"if the label type exists in the Sentence, return it",0
return empty list if none of the above,0
labels also need to be deleted at all tokens,0
labels also need to be deleted at all known spans,0
remove spans without labels,0
delete labels at object itself,0
set name,0
abort if no data is provided,0
sample test data from train if none is provided,0
sample dev data from train if none is provided,0
set train dev and test data,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
count all label types per sentence,0
go through all labels of label_type and count values,0
check if there are any span labels,0
"if an unk threshold is set, UNK all label values below this threshold",0
sample randomly from a label distribution according to the probabilities defined by the noise transition matrix,0
replace the old label with the new one,0
keep track of the old (clean) label using another label type category,0
keep track of how many labels in total are flipped,0
sample randomly from a label distribution according to the probabilities defined by the desired noise share,0
replace the old label with the new one,0
keep track of the old (clean) label using another label type category,0
keep track of how many labels in total are flipped,0
Make the tag dictionary,0
"add a dummy ""O"" to close final prediction",0
return complex list,0
internal variables,0
non-set tags are OUT tags,0
anything that is not OUT is IN,0
does this prediction start a new span?,0
begin and single tags start new spans,0
"in IOB format, an I tag starts a span if it follows an O or is a different span",0
single tags that change prediction start new spans,0
if an existing span is ended (either by reaching O or starting a new span),0
determine score and value,0
append to result list,0
reset for-loop variables for new span,0
remember previous tag,0
global variable: cache_root,0
global variable: device,0
"No need for correctness checks, torch is doing it",0
global variable: version,0
global variable: arrow symbol,0
dummy return to fulfill trainer.train() needs,0
print(vec),0
Attach optimizer,0
"convert `metrics` to float, in case it's a zero-dim Tensor",0
if memory mode option 'none' delete everything,0
"if dynamic embedding keys not passed, identify them automatically",0
always delete dynamic embeddings,0
"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)",0
optional metric space decoder if prototypes have different length than embedding,0
create initial prototypes for all classes (all initial prototypes are a vector of all 1s),0
"if set, create initial prototypes from normal distribution",0
"if set, use a radius",0
all parameters will be pushed internally to the specified device,0
decode embeddings into prototype space,0
"if unlabeled distance is set, mask out loss to unlabeled class prototype",0
Always include the name of the Model class for which the state dict holds,0
"in Flair <0.9.1, optimizer and scheduler used to train model are not saved",0
"write out a ""model card"" if one is set",0
special handling for optimizer:,0
remember optimizer class and state dictionary,0
save model,0
restore optimizer and scheduler to model card if set,0
"if this class is abstract, go through all inheriting classes and try to fetch and load the model",0
get all non-abstract subclasses,0
"try to fetch the model for each subclass. if fetching is possible, load model and return it",0
"skip any invalid loadings, e.g. not found on huggingface hub",0
"if the model cannot be fetched, load as a file",0
try to get model class from state,0
"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses",0
"if str(model_cls) == ""<class 'flair.models.pairwise_classification_model.TextPairClassifier'>"": continue",0
"skip any invalid loadings, e.g. not found on huggingface hub",0
"if this class is not abstract, fetch the model and load it",0
"make sure <unk> is contained in gold_label_dictionary, if given",0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
loss calculation,0
variables for printing,0
variables for computing scores,0
remove any previously predicted labels,0
predict for batch,0
get the gold labels,0
add to all_predicted_values,0
make printout lines,0
convert true and predicted values to two span-aligned lists,0
delete exluded labels if exclude_labels is given,0
"if after excluding labels, no label is left, ignore the datapoint",0
write all_predicted_values to out_file if set,0
make the evaluation dictionary,0
check if this is a multi-label problem,0
compute numbers by formatting true and predicted such that Scikit-Learn can use them,0
multi-label problems require a multi-hot vector for each true and predicted label,0
single-label problems can do with a single index for each true and predicted label,0
"now, calculate evaluation numbers",0
there is at least one gold label or one prediction (default),0
"if there is only one label, then ""micro avg"" = ""macro avg""",0
"micro average is only computed if zero-label exists (for instance ""O"")",0
if no zero-label exists (such as in POS tagging) micro average is equal to accuracy,0
same for the main score,0
issue error and default all evaluation numbers to 0.,0
line for log file,0
check if there is a label mismatch,0
print info,0
set the embeddings,0
initialize the label dictionary,0
initialize the decoder,0
set up multi-label logic,0
init dropouts,0
loss weights and loss function,0
Initialize the weight tensor,0
set up gradient reversal if so specified,0
embed sentences,0
get a tensor of data points,0
do dropout,0
make a forward pass to produce embedded data points and labels,0
get the data points for which to predict labels,0
get their gold labels as a tensor,0
pass data points through network to get encoded data point tensor,0
decode,0
an optional masking step (no masking in most cases),0
calculate the loss,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
filter data points in batch,0
stop if all sentences are empty,0
pass data points through network and decode,0
if anything could possibly be predicted,0
remove previously predicted labels of this type,0
add DefaultClassifier arguments,0
add variables of DefaultClassifier,0
Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23,0
Get projected 1st dimension,0
Compute bilinear form,0
Arcosh,0
Project the input data to n+1 dimensions,0
"The first dimension, is recomputed in the distance module",0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for",0
"train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
plot i,0
save plots,0
save plots,0
plt.show(),0
save plot,0
take the average over the last three scores of training,0
take average over the scores from the different training runs,0
auto-spawn on GPU if available,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
TODO: not saving lines yet,1
TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.,1
- MaskedRelationClassifier ?,0
This depends if this relation classification architecture should replace or offer as an alternative.,0
Set label type and prepare label dictionary,0
Initialize super default classifier,0
Add the special tokens from the encoding strategy,0
"Auto-spawn on GPU, if available",0
Only use entities labelled with the specified labels for each label type,0
Only use entities above the specified threshold,0
Use a dictionary to find gold relation annotations for a given entity pair,0
Yield head and tail entity pairs from the cross product of all entities,0
Remove identity relation entity pairs,0
Remove entity pairs with labels that do not match any,0
of the specified relations in `self.entity_pair_labels`,0
"Obtain gold label, if existing",0
Some sanity checks,0
Pre-compute non-leading head and tail tokens for entity masking,0
We can not use the plaintext of the head/tail span in the sentence as the mask/marker,0
since there may be multiple occurrences of the same entity mentioned in the sentence.,0
"Therefore, we use the span's position in the sentence.",0
Create masked sentence,0
Add gold relation annotation as sentence label,0
"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,",0
"during prediction, the forward pass does not need any knowledge about the entities in the sentence.",0
"If we sample missing splits, the encoded sentences that correspond to the same original sentences",0
"may get distributed into different splits. For training purposes, this is always undesired.",0
Ensure that all sentences are encoded properly,0
Deal with the case where all sentences are encoded sentences,0
"mypy does not infer the type of ""sentences"" restricted by the if statement",0
Deal with the case where all sentences are standard (non-encoded) sentences,0
"For each encoded sentence, transfer its prediction onto the original relation",0
auto-spawn on GPU if available,0
pad strings with whitespaces to longest sentence,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
"""document_delimiter"" property may be missing in some older pre-trained models",0
serialize the language models and the constructor arguments (but nothing else),0
special handling for deserializing language models,0
re-initialize language model with constructor arguments,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
Transform input data into TARS format,0
"if there are no labels, return a random sample as negatives",0
"otherwise, go through all labels",0
make sure the probabilities always sum up to 1,0
get and embed all labels by making a Sentence object that contains only the label text,0
get each label embedding and scale between 0 and 1,0
compute similarity matrix,0
"the higher the similarity, the greater the chance that a label is",0
sampled as negative example,0
make label dictionary if no Dictionary object is passed,0
prepare dictionary of tags (without B- I- prefixes and without UNK),0
check if candidate_label_set is empty,0
make list if only one candidate label is passed,0
create label dictionary,0
note current task,0
create a temporary task,0
make zero shot predictions,0
switch to the pre-existing task,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
make a tars sentence where all labels are O by default,0
init new TARS classifier,0
set all task information,0
return,0
with torch.no_grad():,0
progress bar for verbosity,0
stop if all sentences are empty,0
go through each sentence in the batch,0
always remove tags first,0
get the span and its label,0
determine whether tokens in this span already have a label,0
only add if all tokens have no label,0
make and add a corresponding predicted span,0
set indices so that no token can be tagged twice,0
clearing token embeddings to save memory,0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
get the serialized embeddings,0
remap state dict for models serialized with Flair <= 0.11.3,0
init new TARS classifier,0
set all task information,0
with torch.no_grad():,0
progress bar for verbosity,0
stop if all sentences are empty,0
go through each sentence in the batch,0
always remove tags first,0
add all labels that according to TARS match the text and are above threshold,0
do not add labels below confidence threshold,0
only use label with highest confidence if enforcing single-label predictions,0
get all label scores and do an argmax to get the best label,0
remove previously added labels and only add the best label,0
clearing token embeddings to save memory,0
set separator to concatenate two sentences,0
auto-spawn on GPU if available,0
pooling operation to get embeddings for entites,0
set embeddings,0
set relation and entity label types,0
"whether to use gold entity pairs, and whether to filter entity pairs by type",0
filter entity pairs according to their tags if set,0
whether to encode characters and whether to use attention (attention can only be used if chars are encoded),0
character dictionary for decoding and encoding,0
make sure <unk> is in dictionary for handling of unknown characters,0
add special symbols to dictionary if necessary and save respective indices,0
---- ENCODER ----,0
encoder character embeddings,0
encoder pre-trained embeddings,0
encoder RNN,0
additional encoder linear layer if bidirectional encoding,0
---- DECODER ----,0
decoder: linear layers to transform vectors to and from alphabet_size,0
when using attention we concatenate attention outcome and decoder hidden states,0
decoder RNN,0
loss and softmax,0
self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction,0
add additional columns for special symbols if necessary,0
initialize with dummy symbols,0
encode inputs,0
get labels (we assume each token has a lemma label),0
get char indices for labels of sentence,0
"(batch_size, max_sequence_length) batch_size = #words in sentence,",0
max_sequence_length = length of longest label of sentence + 1,0
get char embeddings,0
"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size",0
take decoder input and initial hidden and pass through RNN,0
"if all encoder outputs are provided, use attention",0
take convex combinations of encoder hidden states as new output using the computed attention coefficients,0
"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)",0
get all tokens,0
encode input characters by sending them through RNN,0
get one-hots for characters and add special symbols / padding,0
determine length of each token,0
embed sentences,0
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)",0
variable to store initial hidden states for decoder,0
encode input characters by sending them through RNN,0
test packing and padding,0
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder,0
concatenate the final hidden states of the encoder. These will be projected to hidden_size of,0
decoder later with self.emb_to_hidden,0
mask out vectors that correspond to a dummy symbol (TODO: check attention masking),1
use token embedding as initial hidden state for decoder,0
concatenate everything together and project to appropriate size for decoder,0
variable to store initial hidden states for decoder,0
encode input characters by sending them through RNN,0
note that we do not need to fill up with dummy symbols since we process each token seperately,0
embed character one-hots,0
send through encoder RNN (produces initial hidden for decoder),0
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder,0
project 2*hidden_size to hidden_size,0
concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder,0
later with self.emb_to_hidden,0
use token embedding as initial hidden state for decoder,0
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)",0
concatenate everything together and project to appropriate size for decoder,0
"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)",0
"create target vector (batch_size, max_label_seq_length + 1)",0
filter empty sentences,0
max length of the predicted sequences,0
for printing,0
stop if all sentences are empty,0
remove previously predicted labels of this type,0
create list of tokens in batch,0
encode inputs,0
"create input for first pass (batch_size, 1, input_size), first letter is special character <S>",0
sequence length is always set to one in prediction,0
option 1: greedy decoding,0
predictions,0
decode next character,0
pick top beam size many outputs with highest probabilities,0
option 2: beam search,0
out_probs = self.softmax(output_vectors).squeeze(1),0
make sure no dummy symbol <> or start symbol <S> is predicted,0
pick top beam size many outputs with highest probabilities,0
"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1",0
"leading_indices and probabilities have size (batch_size, beam_size)",0
keep scores of beam_size many hypothesis for each token in the batch,0
stack all leading indices of all hypothesis and corresponding hidden states in two tensors,0
save sequences so far,0
keep track of how many hypothesis were completed for each token,0
"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)",0
decode with log softmax,0
make sure no dummy symbol <> or start symbol <S> is predicted,0
"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside",0
"if the sequence is already ended, do not record as candidate",0
index of token in in list tokens_in_batch,0
print(token_number),0
hypothesis score,0
TODO: remove token if number of completed hypothesis exceeds given value,1
set score of corresponding entry to -inf so it will not be expanded,0
get leading_indices for next expansion,0
find highest scoring hypothesis among beam_size*beam_size possible ones for each token,0
take beam_size many copies of scores vector and add scores of possible new extensions,0
"size (beam_size*batch_size, beam_size)",0
print(hypothesis_scores),0
"reshape to vector of size (batch_size, beam_size*beam_size),",0
each row contains beam_size*beam_size scores of the new possible hypothesis,0
print(hypothesis_scores_per_token),0
"choose beam_size best for each token - size (batch_size, beam_size)",0
out of indices_per_token we now need to recompute the original indices of the hypothesis in,0
a list of length beam_size*batch_size,0
"where the first three inidices belong to the first token, the next three to the second token,",0
and so on,0
with these indices we can compute the tensors for the next iteration,0
expand sequences with corresponding index,0
add log-probabilities to the scores,0
save new leading indices,0
save corresponding hidden states,0
it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations,0
in that case we append one of the final seuqences without end symbol to the final_candidates,0
get best final hypothesis for each token,0
get characters from index sequences and add predicted label to token,0
dictionaries,0
all parameters will be pushed internally to the specified device,0
now print labels in CoNLL format,0
internal candidate lists of generator,0
load Zelda candidates if so passed,0
create candidate lists,0
"if lower casing is enabled, create candidate lists of lower cased versions",0
create a new dictionary for lower cased mentions,0
go through each mention and its candidates,0
"check if backoff mention already seen. If so, add candidates. Else, create new entry.",0
set lowercased version as map,0
remap state dict for models serialized with Flair <= 0.11.3,0
get the candidates,0
"during training, add the gold value as candidate",0
----- Create the internal tag dictionary -----,0
span-labels need special encoding (BIO or BIOES),0
the big question is whether the label dictionary should contain an UNK or not,0
"without UNK, we cannot evaluate on data that contains labels not seen in test",0
"with UNK, the model learns less well if there are no UNK examples",0
is this a span prediction problem?,0
----- Embeddings -----,0
----- Initial loss weights parameters -----,0
----- RNN specific parameters -----,0
----- Conditional Random Field parameters -----,0
"Previously trained models have been trained without an explicit CRF, thus it is required to check",0
whether we are loading a model from state dict in order to skip or add START and STOP token,0
----- Dropout parameters -----,0
dropouts,0
remove word dropout if there is no contact over the sequence dimension.,0
----- Model layers -----,0
----- RNN layer -----,0
"If shared RNN provided, else create one for model",0
Whether to train initial hidden state,0
final linear map to tag space,0
"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss",0
"if using CRF, we also require a CRF and a Viterbi decoder",0
"if there are no sentences, there is no loss",0
forward pass to get scores,0
calculate loss given scores and labels,0
make a zero-padded tensor for the whole sentence,0
linear map to tag space,0
"Depending on whether we are using CRF or a linear layer, scores is either:",0
"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF",0
"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer",0
spans need to be encoded as token-level predictions,0
all others are regular labels for each token,0
make sure it's a list,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
get features from forward propagation,0
remove previously predicted labels of this type,0
"if return_loss, get loss value",0
make predictions,0
add predictions to Sentence,0
BIOES-labels need to be converted to spans,0
"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)",0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
core Flair models on Huggingface ModelHub,0
"Large NER models,",0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Language-specific NER models,0
Language-specific POS models,0
English NER models,0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Danish models,0
German models,0
French models,0
Dutch models,0
Malayalam models,0
Portuguese models,0
Keyphase models,0
Biomedical models,0
check if model name is a valid local file,0
"check if model key is remapped to HF key - if so, print out information",0
get mapped name,0
use mapped name instead,0
"if not, check if model key is remapped to direct download location. If so, download model",0
special handling for the taggers by the @redewiegergabe project (TODO: move to model hub),1
"for all other cases (not local file or special download location), use HF model hub",0
"if not a local file, get from model hub",0
use model name as subfolder,0
Lazy import,0
output information,0
## Demo: How to use in Flair,0
load tagger,0
make example sentence,0
predict NER tags,0
print sentence,0
print predicted NER spans,0
iterate over entities and print,0
Lazy import,0
Save model weight,0
Determine if model card already exists,0
Generate and save model card,0
Upload files,0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
print labels in CoNLL format,0
the multi task model has several labels,0
biomedical models,0
entity linker,0
auto-spawn on GPU if available,0
remap state dict for models serialized with Flair <= 0.11.3,0
English sentiment models,0
Communicative Functions Model,0
"scores_at_targets[range(features.shape[0]), lengths.values -1]",0
Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices,0
"Initially, get scores from <start> tag to all other tags",0
"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp",0
"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep",0
Create a tensor to hold accumulated sequence scores at each current tag,0
Create a tensor to hold back-pointers,0
"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag",0
"Let pads be the <end> tag index, since that was the last tag in the decoded sequence",0
"We add scores at current timestep to scores accumulated up to previous timestep, and",0
choose the previous timestep that corresponds to the max. accumulated score for each current timestep,0
"If sentence is over, add transition to STOP-tag",0
Decode/trace best path backwards,0
Sanity check,0
remove start-tag and backscore to stop-tag,0
Max + Softmax to get confidence score for predicted label and append label to each token,0
"Transitions are used in the following way: transitions[to, from].",0
"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag",0
to START-tag and from STOP-tag to any other tag to -10000.,0
create a model card for this model with Flair and PyTorch version,0
also record Transformers version if library is loaded,0
remember all parameters used in train() call,0
add model card to model,0
"if optimizer class is passed, instantiate:",0
"determine what splits (train, dev, test) to evaluate and log",0
prepare loss logging file and set up header,0
"from here on, use list of learning rates",0
load existing optimizer state dictionary if it exists,0
"minimize training loss if training with dev data, else maximize dev score",0
"if scheduler is passed as a class, instantiate",0
"if we load a checkpoint, we have already trained for epoch",0
"Determine whether to log ""bad epochs"" information",0
load existing scheduler state dictionary if it exists,0
update optimizer and scheduler in model card,0
"if training also uses dev/train data, include in training set",0
initialize sampler if provided,0
init with default values if only class is provided,0
set dataset to sample from,0
this field stores the names of all dynamic embeddings in the model (determined after first forward pass),0
At any point you can hit Ctrl + C to break out of training early.,0
update epoch in model card,0
get new learning rate,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
"if shuffle_first_epoch==False, the first epoch is not shuffled",0
process mini-batches,0
zero the gradients on the model and optimizer,0
"if necessary, make batch_steps",0
forward and backward for batch,0
forward pass,0
Backward,0
identify dynamic embeddings (always deleted) on first sentence,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
do the optimizer step,0
do the scheduler step if one-cycle or linear decay,0
get new learning rate,0
evaluate on train / dev / test split depending on training settings,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
calculate scores using dev data if available,0
append dev score to score history,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
determine if this is the best model or if we need to anneal,0
default mode: anneal against dev score,0
alternative: anneal against dev loss,0
alternative: anneal against train loss,0
determine bad epoch number,0
lr unchanged,0
log bad epochs,0
output log file,0
make headers on first epoch,0
"if checkpoint is enabled, save model at each epoch",0
Check whether to save best model,0
"if we do not use dev data for model selection, save final model",0
test best model if test data is present,0
recover all arguments that were used to train this model,0
you can overwrite params with your own,0
surface nested arguments,0
resume training with these parameters,0
"If set, add a factor to the learning rate of all parameters with 'embeddings' not in name",0
get and return the final test score of best model,0
cast string to Path,0
forward pass,0
update optimizer and scheduler,0
"TextDataset returns a list. valid and test are only one file,",0
so return the first element,0
cast string to Path,0
error message if the validation dataset is too small,0
Shuffle training files randomly after serially iterating,0
through corpus one,0
"iterate through training data, starting at",0
self.split (for checkpointing),0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
Backward,0
`clip_grad_norm` helps prevent the exploding gradient,0
problem in RNNs / LSTMs.,0
We detach the hidden state from how it was,0
previously produced.,0
"If we didn't, the model would try backpropagating",0
all the way to start of the dataset.,0
explicitly remove loss to clear up memory,0
#########################################################,0
Save the model if the validation loss is the best we've,0
seen so far.,0
#########################################################,0
print info,0
#########################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn,0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
1-camembert-base -> camembert-base,0
1-xlm-roberta-large -> xlm-roberta-large,0
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol,0
The mask has 1 for real tokens and 0 for padding tokens. Only real,0
tokens are attended to.,0
Zero-pad up to the sequence length.,0
"first, find longest sentence in batch",0
prepare id maps for BERT model,0
put encoded batch through BERT model to get all hidden states of all encoder layers,0
get aggregated embeddings for each BERT-subtoken in sentence,0
get the current sentence object,0
add concatenated embedding to sentence,0
use first subword embedding if pooling operation is 'first',0
"otherwise, do a mean over all subwords in token",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
embed a dummy sentence to determine embedding_length,0
Avoid conflicts with flair's Token class,0
"legacy pickle-like saving for image embeddings, as implementation details are not obvious",0
"legacy pickle-like loading for image embeddings, as implementation details are not obvious",0
"<cls> token initially set to 1/D, so it attends to all image features equally",0
add positional encodings,0
reshape the pixels into the sequence,0
layer norm after convolution and positional encodings,0
add <cls> token,0
"transformer requires input in the shape [h*w+1, b, d]",0
the output is an embedding of <cls> token,0
this parameter is fixed,0
optional fine-tuning on top of embedding layer,0
"if only one sentence is passed, convert to list of sentence",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push through RNN,0
after-RNN dropout,0
extract embeddings from RNN,0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
serialize the language models and the constructor arguments (but nothing else),0
re-initialize language model with constructor arguments,0
special handling for deserializing language models,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
IMPORTANT: add embeddings as torch modules,0
iterate over sentences,0
"if its a forward LM, take last state",0
"convert to plain strings, embedded in a list for the encode function",0
CNN,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push CNN,0
after-CNN dropout,0
extract embeddings from CNN,0
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency",0
"if only one sentence is passed, convert to list of sentence",0
Expose base classses,0
Expose document embedding classes,0
Expose image embedding classes,0
Expose legacy embedding classes,0
Expose token embedding classes,0
in some cases we need to insert zero vectors for tokens without embedding.,0
padding,0
remove special markup,0
check if special tokens exist to circumvent error message,0
iterate over subtokens and reconstruct tokens,0
remove special markup,0
check if reconstructed token is special begin token ([CLS] or similar),0
some BERT tokenizers somehow omit words - in such cases skip to next token,0
"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token.",0
if tokens are unaccounted for,0
check if all tokens were matched to subtokens,0
The layoutlm tokenizer doesn't handle ocr themselves,0
"transformers returns the ""added_tokens.json"" even if it doesn't create it",0
"transformers returns the ""added_tokens.json"" even if it doesn't create it",0
in case of doubt: token embedding has higher priority than document embedding,0
random check some tokens to save performance.,0
Models such as FNet do not have an attention_mask,0
set language IDs for XLM-style transformers,0
"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have",0
"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids.",0
set context if not set already,0
flair specific pre-tokenization,0
fields to store left and right context,0
expand context only if context_length is set,0
"if context_dropout is set, randomly deactivate left context during training",0
"if context_dropout is set, randomly deactivate right context during training",0
"if use_context_separator is set, add a [FLERT] token",0
return expanded sentence and context length information,0
"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first.",0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
do not print transformer warnings as these are confusing in this case,0
load tokenizer and transformer model,0
load tokenizer from inmemory zip-file,0
model name,0
embedding parameters,0
send mini-token through to check how many layers the model has,0
return length,0
"If we use a context separator, add a new special token",0
"most models have an initial BOS token, except for XLNet, T5 and GPT2",0
"when initializing, embeddings are in eval mode by default",0
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial",0
"cannot run `.encode` if ocr boxes are required, assume",0
in case of doubt: token embedding has higher priority than document embedding,0
in case of doubt: token embedding has higher priority than document embedding,0
legacy TransformerDocumentEmbedding,0
legacy TransformerTokenEmbedding,0
legacy Flair <= 0.12,0
legacy Flair <= 0.7,0
legacy TransformerTokenEmbedding,0
Legacy TransformerDocumentEmbedding,0
legacy TransformerTokenEmbedding,0
legacy TransformerDocumentEmbedding,0
copy values from new embedding,0
cls first pooling can be done without recreating sentence hidden states,0
make the tuple a tensor; makes working with it easier.,0
"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention",0
only use layers that will be outputted,0
this parameter is fixed,0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
make compatible with serialized models,0
gensim version 4,0
gensim version 3,0
"if no embedding is set, the vocab and embedding length is requried",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNINOS embeddings,0
pubmed embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
fix serialized models,0
"this is required to force the module on the cpu,",0
"if a parent module is put to gpu, the _apply is called to each sub_module",0
self.to(..) actually sets the device properly,0
this ignores the get_cached_vec method when loading older versions,0
it is needed for compatibility reasons,0
gensim version 4,0
gensim version 3,0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
multilingual models,0
English models,0
Arabic,0
Bulgarian,0
Czech,0
Danish,0
German,0
Spanish,0
Basque,0
Persian,0
Finnish,0
French,0
Hebrew,0
Hindi,0
Croatian,0
Indonesian,0
Italian,0
Japanese,0
Malayalam,0
Dutch,0
Norwegian,0
Polish,0
Portuguese,0
Pubmed,0
Slovenian,0
Swedish,0
Tamil,0
Spanish clinical,0
CLEF HIPE Shared task,0
Amharic,0
Ukrainian,0
load model if in pretrained model map,0
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir),0
CLEF HIPE models are lowercased,0
embeddings are static if we don't do finetuning,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
make compatible with serialized models (TODO: remove),1
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout",0
make compatible with serialized models (TODO: remove),1
gradients are enable if fine-tuning is enabled,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
offset mode that extracts at whitespace after last character,0
offset mode that extracts at last character,0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
check token.text is empty or not,0
set aggregation operation,0
add embeddings after updating,0
model architecture,0
model architecture,0
"""pl"",",0
download if necessary,0
load the model,0
"TODO: keep for backwards compatibility, but remove in future",1
save the sentence piece model as binary file (not as path which may change),0
write out the binary sentence piece model into the expected directory,0
"if the model was saved as binary and it is not found on disk, write to appropriate path",0
"otherwise, use normal process and potentially trigger another download",0
"once the modes if there, load it with sentence piece",0
empty words get no embedding,0
all other words get embedded,0
GLOVE embeddings,0
no need to recreate as NILCEmbeddings,0
read in test file if exists,0
read in dev file if exists,0
"find train, dev and test files if not specified",0
Add tags for each annotated span,0
Remove leading and trailing whitespaces from annotated spans,0
Search start and end token index for current span,0
If end index is not found set to last token,0
Throw error if indices are not valid,0
get train data,0
read in test file if exists,0
read in dev file if exists,0
"find train, dev and test files if not specified",0
special key for space after,0
special key for feature columns,0
special key for dependency head id,0
"store either Sentence objects in memory, or only file offsets",0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
determine encoding of text file,0
identify which columns are spans and which are word-level,0
now load all sentences,0
skip first line if to selected,0
option 1: keep Sentence objects in memory,0
pointer to previous,0
parse next sentence,0
quit if last sentence reached,0
skip banned sentences,0
set previous and next sentence for context,0
append parsed sentence to list in memory,0
option 2: keep source data in memory,0
"read lines for next sentence, but don't parse",0
quit if last sentence reached,0
append raw lines for each sentence,0
we make a distinction between word-level tags and span-level tags,0
read first sentence to determine which columns are span-labels,0
skip first line if to selected,0
check the first 5 sentences,0
go through all annotations and identify word- and span-level annotations,0
- if a column has at least one BIES we know it's a Span label,0
"- if a column has at least one tag that is not BIOES, we know it's a Token label",0
- problem cases are columns for which we see only O - in this case we default to Span,0
skip assigned columns,0
the space after key is always word-levels,0
"if at least one token has a BIES, we know it's a span label",0
"if at least one token has a label other than BIOES, we know it's a token label",0
all remaining columns that are not word-level are span-level,0
for column in self.word_level_tag_columns:,0
"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")",0
"if sentence ends, break",0
parse comments if possible,0
"otherwise, this line is a token. parse and add to sentence",0
check if this sentence is a document boundary,0
add span labels,0
discard tags from tokens that are not added to the sentence,0
parse relations if they are set,0
head and tail span indices are 1-indexed and end index is inclusive,0
parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas',0
"to set the metadata ""domain"" to ""de-orcas""",0
get fields from line,0
get head_id if exists (only in dependency parses),0
initialize token,0
go through all columns,0
'feats' and 'misc' column should be split into different fields,0
special handling for whitespace after,0
add each other feature as label-value pair,0
get the task name (e.g. 'ner'),0
get the label value,0
add label,0
remap regular tag names,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
set sentence context using partials TODO: pointer to dataset is really inefficient,1
use all domains,0
iter over all domains / sources and create target files,0
Parameters,0
The conll representation of coref spans allows spans to,0
"overlap. If spans end or begin at the same word, they are",0
"separated by a ""|"".",0
The span begins at this word.,0
The span begins and ends at this word (single word span).,0
"The span is starting, so we record the index of the word.",0
"The span for this id is ending, but didn't start at this word.",0
Retrieve the start index from the document state and,0
add the span to the clusters for this id.,0
Parameters,0
strip all bracketing information to,0
get the actual propbank label.,0
Entering into a span for a particular semantic role label.,0
We append the label and set the current span for this annotation.,0
"If there's no '(' token, but the current_span_label is not None,",0
then we are inside a span.,0
We're outside a span.,0
"Exiting a span, so we reset the current span label for this annotation.",0
The words in the sentence.,0
The pos tags of the words in the sentence.,0
the pieces of the parse tree.,0
The lemmatised form of the words in the sentence which,0
have SRL or word sense information.,0
The FrameNet ID of the predicate.,0
"The sense of the word, if available.",0
"The current speaker, if available.",0
"Cluster id -> List of (start_index, end_index) spans.",0
Cluster id -> List of start_indices which are open for this id.,0
Replace brackets in text and pos tags,0
with a different token for parse trees.,0
only keep ')' if there are nested brackets with nothing in them.,0
There are some bad annotations in the CONLL data.,0
"They contain no information, so to make this explicit,",0
we just set the parse piece to be None which will result,0
in the overall parse tree being None.,0
"If this is the first word in the sentence, create",0
empty lists to collect the NER and SRL BIO labels.,0
"We can't do this upfront, because we don't know how many",0
"components we are collecting, as a sentence can have",0
variable numbers of SRL frames.,0
Create variables representing the current label for each label,0
sequence we are collecting.,0
"If any annotation marks this word as a verb predicate,",0
we need to record its index. This also has the side effect,0
of ordering the verbal predicates by their location in the,0
"sentence, automatically aligning them with the annotations.",0
"this would not be reached if parse_pieces contained None, hence the cast",0
Non-empty line. Collect the annotation.,0
Collect any stragglers or files which might not,0
have the '#end document' format for the end of the file.,0
this dataset name,0
check if data there,0
column format,0
this dataset name,0
check if data there,0
column format,0
this dataset name,0
download data if necessary,0
download files if not present locally,0
we need to slightly modify the original files by adding some new lines after document separators,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
Remove CoNLL-U meta information in the last column,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
entity_mapping,0
this dataset name,0
download data if necessary,0
data validation,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download files if not present locallys,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
# download zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
download data if necessary,0
"unzip the downloaded repo and merge the train, dev and test datasets",0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
check if data there,0
create folder,0
download dataset,0
column format,0
this dataset name,0
download and parse data if necessary,0
create train test dev if not exist,0
column format,0
this dataset name,0
If the extracted corpus file is not yet present in dir,0
download zip if necessary,0
"extracted corpus is not present , so unpacking it.",0
column format,0
this dataset name,0
download zip,0
unpacking the zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)",0
download files if not present locally,0
we need to modify the original files by adding new lines after after the end of each sentence,0
if only one language is given,0
column format,0
this dataset name,0
"use all languages if explicitly set to ""all""",0
download data if necessary,0
initialize comlumncorpus and add it to list,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
this list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
get google drive id from list,0
download from google drive,0
unzip,0
"tar.extractall(language_folder,members=[tar.getmember(file_name)])",0
transform data into required format,0
"the processed dataset has the additional ending ""_new""",0
remove the unprocessed dataset,0
initialize comlumncorpus and add it to list,0
if no languages are given as argument all languages used in XTREME will be loaded,0
if only one language is given,0
column format,0
this dataset name,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
This list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
download from HU Server,0
unzip,0
transform data into required format,0
initialize comlumncorpus and add it to list,0
if only one language is given,0
column format,0
this dataset name,0
download data if necessary,0
initialize comlumncorpus and add it to list,0
download data if necessary,0
unpack and write out in CoNLL column-like format,0
column format,0
this dataset name,0
download data if necessary,0
data is not in IOB2 format. Thus we transform it to IOB2,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
column format,0
this dataset name,0
rename according to train - test - dev - convention,0
column format,0
this dataset name,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
Add missing newline after header,0
Workaround for empty tokens,1
"Add ""real"" document marker",0
Dataset split mapping,0
v2.0 only adds new language and splits for AJMC dataset,0
Special document marker for sample splits in AJMC dataset,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
paths to train and test splits,0
init corpus,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
iterate over all html files,0
"get rid of html syntax, we only need the text",0
between all documents we write a separator symbol,0
skip empty strings,0
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)",0
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention",0
sentence splitting and tokenization,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out-file in column format,0
"in case something goes wrong, delete the dataset and raise error",0
this dataset name,0
download and parse data if necessary,0
from qwikidata.linked_data_interface import get_entity_dict_from_api,0
generate qid wikiname dictionaries,0
merge dictionaries,0
ignore first line,0
commented and empty lines,0
read all Q-IDs,0
ignore first line,0
request,0
this dataset name,0
we use the wikiids in the data instead of directly utilizing the wikipedia urls.,0
like this we can quickly check if the corresponding page exists,0
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi,0
delete unprocessed file,0
collect all wikiids,0
create the dictionary,0
request,0
this dataset name,0
names of raw text documents,0
open output_file,0
iterate through all documents,0
split sentences and tokenize,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out file,0
this dataset name,0
download and parse data if necessary,0
this dataset name,0
download and parse data if necessary,0
First parse the post titles,0
Keep track of how many and which entity mentions does a given post title have,0
Check if the current post title has an entity link and parse accordingly,0
Post titles with entity mentions (if any) are handled via this function,0
Then parse the comments,0
"Iterate over the comments.tsv file, until the end is reached",0
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched.",0
Each comment thread is handled as one 'document'.,0
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.,0
This if-condition is needed to handle this problem.,0
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure",0
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above",0
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle.",0
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,",0
and not just single letters into single rows.,0
If there are annotated entity mentions for given post title or a comment thread,0
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence",0
Write the token with a corresponding tag to file,0
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed",0
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag",0
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized,0
"incorrectly, in order to keep the desired format (empty line as a sentence separator).",0
"Thrown when the second check above happens, but the last token of a sentence is reached.",0
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below.",0
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS",0
Check if further annotations belong to the current post title or comment thread as well,0
Stop when the end of an annotation file is reached,0
Check if further annotations belong to the current sentence as well,0
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)",0
Docstart,0
if there is more than one word in the chunk we write each in a separate line,0
print(chunks),0
empty line after each sentence,0
convert the file to CoNLL,0
this dataset name,0
"check if data there, if not, download the data",0
create folder,0
download data,0
transform data into column format if necessary,0
if no filenames are specified we use all the data,0
"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled",0
also we remove 'raganato_ALL' from filenames in case its in the list,0
generate the test file,0
make column file and save to data_folder,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
this dataset name,0
download data if necessary,0
if True:,0
write CoNLL-U Plus header,0
"Some special cases (e.g., missing spaces before entity marker)",0
necessary if text should be whitespace tokenizeable,0
Handle case where tail may occur before the head,0
this dataset name,0
write CoNLL-U Plus header,0
this dataset name,0
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors,1
download data if necessary,0
write CoNLL-U Plus header,0
The span has ended.,0
We are entering a new span; reset indices,0
and active tag to new span.,0
We're inside a span.,0
Last token might have been a part of a valid span.,0
this dataset name,0
write CoNLL-U Plus header,0
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):",0
"with zip_file.open(source_file_path, mode=""r"") as source_file:",0
target_file_path = Path(data_folder) / target_filename,0
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:",0
# write CoNLL-U Plus header,0
"target_file.write(""# global.columns = id form ner\n"")",0
for example in json.load(source_file):,0
token_list = self._tacred_example_to_token_list(example),0
target_file.write(token_list.serialize()),0
check if first tag row is already occupied,0
"if first tag row is occupied, use second tag row",0
hardcoded mapping TODO: perhaps find nicer solution,1
remap regular tag names,0
else skip to position in file where sentence begins,0
set sentence context using partials TODO: pointer to dataset is really inefficient,1
read in dev file if exists,0
read in test file if exists,0
the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44,0
"find train, dev and test files if not specified",0
use test_file to create test split if available,0
use dev_file to create test split if available,0
"if data point contains black-listed label, do not use",0
first check if valid sentence,0
"if so, add to indices",0
"find train, dev and test files if not specified",0
variables,0
different handling of in_memory data than streaming data,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
test if format is OK,0
test if at least one label given,0
make sentence from text (and filter for length),0
"if a pair column is defined, make a sentence pair object",0
noinspection PyDefaultArgument,0
dataset name includes the split size,0
default dataset folder is the cache root,0
download data if necessary,0
download each of the 28 splits,0
create dataset directory if necessary,0
download senteval datasets if necessary und unzip,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
handle labels file,0
handle data file,0
Create flair compatible labels,0
"by defaut, map point score to POSITIVE / NEGATIVE values",0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file from CSV,0
create test.txt file from CSV,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create train dev and test files in fasttext format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
convert to FastText format,0
download data if necessary,0
"if data is not downloaded yet, download it",0
get the zip file,0
move original .tsv files to another folder,0
create train and dev splits in fasttext format,0
create eval_dataset file with no labels,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download datasets if necessary,0
create dataset directory if necessary,0
create correctly formated txt files,0
multiple labels are possible,0
this dataset name,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
create a separate directory for different tasks,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
check if dataset is supported,0
set file names,0
set file names,0
download and unzip in file structure if necessary,0
instantiate corpus,0
"find train, dev and test files if not specified",0
"create DataPairDataset for train, test and dev file, if they are given",0
stop if file does not exist,0
create a DataPair object from strings,0
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"reorder dev datasets to have same columns as in train set: 8, 9, and 11",0
dev sets include 5 different annotations but we will only keep the gold label,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get test and dev sets,0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data not downloaded yet, download it",0
get the zip file,0
"the downloaded files have json format, we transform them to tsv",0
Function to transform JSON file to tsv for Recognizing Textual Entailment Data,0
remove json file,0
Uses dynamic programming approach to calculate maximum independent set in interval graph,0
with sum of all entity lengths as secondary key,0
calculate offset without current text,0
because we stick all passages of a document together,0
TODO For split entities we also annotate everything inbetween which might be a bad idea?,1
Try to fix incorrect annotations,0
print(,0
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}""",0
),0
Ignore empty lines or relation annotations,0
FIX annotation of whitespaces (necessary for PDR),0
One token may contain multiple entities -> deque all of them,0
column format,0
this dataset name,0
Create tokenization-dependent CONLL files. This is necessary to prevent,0
from caching issues (e.g. loading the same corpus with different sentence splitters),0
column format,0
this dataset name,0
column format,0
this dataset name,0
Edge case: last token starts a new entity,0
Last document in file,0
column format,0
this dataset name,0
column format,0
this dataset name,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
Edge case: last token starts a new entity,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
column format,0
this dataset name,0
Read texts,0
Read annotations,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
We need to apply a patch to correct the original training file,0
Articles title,0
Article abstract,0
Entity annotations,0
column format,0
this dataset name,0
Edge case: last token starts a new entity,0
Map all entities to chemicals,0
Map all entities to disease,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
Incomplete article,0
Invalid XML syntax,0
column format,0
this dataset name,0
column format,0
this dataset name,0
if len(mid) != 3:,0
continue,0
Try to fix entity offsets,0
column format,0
this dataset name,0
There is still one illegal annotation in the file ..,0
column format,0
this dataset name,0
"Abstract first, title second to prevent issues with sentence splitting",0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
column format,0
this dataset name,0
"Filter for specific entity types, by default no entities will be filtered",0
Get original HUNER splits to retrieve a list of all document ids contained in V2,0
train and dev split of V2 will be train in V4,0
test split of V2 will be dev in V4,0
New documents in V4 will become test documents,0
column format,0
this dataset name,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
cache Feidegger config file,0
cache Feidegger images,0
replace image URL with local cached file,0
append Sentence-Image data point,0
"in certain cases, multi-CPU data loading makes no sense and slows",0
"everything down. For this reason, we detect if a dataset is in-memory:",0
"if so, num_workers is set to 0 for faster processing",0
cast to list if necessary,0
cast to list if necessary,0
"first, check if pymongo is installed",0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
Expose base classses,0
Expose all biomedical data sets used for the evaluation of BioBERT,0
-,0
-,0
-,0
-,0
Expose all biomedical data sets using the HUNER splits,0
Expose all biomedical data sets,0
Expose all document classification datasets,0
word sense disambiguation,0
Expose all entity linking datasets,0
Expose all relation extraction datasets,0
universal proposition banks,0
keyphrase detection datasets,0
other NER datasets,0
standard NER datasets,0
Expose all sequence labeling datasets,0
Expose all text-image datasets,0
Expose all text-text datasets,0
Expose all treebanks,0
"find train, dev and test files if not specified",0
get train data,0
get test data,0
get dev data,0
option 1: read only sentence boundaries as offset positions,0
option 2: keep everything in memory,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
current token ID,0
handling for the awful UD multiword format,0
end of sentence,0
comments,0
ellipsis,0
if token is a multi-word,0
normal single-word tokens,0
"if we don't split multiwords, skip over component words",0
add token,0
add morphological tags,0
derive whitespace logic for multiwords,0
print(token),0
print(current_multiword_last_token),0
print(current_multiword_first_token),0
"if multi-word equals component tokens, there should be no whitespace",0
go through all tokens in subword and set whitespace_after information,0
print(i),0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
this dataset name,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"finally, print model card for information",0
test corpus,0
create a TARS classifier,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
test corpus,0
create a TARS classifier,0
switch to a new task (TARS can do multiple tasks so you must define one),0
initialize the text classifier trainer,0
start the training,0
"With end symbol, without start symbol, padding in front",0
"Without end symbol, with start symbol, padding in back",0
"Without end symbol, without start symbol, padding in front",0
initialize trainer,0
initialize trainer,0
train model for 2 epochs,0
load the checkpoint model and train until epoch 4,0
clean up results directory,0
initialize trainer,0
initialize trainer,0
increment for last token in sentence if not followed by whitespace,0
clean up directory,0
clean up directory,0
example sentence,0
set 4 labels for 2 tokens ('love' is tagged twice),0
check if there are three POS labels with correct text and values,0
check if there are is one SENTIMENT label with correct text and values,0
check if all tokens are correctly labeled,0
remove the pos label from the last word,0
there should be 2 POS labels left,0
now remove all pos tags,0
set 3 labels for 2 spans (HU is tagged twice),0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
"now delete the NER tags of ""Humboldt-Universitt zu Berlin""",0
should be only one NER label left,0
and only one NER span,0
set 3 labels for 2 spans (HU is tagged twice with different tags),0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
"now delete the NER tags of ""Humboldt-Universitt zu Berlin""",0
should be only one NER label left,0
and only one NER span,0
but there is also one orgtype span and label,0
and only one NER span,0
let's add the NER tag back,0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
now remove all NER tags,0
set 3 labels for 2 spans (HU is tagged twice with different tags),0
create two relation label,0
there should be two relation labels,0
there should be one syntactic labels,0
"there should be two relations, one with two and one with one label",0
example sentence,0
add another topic label,0
example sentence,0
has sentiment value,0
has 4 part of speech tags,0
has 1 NER tag,0
should be in total 6 labels,0
example sentence,0
add two NER labels,0
get the four labels,0
check that only two of the respective data points are equal,0
make a sentence and some right context,0
TODO: is this desirable? Or should two sentences with same text be considered same objects?,1
Initializing a Sentence this way assumes that there is a space after each token,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
define search space,0
sequence tagger parameter,0
model trainer parameter,0
training parameter,0
find best parameter settings,0
clean up results directory,0
document embeddings parameter,0
training parameter,0
clean up results directory,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
load column dataset with one entry,0
load column dataset with two entries,0
load column dataset with three entries,0
"get training, test and dev data",0
"get training, test and dev data",0
check if Token labels are correct,0
"get training, test and dev data",0
check if Token labels for frames are correct,0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
"assert [token.get_tag(""head"").value for token in sent1.tokens] == [",0
"""2"",",0
"""0"",",0
"""4"",",0
"""2"",",0
"""2"",",0
"""2"",",0
],0
"Here, we use the default token annotation fields.",0
"We have manually checked, that these numbers are correct:",0
"+1 offset, because of missing EOS marker at EOD",0
Test data for v2.1 release,0
--- Embeddings that are shared by both models --- #,0
--- Task 1: Sentiment Analysis (5-class) --- #,0
Define corpus and model,0
-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #,0
Define corpus and model,0
-- Define mapping (which tagger should train on which model) -- #,0
-- Create model trainer and train -- #,0
clean up file,0
no need for label_dict,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
Intel ----founded_by---> Gordon Moore,0
Intel ----founded_by---> Robert Noyce,0
Check sentence masking and relation label annotation on,0
"training, validation and test dataset (in this test the splits are the same)",0
"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google .""",0
"Entity pair permutations of: ""Microsoft was founded by Bill Gates .""",0
"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 .""",0
"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany.""",0
This sentence is only included if we transform the corpus with cross augmentation,0
Ensure this is an example that predicts no classes in multilabel,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
ensure that the prepared tensors is what we expect,0
use a SequenceTagger to save and reload the embedding in the manner it is supposed to work,0
previous and next sentence as context,0
test expansion for sentence without context,0
test expansion for with previous and next as context,0
test expansion if first sentence is document boundary,0
test expansion if we don't use context,0
"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher.",0
dummy model with embeddings,0
save the dummy and load it again,0
check that context_length and use_context_separator is the same for both,0
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
Extract all the contents of zip file in current directory,0
Extract all the contents of zip file in current directory,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
load_big_file is a workaround byhttps://github.com/highway11git,1
to load models on some Mac/Windows setups,0
see https://github.com/zalandoresearch/flair/issues/351,0
first determine the distribution of classes in the dataset,0
weight for each sample,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
increment for last token in sentence if not followed by whitespace,0
this is the default init size of a lmdb database for embeddings,0
get db filename from embedding name,0
"In case initialization of cached version failed, just fallback to the original WordEmbeddings",0
SequenceTagger,0
TextClassifier,0
get db filename from embedding name,0
if embedding database already exists,0
"otherwise, push embedding to database",0
if embedding database already exists,0
open the database in read mode,0
we need to set self.k,0
create and load the database in write mode,0
"no idea why, but we need to close and reopen the environment to avoid",0
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot,0
when opening new transaction !,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
set 'add_unk' if the dictionary was created with a version of Flair older than 0.9,0
set 'add_unk' depending on whether <unk> is a key,0
"if one embedding name, directly return it",0
"if multiple embedding names, concatenate them",0
TODO: does it make sense to exclude labels? Two data points of identical text (but different labels),1
would be equal now.,0
First we remove any existing labels for this PartOfSentence in self.sentence,0
labels also need to be deleted at Sentence object,0
delete labels at object itself,0
private field for all known spans,0
the tokenizer used for this sentence,0
some sentences represent a document boundary (but most do not),0
internal variables to denote position inside dataset,0
"if text is passed, instantiate sentence with tokens (words)",0
determine token positions and whitespace_after flag,0
the last token has no whitespace after,0
log a warning if the dataset is empty,0
data with zero-width characters cannot be handled,0
set token idx and sentence,0
append token to sentence,0
register token annotations on sentence,0
move sentence embeddings to device,0
also move token embeddings to device,0
clear token embeddings,0
infer whitespace after field,0
"if sentence has no tokens, return empty string",0
"otherwise, return concatenation of tokens with the correct offsets",0
The sentence's start position is not propagated to its tokens.,0
"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces.",0
No character at the corresponding code point: remove it,0
"if no label if specified, return all labels",0
"if the label type exists in the Sentence, return it",0
return empty list if none of the above,0
labels also need to be deleted at all tokens,0
labels also need to be deleted at all known spans,0
remove spans without labels,0
delete labels at object itself,0
set name,0
abort if no data is provided,0
sample test data from train if none is provided,0
sample dev data from train if none is provided,0
set train dev and test data,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
count all label types per sentence,0
go through all labels of label_type and count values,0
check if there are any span labels,0
"if an unk threshold is set, UNK all label values below this threshold",0
sample randomly from a label distribution according to the probabilities defined by the desired noise share,0
replace the old label with the new one,0
keep track of the old (clean) label using another label type category,0
keep track of how many labels in total are flipped,0
Make the tag dictionary,0
"add a dummy ""O"" to close final prediction",0
return complex list,0
internal variables,0
non-set tags are OUT tags,0
anything that is not OUT is IN,0
does this prediction start a new span?,0
begin and single tags start new spans,0
"in IOB format, an I tag starts a span if it follows an O or is a different span",0
single tags that change prediction start new spans,0
if an existing span is ended (either by reaching O or starting a new span),0
determine score and value,0
append to result list,0
reset for-loop variables for new span,0
remember previous tag,0
global variable: cache_root,0
global variable: device,0
global variable: version,0
global variable: arrow symbol,0
dummy return to fulfill trainer.train() needs,0
print(vec),0
Attach optimizer,0
"convert `metrics` to float, in case it's a zero-dim Tensor",0
if memory mode option 'none' delete everything,0
"if dynamic embedding keys not passed, identify them automatically",0
always delete dynamic embeddings,0
"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)",0
optional metric space decoder if prototypes have different length than embedding,0
create initial prototypes for all classes (all initial prototypes are a vector of all 1s),0
"if set, create initial prototypes from normal distribution",0
"if set, use a radius",0
all parameters will be pushed internally to the specified device,0
decode embeddings into prototype space,0
"if unlabeled distance is set, mask out loss to unlabeled class prototype",0
Always include the name of the Model class for which the state dict holds,0
"in Flair <0.9.1, optimizer and scheduler used to train model are not saved",0
"write out a ""model card"" if one is set",0
special handling for optimizer:,0
remember optimizer class and state dictionary,0
save model,0
restore optimizer and scheduler to model card if set,0
"if this class is abstract, go through all inheriting classes and try to fetch and load the model",0
get all non-abstract subclasses,0
"try to fetch the model for each subclass. if fetching is possible, load model and return it",0
"skip any invalid loadings, e.g. not found on huggingface hub",0
"if the model cannot be fetched, load as a file",0
try to get model class from state,0
"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses",0
"if str(model_cls) == ""<class 'flair.models.pairwise_classification_model.TextPairClassifier'>"": continue",0
"skip any invalid loadings, e.g. not found on huggingface hub",0
"if this class is not abstract, fetch the model and load it",0
"make sure <unk> is contained in gold_label_dictionary, if given",0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
loss calculation,0
variables for printing,0
variables for computing scores,0
remove any previously predicted labels,0
predict for batch,0
get the gold labels,0
add to all_predicted_values,0
make printout lines,0
convert true and predicted values to two span-aligned lists,0
delete exluded labels if exclude_labels is given,0
"if after excluding labels, no label is left, ignore the datapoint",0
write all_predicted_values to out_file if set,0
make the evaluation dictionary,0
check if this is a multi-label problem,0
compute numbers by formatting true and predicted such that Scikit-Learn can use them,0
multi-label problems require a multi-hot vector for each true and predicted label,0
single-label problems can do with a single index for each true and predicted label,0
"now, calculate evaluation numbers",0
there is at least one gold label or one prediction (default),0
"if there is only one label, then ""micro avg"" = ""macro avg""",0
"micro average is only computed if zero-label exists (for instance ""O"")",0
if no zero-label exists (such as in POS tagging) micro average is equal to accuracy,0
same for the main score,0
issue error and default all evaluation numbers to 0.,0
line for log file,0
check if there is a label mismatch,0
print info,0
set the embeddings,0
initialize the label dictionary,0
initialize the decoder,0
set up multi-label logic,0
init dropouts,0
loss weights and loss function,0
Initialize the weight tensor,0
set up gradient reversal if so specified,0
embed sentences,0
get a tensor of data points,0
do dropout,0
make a forward pass to produce embedded data points and labels,0
get the data points for which to predict labels,0
get their gold labels as a tensor,0
pass data points through network to get encoded data point tensor,0
decode,0
an optional masking step (no masking in most cases),0
calculate the loss,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
filter data points in batch,0
stop if all sentences are empty,0
pass data points through network and decode,0
if anything could possibly be predicted,0
remove previously predicted labels of this type,0
add DefaultClassifier arguments,0
add variables of DefaultClassifier,0
Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23,0
Get projected 1st dimension,0
Compute bilinear form,0
Arcosh,0
Project the input data to n+1 dimensions,0
"The first dimension, is recomputed in the distance module",0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for",0
"train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
plot i,0
save plots,0
save plots,0
plt.show(),0
save plot,0
take the average over the last three scores of training,0
take average over the scores from the different training runs,0
auto-spawn on GPU if available,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
TODO: not saving lines yet,1
TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.,1
- MaskedRelationClassifier ?,0
This depends if this relation classification architecture should replace or offer as an alternative.,0
Set label type and prepare label dictionary,0
Initialize super default classifier,0
Add the special tokens from the encoding strategy,0
"Auto-spawn on GPU, if available",0
Only use entities labelled with the specified labels for each label type,0
Only use entities above the specified threshold,0
Use a dictionary to find gold relation annotations for a given entity pair,0
Yield head and tail entity pairs from the cross product of all entities,0
Remove identity relation entity pairs,0
Remove entity pairs with labels that do not match any,0
of the specified relations in `self.entity_pair_labels`,0
"Obtain gold label, if existing",0
Some sanity checks,0
Pre-compute non-leading head and tail tokens for entity masking,0
We can not use the plaintext of the head/tail span in the sentence as the mask/marker,0
since there may be multiple occurrences of the same entity mentioned in the sentence.,0
"Therefore, we use the span's position in the sentence.",0
Create masked sentence,0
Add gold relation annotation as sentence label,0
"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,",0
"during prediction, the forward pass does not need any knowledge about the entities in the sentence.",0
"If we sample missing splits, the encoded sentences that correspond to the same original sentences",0
"may get distributed into different splits. For training purposes, this is always undesired.",0
Ensure that all sentences are encoded properly,0
Deal with the case where all sentences are encoded sentences,0
"mypy does not infer the type of ""sentences"" restricted by the if statement",0
Deal with the case where all sentences are standard (non-encoded) sentences,0
"For each encoded sentence, transfer its prediction onto the original relation",0
auto-spawn on GPU if available,0
pad strings with whitespaces to longest sentence,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
"""document_delimiter"" property may be missing in some older pre-trained models",0
serialize the language models and the constructor arguments (but nothing else),0
special handling for deserializing language models,0
re-initialize language model with constructor arguments,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
Transform input data into TARS format,0
"if there are no labels, return a random sample as negatives",0
"otherwise, go through all labels",0
make sure the probabilities always sum up to 1,0
get and embed all labels by making a Sentence object that contains only the label text,0
get each label embedding and scale between 0 and 1,0
compute similarity matrix,0
"the higher the similarity, the greater the chance that a label is",0
sampled as negative example,0
make label dictionary if no Dictionary object is passed,0
prepare dictionary of tags (without B- I- prefixes and without UNK),0
check if candidate_label_set is empty,0
make list if only one candidate label is passed,0
create label dictionary,0
note current task,0
create a temporary task,0
make zero shot predictions,0
switch to the pre-existing task,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
make a tars sentence where all labels are O by default,0
init new TARS classifier,0
set all task information,0
return,0
with torch.no_grad():,0
progress bar for verbosity,0
stop if all sentences are empty,0
go through each sentence in the batch,0
always remove tags first,0
get the span and its label,0
determine whether tokens in this span already have a label,0
only add if all tokens have no label,0
make and add a corresponding predicted span,0
set indices so that no token can be tagged twice,0
clearing token embeddings to save memory,0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
get the serialized embeddings,0
remap state dict for models serialized with Flair <= 0.11.3,0
init new TARS classifier,0
set all task information,0
with torch.no_grad():,0
progress bar for verbosity,0
stop if all sentences are empty,0
go through each sentence in the batch,0
always remove tags first,0
add all labels that according to TARS match the text and are above threshold,0
do not add labels below confidence threshold,0
only use label with highest confidence if enforcing single-label predictions,0
get all label scores and do an argmax to get the best label,0
remove previously added labels and only add the best label,0
clearing token embeddings to save memory,0
set separator to concatenate two sentences,0
auto-spawn on GPU if available,0
pooling operation to get embeddings for entites,0
set embeddings,0
set relation and entity label types,0
"whether to use gold entity pairs, and whether to filter entity pairs by type",0
filter entity pairs according to their tags if set,0
whether to encode characters and whether to use attention (attention can only be used if chars are encoded),0
character dictionary for decoding and encoding,0
make sure <unk> is in dictionary for handling of unknown characters,0
add special symbols to dictionary if necessary and save respective indices,0
---- ENCODER ----,0
encoder character embeddings,0
encoder pre-trained embeddings,0
encoder RNN,0
additional encoder linear layer if bidirectional encoding,0
---- DECODER ----,0
decoder: linear layers to transform vectors to and from alphabet_size,0
when using attention we concatenate attention outcome and decoder hidden states,0
decoder RNN,0
loss and softmax,0
self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction,0
add additional columns for special symbols if necessary,0
initialize with dummy symbols,0
encode inputs,0
get labels (we assume each token has a lemma label),0
get char indices for labels of sentence,0
"(batch_size, max_sequence_length) batch_size = #words in sentence,",0
max_sequence_length = length of longest label of sentence + 1,0
get char embeddings,0
"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size",0
take decoder input and initial hidden and pass through RNN,0
"if all encoder outputs are provided, use attention",0
take convex combinations of encoder hidden states as new output using the computed attention coefficients,0
"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)",0
get all tokens,0
encode input characters by sending them through RNN,0
get one-hots for characters and add special symbols / padding,0
determine length of each token,0
embed sentences,0
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)",0
variable to store initial hidden states for decoder,0
encode input characters by sending them through RNN,0
test packing and padding,0
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder,0
concatenate the final hidden states of the encoder. These will be projected to hidden_size of,0
decoder later with self.emb_to_hidden,0
mask out vectors that correspond to a dummy symbol (TODO: check attention masking),1
use token embedding as initial hidden state for decoder,0
concatenate everything together and project to appropriate size for decoder,0
variable to store initial hidden states for decoder,0
encode input characters by sending them through RNN,0
note that we do not need to fill up with dummy symbols since we process each token seperately,0
embed character one-hots,0
send through encoder RNN (produces initial hidden for decoder),0
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder,0
project 2*hidden_size to hidden_size,0
concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder,0
later with self.emb_to_hidden,0
use token embedding as initial hidden state for decoder,0
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)",0
concatenate everything together and project to appropriate size for decoder,0
"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)",0
"create target vector (batch_size, max_label_seq_length + 1)",0
filter empty sentences,0
max length of the predicted sequences,0
for printing,0
stop if all sentences are empty,0
remove previously predicted labels of this type,0
create list of tokens in batch,0
encode inputs,0
"create input for first pass (batch_size, 1, input_size), first letter is special character <S>",0
sequence length is always set to one in prediction,0
option 1: greedy decoding,0
predictions,0
decode next character,0
pick top beam size many outputs with highest probabilities,0
option 2: beam search,0
out_probs = self.softmax(output_vectors).squeeze(1),0
make sure no dummy symbol <> or start symbol <S> is predicted,0
pick top beam size many outputs with highest probabilities,0
"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1",0
"leading_indices and probabilities have size (batch_size, beam_size)",0
keep scores of beam_size many hypothesis for each token in the batch,0
stack all leading indices of all hypothesis and corresponding hidden states in two tensors,0
save sequences so far,0
keep track of how many hypothesis were completed for each token,0
"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)",0
decode with log softmax,0
make sure no dummy symbol <> or start symbol <S> is predicted,0
"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside",0
"if the sequence is already ended, do not record as candidate",0
index of token in in list tokens_in_batch,0
print(token_number),0
hypothesis score,0
TODO: remove token if number of completed hypothesis exceeds given value,1
set score of corresponding entry to -inf so it will not be expanded,0
get leading_indices for next expansion,0
find highest scoring hypothesis among beam_size*beam_size possible ones for each token,0
take beam_size many copies of scores vector and add scores of possible new extensions,0
"size (beam_size*batch_size, beam_size)",0
print(hypothesis_scores),0
"reshape to vector of size (batch_size, beam_size*beam_size),",0
each row contains beam_size*beam_size scores of the new possible hypothesis,0
print(hypothesis_scores_per_token),0
"choose beam_size best for each token - size (batch_size, beam_size)",0
out of indices_per_token we now need to recompute the original indices of the hypothesis in,0
a list of length beam_size*batch_size,0
"where the first three inidices belong to the first token, the next three to the second token,",0
and so on,0
with these indices we can compute the tensors for the next iteration,0
expand sequences with corresponding index,0
add log-probabilities to the scores,0
save new leading indices,0
save corresponding hidden states,0
it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations,0
in that case we append one of the final seuqences without end symbol to the final_candidates,0
get best final hypothesis for each token,0
get characters from index sequences and add predicted label to token,0
dictionaries,0
all parameters will be pushed internally to the specified device,0
now print labels in CoNLL format,0
internal candidate lists of generator,0
load Zelda candidates if so passed,0
create candidate lists,0
"if lower casing is enabled, create candidate lists of lower cased versions",0
create a new dictionary for lower cased mentions,0
go through each mention and its candidates,0
"check if backoff mention already seen. If so, add candidates. Else, create new entry.",0
set lowercased version as map,0
remap state dict for models serialized with Flair <= 0.11.3,0
get the candidates,0
"during training, add the gold value as candidate",0
----- Create the internal tag dictionary -----,0
span-labels need special encoding (BIO or BIOES),0
the big question is whether the label dictionary should contain an UNK or not,0
"without UNK, we cannot evaluate on data that contains labels not seen in test",0
"with UNK, the model learns less well if there are no UNK examples",0
is this a span prediction problem?,0
----- Embeddings -----,0
----- Initial loss weights parameters -----,0
----- RNN specific parameters -----,0
----- Conditional Random Field parameters -----,0
"Previously trained models have been trained without an explicit CRF, thus it is required to check",0
whether we are loading a model from state dict in order to skip or add START and STOP token,0
----- Dropout parameters -----,0
dropouts,0
remove word dropout if there is no contact over the sequence dimension.,0
----- Model layers -----,0
----- RNN layer -----,0
"If shared RNN provided, else create one for model",0
Whether to train initial hidden state,0
final linear map to tag space,0
"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss",0
"if using CRF, we also require a CRF and a Viterbi decoder",0
"if there are no sentences, there is no loss",0
forward pass to get scores,0
calculate loss given scores and labels,0
make a zero-padded tensor for the whole sentence,0
linear map to tag space,0
"Depending on whether we are using CRF or a linear layer, scores is either:",0
"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF",0
"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer",0
spans need to be encoded as token-level predictions,0
all others are regular labels for each token,0
make sure it's a list,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
get features from forward propagation,0
remove previously predicted labels of this type,0
"if return_loss, get loss value",0
make predictions,0
add predictions to Sentence,0
BIOES-labels need to be converted to spans,0
"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)",0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
core Flair models on Huggingface ModelHub,0
"Large NER models,",0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Language-specific NER models,0
Language-specific POS models,0
English NER models,0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Danish models,0
German models,0
French models,0
Dutch models,0
Malayalam models,0
Portuguese models,0
Keyphase models,0
Biomedical models,0
check if model name is a valid local file,0
"check if model key is remapped to HF key - if so, print out information",0
get mapped name,0
use mapped name instead,0
"if not, check if model key is remapped to direct download location. If so, download model",0
special handling for the taggers by the @redewiegergabe project (TODO: move to model hub),1
"for all other cases (not local file or special download location), use HF model hub",0
"if not a local file, get from model hub",0
use model name as subfolder,0
Lazy import,0
output information,0
## Demo: How to use in Flair,0
load tagger,0
make example sentence,0
predict NER tags,0
print sentence,0
print predicted NER spans,0
iterate over entities and print,0
Lazy import,0
Save model weight,0
Determine if model card already exists,0
Generate and save model card,0
Upload files,0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
print labels in CoNLL format,0
the multi task model has several labels,0
biomedical models,0
entity linker,0
auto-spawn on GPU if available,0
remap state dict for models serialized with Flair <= 0.11.3,0
English sentiment models,0
Communicative Functions Model,0
"scores_at_targets[range(features.shape[0]), lengths.values -1]",0
Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices,0
"Initially, get scores from <start> tag to all other tags",0
"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp",0
"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep",0
Create a tensor to hold accumulated sequence scores at each current tag,0
Create a tensor to hold back-pointers,0
"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag",0
"Let pads be the <end> tag index, since that was the last tag in the decoded sequence",0
"We add scores at current timestep to scores accumulated up to previous timestep, and",0
choose the previous timestep that corresponds to the max. accumulated score for each current timestep,0
"If sentence is over, add transition to STOP-tag",0
Decode/trace best path backwards,0
Sanity check,0
remove start-tag and backscore to stop-tag,0
Max + Softmax to get confidence score for predicted label and append label to each token,0
"Transitions are used in the following way: transitions[to, from].",0
"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag",0
to START-tag and from STOP-tag to any other tag to -10000.,0
create a model card for this model with Flair and PyTorch version,0
also record Transformers version if library is loaded,0
remember all parameters used in train() call,0
add model card to model,0
"if optimizer class is passed, instantiate:",0
"determine what splits (train, dev, test) to evaluate and log",0
prepare loss logging file and set up header,0
"from here on, use list of learning rates",0
load existing optimizer state dictionary if it exists,0
"minimize training loss if training with dev data, else maximize dev score",0
"if scheduler is passed as a class, instantiate",0
"if we load a checkpoint, we have already trained for epoch",0
"Determine whether to log ""bad epochs"" information",0
load existing scheduler state dictionary if it exists,0
update optimizer and scheduler in model card,0
"if training also uses dev/train data, include in training set",0
initialize sampler if provided,0
init with default values if only class is provided,0
set dataset to sample from,0
this field stores the names of all dynamic embeddings in the model (determined after first forward pass),0
At any point you can hit Ctrl + C to break out of training early.,0
update epoch in model card,0
get new learning rate,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
"if shuffle_first_epoch==False, the first epoch is not shuffled",0
process mini-batches,0
zero the gradients on the model and optimizer,0
"if necessary, make batch_steps",0
forward and backward for batch,0
forward pass,0
Backward,0
identify dynamic embeddings (always deleted) on first sentence,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
do the optimizer step,0
do the scheduler step if one-cycle or linear decay,0
get new learning rate,0
evaluate on train / dev / test split depending on training settings,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
calculate scores using dev data if available,0
append dev score to score history,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
determine if this is the best model or if we need to anneal,0
default mode: anneal against dev score,0
alternative: anneal against dev loss,0
alternative: anneal against train loss,0
determine bad epoch number,0
lr unchanged,0
log bad epochs,0
output log file,0
make headers on first epoch,0
"if checkpoint is enabled, save model at each epoch",0
Check whether to save best model,0
"if we do not use dev data for model selection, save final model",0
test best model if test data is present,0
recover all arguments that were used to train this model,0
you can overwrite params with your own,0
surface nested arguments,0
resume training with these parameters,0
"If set, add a factor to the learning rate of all parameters with 'embeddings' not in name",0
get and return the final test score of best model,0
cast string to Path,0
forward pass,0
update optimizer and scheduler,0
"TextDataset returns a list. valid and test are only one file,",0
so return the first element,0
cast string to Path,0
error message if the validation dataset is too small,0
Shuffle training files randomly after serially iterating,0
through corpus one,0
"iterate through training data, starting at",0
self.split (for checkpointing),0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
Backward,0
`clip_grad_norm` helps prevent the exploding gradient,0
problem in RNNs / LSTMs.,0
We detach the hidden state from how it was,0
previously produced.,0
"If we didn't, the model would try backpropagating",0
all the way to start of the dataset.,0
explicitly remove loss to clear up memory,0
#########################################################,0
Save the model if the validation loss is the best we've,0
seen so far.,0
#########################################################,0
print info,0
#########################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn,0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
1-camembert-base -> camembert-base,0
1-xlm-roberta-large -> xlm-roberta-large,0
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol,0
The mask has 1 for real tokens and 0 for padding tokens. Only real,0
tokens are attended to.,0
Zero-pad up to the sequence length.,0
"first, find longest sentence in batch",0
prepare id maps for BERT model,0
put encoded batch through BERT model to get all hidden states of all encoder layers,0
get aggregated embeddings for each BERT-subtoken in sentence,0
get the current sentence object,0
add concatenated embedding to sentence,0
use first subword embedding if pooling operation is 'first',0
"otherwise, do a mean over all subwords in token",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
embed a dummy sentence to determine embedding_length,0
Avoid conflicts with flair's Token class,0
"legacy pickle-like saving for image embeddings, as implementation details are not obvious",0
"legacy pickle-like loading for image embeddings, as implementation details are not obvious",0
"<cls> token initially set to 1/D, so it attends to all image features equally",0
add positional encodings,0
reshape the pixels into the sequence,0
layer norm after convolution and positional encodings,0
add <cls> token,0
"transformer requires input in the shape [h*w+1, b, d]",0
the output is an embedding of <cls> token,0
this parameter is fixed,0
optional fine-tuning on top of embedding layer,0
"if only one sentence is passed, convert to list of sentence",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push through RNN,0
after-RNN dropout,0
extract embeddings from RNN,0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
serialize the language models and the constructor arguments (but nothing else),0
re-initialize language model with constructor arguments,0
special handling for deserializing language models,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
IMPORTANT: add embeddings as torch modules,0
iterate over sentences,0
"if its a forward LM, take last state",0
"convert to plain strings, embedded in a list for the encode function",0
CNN,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push CNN,0
after-CNN dropout,0
extract embeddings from CNN,0
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency",0
"if only one sentence is passed, convert to list of sentence",0
Expose base classses,0
Expose document embedding classes,0
Expose image embedding classes,0
Expose legacy embedding classes,0
Expose token embedding classes,0
in some cases we need to insert zero vectors for tokens without embedding.,0
padding,0
remove special markup,0
check if special tokens exist to circumvent error message,0
iterate over subtokens and reconstruct tokens,0
remove special markup,0
check if reconstructed token is special begin token ([CLS] or similar),0
some BERT tokenizers somehow omit words - in such cases skip to next token,0
"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token.",0
if tokens are unaccounted for,0
check if all tokens were matched to subtokens,0
The layoutlm tokenizer doesn't handle ocr themselves,0
"transformers returns the ""added_tokens.json"" even if it doesn't create it",0
"transformers returns the ""added_tokens.json"" even if it doesn't create it",0
in case of doubt: token embedding has higher priority than document embedding,0
random check some tokens to save performance.,0
Models such as FNet do not have an attention_mask,0
set language IDs for XLM-style transformers,0
"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have",0
"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids.",0
set context if not set already,0
flair specific pre-tokenization,0
"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first.",0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
do not print transformer warnings as these are confusing in this case,0
load tokenizer and transformer model,0
load tokenizer from inmemory zip-file,0
model name,0
embedding parameters,0
send mini-token through to check how many layers the model has,0
return length,0
"If we use a context separator, add a new special token",0
"most models have an initial BOS token, except for XLNet, T5 and GPT2",0
"when initializing, embeddings are in eval mode by default",0
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial",0
"cannot run `.encode` if ocr boxes are required, assume",0
in case of doubt: token embedding has higher priority than document embedding,0
in case of doubt: token embedding has higher priority than document embedding,0
legacy TransformerDocumentEmbedding,0
legacy TransformerTokenEmbedding,0
legacy Flair <= 0.12,0
legacy Flair <= 0.7,0
legacy TransformerTokenEmbedding,0
Legacy TransformerDocumentEmbedding,0
legacy TransformerTokenEmbedding,0
legacy TransformerDocumentEmbedding,0
copy values from new embedding,0
cls first pooling can be done without recreating sentence hidden states,0
make the tuple a tensor; makes working with it easier.,0
"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention",0
only use layers that will be outputted,0
this parameter is fixed,0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
make compatible with serialized models,0
gensim version 4,0
gensim version 3,0
"if no embedding is set, the vocab and embedding length is requried",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNINOS embeddings,0
pubmed embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
fix serialized models,0
"this is required to force the module on the cpu,",0
"if a parent module is put to gpu, the _apply is called to each sub_module",0
self.to(..) actually sets the device properly,0
this ignores the get_cached_vec method when loading older versions,0
it is needed for compatibility reasons,0
gensim version 4,0
gensim version 3,0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
multilingual models,0
English models,0
Arabic,0
Bulgarian,0
Czech,0
Danish,0
German,0
Spanish,0
Basque,0
Persian,0
Finnish,0
French,0
Hebrew,0
Hindi,0
Croatian,0
Indonesian,0
Italian,0
Japanese,0
Malayalam,0
Dutch,0
Norwegian,0
Polish,0
Portuguese,0
Pubmed,0
Slovenian,0
Swedish,0
Tamil,0
Spanish clinical,0
CLEF HIPE Shared task,0
Amharic,0
Ukrainian,0
load model if in pretrained model map,0
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir),0
CLEF HIPE models are lowercased,0
embeddings are static if we don't do finetuning,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
make compatible with serialized models (TODO: remove),1
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout",0
make compatible with serialized models (TODO: remove),1
gradients are enable if fine-tuning is enabled,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
offset mode that extracts at whitespace after last character,0
offset mode that extracts at last character,0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
check token.text is empty or not,0
set aggregation operation,0
add embeddings after updating,0
model architecture,0
model architecture,0
"""pl"",",0
download if necessary,0
load the model,0
"TODO: keep for backwards compatibility, but remove in future",1
save the sentence piece model as binary file (not as path which may change),0
write out the binary sentence piece model into the expected directory,0
"if the model was saved as binary and it is not found on disk, write to appropriate path",0
"otherwise, use normal process and potentially trigger another download",0
"once the modes if there, load it with sentence piece",0
empty words get no embedding,0
all other words get embedded,0
GLOVE embeddings,0
no need to recreate as NILCEmbeddings,0
read in test file if exists,0
read in dev file if exists,0
"find train, dev and test files if not specified",0
Add tags for each annotated span,0
Remove leading and trailing whitespaces from annotated spans,0
Search start and end token index for current span,0
If end index is not found set to last token,0
Throw error if indices are not valid,0
get train data,0
read in test file if exists,0
read in dev file if exists,0
"find train, dev and test files if not specified",0
special key for space after,0
special key for feature columns,0
special key for dependency head id,0
"store either Sentence objects in memory, or only file offsets",0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
determine encoding of text file,0
identify which columns are spans and which are word-level,0
now load all sentences,0
skip first line if to selected,0
option 1: keep Sentence objects in memory,0
pointer to previous,0
parse next sentence,0
quit if last sentence reached,0
skip banned sentences,0
set previous and next sentence for context,0
append parsed sentence to list in memory,0
option 2: keep source data in memory,0
"read lines for next sentence, but don't parse",0
quit if last sentence reached,0
append raw lines for each sentence,0
we make a distinction between word-level tags and span-level tags,0
read first sentence to determine which columns are span-labels,0
skip first line if to selected,0
check the first 5 sentences,0
go through all annotations and identify word- and span-level annotations,0
- if a column has at least one BIES we know it's a Span label,0
"- if a column has at least one tag that is not BIOES, we know it's a Token label",0
- problem cases are columns for which we see only O - in this case we default to Span,0
skip assigned columns,0
the space after key is always word-levels,0
"if at least one token has a BIES, we know it's a span label",0
"if at least one token has a label other than BIOES, we know it's a token label",0
all remaining columns that are not word-level are span-level,0
for column in self.word_level_tag_columns:,0
"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")",0
"if sentence ends, break",0
parse comments if possible,0
"otherwise, this line is a token. parse and add to sentence",0
check if this sentence is a document boundary,0
add span labels,0
discard tags from tokens that are not added to the sentence,0
parse relations if they are set,0
head and tail span indices are 1-indexed and end index is inclusive,0
parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas',0
"to set the metadata ""domain"" to ""de-orcas""",0
get fields from line,0
get head_id if exists (only in dependency parses),0
initialize token,0
go through all columns,0
'feats' and 'misc' column should be split into different fields,0
special handling for whitespace after,0
add each other feature as label-value pair,0
get the task name (e.g. 'ner'),0
get the label value,0
add label,0
remap regular tag names,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
set sentence context using partials TODO: pointer to dataset is really inefficient,1
use all domains,0
iter over all domains / sources and create target files,0
Parameters,0
The conll representation of coref spans allows spans to,0
"overlap. If spans end or begin at the same word, they are",0
"separated by a ""|"".",0
The span begins at this word.,0
The span begins and ends at this word (single word span).,0
"The span is starting, so we record the index of the word.",0
"The span for this id is ending, but didn't start at this word.",0
Retrieve the start index from the document state and,0
add the span to the clusters for this id.,0
Parameters,0
strip all bracketing information to,0
get the actual propbank label.,0
Entering into a span for a particular semantic role label.,0
We append the label and set the current span for this annotation.,0
"If there's no '(' token, but the current_span_label is not None,",0
then we are inside a span.,0
We're outside a span.,0
"Exiting a span, so we reset the current span label for this annotation.",0
The words in the sentence.,0
The pos tags of the words in the sentence.,0
the pieces of the parse tree.,0
The lemmatised form of the words in the sentence which,0
have SRL or word sense information.,0
The FrameNet ID of the predicate.,0
"The sense of the word, if available.",0
"The current speaker, if available.",0
"Cluster id -> List of (start_index, end_index) spans.",0
Cluster id -> List of start_indices which are open for this id.,0
Replace brackets in text and pos tags,0
with a different token for parse trees.,0
only keep ')' if there are nested brackets with nothing in them.,0
There are some bad annotations in the CONLL data.,0
"They contain no information, so to make this explicit,",0
we just set the parse piece to be None which will result,0
in the overall parse tree being None.,0
"If this is the first word in the sentence, create",0
empty lists to collect the NER and SRL BIO labels.,0
"We can't do this upfront, because we don't know how many",0
"components we are collecting, as a sentence can have",0
variable numbers of SRL frames.,0
Create variables representing the current label for each label,0
sequence we are collecting.,0
"If any annotation marks this word as a verb predicate,",0
we need to record its index. This also has the side effect,0
of ordering the verbal predicates by their location in the,0
"sentence, automatically aligning them with the annotations.",0
"this would not be reached if parse_pieces contained None, hence the cast",0
Non-empty line. Collect the annotation.,0
Collect any stragglers or files which might not,0
have the '#end document' format for the end of the file.,0
this dataset name,0
check if data there,0
column format,0
this dataset name,0
check if data there,0
column format,0
this dataset name,0
download data if necessary,0
download files if not present locally,0
we need to slightly modify the original files by adding some new lines after document separators,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
Remove CoNLL-U meta information in the last column,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
entity_mapping,0
this dataset name,0
download data if necessary,0
data validation,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download files if not present locallys,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
# download zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
download data if necessary,0
"unzip the downloaded repo and merge the train, dev and test datasets",0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
check if data there,0
create folder,0
download dataset,0
column format,0
this dataset name,0
download and parse data if necessary,0
create train test dev if not exist,0
column format,0
this dataset name,0
If the extracted corpus file is not yet present in dir,0
download zip if necessary,0
"extracted corpus is not present , so unpacking it.",0
column format,0
this dataset name,0
download zip,0
unpacking the zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)",0
download files if not present locally,0
we need to modify the original files by adding new lines after after the end of each sentence,0
if only one language is given,0
column format,0
this dataset name,0
"use all languages if explicitly set to ""all""",0
download data if necessary,0
initialize comlumncorpus and add it to list,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
this list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
get google drive id from list,0
download from google drive,0
unzip,0
"tar.extractall(language_folder,members=[tar.getmember(file_name)])",0
transform data into required format,0
"the processed dataset has the additional ending ""_new""",0
remove the unprocessed dataset,0
initialize comlumncorpus and add it to list,0
if no languages are given as argument all languages used in XTREME will be loaded,0
if only one language is given,0
column format,0
this dataset name,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
This list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
download from HU Server,0
unzip,0
transform data into required format,0
initialize comlumncorpus and add it to list,0
if only one language is given,0
column format,0
this dataset name,0
download data if necessary,0
initialize comlumncorpus and add it to list,0
download data if necessary,0
unpack and write out in CoNLL column-like format,0
column format,0
this dataset name,0
download data if necessary,0
data is not in IOB2 format. Thus we transform it to IOB2,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
column format,0
this dataset name,0
rename according to train - test - dev - convention,0
column format,0
this dataset name,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
Add missing newline after header,0
Workaround for empty tokens,1
"Add ""real"" document marker",0
Dataset split mapping,0
v2.0 only adds new language and splits for AJMC dataset,0
Special document marker for sample splits in AJMC dataset,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
paths to train and test splits,0
init corpus,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
iterate over all html files,0
"get rid of html syntax, we only need the text",0
between all documents we write a separator symbol,0
skip empty strings,0
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)",0
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention",0
sentence splitting and tokenization,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out-file in column format,0
"in case something goes wrong, delete the dataset and raise error",0
this dataset name,0
download and parse data if necessary,0
from qwikidata.linked_data_interface import get_entity_dict_from_api,0
generate qid wikiname dictionaries,0
merge dictionaries,0
ignore first line,0
commented and empty lines,0
read all Q-IDs,0
ignore first line,0
request,0
this dataset name,0
we use the wikiids in the data instead of directly utilizing the wikipedia urls.,0
like this we can quickly check if the corresponding page exists,0
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi,0
delete unprocessed file,0
collect all wikiids,0
create the dictionary,0
request,0
this dataset name,0
names of raw text documents,0
open output_file,0
iterate through all documents,0
split sentences and tokenize,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out file,0
this dataset name,0
download and parse data if necessary,0
this dataset name,0
download and parse data if necessary,0
First parse the post titles,0
Keep track of how many and which entity mentions does a given post title have,0
Check if the current post title has an entity link and parse accordingly,0
Post titles with entity mentions (if any) are handled via this function,0
Then parse the comments,0
"Iterate over the comments.tsv file, until the end is reached",0
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched.",0
Each comment thread is handled as one 'document'.,0
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.,0
This if-condition is needed to handle this problem.,0
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure",0
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above",0
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle.",0
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,",0
and not just single letters into single rows.,0
If there are annotated entity mentions for given post title or a comment thread,0
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence",0
Write the token with a corresponding tag to file,0
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed",0
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag",0
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized,0
"incorrectly, in order to keep the desired format (empty line as a sentence separator).",0
"Thrown when the second check above happens, but the last token of a sentence is reached.",0
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below.",0
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS",0
Check if further annotations belong to the current post title or comment thread as well,0
Stop when the end of an annotation file is reached,0
Check if further annotations belong to the current sentence as well,0
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)",0
Docstart,0
if there is more than one word in the chunk we write each in a separate line,0
print(chunks),0
empty line after each sentence,0
convert the file to CoNLL,0
this dataset name,0
"check if data there, if not, download the data",0
create folder,0
download data,0
transform data into column format if necessary,0
if no filenames are specified we use all the data,0
"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled",0
also we remove 'raganato_ALL' from filenames in case its in the list,0
generate the test file,0
make column file and save to data_folder,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
this dataset name,0
download data if necessary,0
if True:,0
write CoNLL-U Plus header,0
"Some special cases (e.g., missing spaces before entity marker)",0
necessary if text should be whitespace tokenizeable,0
Handle case where tail may occur before the head,0
this dataset name,0
write CoNLL-U Plus header,0
this dataset name,0
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors,1
download data if necessary,0
write CoNLL-U Plus header,0
The span has ended.,0
We are entering a new span; reset indices,0
and active tag to new span.,0
We're inside a span.,0
Last token might have been a part of a valid span.,0
this dataset name,0
write CoNLL-U Plus header,0
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):",0
"with zip_file.open(source_file_path, mode=""r"") as source_file:",0
target_file_path = Path(data_folder) / target_filename,0
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:",0
# write CoNLL-U Plus header,0
"target_file.write(""# global.columns = id form ner\n"")",0
for example in json.load(source_file):,0
token_list = self._tacred_example_to_token_list(example),0
target_file.write(token_list.serialize()),0
check if first tag row is already occupied,0
"if first tag row is occupied, use second tag row",0
hardcoded mapping TODO: perhaps find nicer solution,1
remap regular tag names,0
else skip to position in file where sentence begins,0
set sentence context using partials TODO: pointer to dataset is really inefficient,1
read in dev file if exists,0
read in test file if exists,0
the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44,0
"find train, dev and test files if not specified",0
use test_file to create test split if available,0
use dev_file to create test split if available,0
"if data point contains black-listed label, do not use",0
first check if valid sentence,0
"if so, add to indices",0
"find train, dev and test files if not specified",0
variables,0
different handling of in_memory data than streaming data,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
test if format is OK,0
test if at least one label given,0
make sentence from text (and filter for length),0
"if a pair column is defined, make a sentence pair object",0
noinspection PyDefaultArgument,0
dataset name includes the split size,0
default dataset folder is the cache root,0
download data if necessary,0
download each of the 28 splits,0
create dataset directory if necessary,0
download senteval datasets if necessary und unzip,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
handle labels file,0
handle data file,0
Create flair compatible labels,0
"by defaut, map point score to POSITIVE / NEGATIVE values",0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file from CSV,0
create test.txt file from CSV,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create train dev and test files in fasttext format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
convert to FastText format,0
download data if necessary,0
"if data is not downloaded yet, download it",0
get the zip file,0
move original .tsv files to another folder,0
create train and dev splits in fasttext format,0
create eval_dataset file with no labels,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download datasets if necessary,0
create dataset directory if necessary,0
create correctly formated txt files,0
multiple labels are possible,0
this dataset name,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
create a separate directory for different tasks,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
check if dataset is supported,0
set file names,0
set file names,0
download and unzip in file structure if necessary,0
instantiate corpus,0
"find train, dev and test files if not specified",0
"create DataPairDataset for train, test and dev file, if they are given",0
stop if file does not exist,0
create a DataPair object from strings,0
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"reorder dev datasets to have same columns as in train set: 8, 9, and 11",0
dev sets include 5 different annotations but we will only keep the gold label,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get test and dev sets,0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data not downloaded yet, download it",0
get the zip file,0
"the downloaded files have json format, we transform them to tsv",0
Function to transform JSON file to tsv for Recognizing Textual Entailment Data,0
remove json file,0
Uses dynamic programming approach to calculate maximum independent set in interval graph,0
with sum of all entity lengths as secondary key,0
calculate offset without current text,0
because we stick all passages of a document together,0
TODO For split entities we also annotate everything inbetween which might be a bad idea?,1
Try to fix incorrect annotations,0
print(,0
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}""",0
),0
Ignore empty lines or relation annotations,0
FIX annotation of whitespaces (necessary for PDR),0
One token may contain multiple entities -> deque all of them,0
column format,0
this dataset name,0
Create tokenization-dependent CONLL files. This is necessary to prevent,0
from caching issues (e.g. loading the same corpus with different sentence splitters),0
column format,0
this dataset name,0
column format,0
this dataset name,0
Edge case: last token starts a new entity,0
Last document in file,0
column format,0
this dataset name,0
column format,0
this dataset name,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
Edge case: last token starts a new entity,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
column format,0
this dataset name,0
Read texts,0
Read annotations,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
We need to apply a patch to correct the original training file,0
Articles title,0
Article abstract,0
Entity annotations,0
column format,0
this dataset name,0
Edge case: last token starts a new entity,0
Map all entities to chemicals,0
Map all entities to disease,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
Incomplete article,0
Invalid XML syntax,0
column format,0
this dataset name,0
column format,0
this dataset name,0
if len(mid) != 3:,0
continue,0
Try to fix entity offsets,0
column format,0
this dataset name,0
There is still one illegal annotation in the file ..,0
column format,0
this dataset name,0
"Abstract first, title second to prevent issues with sentence splitting",0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
column format,0
this dataset name,0
"Filter for specific entity types, by default no entities will be filtered",0
Get original HUNER splits to retrieve a list of all document ids contained in V2,0
train and dev split of V2 will be train in V4,0
test split of V2 will be dev in V4,0
New documents in V4 will become test documents,0
column format,0
this dataset name,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
cache Feidegger config file,0
cache Feidegger images,0
replace image URL with local cached file,0
append Sentence-Image data point,0
"in certain cases, multi-CPU data loading makes no sense and slows",0
"everything down. For this reason, we detect if a dataset is in-memory:",0
"if so, num_workers is set to 0 for faster processing",0
cast to list if necessary,0
cast to list if necessary,0
"first, check if pymongo is installed",0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
Expose base classses,0
Expose all biomedical data sets used for the evaluation of BioBERT,0
-,0
-,0
-,0
-,0
Expose all biomedical data sets using the HUNER splits,0
Expose all biomedical data sets,0
Expose all document classification datasets,0
word sense disambiguation,0
Expose all entity linking datasets,0
Expose all relation extraction datasets,0
universal proposition banks,0
keyphrase detection datasets,0
other NER datasets,0
standard NER datasets,0
Expose all sequence labeling datasets,0
Expose all text-image datasets,0
Expose all text-text datasets,0
Expose all treebanks,0
"find train, dev and test files if not specified",0
get train data,0
get test data,0
get dev data,0
option 1: read only sentence boundaries as offset positions,0
option 2: keep everything in memory,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
current token ID,0
handling for the awful UD multiword format,0
end of sentence,0
comments,0
ellipsis,0
if token is a multi-word,0
normal single-word tokens,0
"if we don't split multiwords, skip over component words",0
add token,0
add morphological tags,0
derive whitespace logic for multiwords,0
print(token),0
print(current_multiword_last_token),0
print(current_multiword_first_token),0
"if multi-word equals component tokens, there should be no whitespace",0
go through all tokens in subword and set whitespace_after information,0
print(i),0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
this dataset name,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"finally, print model card for information",0
test corpus,0
create a TARS classifier,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
test corpus,0
create a TARS classifier,0
switch to a new task (TARS can do multiple tasks so you must define one),0
initialize the text classifier trainer,0
start the training,0
"With end symbol, without start symbol, padding in front",0
"Without end symbol, with start symbol, padding in back",0
"Without end symbol, without start symbol, padding in front",0
initialize trainer,0
initialize trainer,0
train model for 2 epochs,0
load the checkpoint model and train until epoch 4,0
clean up results directory,0
initialize trainer,0
initialize trainer,0
increment for last token in sentence if not followed by whitespace,0
clean up directory,0
clean up directory,0
example sentence,0
set 4 labels for 2 tokens ('love' is tagged twice),0
check if there are three POS labels with correct text and values,0
check if there are is one SENTIMENT label with correct text and values,0
check if all tokens are correctly labeled,0
remove the pos label from the last word,0
there should be 2 POS labels left,0
now remove all pos tags,0
set 3 labels for 2 spans (HU is tagged twice),0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
"now delete the NER tags of ""Humboldt-Universitt zu Berlin""",0
should be only one NER label left,0
and only one NER span,0
set 3 labels for 2 spans (HU is tagged twice with different tags),0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
"now delete the NER tags of ""Humboldt-Universitt zu Berlin""",0
should be only one NER label left,0
and only one NER span,0
but there is also one orgtype span and label,0
and only one NER span,0
let's add the NER tag back,0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
now remove all NER tags,0
set 3 labels for 2 spans (HU is tagged twice with different tags),0
create two relation label,0
there should be two relation labels,0
there should be one syntactic labels,0
"there should be two relations, one with two and one with one label",0
example sentence,0
add another topic label,0
example sentence,0
has sentiment value,0
has 4 part of speech tags,0
has 1 NER tag,0
should be in total 6 labels,0
example sentence,0
add two NER labels,0
get the four labels,0
check that only two of the respective data points are equal,0
make a sentence and some right context,0
TODO: is this desirable? Or should two sentences with same text still be considered different objects?,1
Initializing a Sentence this way assumes that there is a space after each token,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
define search space,0
sequence tagger parameter,0
model trainer parameter,0
training parameter,0
find best parameter settings,0
clean up results directory,0
document embeddings parameter,0
training parameter,0
clean up results directory,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
load column dataset with one entry,0
load column dataset with two entries,0
load column dataset with three entries,0
"get training, test and dev data",0
"get training, test and dev data",0
check if Token labels are correct,0
"get training, test and dev data",0
check if Token labels for frames are correct,0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
"assert [token.get_tag(""head"").value for token in sent1.tokens] == [",0
"""2"",",0
"""0"",",0
"""4"",",0
"""2"",",0
"""2"",",0
"""2"",",0
],0
"Here, we use the default token annotation fields.",0
"We have manually checked, that these numbers are correct:",0
"+1 offset, because of missing EOS marker at EOD",0
Test data for v2.1 release,0
--- Embeddings that are shared by both models --- #,0
--- Task 1: Sentiment Analysis (5-class) --- #,0
Define corpus and model,0
-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #,0
Define corpus and model,0
-- Define mapping (which tagger should train on which model) -- #,0
-- Create model trainer and train -- #,0
load dataset,0
tagger without CRF,0
train,0
check if loaded model can predict,0
check if loaded model successfully fit the training data,0
load dataset,0
tagger without CRF,0
train,0
check if loaded model can predict,0
check if loaded model successfully fit the training data,0
load dataset,0
tagger without CRF,0
train,0
check if loaded model can predict,0
check if loaded model successfully fit the training data,0
load dataset,0
tagger without CRF,0
train,0
check if loaded model can predict,0
check if loaded model successfully fit the training data,0
check if model can predict,0
load model,0
chcek if model predicts correct label,0
check if loaded model successfully fit the training data,0
check if model can predict,0
load model,0
chcek if model predicts correct label,0
check if loaded model successfully fit the training data,0
check if model can predict,0
load model,0
chcek if model predicts correct label,0
check if loaded model successfully fit the training data,0
clean up file,0
no need for label_dict,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
Intel ----founded_by---> Gordon Moore,0
Intel ----founded_by---> Robert Noyce,0
Check sentence masking and relation label annotation on,0
"training, validation and test dataset (in this test the splits are the same)",0
"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google .""",0
"Entity pair permutations of: ""Microsoft was founded by Bill Gates .""",0
"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 .""",0
"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany.""",0
This sentence is only included if we transform the corpus with cross augmentation,0
Ensure this is an example that predicts no classes in multilabel,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
ensure that the prepared tensors is what we expect,0
use a SequenceTagger to save and reload the embedding in the manner it is supposed to work,0
previous and next sentence as context,0
test expansion for sentence without context,0
test expansion for with previous and next as context,0
test expansion if first sentence is document boundary,0
test expansion if we don't use context,0
"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher.",0
dummy model with embeddings,0
save the dummy and load it again,0
check that context_length and use_context_separator is the same for both,0
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
Extract all the contents of zip file in current directory,0
Extract all the contents of zip file in current directory,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
load_big_file is a workaround byhttps://github.com/highway11git,1
to load models on some Mac/Windows setups,0
see https://github.com/zalandoresearch/flair/issues/351,0
first determine the distribution of classes in the dataset,0
weight for each sample,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
increment for last token in sentence if not followed by whitespace,0
this is the default init size of a lmdb database for embeddings,0
get db filename from embedding name,0
"In case initialization of cached version failed, just fallback to the original WordEmbeddings",0
SequenceTagger,0
TextClassifier,0
get db filename from embedding name,0
if embedding database already exists,0
"otherwise, push embedding to database",0
if embedding database already exists,0
open the database in read mode,0
we need to set self.k,0
create and load the database in write mode,0
"no idea why, but we need to close and reopen the environment to avoid",0
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot,0
when opening new transaction !,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
set 'add_unk' if the dictionary was created with a version of Flair older than 0.9,0
set 'add_unk' depending on whether <unk> is a key,0
"if one embedding name, directly return it",0
"if multiple embedding names, concatenate them",0
TODO: does it make sense to exclude labels? Two data points of identical text (but different labels),1
would be equal now.,0
First we remove any existing labels for this PartOfSentence in self.sentence,0
labels also need to be deleted at Sentence object,0
delete labels at object itself,0
private field for all known spans,0
the tokenizer used for this sentence,0
some sentences represent a document boundary (but most do not),0
internal variables to denote position inside dataset,0
"if text is passed, instantiate sentence with tokens (words)",0
determine token positions and whitespace_after flag,0
the last token has no whitespace after,0
log a warning if the dataset is empty,0
data with zero-width characters cannot be handled,0
set token idx and sentence,0
append token to sentence,0
register token annotations on sentence,0
move sentence embeddings to device,0
also move token embeddings to device,0
clear token embeddings,0
infer whitespace after field,0
"if sentence has no tokens, return empty string",0
"otherwise, return concatenation of tokens with the correct offsets",0
The sentence's start position is not propagated to its tokens.,0
"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces.",0
No character at the corresponding code point: remove it,0
"if no label if specified, return all labels",0
"if the label type exists in the Sentence, return it",0
return empty list if none of the above,0
labels also need to be deleted at all tokens,0
labels also need to be deleted at all known spans,0
remove spans without labels,0
delete labels at object itself,0
set name,0
abort if no data is provided,0
sample test data from train if none is provided,0
sample dev data from train if none is provided,0
set train dev and test data,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
count all label types per sentence,0
go through all labels of label_type and count values,0
check if there are any span labels,0
"if an unk threshold is set, UNK all label values below this threshold",0
sample randomly from a label distribution according to the probabilities defined by the desired noise share,0
replace the old label with the new one,0
keep track of the old (clean) label using another label type category,0
keep track of how many labels in total are flipped,0
Make the tag dictionary,0
"add a dummy ""O"" to close final prediction",0
return complex list,0
internal variables,0
non-set tags are OUT tags,0
anything that is not OUT is IN,0
does this prediction start a new span?,0
begin and single tags start new spans,0
"in IOB format, an I tag starts a span if it follows an O or is a different span",0
single tags that change prediction start new spans,0
if an existing span is ended (either by reaching O or starting a new span),0
determine score and value,0
append to result list,0
reset for-loop variables for new span,0
remember previous tag,0
global variable: cache_root,0
global variable: device,0
global variable: version,0
global variable: arrow symbol,0
dummy return to fulfill trainer.train() needs,0
print(vec),0
Attach optimizer,0
"convert `metrics` to float, in case it's a zero-dim Tensor",0
if memory mode option 'none' delete everything,0
"if dynamic embedding keys not passed, identify them automatically",0
always delete dynamic embeddings,0
"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)",0
optional metric space decoder if prototypes have different length than embedding,0
create initial prototypes for all classes (all initial prototypes are a vector of all 1s),0
"if set, create initial prototypes from normal distribution",0
"if set, use a radius",0
all parameters will be pushed internally to the specified device,0
decode embeddings into prototype space,0
"if unlabeled distance is set, mask out loss to unlabeled class prototype",0
Always include the name of the Model class for which the state dict holds,0
"in Flair <0.9.1, optimizer and scheduler used to train model are not saved",0
"write out a ""model card"" if one is set",0
special handling for optimizer:,0
remember optimizer class and state dictionary,0
save model,0
restore optimizer and scheduler to model card if set,0
"if this class is abstract, go through all inheriting classes and try to fetch and load the model",0
get all non-abstract subclasses,0
"try to fetch the model for each subclass. if fetching is possible, load model and return it",0
"skip any invalid loadings, e.g. not found on huggingface hub",0
"if the model cannot be fetched, load as a file",0
try to get model class from state,0
"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses",0
"if str(model_cls) == ""<class 'flair.models.pairwise_classification_model.TextPairClassifier'>"": continue",0
"skip any invalid loadings, e.g. not found on huggingface hub",0
"if this class is not abstract, fetch the model and load it",0
"make sure <unk> is contained in gold_label_dictionary, if given",0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
loss calculation,0
variables for printing,0
variables for computing scores,0
remove any previously predicted labels,0
predict for batch,0
get the gold labels,0
add to all_predicted_values,0
make printout lines,0
convert true and predicted values to two span-aligned lists,0
delete exluded labels if exclude_labels is given,0
"if after excluding labels, no label is left, ignore the datapoint",0
write all_predicted_values to out_file if set,0
make the evaluation dictionary,0
check if this is a multi-label problem,0
compute numbers by formatting true and predicted such that Scikit-Learn can use them,0
multi-label problems require a multi-hot vector for each true and predicted label,0
single-label problems can do with a single index for each true and predicted label,0
"now, calculate evaluation numbers",0
there is at least one gold label or one prediction (default),0
"if there is only one label, then ""micro avg"" = ""macro avg""",0
"micro average is only computed if zero-label exists (for instance ""O"")",0
if no zero-label exists (such as in POS tagging) micro average is equal to accuracy,0
same for the main score,0
issue error and default all evaluation numbers to 0.,0
line for log file,0
check if there is a label mismatch,0
print info,0
set the embeddings,0
initialize the label dictionary,0
initialize the decoder,0
set up multi-label logic,0
init dropouts,0
loss weights and loss function,0
Initialize the weight tensor,0
set up gradient reversal if so specified,0
embed sentences,0
get a tensor of data points,0
do dropout,0
make a forward pass to produce embedded data points and labels,0
get the data points for which to predict labels,0
get their gold labels as a tensor,0
pass data points through network to get encoded data point tensor,0
decode,0
an optional masking step (no masking in most cases),0
calculate the loss,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
filter data points in batch,0
stop if all sentences are empty,0
pass data points through network and decode,0
if anything could possibly be predicted,0
remove previously predicted labels of this type,0
add DefaultClassifier arguments,0
add variables of DefaultClassifier,0
Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23,0
Get projected 1st dimension,0
Compute bilinear form,0
Arcosh,0
Project the input data to n+1 dimensions,0
"The first dimension, is recomputed in the distance module",0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for",0
"train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
plot i,0
save plots,0
save plots,0
plt.show(),0
save plot,0
take the average over the last three scores of training,0
take average over the scores from the different training runs,0
auto-spawn on GPU if available,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
TODO: not saving lines yet,1
TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.,1
- MaskedRelationClassifier ?,0
This depends if this relation classification architecture should replace or offer as an alternative.,0
Set label type and prepare label dictionary,0
Initialize super default classifier,0
Add the special tokens from the encoding strategy,0
"Auto-spawn on GPU, if available",0
Only use entities labelled with the specified labels for each label type,0
Only use entities above the specified threshold,0
Use a dictionary to find gold relation annotations for a given entity pair,0
Yield head and tail entity pairs from the cross product of all entities,0
Remove identity relation entity pairs,0
Remove entity pairs with labels that do not match any,0
of the specified relations in `self.entity_pair_labels`,0
"Obtain gold label, if existing",0
Some sanity checks,0
Pre-compute non-leading head and tail tokens for entity masking,0
We can not use the plaintext of the head/tail span in the sentence as the mask/marker,0
since there may be multiple occurrences of the same entity mentioned in the sentence.,0
"Therefore, we use the span's position in the sentence.",0
Create masked sentence,0
Add gold relation annotation as sentence label,0
"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,",0
"during prediction, the forward pass does not need any knowledge about the entities in the sentence.",0
"If we sample missing splits, the encoded sentences that correspond to the same original sentences",0
"may get distributed into different splits. For training purposes, this is always undesired.",0
Ensure that all sentences are encoded properly,0
Deal with the case where all sentences are encoded sentences,0
"mypy does not infer the type of ""sentences"" restricted by the if statement",0
Deal with the case where all sentences are standard (non-encoded) sentences,0
"For each encoded sentence, transfer its prediction onto the original relation",0
auto-spawn on GPU if available,0
pad strings with whitespaces to longest sentence,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
"""document_delimiter"" property may be missing in some older pre-trained models",0
serialize the language models and the constructor arguments (but nothing else),0
special handling for deserializing language models,0
re-initialize language model with constructor arguments,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
Transform input data into TARS format,0
"if there are no labels, return a random sample as negatives",0
"otherwise, go through all labels",0
make sure the probabilities always sum up to 1,0
get and embed all labels by making a Sentence object that contains only the label text,0
get each label embedding and scale between 0 and 1,0
compute similarity matrix,0
"the higher the similarity, the greater the chance that a label is",0
sampled as negative example,0
make label dictionary if no Dictionary object is passed,0
prepare dictionary of tags (without B- I- prefixes and without UNK),0
check if candidate_label_set is empty,0
make list if only one candidate label is passed,0
create label dictionary,0
note current task,0
create a temporary task,0
make zero shot predictions,0
switch to the pre-existing task,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
make a tars sentence where all labels are O by default,0
init new TARS classifier,0
set all task information,0
return,0
with torch.no_grad():,0
progress bar for verbosity,0
stop if all sentences are empty,0
go through each sentence in the batch,0
always remove tags first,0
get the span and its label,0
determine whether tokens in this span already have a label,0
only add if all tokens have no label,0
make and add a corresponding predicted span,0
set indices so that no token can be tagged twice,0
clearing token embeddings to save memory,0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
get the serialized embeddings,0
remap state dict for models serialized with Flair <= 0.11.3,0
init new TARS classifier,0
set all task information,0
with torch.no_grad():,0
progress bar for verbosity,0
stop if all sentences are empty,0
go through each sentence in the batch,0
always remove tags first,0
add all labels that according to TARS match the text and are above threshold,0
do not add labels below confidence threshold,0
only use label with highest confidence if enforcing single-label predictions,0
get all label scores and do an argmax to get the best label,0
remove previously added labels and only add the best label,0
clearing token embeddings to save memory,0
set separator to concatenate two sentences,0
auto-spawn on GPU if available,0
pooling operation to get embeddings for entites,0
set embeddings,0
set relation and entity label types,0
"whether to use gold entity pairs, and whether to filter entity pairs by type",0
filter entity pairs according to their tags if set,0
whether to encode characters and whether to use attention (attention can only be used if chars are encoded),0
character dictionary for decoding and encoding,0
make sure <unk> is in dictionary for handling of unknown characters,0
add special symbols to dictionary if necessary and save respective indices,0
---- ENCODER ----,0
encoder character embeddings,0
encoder pre-trained embeddings,0
encoder RNN,0
additional encoder linear layer if bidirectional encoding,0
---- DECODER ----,0
decoder: linear layers to transform vectors to and from alphabet_size,0
when using attention we concatenate attention outcome and decoder hidden states,0
decoder RNN,0
loss and softmax,0
self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction,0
add additional columns for special symbols if necessary,0
initialize with dummy symbols,0
encode inputs,0
get labels (we assume each token has a lemma label),0
get char indices for labels of sentence,0
"(batch_size, max_sequence_length) batch_size = #words in sentence,",0
max_sequence_length = length of longest label of sentence + 1,0
get char embeddings,0
"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size",0
take decoder input and initial hidden and pass through RNN,0
"if all encoder outputs are provided, use attention",0
take convex combinations of encoder hidden states as new output using the computed attention coefficients,0
"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)",0
get all tokens,0
encode input characters by sending them through RNN,0
get one-hots for characters and add special symbols / padding,0
determine length of each token,0
embed sentences,0
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)",0
variable to store initial hidden states for decoder,0
encode input characters by sending them through RNN,0
test packing and padding,0
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder,0
concatenate the final hidden states of the encoder. These will be projected to hidden_size of,0
decoder later with self.emb_to_hidden,0
mask out vectors that correspond to a dummy symbol (TODO: check attention masking),1
use token embedding as initial hidden state for decoder,0
concatenate everything together and project to appropriate size for decoder,0
variable to store initial hidden states for decoder,0
encode input characters by sending them through RNN,0
note that we do not need to fill up with dummy symbols since we process each token seperately,0
embed character one-hots,0
send through encoder RNN (produces initial hidden for decoder),0
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder,0
project 2*hidden_size to hidden_size,0
concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder,0
later with self.emb_to_hidden,0
use token embedding as initial hidden state for decoder,0
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)",0
concatenate everything together and project to appropriate size for decoder,0
"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)",0
"create target vector (batch_size, max_label_seq_length + 1)",0
filter empty sentences,0
max length of the predicted sequences,0
for printing,0
stop if all sentences are empty,0
remove previously predicted labels of this type,0
create list of tokens in batch,0
encode inputs,0
"create input for first pass (batch_size, 1, input_size), first letter is special character <S>",0
sequence length is always set to one in prediction,0
option 1: greedy decoding,0
predictions,0
decode next character,0
pick top beam size many outputs with highest probabilities,0
option 2: beam search,0
out_probs = self.softmax(output_vectors).squeeze(1),0
make sure no dummy symbol <> or start symbol <S> is predicted,0
pick top beam size many outputs with highest probabilities,0
"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1",0
"leading_indices and probabilities have size (batch_size, beam_size)",0
keep scores of beam_size many hypothesis for each token in the batch,0
stack all leading indices of all hypothesis and corresponding hidden states in two tensors,0
save sequences so far,0
keep track of how many hypothesis were completed for each token,0
"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)",0
decode with log softmax,0
make sure no dummy symbol <> or start symbol <S> is predicted,0
"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside",0
"if the sequence is already ended, do not record as candidate",0
index of token in in list tokens_in_batch,0
print(token_number),0
hypothesis score,0
TODO: remove token if number of completed hypothesis exceeds given value,1
set score of corresponding entry to -inf so it will not be expanded,0
get leading_indices for next expansion,0
find highest scoring hypothesis among beam_size*beam_size possible ones for each token,0
take beam_size many copies of scores vector and add scores of possible new extensions,0
"size (beam_size*batch_size, beam_size)",0
print(hypothesis_scores),0
"reshape to vector of size (batch_size, beam_size*beam_size),",0
each row contains beam_size*beam_size scores of the new possible hypothesis,0
print(hypothesis_scores_per_token),0
"choose beam_size best for each token - size (batch_size, beam_size)",0
out of indices_per_token we now need to recompute the original indices of the hypothesis in,0
a list of length beam_size*batch_size,0
"where the first three inidices belong to the first token, the next three to the second token,",0
and so on,0
with these indices we can compute the tensors for the next iteration,0
expand sequences with corresponding index,0
add log-probabilities to the scores,0
save new leading indices,0
save corresponding hidden states,0
it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations,0
in that case we append one of the final seuqences without end symbol to the final_candidates,0
get best final hypothesis for each token,0
get characters from index sequences and add predicted label to token,0
dictionaries,0
all parameters will be pushed internally to the specified device,0
now print labels in CoNLL format,0
internal candidate lists of generator,0
load Zelda candidates if so passed,0
create candidate lists,0
"if lower casing is enabled, create candidate lists of lower cased versions",0
create a new dictionary for lower cased mentions,0
go through each mention and its candidates,0
"check if backoff mention already seen. If so, add candidates. Else, create new entry.",0
set lowercased version as map,0
remap state dict for models serialized with Flair <= 0.11.3,0
get the candidates,0
"during training, add the gold value as candidate",0
----- Create the internal tag dictionary -----,0
span-labels need special encoding (BIO or BIOES),0
the big question is whether the label dictionary should contain an UNK or not,0
"without UNK, we cannot evaluate on data that contains labels not seen in test",0
"with UNK, the model learns less well if there are no UNK examples",0
is this a span prediction problem?,0
----- Embeddings -----,0
----- Initial loss weights parameters -----,0
----- RNN specific parameters -----,0
----- Conditional Random Field parameters -----,0
"Previously trained models have been trained without an explicit CRF, thus it is required to check",0
whether we are loading a model from state dict in order to skip or add START and STOP token,0
----- Dropout parameters -----,0
dropouts,0
remove word dropout if there is no contact over the sequence dimension.,0
----- Model layers -----,0
----- RNN layer -----,0
"If shared RNN provided, else create one for model",0
Whether to train initial hidden state,0
final linear map to tag space,0
"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss",0
"if using CRF, we also require a CRF and a Viterbi decoder",0
"if there are no sentences, there is no loss",0
forward pass to get scores,0
calculate loss given scores and labels,0
make a zero-padded tensor for the whole sentence,0
linear map to tag space,0
"Depending on whether we are using CRF or a linear layer, scores is either:",0
"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF",0
"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer",0
spans need to be encoded as token-level predictions,0
all others are regular labels for each token,0
make sure it's a list,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
get features from forward propagation,0
remove previously predicted labels of this type,0
"if return_loss, get loss value",0
make predictions,0
add predictions to Sentence,0
BIOES-labels need to be converted to spans,0
"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)",0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
core Flair models on Huggingface ModelHub,0
"Large NER models,",0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Language-specific NER models,0
Language-specific POS models,0
English NER models,0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Danish models,0
German models,0
French models,0
Dutch models,0
Malayalam models,0
Portuguese models,0
Keyphase models,0
Biomedical models,0
check if model name is a valid local file,0
"check if model key is remapped to HF key - if so, print out information",0
get mapped name,0
use mapped name instead,0
"if not, check if model key is remapped to direct download location. If so, download model",0
special handling for the taggers by the @redewiegergabe project (TODO: move to model hub),1
"for all other cases (not local file or special download location), use HF model hub",0
"if not a local file, get from model hub",0
use model name as subfolder,0
Lazy import,0
output information,0
## Demo: How to use in Flair,0
load tagger,0
make example sentence,0
predict NER tags,0
print sentence,0
print predicted NER spans,0
iterate over entities and print,0
Lazy import,0
Save model weight,0
Determine if model card already exists,0
Generate and save model card,0
Upload files,0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
print labels in CoNLL format,0
the multi task model has several labels,0
biomedical models,0
entity linker,0
auto-spawn on GPU if available,0
remap state dict for models serialized with Flair <= 0.11.3,0
English sentiment models,0
Communicative Functions Model,0
"scores_at_targets[range(features.shape[0]), lengths.values -1]",0
Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices,0
"Initially, get scores from <start> tag to all other tags",0
"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp",0
"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep",0
Create a tensor to hold accumulated sequence scores at each current tag,0
Create a tensor to hold back-pointers,0
"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag",0
"Let pads be the <end> tag index, since that was the last tag in the decoded sequence",0
"We add scores at current timestep to scores accumulated up to previous timestep, and",0
choose the previous timestep that corresponds to the max. accumulated score for each current timestep,0
"If sentence is over, add transition to STOP-tag",0
Decode/trace best path backwards,0
Sanity check,0
remove start-tag and backscore to stop-tag,0
Max + Softmax to get confidence score for predicted label and append label to each token,0
"Transitions are used in the following way: transitions[to, from].",0
"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag",0
to START-tag and from STOP-tag to any other tag to -10000.,0
create a model card for this model with Flair and PyTorch version,0
also record Transformers version if library is loaded,0
remember all parameters used in train() call,0
add model card to model,0
"if optimizer class is passed, instantiate:",0
"determine what splits (train, dev, test) to evaluate and log",0
prepare loss logging file and set up header,0
"from here on, use list of learning rates",0
load existing optimizer state dictionary if it exists,0
"minimize training loss if training with dev data, else maximize dev score",0
"if scheduler is passed as a class, instantiate",0
"if we load a checkpoint, we have already trained for epoch",0
"Determine whether to log ""bad epochs"" information",0
load existing scheduler state dictionary if it exists,0
update optimizer and scheduler in model card,0
"if training also uses dev/train data, include in training set",0
initialize sampler if provided,0
init with default values if only class is provided,0
set dataset to sample from,0
this field stores the names of all dynamic embeddings in the model (determined after first forward pass),0
At any point you can hit Ctrl + C to break out of training early.,0
update epoch in model card,0
get new learning rate,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
"if shuffle_first_epoch==False, the first epoch is not shuffled",0
process mini-batches,0
zero the gradients on the model and optimizer,0
"if necessary, make batch_steps",0
forward and backward for batch,0
forward pass,0
Backward,0
identify dynamic embeddings (always deleted) on first sentence,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
do the optimizer step,0
do the scheduler step if one-cycle or linear decay,0
get new learning rate,0
evaluate on train / dev / test split depending on training settings,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
calculate scores using dev data if available,0
append dev score to score history,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
determine if this is the best model or if we need to anneal,0
default mode: anneal against dev score,0
alternative: anneal against dev loss,0
alternative: anneal against train loss,0
determine bad epoch number,0
lr unchanged,0
log bad epochs,0
output log file,0
make headers on first epoch,0
"if checkpoint is enabled, save model at each epoch",0
Check whether to save best model,0
"if we do not use dev data for model selection, save final model",0
test best model if test data is present,0
recover all arguments that were used to train this model,0
you can overwrite params with your own,0
surface nested arguments,0
resume training with these parameters,0
"If set, add a factor to the learning rate of all parameters with 'embeddings' not in name",0
get and return the final test score of best model,0
cast string to Path,0
forward pass,0
update optimizer and scheduler,0
"TextDataset returns a list. valid and test are only one file,",0
so return the first element,0
cast string to Path,0
error message if the validation dataset is too small,0
Shuffle training files randomly after serially iterating,0
through corpus one,0
"iterate through training data, starting at",0
self.split (for checkpointing),0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
Backward,0
`clip_grad_norm` helps prevent the exploding gradient,0
problem in RNNs / LSTMs.,0
We detach the hidden state from how it was,0
previously produced.,0
"If we didn't, the model would try backpropagating",0
all the way to start of the dataset.,0
explicitly remove loss to clear up memory,0
#########################################################,0
Save the model if the validation loss is the best we've,0
seen so far.,0
#########################################################,0
print info,0
#########################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn,0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
1-camembert-base -> camembert-base,0
1-xlm-roberta-large -> xlm-roberta-large,0
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol,0
The mask has 1 for real tokens and 0 for padding tokens. Only real,0
tokens are attended to.,0
Zero-pad up to the sequence length.,0
"first, find longest sentence in batch",0
prepare id maps for BERT model,0
put encoded batch through BERT model to get all hidden states of all encoder layers,0
get aggregated embeddings for each BERT-subtoken in sentence,0
get the current sentence object,0
add concatenated embedding to sentence,0
use first subword embedding if pooling operation is 'first',0
"otherwise, do a mean over all subwords in token",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
embed a dummy sentence to determine embedding_length,0
Avoid conflicts with flair's Token class,0
"legacy pickle-like saving for image embeddings, as implementation details are not obvious",0
"legacy pickle-like loading for image embeddings, as implementation details are not obvious",0
"<cls> token initially set to 1/D, so it attends to all image features equally",0
add positional encodings,0
reshape the pixels into the sequence,0
layer norm after convolution and positional encodings,0
add <cls> token,0
"transformer requires input in the shape [h*w+1, b, d]",0
the output is an embedding of <cls> token,0
this parameter is fixed,0
optional fine-tuning on top of embedding layer,0
"if only one sentence is passed, convert to list of sentence",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push through RNN,0
after-RNN dropout,0
extract embeddings from RNN,0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
serialize the language models and the constructor arguments (but nothing else),0
re-initialize language model with constructor arguments,0
special handling for deserializing language models,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
IMPORTANT: add embeddings as torch modules,0
iterate over sentences,0
"if its a forward LM, take last state",0
"convert to plain strings, embedded in a list for the encode function",0
CNN,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push CNN,0
after-CNN dropout,0
extract embeddings from CNN,0
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency",0
"if only one sentence is passed, convert to list of sentence",0
Expose base classses,0
Expose document embedding classes,0
Expose image embedding classes,0
Expose legacy embedding classes,0
Expose token embedding classes,0
in some cases we need to insert zero vectors for tokens without embedding.,0
padding,0
remove special markup,0
check if special tokens exist to circumvent error message,0
iterate over subtokens and reconstruct tokens,0
remove special markup,0
check if reconstructed token is special begin token ([CLS] or similar),0
some BERT tokenizers somehow omit words - in such cases skip to next token,0
"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token.",0
if tokens are unaccounted for,0
check if all tokens were matched to subtokens,0
The layoutlm tokenizer doesn't handle ocr themselves,0
"transformers returns the ""added_tokens.json"" even if it doesn't create it",0
"transformers returns the ""added_tokens.json"" even if it doesn't create it",0
in case of doubt: token embedding has higher priority than document embedding,0
random check some tokens to save performance.,0
Models such as FNet do not have an attention_mask,0
set language IDs for XLM-style transformers,0
"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have",0
"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids.",0
set context if not set already,0
flair specific pre-tokenization,0
"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first.",0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
do not print transformer warnings as these are confusing in this case,0
load tokenizer and transformer model,0
load tokenizer from inmemory zip-file,0
model name,0
embedding parameters,0
send mini-token through to check how many layers the model has,0
return length,0
"If we use a context separator, add a new special token",0
"most models have an initial BOS token, except for XLNet, T5 and GPT2",0
"when initializing, embeddings are in eval mode by default",0
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial",0
"cannot run `.encode` if ocr boxes are required, assume",0
in case of doubt: token embedding has higher priority than document embedding,0
in case of doubt: token embedding has higher priority than document embedding,0
legacy TransformerDocumentEmbedding,0
legacy TransformerTokenEmbedding,0
legacy Flair <= 0.12,0
legacy Flair <= 0.7,0
legacy TransformerTokenEmbedding,0
Legacy TransformerDocumentEmbedding,0
legacy TransformerTokenEmbedding,0
legacy TransformerDocumentEmbedding,0
copy values from new embedding,0
cls first pooling can be done without recreating sentence hidden states,0
make the tuple a tensor; makes working with it easier.,0
"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention",0
only use layers that will be outputted,0
this parameter is fixed,0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
make compatible with serialized models,0
gensim version 4,0
gensim version 3,0
"if no embedding is set, the vocab and embedding length is requried",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNINOS embeddings,0
pubmed embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
fix serialized models,0
"this is required to force the module on the cpu,",0
"if a parent module is put to gpu, the _apply is called to each sub_module",0
self.to(..) actually sets the device properly,0
this ignores the get_cached_vec method when loading older versions,0
it is needed for compatibility reasons,0
gensim version 4,0
gensim version 3,0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
multilingual models,0
English models,0
Arabic,0
Bulgarian,0
Czech,0
Danish,0
German,0
Spanish,0
Basque,0
Persian,0
Finnish,0
French,0
Hebrew,0
Hindi,0
Croatian,0
Indonesian,0
Italian,0
Japanese,0
Malayalam,0
Dutch,0
Norwegian,0
Polish,0
Portuguese,0
Pubmed,0
Slovenian,0
Swedish,0
Tamil,0
Spanish clinical,0
CLEF HIPE Shared task,0
Amharic,0
Ukrainian,0
load model if in pretrained model map,0
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir),0
CLEF HIPE models are lowercased,0
embeddings are static if we don't do finetuning,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
make compatible with serialized models (TODO: remove),1
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout",0
make compatible with serialized models (TODO: remove),1
gradients are enable if fine-tuning is enabled,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
offset mode that extracts at whitespace after last character,0
offset mode that extracts at last character,0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
check token.text is empty or not,0
set aggregation operation,0
add embeddings after updating,0
model architecture,0
model architecture,0
"""pl"",",0
download if necessary,0
load the model,0
"TODO: keep for backwards compatibility, but remove in future",1
save the sentence piece model as binary file (not as path which may change),0
write out the binary sentence piece model into the expected directory,0
"if the model was saved as binary and it is not found on disk, write to appropriate path",0
"otherwise, use normal process and potentially trigger another download",0
"once the modes if there, load it with sentence piece",0
empty words get no embedding,0
all other words get embedded,0
GLOVE embeddings,0
no need to recreate as NILCEmbeddings,0
read in test file if exists,0
read in dev file if exists,0
"find train, dev and test files if not specified",0
Add tags for each annotated span,0
Remove leading and trailing whitespaces from annotated spans,0
Search start and end token index for current span,0
If end index is not found set to last token,0
Throw error if indices are not valid,0
get train data,0
read in test file if exists,0
read in dev file if exists,0
"find train, dev and test files if not specified",0
special key for space after,0
special key for feature columns,0
special key for dependency head id,0
"store either Sentence objects in memory, or only file offsets",0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
determine encoding of text file,0
identify which columns are spans and which are word-level,0
now load all sentences,0
skip first line if to selected,0
option 1: keep Sentence objects in memory,0
pointer to previous,0
parse next sentence,0
quit if last sentence reached,0
skip banned sentences,0
set previous and next sentence for context,0
append parsed sentence to list in memory,0
option 2: keep source data in memory,0
"read lines for next sentence, but don't parse",0
quit if last sentence reached,0
append raw lines for each sentence,0
we make a distinction between word-level tags and span-level tags,0
read first sentence to determine which columns are span-labels,0
skip first line if to selected,0
check the first 5 sentences,0
go through all annotations and identify word- and span-level annotations,0
- if a column has at least one BIES we know it's a Span label,0
"- if a column has at least one tag that is not BIOES, we know it's a Token label",0
- problem cases are columns for which we see only O - in this case we default to Span,0
skip assigned columns,0
the space after key is always word-levels,0
"if at least one token has a BIES, we know it's a span label",0
"if at least one token has a label other than BIOES, we know it's a token label",0
all remaining columns that are not word-level are span-level,0
for column in self.word_level_tag_columns:,0
"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")",0
"if sentence ends, break",0
parse comments if possible,0
"otherwise, this line is a token. parse and add to sentence",0
check if this sentence is a document boundary,0
add span labels,0
discard tags from tokens that are not added to the sentence,0
parse relations if they are set,0
head and tail span indices are 1-indexed and end index is inclusive,0
parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas',0
"to set the metadata ""domain"" to ""de-orcas""",0
get fields from line,0
get head_id if exists (only in dependency parses),0
initialize token,0
go through all columns,0
'feats' and 'misc' column should be split into different fields,0
special handling for whitespace after,0
add each other feature as label-value pair,0
get the task name (e.g. 'ner'),0
get the label value,0
add label,0
remap regular tag names,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
set sentence context using partials TODO: pointer to dataset is really inefficient,1
use all domains,0
iter over all domains / sources and create target files,0
Parameters,0
The conll representation of coref spans allows spans to,0
"overlap. If spans end or begin at the same word, they are",0
"separated by a ""|"".",0
The span begins at this word.,0
The span begins and ends at this word (single word span).,0
"The span is starting, so we record the index of the word.",0
"The span for this id is ending, but didn't start at this word.",0
Retrieve the start index from the document state and,0
add the span to the clusters for this id.,0
Parameters,0
strip all bracketing information to,0
get the actual propbank label.,0
Entering into a span for a particular semantic role label.,0
We append the label and set the current span for this annotation.,0
"If there's no '(' token, but the current_span_label is not None,",0
then we are inside a span.,0
We're outside a span.,0
"Exiting a span, so we reset the current span label for this annotation.",0
The words in the sentence.,0
The pos tags of the words in the sentence.,0
the pieces of the parse tree.,0
The lemmatised form of the words in the sentence which,0
have SRL or word sense information.,0
The FrameNet ID of the predicate.,0
"The sense of the word, if available.",0
"The current speaker, if available.",0
"Cluster id -> List of (start_index, end_index) spans.",0
Cluster id -> List of start_indices which are open for this id.,0
Replace brackets in text and pos tags,0
with a different token for parse trees.,0
only keep ')' if there are nested brackets with nothing in them.,0
There are some bad annotations in the CONLL data.,0
"They contain no information, so to make this explicit,",0
we just set the parse piece to be None which will result,0
in the overall parse tree being None.,0
"If this is the first word in the sentence, create",0
empty lists to collect the NER and SRL BIO labels.,0
"We can't do this upfront, because we don't know how many",0
"components we are collecting, as a sentence can have",0
variable numbers of SRL frames.,0
Create variables representing the current label for each label,0
sequence we are collecting.,0
"If any annotation marks this word as a verb predicate,",0
we need to record its index. This also has the side effect,0
of ordering the verbal predicates by their location in the,0
"sentence, automatically aligning them with the annotations.",0
"this would not be reached if parse_pieces contained None, hence the cast",0
Non-empty line. Collect the annotation.,0
Collect any stragglers or files which might not,0
have the '#end document' format for the end of the file.,0
this dataset name,0
check if data there,0
column format,0
this dataset name,0
check if data there,0
column format,0
this dataset name,0
download data if necessary,0
download files if not present locally,0
we need to slightly modify the original files by adding some new lines after document separators,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
Remove CoNLL-U meta information in the last column,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
entity_mapping,0
this dataset name,0
download data if necessary,0
data validation,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download files if not present locallys,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
# download zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
download data if necessary,0
"unzip the downloaded repo and merge the train, dev and test datasets",0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
check if data there,0
create folder,0
download dataset,0
column format,0
this dataset name,0
download and parse data if necessary,0
create train test dev if not exist,0
column format,0
this dataset name,0
If the extracted corpus file is not yet present in dir,0
download zip if necessary,0
"extracted corpus is not present , so unpacking it.",0
column format,0
this dataset name,0
download zip,0
unpacking the zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)",0
download files if not present locally,0
we need to modify the original files by adding new lines after after the end of each sentence,0
if only one language is given,0
column format,0
this dataset name,0
"use all languages if explicitly set to ""all""",0
download data if necessary,0
initialize comlumncorpus and add it to list,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
this list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
get google drive id from list,0
download from google drive,0
unzip,0
"tar.extractall(language_folder,members=[tar.getmember(file_name)])",0
transform data into required format,0
"the processed dataset has the additional ending ""_new""",0
remove the unprocessed dataset,0
initialize comlumncorpus and add it to list,0
if no languages are given as argument all languages used in XTREME will be loaded,0
if only one language is given,0
column format,0
this dataset name,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
This list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
download from HU Server,0
unzip,0
transform data into required format,0
initialize comlumncorpus and add it to list,0
if only one language is given,0
column format,0
this dataset name,0
download data if necessary,0
initialize comlumncorpus and add it to list,0
download data if necessary,0
unpack and write out in CoNLL column-like format,0
column format,0
this dataset name,0
download data if necessary,0
data is not in IOB2 format. Thus we transform it to IOB2,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
column format,0
this dataset name,0
rename according to train - test - dev - convention,0
column format,0
this dataset name,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
Add missing newline after header,0
Workaround for empty tokens,1
"Add ""real"" document marker",0
Dataset split mapping,0
v2.0 only adds new language and splits for AJMC dataset,0
Special document marker for sample splits in AJMC dataset,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
paths to train and test splits,0
init corpus,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
iterate over all html files,0
"get rid of html syntax, we only need the text",0
between all documents we write a separator symbol,0
skip empty strings,0
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)",0
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention",0
sentence splitting and tokenization,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out-file in column format,0
"in case something goes wrong, delete the dataset and raise error",0
this dataset name,0
download and parse data if necessary,0
from qwikidata.linked_data_interface import get_entity_dict_from_api,0
generate qid wikiname dictionaries,0
merge dictionaries,0
ignore first line,0
commented and empty lines,0
read all Q-IDs,0
ignore first line,0
request,0
this dataset name,0
we use the wikiids in the data instead of directly utilizing the wikipedia urls.,0
like this we can quickly check if the corresponding page exists,0
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi,0
delete unprocessed file,0
collect all wikiids,0
create the dictionary,0
request,0
this dataset name,0
names of raw text documents,0
open output_file,0
iterate through all documents,0
split sentences and tokenize,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out file,0
this dataset name,0
download and parse data if necessary,0
this dataset name,0
download and parse data if necessary,0
First parse the post titles,0
Keep track of how many and which entity mentions does a given post title have,0
Check if the current post title has an entity link and parse accordingly,0
Post titles with entity mentions (if any) are handled via this function,0
Then parse the comments,0
"Iterate over the comments.tsv file, until the end is reached",0
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched.",0
Each comment thread is handled as one 'document'.,0
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.,0
This if-condition is needed to handle this problem.,0
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure",0
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above",0
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle.",0
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,",0
and not just single letters into single rows.,0
If there are annotated entity mentions for given post title or a comment thread,0
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence",0
Write the token with a corresponding tag to file,0
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed",0
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag",0
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized,0
"incorrectly, in order to keep the desired format (empty line as a sentence separator).",0
"Thrown when the second check above happens, but the last token of a sentence is reached.",0
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below.",0
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS",0
Check if further annotations belong to the current post title or comment thread as well,0
Stop when the end of an annotation file is reached,0
Check if further annotations belong to the current sentence as well,0
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)",0
Docstart,0
if there is more than one word in the chunk we write each in a separate line,0
print(chunks),0
empty line after each sentence,0
convert the file to CoNLL,0
this dataset name,0
"check if data there, if not, download the data",0
create folder,0
download data,0
transform data into column format if necessary,0
if no filenames are specified we use all the data,0
"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled",0
also we remove 'raganato_ALL' from filenames in case its in the list,0
generate the test file,0
make column file and save to data_folder,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
this dataset name,0
download data if necessary,0
if True:,0
write CoNLL-U Plus header,0
"Some special cases (e.g., missing spaces before entity marker)",0
necessary if text should be whitespace tokenizeable,0
Handle case where tail may occur before the head,0
this dataset name,0
write CoNLL-U Plus header,0
this dataset name,0
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors,1
download data if necessary,0
write CoNLL-U Plus header,0
The span has ended.,0
We are entering a new span; reset indices,0
and active tag to new span.,0
We're inside a span.,0
Last token might have been a part of a valid span.,0
this dataset name,0
write CoNLL-U Plus header,0
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):",0
"with zip_file.open(source_file_path, mode=""r"") as source_file:",0
target_file_path = Path(data_folder) / target_filename,0
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:",0
# write CoNLL-U Plus header,0
"target_file.write(""# global.columns = id form ner\n"")",0
for example in json.load(source_file):,0
token_list = self._tacred_example_to_token_list(example),0
target_file.write(token_list.serialize()),0
check if first tag row is already occupied,0
"if first tag row is occupied, use second tag row",0
hardcoded mapping TODO: perhaps find nicer solution,1
remap regular tag names,0
else skip to position in file where sentence begins,0
set sentence context using partials TODO: pointer to dataset is really inefficient,1
read in dev file if exists,0
read in test file if exists,0
the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44,0
"find train, dev and test files if not specified",0
use test_file to create test split if available,0
use dev_file to create test split if available,0
"if data point contains black-listed label, do not use",0
first check if valid sentence,0
"if so, add to indices",0
"find train, dev and test files if not specified",0
variables,0
different handling of in_memory data than streaming data,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
test if format is OK,0
test if at least one label given,0
make sentence from text (and filter for length),0
"if a pair column is defined, make a sentence pair object",0
noinspection PyDefaultArgument,0
dataset name includes the split size,0
default dataset folder is the cache root,0
download data if necessary,0
download each of the 28 splits,0
create dataset directory if necessary,0
download senteval datasets if necessary und unzip,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
handle labels file,0
handle data file,0
Create flair compatible labels,0
"by defaut, map point score to POSITIVE / NEGATIVE values",0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file from CSV,0
create test.txt file from CSV,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create train dev and test files in fasttext format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
convert to FastText format,0
download data if necessary,0
"if data is not downloaded yet, download it",0
get the zip file,0
move original .tsv files to another folder,0
create train and dev splits in fasttext format,0
create eval_dataset file with no labels,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download datasets if necessary,0
create dataset directory if necessary,0
create correctly formated txt files,0
multiple labels are possible,0
this dataset name,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
create a separate directory for different tasks,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
check if dataset is supported,0
set file names,0
set file names,0
download and unzip in file structure if necessary,0
instantiate corpus,0
"find train, dev and test files if not specified",0
"create DataPairDataset for train, test and dev file, if they are given",0
stop if file does not exist,0
create a DataPair object from strings,0
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"reorder dev datasets to have same columns as in train set: 8, 9, and 11",0
dev sets include 5 different annotations but we will only keep the gold label,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get test and dev sets,0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data not downloaded yet, download it",0
get the zip file,0
"the downloaded files have json format, we transform them to tsv",0
Function to transform JSON file to tsv for Recognizing Textual Entailment Data,0
remove json file,0
Uses dynamic programming approach to calculate maximum independent set in interval graph,0
with sum of all entity lengths as secondary key,0
calculate offset without current text,0
because we stick all passages of a document together,0
TODO For split entities we also annotate everything inbetween which might be a bad idea?,1
Try to fix incorrect annotations,0
print(,0
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}""",0
),0
Ignore empty lines or relation annotations,0
FIX annotation of whitespaces (necessary for PDR),0
One token may contain multiple entities -> deque all of them,0
column format,0
this dataset name,0
Create tokenization-dependent CONLL files. This is necessary to prevent,0
from caching issues (e.g. loading the same corpus with different sentence splitters),0
column format,0
this dataset name,0
column format,0
this dataset name,0
Edge case: last token starts a new entity,0
Last document in file,0
column format,0
this dataset name,0
column format,0
this dataset name,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
Edge case: last token starts a new entity,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
column format,0
this dataset name,0
Read texts,0
Read annotations,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
We need to apply a patch to correct the original training file,0
Articles title,0
Article abstract,0
Entity annotations,0
column format,0
this dataset name,0
Edge case: last token starts a new entity,0
Map all entities to chemicals,0
Map all entities to disease,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
Incomplete article,0
Invalid XML syntax,0
column format,0
this dataset name,0
column format,0
this dataset name,0
if len(mid) != 3:,0
continue,0
Try to fix entity offsets,0
column format,0
this dataset name,0
There is still one illegal annotation in the file ..,0
column format,0
this dataset name,0
"Abstract first, title second to prevent issues with sentence splitting",0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
column format,0
this dataset name,0
"Filter for specific entity types, by default no entities will be filtered",0
Get original HUNER splits to retrieve a list of all document ids contained in V2,0
train and dev split of V2 will be train in V4,0
test split of V2 will be dev in V4,0
New documents in V4 will become test documents,0
column format,0
this dataset name,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
cache Feidegger config file,0
cache Feidegger images,0
replace image URL with local cached file,0
append Sentence-Image data point,0
"in certain cases, multi-CPU data loading makes no sense and slows",0
"everything down. For this reason, we detect if a dataset is in-memory:",0
"if so, num_workers is set to 0 for faster processing",0
cast to list if necessary,0
cast to list if necessary,0
"first, check if pymongo is installed",0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
Expose base classses,0
Expose all biomedical data sets used for the evaluation of BioBERT,0
-,0
-,0
-,0
-,0
Expose all biomedical data sets using the HUNER splits,0
Expose all biomedical data sets,0
Expose all document classification datasets,0
word sense disambiguation,0
Expose all entity linking datasets,0
Expose all relation extraction datasets,0
universal proposition banks,0
keyphrase detection datasets,0
other NER datasets,0
standard NER datasets,0
Expose all sequence labeling datasets,0
Expose all text-image datasets,0
Expose all text-text datasets,0
Expose all treebanks,0
"find train, dev and test files if not specified",0
get train data,0
get test data,0
get dev data,0
option 1: read only sentence boundaries as offset positions,0
option 2: keep everything in memory,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
current token ID,0
handling for the awful UD multiword format,0
end of sentence,0
comments,0
ellipsis,0
if token is a multi-word,0
normal single-word tokens,0
"if we don't split multiwords, skip over component words",0
add token,0
add morphological tags,0
derive whitespace logic for multiwords,0
print(token),0
print(current_multiword_last_token),0
print(current_multiword_first_token),0
"if multi-word equals component tokens, there should be no whitespace",0
go through all tokens in subword and set whitespace_after information,0
print(i),0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
this dataset name,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"finally, print model card for information",0
test corpus,0
create a TARS classifier,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
test corpus,0
create a TARS classifier,0
switch to a new task (TARS can do multiple tasks so you must define one),0
initialize the text classifier trainer,0
start the training,0
"With end symbol, without start symbol, padding in front",0
"Without end symbol, with start symbol, padding in back",0
"Without end symbol, without start symbol, padding in front",0
initialize trainer,0
initialize trainer,0
train model for 2 epochs,0
load the checkpoint model and train until epoch 4,0
clean up results directory,0
initialize trainer,0
initialize trainer,0
increment for last token in sentence if not followed by whitespace,0
clean up directory,0
clean up directory,0
example sentence,0
set 4 labels for 2 tokens ('love' is tagged twice),0
check if there are three POS labels with correct text and values,0
check if there are is one SENTIMENT label with correct text and values,0
check if all tokens are correctly labeled,0
remove the pos label from the last word,0
there should be 2 POS labels left,0
now remove all pos tags,0
set 3 labels for 2 spans (HU is tagged twice),0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
"now delete the NER tags of ""Humboldt-Universitt zu Berlin""",0
should be only one NER label left,0
and only one NER span,0
set 3 labels for 2 spans (HU is tagged twice with different tags),0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
"now delete the NER tags of ""Humboldt-Universitt zu Berlin""",0
should be only one NER label left,0
and only one NER span,0
but there is also one orgtype span and label,0
and only one NER span,0
let's add the NER tag back,0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
now remove all NER tags,0
set 3 labels for 2 spans (HU is tagged twice with different tags),0
create two relation label,0
there should be two relation labels,0
there should be one syntactic labels,0
"there should be two relations, one with two and one with one label",0
example sentence,0
add another topic label,0
example sentence,0
has sentiment value,0
has 4 part of speech tags,0
has 1 NER tag,0
should be in total 6 labels,0
example sentence,0
add two NER labels,0
get the four labels,0
check that only two of the respective data points are equal,0
make a sentence and some right context,0
TODO: is this desirable? Or should two sentences with same text still be considered different objects?,1
Initializing a Sentence this way assumes that there is a space after each token,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
define search space,0
sequence tagger parameter,0
model trainer parameter,0
training parameter,0
find best parameter settings,0
clean up results directory,0
document embeddings parameter,0
training parameter,0
clean up results directory,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
load column dataset with one entry,0
load column dataset with two entries,0
load column dataset with three entries,0
"get training, test and dev data",0
"get training, test and dev data",0
check if Token labels are correct,0
"get training, test and dev data",0
check if Token labels for frames are correct,0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
"assert [token.get_tag(""head"").value for token in sent1.tokens] == [",0
"""2"",",0
"""0"",",0
"""4"",",0
"""2"",",0
"""2"",",0
"""2"",",0
],0
"Here, we use the default token annotation fields.",0
"We have manually checked, that these numbers are correct:",0
"+1 offset, because of missing EOS marker at EOD",0
Test data for v2.1 release,0
--- Embeddings that are shared by both models --- #,0
--- Task 1: Sentiment Analysis (5-class) --- #,0
Define corpus and model,0
-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #,0
Define corpus and model,0
-- Define mapping (which tagger should train on which model) -- #,0
-- Create model trainer and train -- #,0
load dataset,0
tagger without CRF,0
train,0
check if loaded model can predict,0
check if loaded model successfully fit the training data,0
load dataset,0
tagger without CRF,0
train,0
check if loaded model can predict,0
check if loaded model successfully fit the training data,0
load dataset,0
tagger without CRF,0
train,0
check if loaded model can predict,0
check if loaded model successfully fit the training data,0
load dataset,0
tagger without CRF,0
train,0
check if loaded model can predict,0
check if loaded model successfully fit the training data,0
check if model can predict,0
load model,0
chcek if model predicts correct label,0
check if loaded model successfully fit the training data,0
check if model can predict,0
load model,0
chcek if model predicts correct label,0
check if loaded model successfully fit the training data,0
check if model can predict,0
load model,0
chcek if model predicts correct label,0
check if loaded model successfully fit the training data,0
clean up file,0
no need for label_dict,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
Intel ----founded_by---> Gordon Moore,0
Intel ----founded_by---> Robert Noyce,0
Check sentence masking and relation label annotation on,0
"training, validation and test dataset (in this test the splits are the same)",0
"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google .""",0
"Entity pair permutations of: ""Microsoft was founded by Bill Gates .""",0
"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 .""",0
"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany.""",0
This sentence is only included if we transform the corpus with cross augmentation,0
Ensure this is an example that predicts no classes in multilabel,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
ensure that the prepared tensors is what we expect,0
use a SequenceTagger to save and reload the embedding in the manner it is supposed to work,0
previous and next sentence as context,0
test expansion for sentence without context,0
test expansion for with previous and next as context,0
test expansion if first sentence is document boundary,0
test expansion if we don't use context,0
"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher.",0
dummy model with embeddings,0
save the dummy and load it again,0
check that context_length and use_context_separator is the same for both,0
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
Extract all the contents of zip file in current directory,0
Extract all the contents of zip file in current directory,0
get cache path to put the file,0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
first determine the distribution of classes in the dataset,0
weight for each sample,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
increment for last token in sentence if not followed by whitespace,0
this is the default init size of a lmdb database for embeddings,0
get db filename from embedding name,0
"In case initialization of cached version failed, just fallback to the original WordEmbeddings",0
SequenceTagger,0
TextClassifier,0
get db filename from embedding name,0
if embedding database already exists,0
"otherwise, push embedding to database",0
if embedding database already exists,0
open the database in read mode,0
we need to set self.k,0
create and load the database in write mode,0
"no idea why, but we need to close and reopen the environment to avoid",0
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot,0
when opening new transaction !,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
set 'add_unk' if the dictionary was created with a version of Flair older than 0.9,0
set 'add_unk' depending on whether <unk> is a key,0
TODO: does it make sense to exclude labels? Two data points of identical text (but different labels),1
would be equal now.,0
labels also need to be deleted at Sentence object,0
delete labels at object itself,0
private field for all known spans,0
the tokenizer used for this sentence,0
"if text is passed, instantiate sentence with tokens (words)",0
determine token positions and whitespace_after flag,0
the last token has no whitespace after,0
log a warning if the dataset is empty,0
some sentences represent a document boundary (but most do not),0
internal variables to denote position inside dataset,0
data with zero-width characters cannot be handled,0
set token idx and sentence,0
append token to sentence,0
register token annotations on sentence,0
move sentence embeddings to device,0
also move token embeddings to device,0
clear token embeddings,0
infer whitespace after field,0
No character at the corresponding code point: remove it,0
"if no label if specified, return all labels",0
"if the label type exists in the Sentence, return it",0
return empty list if none of the above,0
labels also need to be deleted at all tokens,0
labels also need to be deleted at all known spans,0
remove spans without labels,0
delete labels at object itself,0
set name,0
abort if no data is provided,0
sample test data from train if none is provided,0
sample dev data from train if none is provided,0
set train dev and test data,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
count all label types per sentence,0
go through all labels of label_type and count values,0
check if there are any span labels,0
"if an unk threshold is set, UNK all label values below this threshold",0
Make the tag dictionary,0
global variable: cache_root,0
global variable: device,0
global variable: version,0
global variable: arrow symbol,0
dummy return to fulfill trainer.train() needs,0
print(vec),0
Attach optimizer,0
"convert `metrics` to float, in case it's a zero-dim Tensor",0
if memory mode option 'none' delete everything,0
"if dynamic embedding keys not passed, identify them automatically",0
always delete dynamic embeddings,0
"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)",0
optional metric space decoder if prototypes have different length than embedding,0
create initial prototypes for all classes (all initial prototypes are a vector of all 1s),0
"if set, create initial prototypes from normal distribution",0
"if set, use a radius",0
all parameters will be pushed internally to the specified device,0
decode embeddings into prototype space,0
"if unlabeled distance is set, mask out loss to unlabeled class prototype",0
Monkey-patching is problematic for mypy (https://github.com/python/mypy/issues/2427),0
gradients are not required for prototype computation,0
reset prototypes for all classes,0
decode embeddings into prototype space,0
embeddings need to be removed so that memory doesn't fill up,0
TODO: changes required,1
"in Flair <0.9.1, optimizer and scheduler used to train model are not saved",0
"write out a ""model card"" if one is set",0
special handling for optimizer:,0
remember optimizer class and state dictionary,0
save model,0
restore optimizer and scheduler to model card if set,0
load_big_file is a workaround byhttps://github.com/highway11git,1
to load models on some Mac/Windows setups,0
see https://github.com/zalandoresearch/flair/issues/351,0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
loss calculation,0
variables for printing,0
variables for computing scores,0
remove any previously predicted labels,0
predict for batch,0
get the gold labels,0
add to all_predicted_values,0
make printout lines,0
convert true and predicted values to two span-aligned lists,0
delete exluded labels if exclude_labels is given,0
"if after excluding labels, no label is left, ignore the datapoint",0
write all_predicted_values to out_file if set,0
make the evaluation dictionary,0
check if this is a multi-label problem,0
compute numbers by formatting true and predicted such that Scikit-Learn can use them,0
multi-label problems require a multi-hot vector for each true and predicted label,0
single-label problems can do with a single index for each true and predicted label,0
"now, calculate evaluation numbers",0
there is at least one gold label or one prediction (default),0
"if there is only one label, then ""micro avg"" = ""macro avg""",0
"micro average is only computed if zero-label exists (for instance ""O"")",0
if no zero-label exists (such as in POS tagging) micro average is equal to accuracy,0
same for the main score,0
issue error and default all evaluation numbers to 0.,0
line for log file,0
check if there is a label mismatch,0
print info,0
initialize the label dictionary,0
initialize the decoder,0
set up multi-label logic,0
init dropouts,0
loss weights and loss function,0
Initialize the weight tensor,0
make a forward pass to produce embedded data points and labels,0
no loss can be calculated if there are no labels,0
use dropout,0
push embedded_data_points through decoder to get the scores,0
calculate the loss,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
if anything could possibly be predicted,0
remove previously predicted labels of this type,0
add DefaultClassifier arguments,0
add variables of DefaultClassifier,0
Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23,0
Get projected 1st dimension,0
Compute bilinear form,0
Arcosh,0
Project the input data to n+1 dimensions,0
"The first dimension, is recomputed in the distance module",0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for",0
"train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
plot i,0
save plots,0
save plots,0
plt.show(),0
save plot,0
take the average over the last three scores of training,0
take average over the scores from the different training runs,0
auto-spawn on GPU if available,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
"read Dataset into data loader, if list of sentences passed, make Dataset first",0
TODO: not saving lines yet,1
== similarity measures ==,0
helper class for ModelSimilarity,0
-- works with binary cross entropy loss --,0
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}",0
-- works with ranking/triplet loss --,0
normalize the embeddings,0
== similarity losses ==,0
"we want that logits for corresponding pairs are high, and for non-corresponding low",0
TODO: this assumes eye matrix,0
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa",0
== similarity learner ==,0
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both",0
assumes that for each data pair there's at least one embedding per modality,0
pre-compute embeddings for all targets in evaluation dataset,0
compute the similarity,0
sort the similarity matrix across modality 1,0
"get the ranks, so +1 to start counting ranks from 1",0
The conversion from old model's constructor interface,0
auto-spawn on GPU if available,0
pad strings with whitespaces to longest sentence,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
"""document_delimiter"" property may be missing in some older pre-trained models",0
serialize the language models and the constructor arguments (but nothing else),0
special handling for deserializing language models,0
re-initialize language model with constructor arguments,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
Transform input data into TARS format,0
"if there are no labels, return a random sample as negatives",0
"otherwise, go through all labels",0
make sure the probabilities always sum up to 1,0
get and embed all labels by making a Sentence object that contains only the label text,0
get each label embedding and scale between 0 and 1,0
compute similarity matrix,0
"the higher the similarity, the greater the chance that a label is",0
sampled as negative example,0
make label dictionary if no Dictionary object is passed,0
prepare dictionary of tags (without B- I- prefixes and without UNK),0
check if candidate_label_set is empty,0
make list if only one candidate label is passed,0
create label dictionary,0
note current task,0
create a temporary task,0
make zero shot predictions,0
switch to the pre-existing task,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
make a tars sentence where all labels are O by default,0
init new TARS classifier,0
set all task information,0
return,0
with torch.no_grad():,0
progress bar for verbosity,0
stop if all sentences are empty,0
go through each sentence in the batch,0
always remove tags first,0
get the span and its label,0
"label = span.get_labels(""tars_temp_label"")[0].value",0
determine whether tokens in this span already have a label,0
only add if all tokens have no label,0
clearing token embeddings to save memory,0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
init new TARS classifier,0
set all task information,0
with torch.no_grad():,0
set context if not set already,0
progress bar for verbosity,0
stop if all sentences are empty,0
go through each sentence in the batch,0
always remove tags first,0
add all labels that according to TARS match the text and are above threshold,0
do not add labels below confidence threshold,0
only use label with highest confidence if enforcing single-label predictions,0
get all label scores and do an argmax to get the best label,0
remove previously added labels and only add the best label,0
clearing token embeddings to save memory,0
set separator to concatenate two sentences,0
auto-spawn on GPU if available,0
pooling operation to get embeddings for entites,0
set embeddings,0
set relation and entity label types,0
"whether to use gold entity pairs, and whether to filter entity pairs by type",0
super lame: make dictionary to find relation annotations for a given entity pair,0
get all entity spans,0
"go through cross product of entities, for each pair concat embeddings",0
filter entity pairs according to their tags if set,0
get gold label for this relation (if one exists),0
"if there is no gold label for this entity pair, set to 'O' (no relation)",0
"if predicting, also remember sentences and label candidates",0
if there's at least one entity pair in the sentence,0
embed sentences and get embeddings for each entity pair,0
get embeddings,0
whether to encode characters and whether to use attention (attention can only be used if chars are encoded),0
character dictionary for decoding and encoding,0
make sure <unk> is in dictionary for handling of unknown characters,0
add special symbols to dictionary if necessary and save respective indices,0
---- ENCODER ----,0
encoder character embeddings,0
encoder pre-trained embeddings,0
encoder RNN,0
additional encoder linear layer if bidirectional encoding,0
---- DECODER ----,0
decoder: linear layers to transform vectors to and from alphabet_size,0
when using attention we concatenate attention outcome and decoder hidden states,0
decoder RNN,0
loss and softmax,0
self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction,0
add additional columns for special symbols if necessary,0
initialize with dummy symbols,0
encode inputs,0
get labels (we assume each token has a lemma label),0
get char indices for labels of sentence,0
"(batch_size, max_sequence_length) batch_size = #words in sentence,",0
max_sequence_length = length of longest label of sentence + 1,0
get char embeddings,0
"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size",0
take decoder input and initial hidden and pass through RNN,0
"if all encoder outputs are provided, use attention",0
take convex combinations of encoder hidden states as new output using the computed attention coefficients,0
"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)",0
get all tokens,0
variable to store initial hidden states for decoder,0
encode input characters by sending them through RNN,0
get one-hots for characters and add special symbols / padding,0
determine length of each token,0
embed character one-hots,0
test packing and padding,0
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder,0
concatenate the final hidden states of the encoder. These will be projected to hidden_size of,0
decoder later with self.emb_to_hidden,0
mask out vectors that correspond to a dummy symbol (TODO: check attention masking),1
use token embedding as initial hidden state for decoder,0
embed sentences,0
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)",0
concatenate everything together and project to appropriate size for decoder,0
variable to store initial hidden states for decoder,0
encode input characters by sending them through RNN,0
note that we do not need to fill up with dummy symbols since we process each token seperately,0
embed character one-hots,0
send through encoder RNN (produces initial hidden for decoder),0
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder,0
project 2*hidden_size to hidden_size,0
concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder,0
later with self.emb_to_hidden,0
use token embedding as initial hidden state for decoder,0
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)",0
concatenate everything together and project to appropriate size for decoder,0
"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)",0
"create target vector (batch_size, max_label_seq_length + 1)",0
filter empty sentences,0
max length of the predicted sequences,0
for printing,0
stop if all sentences are empty,0
remove previously predicted labels of this type,0
create list of tokens in batch,0
encode inputs,0
"create input for first pass (batch_size, 1, input_size), first letter is special character <S>",0
sequence length is always set to one in prediction,0
option 1: greedy decoding,0
predictions,0
decode next character,0
pick top beam size many outputs with highest probabilities,0
option 2: beam search,0
out_probs = self.softmax(output_vectors).squeeze(1),0
make sure no dummy symbol <> or start symbol <S> is predicted,0
pick top beam size many outputs with highest probabilities,0
"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1",0
"leading_indices and probabilities have size (batch_size, beam_size)",0
keep scores of beam_size many hypothesis for each token in the batch,0
stack all leading indices of all hypothesis and corresponding hidden states in two tensors,0
save sequences so far,0
keep track of how many hypothesis were completed for each token,0
"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)",0
decode with log softmax,0
make sure no dummy symbol <> or start symbol <S> is predicted,0
"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside",0
"if the sequence is already ended, do not record as candidate",0
index of token in in list tokens_in_batch,0
print(token_number),0
hypothesis score,0
TODO: remove token if number of completed hypothesis exceeds given value,1
set score of corresponding entry to -inf so it will not be expanded,0
get leading_indices for next expansion,0
find highest scoring hypothesis among beam_size*beam_size possible ones for each token,0
take beam_size many copies of scores vector and add scores of possible new extensions,0
"size (beam_size*batch_size, beam_size)",0
print(hypothesis_scores),0
"reshape to vector of size (batch_size, beam_size*beam_size),",0
each row contains beam_size*beam_size scores of the new possible hypothesis,0
print(hypothesis_scores_per_token),0
"choose beam_size best for each token - size (batch_size, beam_size)",0
out of indices_per_token we now need to recompute the original indices of the hypothesis in,0
a list of length beam_size*batch_size,0
"where the first three inidices belong to the first token, the next three to the second token,",0
and so on,0
with these indices we can compute the tensors for the next iteration,0
expand sequences with corresponding index,0
add log-probabilities to the scores,0
save new leading indices,0
save corresponding hidden states,0
it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations,0
in that case we append one of the final seuqences without end symbol to the final_candidates,0
get best final hypothesis for each token,0
get characters from index sequences and add predicted label to token,0
embeddings,0
dictionaries,0
all parameters will be pushed internally to the specified device,0
get all tokens in this mini-batch,0
now print labels in CoNLL format,0
filter sentences with no candidates (no candidates means nothing can be linked anyway),0
fields to return,0
embed sentences and send through prediction head,0
embed all tokens,0
get the embeddings of the entity mentions,0
get the label of the entity,0
if there is no RNN,0
embed sentences,0
"Main model implementation drops words and tags (independently), instead, we use word dropout!",0
apply MLPs for arc and relations to the BiLSTM output states,0
get scores from the biaffine attentions,0
"[batch_size, seq_len, seq_len]",0
"[batch_size, seq_len, seq_len, n_rels]",0
append both to file for evaluation,0
"if head AND deprel correct, augment correct_rels score",0
----- Create the internal tag dictionary -----,0
span-labels need special encoding (BIO or BIOES),0
the big question is whether the label dictionary should contain an UNK or not,0
"without UNK, we cannot evaluate on data that contains labels not seen in test",0
"with UNK, the model learns less well if there are no UNK examples",0
is this a span prediction problem?,0
----- Embeddings -----,0
----- Initial loss weights parameters -----,0
----- RNN specific parameters -----,0
----- Conditional Random Field parameters -----,0
"Previously trained models have been trained without an explicit CRF, thus it is required to check",0
whether we are loading a model from state dict in order to skip or add START and STOP token,0
----- Dropout parameters -----,0
dropouts,0
----- Model layers -----,0
----- RNN layer -----,0
"If shared RNN provided, else create one for model",0
Whether to train initial hidden state,0
final linear map to tag space,0
"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss",0
"if using CRF, we also require a CRF and a Viterbi decoder",0
"if there are no sentences, there is no loss",0
forward pass to get scores,0
calculate loss given scores and labels,0
make a zero-padded tensor for the whole sentence,0
sort tensor in decreasing order based on lengths of sentences in batch,0
----- Forward Propagation -----,0
linear map to tag space,0
"Depending on whether we are using CRF or a linear layer, scores is either:",0
"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF",0
"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer",0
get the gold labels,0
spans need to be encoded as token-level predictions,0
all others are regular labels for each token,0
make sure its a list,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
get features from forward propagation,0
remove previously predicted labels of this type,0
"if return_loss, get loss value",0
Sort batch in same way as forward propagation,0
make predictions,0
add predictions to Sentence,0
BIOES-labels need to be converted to spans,0
"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)",0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
core Flair models on Huggingface ModelHub,0
"Large NER models,",0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Language-specific NER models,0
English NER models,0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Danish models,0
German models,0
French models,0
Dutch models,0
Malayalam models,0
Portuguese models,0
Keyphase models,0
Biomedical models,0
check if model name is a valid local file,0
"check if model key is remapped to HF key - if so, print out information",0
get mapped name,0
use mapped name instead,0
"if not, check if model key is remapped to direct download location. If so, download model",0
special handling for the taggers by the @redewiegergabe project (TODO: move to model hub),1
"for all other cases (not local file or special download location), use HF model hub",0
"if not a local file, get from model hub",0
use model name as subfolder,0
Lazy import,0
output information,0
"log.error(f"" - Error message: {e}"")",0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
print labels in CoNLL format,0
clear embeddings after predicting,0
load each model,0
check if the same embeddings were already loaded previously,0
"if the model uses StackedEmbedding, make a new stack with previous objects",0
sort embeddings by key alphabetically,0
check previous embeddings and add if found,0
only re-use static embeddings,0
"if not found, use existing embedding",0
initialize new stack,0
"of the model uses regular embedding, re-load if previous version found",0
auto-spawn on GPU if available,0
embed sentences,0
make tensor for all embedded sentences in batch,0
English sentiment models,0
Communicative Functions Model,0
"scores_at_targets[range(features.shape[0]), lengths.values -1]",0
Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices,0
"Initially, get scores from <start> tag to all other tags",0
"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp",0
"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep",0
Create a tensor to hold accumulated sequence scores at each current tag,0
Create a tensor to hold back-pointers,0
"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag",0
"Let pads be the <end> tag index, since that was the last tag in the decoded sequence",0
"We add scores at current timestep to scores accumulated up to previous timestep, and",0
choose the previous timestep that corresponds to the max. accumulated score for each current timestep,0
"If sentence is over, add transition to STOP-tag",0
Decode/trace best path backwards,0
Sanity check,0
remove start-tag and backscore to stop-tag,0
Max + Softmax to get confidence score for predicted label and append label to each token,0
"add a dummy ""O"" to close final prediction",0
return complex list,0
internal variables,0
non-set tags are OUT tags,0
anything that is not OUT is IN,0
does this prediction start a new span?,0
begin and single tags start new spans,0
"in IOB format, an I tag starts a span if it follows an O or is a different span",0
single tags that change prediction start new spans,0
if an existing span is ended (either by reaching O or starting a new span),0
determine score and value,0
append to result list,0
reset for-loop variables for new span,0
remember previous tag,0
"Transitions are used in the following way: transitions[to, from].",0
"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag",0
to START-tag and from STOP-tag to any other tag to -10000.,0
weights for loss function,0
iput size is two times wordembedding size since we use pair of words as input,0
"the output size is max_distance + 1, i.e. we allow 0,1,...,max_distance words between pairs",0
regression,0
input size is two times word embedding size since we use pair of words as input,0
the output size is 1,0
auto-spawn on GPU if available,0
forward allows only a single sentcence!!,0
embed words of sentence,0
go through all pairs of words with a maximum number of max_distance in between,0
go through all pairs,0
2-dim matrix whose rows are the embeddings of word pairs of the sentence,0
So far only one sentence allowed,0
If list of sentences is handed the function works with the first sentence of the list,0
Assume data_points is a single sentence!!!,0
scores are the predictions for each word pair,0
"classification needs labels to be integers, regression needs labels to be float",0
this is due to the different loss functions,0
only single sentences as input,0
gold labels,0
for output text file,0
for buckets,0
for average prediction,0
add some statistics to the output,0
use scikit-learn to evaluate,0
"we iterate over each sentence, instead of batches",0
get single labels from scores,0
gold labels,0
for output text file,0
hot one vector of true value,0
hot one vector of predicted value,0
"speichert embeddings, falls embedding_storage!= 'None'",0
"make ""classification report""",0
get scores,0
"precision_score = round(metrics.precision_score(y_true, y_pred, average='macro', zero_division=0), 4)",0
"recall_score = round(metrics.recall_score(y_true, y_pred, average='macro', zero_division=0), 4)",0
line for log file,0
create a model card for this model with Flair and PyTorch version,0
also record Transformers version if library is loaded,0
remember all parameters used in train() call,0
add model card to model,0
"if optimizer is class, trainer will create a single parameter group",0
"determine what splits (train, dev, test) to evaluate and log",0
prepare loss logging file and set up header,0
"if optimizer class is passed, instantiate:",0
"from here on, use list of learning rates",0
load existing optimizer state dictionary if it exists,0
"minimize training loss if training with dev data, else maximize dev score",0
"if scheduler is passed as a class, instantiate",0
"if we load a checkpoint, we have already trained for epoch",0
load existing scheduler state dictionary if it exists,0
update optimizer and scheduler in model card,0
"if training also uses dev/train data, include in training set",0
initialize sampler if provided,0
init with default values if only class is provided,0
set dataset to sample from,0
this field stores the names of all dynamic embeddings in the model (determined after first forward pass),0
At any point you can hit Ctrl + C to break out of training early.,0
update epoch in model card,0
get new learning rate,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
process mini-batches,0
zero the gradients on the model and optimizer,0
"if necessary, make batch_steps",0
forward and backward for batch,0
forward pass,0
Backward,0
identify dynamic embeddings (always deleted) on first sentence,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
do the optimizer step,0
do the scheduler step if one-cycle or linear decay,0
get new learning rate,0
evaluate on train / dev / test split depending on training settings,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
calculate scores using dev data if available,0
append dev score to score history,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
determine if this is the best model or if we need to anneal,0
default mode: anneal against dev score,0
alternative: anneal against dev loss,0
alternative: anneal against train loss,0
determine bad epoch number,0
lr unchanged,0
log bad epochs,0
output log file,0
make headers on first epoch,0
"if checkpoint is enabled, save model at each epoch",0
Check whether to save best model,0
"if we do not use dev data for model selection, save final model",0
test best model if test data is present,0
recover all arguments that were used to train this model,0
you can overwrite params with your own,0
surface nested arguments,0
resume training with these parameters,0
"if we are training over multiple datasets, do evaluation for each",0
get and return the final test score of best model,0
cast string to Path,0
forward pass,0
update optimizer and scheduler,0
"TextDataset returns a list. valid and test are only one file,",0
so return the first element,0
cast string to Path,0
error message if the validation dataset is too small,0
Shuffle training files randomly after serially iterating,0
through corpus one,0
"iterate through training data, starting at",0
self.split (for checkpointing),0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
Backward,0
`clip_grad_norm` helps prevent the exploding gradient,0
problem in RNNs / LSTMs.,0
We detach the hidden state from how it was,0
previously produced.,0
"If we didn't, the model would try backpropagating",0
all the way to start of the dataset.,0
explicitly remove loss to clear up memory,0
#########################################################,0
Save the model if the validation loss is the best we've,0
seen so far.,0
#########################################################,0
print info,0
#########################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
1-camembert-base -> camembert-base,0
1-xlm-roberta-large -> xlm-roberta-large,0
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol,0
The mask has 1 for real tokens and 0 for padding tokens. Only real,0
tokens are attended to.,0
Zero-pad up to the sequence length.,0
"first, find longest sentence in batch",0
prepare id maps for BERT model,0
put encoded batch through BERT model to get all hidden states of all encoder layers,0
get aggregated embeddings for each BERT-subtoken in sentence,0
get the current sentence object,0
add concatenated embedding to sentence,0
use first subword embedding if pooling operation is 'first',0
"otherwise, do a mean over all subwords in token",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
embed a dummy sentence to determine embedding_length,0
Avoid conflicts with flair's Token class,0
"<cls> token initially set to 1/D, so it attends to all image features equally",0
add positional encodings,0
reshape the pixels into the sequence,0
layer norm after convolution and positional encodings,0
add <cls> token,0
"transformer requires input in the shape [h*w+1, b, d]",0
the output is an embedding of <cls> token,0
this parameter is fixed,0
optional fine-tuning on top of embedding layer,0
"if only one sentence is passed, convert to list of sentence",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push through RNN,0
after-RNN dropout,0
extract embeddings from RNN,0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
serialize the language models and the constructor arguments (but nothing else),0
special handling for deserializing language models,0
re-initialize language model with constructor arguments,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
IMPORTANT: add embeddings as torch modules,0
iterate over sentences,0
"if its a forward LM, take last state",0
"convert to plain strings, embedded in a list for the encode function",0
CNN,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push CNN,0
after-CNN dropout,0
extract embeddings from CNN,0
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency",0
"if only one sentence is passed, convert to list of sentence",0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
do not print transformer warnings as these are confusing in this case,0
load tokenizer and transformer model,0
load tokenizer from inmemory zip-file,0
model name,0
embedding parameters,0
send mini-token through to check how many layers the model has,0
return length,0
check if special tokens exist to circumvent error message,0
"most models have an initial BOS token, except for XLNet, T5 and GPT2",0
"when initializing, embeddings are in eval mode by default",0
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial",0
in case of doubt: token embedding has higher priority than document embedding,0
in case of doubt: token embedding has higher priority than document embedding,0
"transformers returns the ""added_tokens.json"" even if it doesn't create it",0
remove special markup,0
legacy TransformerDocumentEmbedding,0
legacy TransformerTokenEmbedding,0
legacy Flair <= 0.7,0
legacy TransformerTokenEmbedding,0
Legacy TransformerDocumentEmbedding,0
legacy TransformerTokenEmbedding,0
legacy TransformerDocumentEmbedding,0
copy values from new embedding,0
iterate over subtokens and reconstruct tokens,0
remove special markup,0
TODO check if this is necessary is this method is called before prepare_for_model,1
check if reconstructed token is special begin token ([CLS] or similar),0
some BERT tokenizers somehow omit words - in such cases skip to next token,0
append subtoken to reconstruct token,0
check if reconstructed token is the same as current token,0
"if so, add subtoken count",0
reset subtoken count and reconstructed token,0
break from loop if all tokens are accounted for,0
if tokens are unaccounted for,0
check if all tokens were matched to subtokens,0
subtokenize the sentence,0
transformer specific tokenization,0
set zero embeddings for empty sentences and exclude,0
determine into how many subtokens each token is split,0
remember tokenized sentences and their subtokenization,0
Models such as FNet do not have an attention_mask,0
set language IDs for XLM-style transformers,0
cls first pooling can be done without recreating sentence hidden states,0
encode inputs,0
make the tuple a tensor; makes working with it easier.,0
only use layers that will be outputted,0
remove padding tokens,0
set context if not set already,0
create expanded sentence and remember context offsets,0
move embeddings from context back to original sentence (if using context),0
Expose base classses,0
Expose document embedding classes,0
Expose image embedding classes,0
Expose legacy embedding classes,0
Expose token embedding classes,0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNINOS embeddings,0
pubmed embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
gensim version 4,0
gensim version 3,0
fix serialized models,0
"this is required to force the module on the cpu,",0
"if a parent module is put to gpu, the _apply is called to each sub_module",0
self.to(..) actually sets the device properly,0
this ignores the get_cached_vec method when loading older versions,0
it is needed for compatibility reasons,0
gensim version 4,0
gensim version 3,0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
multilingual models,0
English models,0
Arabic,0
Bulgarian,0
Czech,0
Danish,0
German,0
Spanish,0
Basque,0
Persian,0
Finnish,0
French,0
Hebrew,0
Hindi,0
Croatian,0
Indonesian,0
Italian,0
Japanese,0
Malayalam,0
Dutch,0
Norwegian,0
Polish,0
Portuguese,0
Pubmed,0
Slovenian,0
Swedish,0
Tamil,0
Spanish clinical,0
CLEF HIPE Shared task,0
Amharic,0
load model if in pretrained model map,0
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir),0
CLEF HIPE models are lowercased,0
embeddings are static if we don't do finetuning,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
make compatible with serialized models (TODO: remove),1
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout",0
make compatible with serialized models (TODO: remove),1
gradients are enable if fine-tuning is enabled,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
offset mode that extracts at whitespace after last character,0
offset mode that extracts at last character,0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
check token.text is empty or not,0
set aggregation operation,0
add embeddings after updating,0
this parameter is fixed,0
model architecture,0
model architecture,0
"""pl"",",0
download if necessary,0
load the model,0
"TODO: keep for backwards compatibility, but remove in future",1
save the sentence piece model as binary file (not as path which may change),0
write out the binary sentence piece model into the expected directory,0
"if the model was saved as binary and it is not found on disk, write to appropriate path",0
"otherwise, use normal process and potentially trigger another download",0
"once the modes if there, load it with sentence piece",0
empty words get no embedding,0
all other words get embedded,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn,0
GLOVE embeddings,0
read in test file if exists,0
read in dev file if exists,0
"find train, dev and test files if not specified",0
Add tags for each annotated span,0
Tag all other token as Outer (O),0
Remove leading and trailing whitespaces from annotated spans,0
Search start and end token index for current span,0
If end index is not found set to last token,0
Throw error if indices are not valid,0
Add IOB tags,0
get train data,0
read in test file if exists,0
read in dev file if exists,0
"find train, dev and test files if not specified",0
special key for space after,0
special key for feature columns,0
special key for dependency head id,0
"store either Sentence objects in memory, or only file offsets",0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
determine encoding of text file,0
identify which columns are spans and which are word-level,0
now load all sentences,0
skip first line if to selected,0
option 1: keep Sentence objects in memory,0
pointer to previous,0
parse next sentence,0
quit if last sentence reached,0
skip banned sentences,0
set previous and next sentence for context,0
append parsed sentence to list in memory,0
option 2: keep source data in memory,0
"read lines for next sentence, but don't parse",0
quit if last sentence reached,0
append raw lines for each sentence,0
we make a distinction between word-level tags and span-level tags,0
read first sentence to determine which columns are span-labels,0
skip first line if to selected,0
"sentence_2 = self._convert_lines_to_sentence(self._read_next_sentence(file),",0
word_level_tag_columns=column_name_map),0
go through all annotations,0
the space after key is always word-levels,0
for column in self.word_level_tag_columns:,0
"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")",0
"if sentence ends, break",0
parse comments if possible,0
"otherwise, this line is a token. parse and add to sentence",0
check if this sentence is a document boundary,0
add span labels,0
parse relations if they are set,0
head and tail span indices are 1-indexed and end index is inclusive,0
get fields from line,0
get head_id if exists (only in dependency parses),0
initialize token,0
go through all columns,0
'feats' and 'misc' column should be split into different fields,0
special handling for whitespace after,0
add each other feature as label-value pair,0
get the task name (e.g. 'ner'),0
get the label value,0
add label,0
remap regular tag names,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
set sentence context using partials TODO: pointer to dataset is really inefficient,1
this dataset name,0
check if data there,0
column format,0
this dataset name,0
check if data there,0
column format,0
this dataset name,0
download data if necessary,0
download files if not present locally,0
we need to slightly modify the original files by adding some new lines after document separators,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
Remove CoNLL-U meta information in the last column,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
entity_mapping,0
this dataset name,0
download data if necessary,0
data validation,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download files if not present locallys,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
# download zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
download data if necessary,0
"unzip the downloaded repo and merge the train, dev and test datasets",0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
check if data there,0
create folder,0
download dataset,0
column format,0
this dataset name,0
download and parse data if necessary,0
create train test dev if not exist,0
column format,0
this dataset name,0
If the extracted corpus file is not yet present in dir,0
download zip if necessary,0
"extracted corpus is not present , so unpacking it.",0
column format,0
this dataset name,0
download zip,0
unpacking the zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)",0
download files if not present locally,0
we need to modify the original files by adding new lines after after the end of each sentence,0
if only one language is given,0
column format,0
this dataset name,0
"use all languages if explicitly set to ""all""",0
download data if necessary,0
initialize comlumncorpus and add it to list,0
column format,0
this dataset name,0
check if data there,0
code-switch uses the same training data than multi but provides a different test set.,0
"as the test set is not published, those two tasks are the same.",0
column format,0
this dataset name,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
this list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
get google drive id from list,0
download from google drive,0
unzip,0
"tar.extractall(language_folder,members=[tar.getmember(file_name)])",0
transform data into required format,0
"the processed dataset has the additional ending ""_new""",0
remove the unprocessed dataset,0
initialize comlumncorpus and add it to list,0
if no languages are given as argument all languages used in XTREME will be loaded,0
if only one language is given,0
column format,0
this dataset name,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
This list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
download from HU Server,0
unzip,0
transform data into required format,0
initialize comlumncorpus and add it to list,0
if only one language is given,0
column format,0
this dataset name,0
download data if necessary,0
initialize comlumncorpus and add it to list,0
download data if necessary,0
unpack and write out in CoNLL column-like format,0
column format,0
this dataset name,0
download data if necessary,0
data is not in IOB2 format. Thus we transform it to IOB2,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
column format,0
this dataset name,0
rename according to train - test - dev - convention,0
column format,0
this dataset name,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
column format,0
this dataset name,0
download data if necessary,0
Add missing newline after header,0
Workaround for empty tokens,1
"Add ""real"" document marker",0
Dataset split mapping,0
v2.0 only adds new language and splits for AJMC dataset,0
Special document marker for sample splits in AJMC dataset,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
iterate over all html files,0
"get rid of html syntax, we only need the text",0
between all documents we write a separator symbol,0
skip empty strings,0
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)",0
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention",0
sentence splitting and tokenization,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out-file in column format,0
"in case something goes wrong, delete the dataset and raise error",0
this dataset name,0
download and parse data if necessary,0
from qwikidata.linked_data_interface import get_entity_dict_from_api,0
generate qid wikiname dictionaries,0
merge dictionaries,0
ignore first line,0
commented and empty lines,0
read all Q-IDs,0
ignore first line,0
request,0
this dataset name,0
we use the wikiids in the data instead of directly utilizing the wikipedia urls.,0
like this we can quickly check if the corresponding page exists,0
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi,0
delete unprocessed file,0
collect all wikiids,0
create the dictionary,0
request,0
this dataset name,0
names of raw text documents,0
open output_file,0
iterate through all documents,0
split sentences and tokenize,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out file,0
this dataset name,0
download and parse data if necessary,0
this dataset name,0
download and parse data if necessary,0
First parse the post titles,0
Keep track of how many and which entity mentions does a given post title have,0
Check if the current post title has an entity link and parse accordingly,0
Post titles with entity mentions (if any) are handled via this function,0
Then parse the comments,0
"Iterate over the comments.tsv file, until the end is reached",0
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched.",0
Each comment thread is handled as one 'document'.,0
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.,0
This if-condition is needed to handle this problem.,0
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure",0
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above",0
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle.",0
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,",0
and not just single letters into single rows.,0
If there are annotated entity mentions for given post title or a comment thread,0
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence",0
Write the token with a corresponding tag to file,0
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed",0
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag",0
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized,0
"incorrectly, in order to keep the desired format (empty line as a sentence separator).",0
"Thrown when the second check above happens, but the last token of a sentence is reached.",0
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below.",0
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS",0
Check if further annotations belong to the current post title or comment thread as well,0
Stop when the end of an annotation file is reached,0
Check if further annotations belong to the current sentence as well,0
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)",0
Docstart,0
if there is more than one word in the chunk we write each in a separate line,0
print(chunks),0
empty line after each sentence,0
convert the file to CoNLL,0
this dataset name,0
"check if data there, if not, download the data",0
create folder,0
download data,0
transform data into column format if necessary,0
if no filenames are specified we use all the data,0
"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled",0
also we remove 'raganato_ALL' from filenames in case its in the list,0
generate the test file,0
make column file and save to data_folder,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
this dataset name,0
download data if necessary,0
if True:,0
write CoNLL-U Plus header,0
"Some special cases (e.g., missing spaces before entity marker)",0
necessary if text should be whitespace tokenizeable,0
Handle case where tail may occur before the head,0
this dataset name,0
write CoNLL-U Plus header,0
this dataset name,0
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors,1
download data if necessary,0
write CoNLL-U Plus header,0
The span has ended.,0
We are entering a new span; reset indices,0
and active tag to new span.,0
We're inside a span.,0
Last token might have been a part of a valid span.,0
this dataset name,0
write CoNLL-U Plus header,0
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):",0
"with zip_file.open(source_file_path, mode=""r"") as source_file:",0
target_file_path = Path(data_folder) / target_filename,0
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:",0
# write CoNLL-U Plus header,0
"target_file.write(""# global.columns = id form ner\n"")",0
for example in json.load(source_file):,0
token_list = self._tacred_example_to_token_list(example),0
target_file.write(token_list.serialize()),0
check if first tag row is already occupied,0
"if first tag row is occupied, use second tag row",0
hardcoded mapping TODO: perhaps find nicer solution,1
"find train, dev and test files if not specified",0
use test_file to create test split if available,0
use dev_file to create test split if available,0
"if data point contains black-listed label, do not use",0
first check if valid sentence,0
"if so, add to indices",0
"find train, dev and test files if not specified",0
variables,0
different handling of in_memory data than streaming data,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
test if format is OK,0
test if at least one label given,0
make sentence from text (and filter for length),0
"if a pair column is defined, make a sentence pair object",0
noinspection PyDefaultArgument,0
dataset name includes the split size,0
default dataset folder is the cache root,0
download data if necessary,0
download each of the 28 splits,0
create dataset directory if necessary,0
download senteval datasets if necessary und unzip,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
handle labels file,0
handle data file,0
Create flair compatible labels,0
"by defaut, map point score to POSITIVE / NEGATIVE values",0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file from CSV,0
create test.txt file from CSV,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create train dev and test files in fasttext format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
convert to FastText format,0
download data if necessary,0
"if data is not downloaded yet, download it",0
get the zip file,0
move original .tsv files to another folder,0
create train and dev splits in fasttext format,0
create eval_dataset file with no labels,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download datasets if necessary,0
create dataset directory if necessary,0
create correctly formated txt files,0
multiple labels are possible,0
this dataset name,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
create a separate directory for different tasks,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
check if dataset is supported,0
set file names,0
set file names,0
download and unzip in file structure if necessary,0
instantiate corpus,0
"find train, dev and test files if not specified",0
"create DataPairDataset for train, test and dev file, if they are given",0
stop if file does not exist,0
create a DataPair object from strings,0
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"reorder dev datasets to have same columns as in train set: 8, 9, and 11",0
dev sets include 5 different annotations but we will only keep the gold label,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get test and dev sets,0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
"if data not downloaded yet, download it",0
get the zip file,0
"the downloaded files have json format, we transform them to tsv",0
Function to transform JSON file to tsv for Recognizing Textual Entailment Data,0
remove json file,0
Uses dynamic programming approach to calculate maximum independent set in interval graph,0
with sum of all entity lengths as secondary key,0
calculate offset without current text,0
because we stick all passages of a document together,0
TODO For split entities we also annotate everything inbetween which might be a bad idea?,1
Try to fix incorrect annotations,0
print(,0
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}""",0
),0
Ignore empty lines or relation annotations,0
FIX annotation of whitespaces (necessary for PDR),0
One token may contain multiple entities -> deque all of them,0
column format,0
this dataset name,0
Create tokenization-dependent CONLL files. This is necessary to prevent,0
from caching issues (e.g. loading the same corpus with different sentence splitters),0
column format,0
this dataset name,0
column format,0
this dataset name,0
Edge case: last token starts a new entity,0
Last document in file,0
column format,0
this dataset name,0
column format,0
this dataset name,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
Edge case: last token starts a new entity,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
column format,0
this dataset name,0
Read texts,0
Read annotations,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
We need to apply a patch to correct the original training file,0
Articles title,0
Article abstract,0
Entity annotations,0
column format,0
this dataset name,0
Edge case: last token starts a new entity,0
Map all entities to chemicals,0
Map all entities to disease,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
Incomplete article,0
Invalid XML syntax,0
column format,0
this dataset name,0
column format,0
this dataset name,0
if len(mid) != 3:,0
continue,0
Try to fix entity offsets,0
column format,0
this dataset name,0
There is still one illegal annotation in the file ..,0
column format,0
this dataset name,0
"Abstract first, title second to prevent issues with sentence splitting",0
column format,0
this dataset name,0
column format,0
this dataset name,0
column format,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
this dataset name,0
column format,0
this dataset name,0
"Filter for specific entity types, by default no entities will be filtered",0
Get original HUNER splits to retrieve a list of all document ids contained in V2,0
train and dev split of V2 will be train in V4,0
test split of V2 will be dev in V4,0
New documents in V4 will become test documents,0
column format,0
this dataset name,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
cache Feidegger config file,0
cache Feidegger images,0
replace image URL with local cached file,0
append Sentence-Image data point,0
"in certain cases, multi-CPU data loading makes no sense and slows",0
"everything down. For this reason, we detect if a dataset is in-memory:",0
"if so, num_workers is set to 0 for faster processing",0
cast to list if necessary,0
cast to list if necessary,0
"first, check if pymongo is installed",0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
Expose base classses,0
Expose all biomedical data sets used for the evaluation of BioBERT,0
-,0
-,0
-,0
-,0
Expose all biomedical data sets using the HUNER splits,0
Expose all biomedical data sets,0
Expose all document classification datasets,0
word sense disambiguation,0
Expose all entity linking datasets,0
Expose all relation extraction datasets,0
universal proposition banks,0
keyphrase detection datasets,0
other NER datasets,0
standard NER datasets,0
Expose all sequence labeling datasets,0
Expose all text-image datasets,0
Expose all text-text datasets,0
Expose all treebanks,0
"find train, dev and test files if not specified",0
get train data,0
get test data,0
get dev data,0
option 1: read only sentence boundaries as offset positions,0
option 2: keep everything in memory,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
current token ID,0
handling for the awful UD multiword format,0
end of sentence,0
comments,0
ellipsis,0
if token is a multi-word,0
normal single-word tokens,0
"if we don't split multiwords, skip over component words",0
add token,0
add morphological tags,0
derive whitespace logic for multiwords,0
print(token),0
print(current_multiword_last_token),0
print(current_multiword_first_token),0
"if multi-word equals component tokens, there should be no whitespace",0
go through all tokens in subword and set whitespace_after information,0
print(i),0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
this dataset name,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"finally, print model card for information",0
test corpus,0
create a TARS classifier,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
test corpus,0
create a TARS classifier,0
switch to a new task (TARS can do multiple tasks so you must define one),0
initialize the text classifier trainer,0
start the training,0
"With end symbol, without start symbol, padding in front",0
"Without end symbol, with start symbol, padding in back",0
"Without end symbol, without start symbol, padding in front",0
increment for last token in sentence if not followed by whitespace,0
clean up directory,0
clean up directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
initialize trainer,0
initialize trainer,0
initialize trainer,0
initialize trainer,0
initialize trainer,0
train model for 2 epochs,0
load the checkpoint model and train until epoch 4,0
clean up results directory,0
initialize trainer,0
from flair.trainers.trainer_regression import RegressorTrainer,0
def test_trainer_evaluation(tasks_base_path):,0
"corpus, model, trainer = init(tasks_base_path)",0
,0
expected = model.evaluate(corpus.dev),0
,0
assert expected is not None,0
def test_trainer_results(tasks_base_path):,0
"corpus, model, trainer = init(tasks_base_path)",0
"results = trainer.train(""regression_train/"", max_epochs=1)",0
"assert results[""test_score""] > 0",0
"assert len(results[""dev_loss_history""]) == 1",0
"assert len(results[""dev_score_history""]) == 1",0
"assert len(results[""train_loss_history""]) == 1",0
example sentence,0
set 4 labels for 2 tokens ('love' is tagged twice),0
check if there are three POS labels with correct text and values,0
check if there are is one SENTIMENT label with correct text and values,0
check if all tokens are correctly labeled,0
remove the pos label from the last word,0
there should be 2 POS labels left,0
now remove all pos tags,0
set 3 labels for 2 spans (HU is tagged twice),0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
"now delete the NER tags of ""Humboldt-Universitt zu Berlin""",0
should be only one NER label left,0
and only one NER span,0
set 3 labels for 2 spans (HU is tagged twice with different tags),0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
"now delete the NER tags of ""Humboldt-Universitt zu Berlin""",0
should be only one NER label left,0
and only one NER span,0
but there is also one orgtype span and label,0
and only one NER span,0
let's add the NER tag back,0
check if there are three labels with correct text and values,0
check if there are two spans with correct text and values,0
now remove all NER tags,0
set 3 labels for 2 spans (HU is tagged twice with different tags),0
create two relation label,0
there should be two relation labels,0
there should be one syntactic labels,0
"there should be two relations, one with two and one with one label",0
example sentence,0
add another topic label,0
example sentence,0
has sentiment value,0
has 4 part of speech tags,0
has 1 NER tag,0
should be in total 6 labels,0
example sentence,0
add two NER labels,0
get the four labels,0
check that only two of the respective data points are equal,0
make a sentence and some right context,0
TODO: is this desirable? Or should two sentences with same text still be considered different objects?,1
@pytest.mark.integration,0
initialize trainer,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
define search space,0
sequence tagger parameter,0
model trainer parameter,0
training parameter,0
find best parameter settings,0
clean up results directory,0
document embeddings parameter,0
training parameter,0
clean up results directory,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
"assert [token.get_tag(""head"").value for token in sent1.tokens] == [",0
"""2"",",0
"""0"",",0
"""4"",",0
"""2"",",0
"""2"",",0
"""2"",",0
],0
"Here, we use the default token annotation fields.",0
"We have manually checked, that these numbers are correct:",0
"+1 offset, because of missing EOS marker at EOD",0
load dataset,0
tagger without CRF,0
train,0
check if loaded model can predict,0
check if loaded model successfully fit the training data,0
load dataset,0
tagger without CRF,0
train,0
check if loaded model can predict,0
check if loaded model successfully fit the training data,0
load dataset,0
tagger without CRF,0
train,0
check if loaded model can predict,0
check if loaded model successfully fit the training data,0
load dataset,0
tagger without CRF,0
train,0
check if loaded model can predict,0
check if loaded model successfully fit the training data,0
check if model can predict,0
load model,0
chcek if model predicts correct label,0
check if loaded model successfully fit the training data,0
check if model can predict,0
load model,0
chcek if model predicts correct label,0
check if loaded model successfully fit the training data,0
check if model can predict,0
load model,0
chcek if model predicts correct label,0
check if loaded model successfully fit the training data,0
clean up file,0
train model for 2 epochs,0
load the checkpoint model and train until epoch 4,0
from allennlp.common.tqdm import Tqdm,0
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
Extract all the contents of zip file in current directory,0
Extract all the contents of zip file in current directory,0
get cache path to put the file,0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
first determine the distribution of classes in the dataset,0
weight for each sample,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
determine offsets for whitespace_after field,0
increment for last token in sentence if not followed by whitespace,0
determine offsets for whitespace_after field,0
conll 2000 column format,0
conll 03 NER column format,0
WNUT-17,0
-- WikiNER datasets,0
-- Universal Dependencies,0
Germanic,0
Romance,0
West-Slavic,0
South-Slavic,0
East-Slavic,0
Scandinavian,0
Asian,0
Language isolates,0
recent Universal Dependencies,0
other datasets,0
text classification format,0
text regression format,0
"first, try to fetch dataset online",0
default dataset folder is the cache root,0
get string value if enum is passed,0
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)",0
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag",0
the CoNLL 03 task for German has an additional lemma column,0
the CoNLL 03 task for Dutch has no NP column,0
the CoNLL 03 task for Spanish only has two columns,0
the GERMEVAL task only has two columns: text and ner,0
WSD tasks may be put into this column format,0
"the UD corpora follow the CoNLL-U format, for which we have a special reader",0
"for text classifiers, we use our own special format",0
NER corpus for Basque,0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
get train and test data,0
"read in test file if exists, otherwise sample 10% of train data as test dataset",0
"read in dev file if exists, otherwise sample 10% of train data as dev dataset",0
convert tag scheme to iobes,0
automatically identify train / test / dev files,0
automatically identify train / test / dev files,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
conll 2000 chunking task,0
Support both TREC-6 and TREC-50,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
Wikiner NER task,0
unpack and write out in CoNLL column-like format,0
CoNLL 02/03 NER,0
universal dependencies,0
--- UD Germanic,0
--- UD Romance,0
--- UD West-Slavic,0
--- UD Scandinavian,0
--- UD South-Slavic,0
--- UD Asian,0
this is the default init size of a lmdb database for embeddings,0
some non-used parameter to allow print,0
get db filename from embedding name,0
"In case initialization of cached version failed, just fallback to the original WordEmbeddings",0
SequenceTagger,0
TextClassifier,0
get db filename from embedding name,0
if embedding database already exists,0
"otherwise, push embedding to database",0
if embedding database already exists,0
open the database in read mode,0
we need to set self.k,0
create and load the database in write mode,0
"no idea why, but we need to close and reopen the environment to avoid",0
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot,0
when opening new transaction !,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
set 'add_unk' if the dictionary was created with a version of Flair older than 0.9,0
set 'add_unk' depending on whether <unk> is a key,0
"if text is passed, instantiate sentence with tokens (words)",0
log a warning if the dataset is empty,0
some sentences represent a document boundary (but most do not),0
data with zero-width characters cannot be handled,0
set token idx if not set,0
non-set tags are OUT tags,0
anything that is not a BIOES tag is a SINGLE tag,0
anything that is not OUT is IN,0
single and begin tags start a new span,0
remember previous tag,0
"if label type is explicitly specified, get spans for this label type",0
else determine all label types in sentence and get all spans,0
move sentence embeddings to device,0
move token embeddings to device,0
clear sentence embeddings,0
clear token embeddings,0
infer whitespace after field,0
add Sentence labels to output if they exist,0
add Token labels to output if they exist,0
add Sentence labels to output if they exist,0
add Token labels to output if they exist,0
No character at the corresponding code point: remove it,0
TODO: crude hack - replace with something better,1
set name,0
abort if no data is provided,0
sample test data from train if none is provided,0
sample dev data from train if none is provided,0
set train dev and test data,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
"if there are token labels of provided type, use these. Otherwise use sentence labels",0
check for labels of words,0
if we are looking for sentence-level labels,0
check if sentence itself has labels,0
"if this is not a token-level prediction problem, add sentence-level labels to dictionary",0
Make the tag dictionary,0
global variable: cache_root,0
global variable: device,0
global variable: embedding_storage_mode,0
# dummy return to fulfill trainer.train() needs,0
print(vec),0
Attach optimizer,0
"convert `metrics` to float, in case it's a zero-dim Tensor",0
if memory mode option 'none' delete everything,0
else delete only dynamic embeddings (otherwise autograd will keep everything in memory),0
find out which ones are dynamic embeddings,0
find out which ones are dynamic embeddings,0
memory management - option 1: send everything to CPU (pin to memory if we train on GPU),0
record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class),0
"in Flair <0.9.1, optimizer and scheduler used to train model are not saved",0
"write out a ""model card"" if one is set",0
special handling for optimizer: remember optimizer class and state dictionary,0
save model,0
restore optimizer and scheduler to model card if set,0
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups,1
see https://github.com/zalandoresearch/flair/issues/351,0
"read Dataset into data loader (if list of sentences passed, make Dataset first)",0
loss calculation,0
variables for printing,0
variables for computing scores,0
remove any previously predicted labels,0
predict for batch,0
get the gold labels,0
add to all_predicted_values,0
make printout lines,0
"if the model is span-level, transfer to word-level annotations for printout",0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
check if there is a label mismatch,0
print info,0
write all_predicted_values to out_file if set,0
make the evaluation dictionary,0
"finally, compute numbers",0
"now, calculate evaluation numbers",0
there is at least one gold label or one prediction (default),0
issue error and default all evaluation numbers to 0.,0
line for log file,0
initialize the label dictionary,0
set up multi-label logic,0
loss weights and loss function,0
Initialize the weight tensor,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
remove previously predicted labels of this type,0
if anything could possibly be predicted,0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
plot i,0
save plots,0
save plots,0
plt.show(),0
save plot,0
auto-spawn on GPU if available,0
remove previous embeddings,0
clearing token embeddings to save memory,0
"read Dataset into data loader (if list of sentences passed, make Dataset first)",0
#TODO: not saving lines yet,1
== similarity measures ==,0
helper class for ModelSimilarity,0
-- works with binary cross entropy loss --,0
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}",0
-- works with ranking/triplet loss --,0
normalize the embeddings,0
== similarity losses ==,0
"we want that logits for corresponding pairs are high, and for non-corresponding low",0
TODO: this assumes eye matrix,0
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa",0
== similarity learner ==,0
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both",0
assumes that for each data pair there's at least one embedding per modality,0
pre-compute embeddings for all targets in evaluation dataset,0
compute the similarity,0
sort the similarity matrix across modality 1,0
"get the ranks, so +1 to start counting ranks from 1",0
The conversion from old model's constructor interface,0
auto-spawn on GPU if available,0
pad strings with whitespaces to longest sentence,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
serialize the language models and the constructor arguments (but nothing else),0
special handling for deserializing language models,0
re-initialize language model with constructor arguments,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
Transform input data into TARS format,0
print(all_labels),0
"if there are no labels, return a random sample as negatives",0
print(sample),0
"otherwise, go through all labels",0
make sure the probabilities always sum up to 1,0
get and embed all labels by making a Sentence object that contains only the label text,0
get each label embedding and scale between 0 and 1,0
compute similarity matrix,0
"the higher the similarity, the greater the chance that a label is",0
sampled as negative example,0
make label dictionary if no Dictionary object is passed,0
prepare dictionary of tags (without B- I- prefixes and without UNK),0
check if candidate_label_set is empty,0
make list if only one candidate label is passed,0
create label dictionary,0
note current task,0
create a temporary task,0
make zero shot predictions,0
switch to the pre-existing task,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
make a tars sentence where all labels are O by default,0
overwrite O labels with tags,0
init new TARS classifier,0
set all task information,0
linear layers of internal classifier,0
return,0
with torch.no_grad():,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
go through each sentence in the batch,0
always remove tags first,0
get the span and its label,0
determine whether tokens in this span already have a label,0
only add if all tokens have no label,0
clearing token embeddings to save memory,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
init new TARS classifier,0
set all task information,0
linear layers of internal classifier,0
with torch.no_grad():,0
set context if not set already,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
go through each sentence in the batch,0
always remove tags first,0
add all labels that according to TARS match the text and are above threshold,0
do not add labels below confidence threshold,0
only use label with highest confidence if enforcing single-label predictions,0
get all label scores and do an argmax to get the best label,0
remove previously added labels and only add the best label,0
clearing token embeddings to save memory,0
if embed_separately == True the linear layer needs twice the length of the embeddings as input size,0
since we concatenate the embeddings of the two DataPoints in the DataPairs,0
representation for both sentences,0
set separator to concatenate two sentences,0
auto-spawn on GPU if available,0
linear layer,0
minimal return is scores and labels,0
set embeddings,0
set relation and entity label types,0
"whether to use gold entity pairs, and whether to filter entity pairs by type",0
init dropouts,0
pooling operation to get embeddings for entites,0
"entity pairs could also be no relation at all, add default value for this case to dictionary",0
decoder can be linear or nonlinear,0
super lame: make dictionary to find relation annotations for a given entity pair,0
get all entity spans,0
"go through cross product of entities, for each pair concat embeddings",0
filter entity pairs according to their tags if set,0
get gold label for this relation (if one exists),0
"if there is no gold label for this entity pair, set to 'O' (no relation)",0
"if predicting, also remember sentences and label candidates",0
if there's at least one entity pair in the sentence,0
embed sentences and get embeddings for each entity pair,0
get embeddings,0
stack and drop out (squeeze and unsqueeze),0
send through decoder,0
"return either scores and gold labels (for loss calculation), or include label candidates for prediction",0
if we concatenate the embeddings we need double input size in our linear layer,0
filter sentences with no candidates (no candidates means nothing can be linked anyway),0
fields to return,0
"if the entire batch has no sentence with candidates, return empty",0
"otherwise, embed sentence and send through prediction head",0
embed all tokens,0
get the embeddings of the entity mentions,0
minimal return is scores and labels,0
set the dictionaries,0
"if we use a CRF, we must add special START and STOP tags to the dictionary",0
Initialize the weight tensor,0
initialize the network architecture,0
dropouts,0
optional reprojection layer on top of word embeddings,0
bidirectional LSTM on top of embedding layer,0
Create initial hidden state and initialize it,0
TODO: Decide how to initialize the hidden state variables,1
self.hs_initializer(self.lstm_init_h),0
self.hs_initializer(self.lstm_init_c),0
final linear map to tag space,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
clearing token embeddings to save memory,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
"if initial hidden state is trainable, use this state",0
word dropout only before LSTM - TODO: more experimentation needed,1
if self.use_word_dropout > 0.0:,0
sentence_tensor = self.word_dropout(sentence_tensor),0
get the tags in this sentence,0
add tags as tensor,0
pad tags if using batch-CRF decoder,0
reduce raw values to avoid NaN during exp,0
broadcasting will do the job of reshaping and is more efficient than calling repeat,0
default value,0
core Flair models on Huggingface ModelHub,0
"Large NER models,",0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Language-specific NER models,0
English NER models,0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Danish models,0
German models,0
French models,0
Dutch models,0
Malayalam models,0
Portuguese models,0
Keyphase models,0
Biomedical models,0
check if model name is a valid local file,0
"check if model key is remapped to HF key - if so, print out information",0
get mapped name,0
output information,0
use mapped name instead,0
"if not, check if model key is remapped to direct download location. If so, download model",0
special handling for the taggers by the @redewiegergabe project (TODO: move to model hub),1
"for all other cases (not local file or special download location), use HF model hub",0
"if not a local file, get from model hub",0
use model name as subfolder,0
Lazy import,0
output information,0
"log.error(f"" - Error message: {e}"")",0
"all_tag_prob=all_tag_prob,",0
clear embeddings after predicting,0
load each model,0
check if the same embeddings were already loaded previously,0
"if the model uses StackedEmbedding, make a new stack with previous objects",0
sort embeddings by key alphabetically,0
check previous embeddings and add if found,0
only re-use static embeddings,0
"if not found, use existing embedding",0
initialize new stack,0
"of the model uses regular embedding, re-load if previous version found",0
auto-spawn on GPU if available,0
embed sentences,0
make tensor for all embedded sentences in batch,0
send through decoder to get logits,0
minimal return is scores and labels,0
English sentiment models,0
Communicative Functions Model,0
embeddings,0
dictionaries,0
linear layer,0
all parameters will be pushed internally to the specified device,0
get all tokens in this mini-batch,0
minimal return is scores and labels,0
weights for loss function,0
iput size is two times wordembedding size since we use pair of words as input,0
"the output size is max_distance + 1, i.e. we allow 0,1,...,max_distance words between pairs",0
regression,0
input size is two times word embedding size since we use pair of words as input,0
the output size is 1,0
auto-spawn on GPU if available,0
all input should be tensors,0
forward allows only a single sentcence!!,0
embed words of sentence,0
go through all pairs of words with a maximum number of max_distance in between,0
go through all pairs,0
2-dim matrix whose rows are the embeddings of word pairs of the sentence,0
So far only one sentence allowed,0
If list of sentences is handed the function works with the first sentence of the list,0
Assume data_points is a single sentence!!!,0
scores are the predictions for each word pair,0
"classification needs labels to be integers, regression needs labels to be float",0
this is due to the different loss functions,0
only single sentences as input,0
gold labels,0
for output text file,0
for buckets,0
for average prediction,0
add some statistics to the output,0
use scikit-learn to evaluate,0
"we iterate over each sentence, instead of batches",0
get single labels from scores,0
gold labels,0
for output text file,0
hot one vector of true value,0
hot one vector of predicted value,0
"speichert embeddings, falls embedding_storage!= 'None'",0
"make ""classification report""",0
get scores,0
"precision_score = round(metrics.precision_score(y_true, y_pred, average='macro', zero_division=0), 4)",0
"recall_score = round(metrics.recall_score(y_true, y_pred, average='macro', zero_division=0), 4)",0
line for log file,0
create a model card for this model with Flair and PyTorch version,0
also record Transformers version if library is loaded,0
remember all parameters used in train() call,0
add model card to model,0
cast string to Path,0
check for previously saved best models in the current training folder and delete them,0
"determine what splits (train, dev, test) to evaluate and log",0
prepare loss logging file and set up header,0
"if optimizer class is passed, instantiate:",0
load existing optimizer state dictionary if it exists,0
"minimize training loss if training with dev data, else maximize dev score",0
"if scheduler is passed as a class, instantiate",0
"if we load a checkpoint, we have already trained for epoch",0
load existing scheduler state dictionary if it exists,0
update optimizer and scheduler in model card,0
"if training also uses dev/train data, include in training set",0
initialize sampler if provided,0
init with default values if only class is provided,0
set dataset to sample from,0
At any point you can hit Ctrl + C to break out of training early.,0
update epoch in model card,0
get new learning rate,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
process mini-batches,0
zero the gradients on the model and optimizer,0
"if necessary, make batch_steps",0
forward and backward for batch,0
forward pass,0
Backward,0
do the optimizer step,0
do the scheduler step if one-cycle or linear decay,0
get new learning rate,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
evaluate on train / dev / test split depending on training settings,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
calculate scores using dev data if available,0
append dev score to score history,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
determine if this is the best model or if we need to anneal,0
default mode: anneal against dev score,0
alternative: anneal against dev loss,0
alternative: anneal against train loss,0
determine bad epoch number,0
log bad epochs,0
output log file,0
make headers on first epoch,0
"if checkpoint is enabled, save model at each epoch",0
Check whether to save best model,0
"if we do not use dev data for model selection, save final model",0
test best model if test data is present,0
recover all arguments that were used to train this model,0
you can overwrite params with your own,0
surface nested arguments,0
resume training with these parameters,0
"if we are training over multiple datasets, do evaluation for each",0
get and return the final test score of best model,0
cast string to Path,0
forward pass,0
update optimizer and scheduler,0
append current loss to list of losses for all iterations,0
compute averaged loss,0
"TextDataset returns a list. valid and test are only one file, so return the first element",0
cast string to Path,0
error message if the validation dataset is too small,0
Shuffle training files randomly after serially iterating through corpus one,0
"iterate through training data, starting at self.split (for checkpointing)",0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
Backward,0
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.,0
We detach the hidden state from how it was previously produced.,0
"If we didn't, the model would try backpropagating all the way to start of the dataset.",0
explicitly remove loss to clear up memory,0
##############################################################################,0
Save the model if the validation loss is the best we've seen so far.,0
##############################################################################,0
print info,0
##############################################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
1-camembert-base -> camembert-base,0
1-xlm-roberta-large -> xlm-roberta-large,0
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol,0
The mask has 1 for real tokens and 0 for padding tokens. Only real,0
tokens are attended to.,0
Zero-pad up to the sequence length.,0
"first, find longest sentence in batch",0
prepare id maps for BERT model,0
put encoded batch through BERT model to get all hidden states of all encoder layers,0
get aggregated embeddings for each BERT-subtoken in sentence,0
get the current sentence object,0
add concatenated embedding to sentence,0
use first subword embedding if pooling operation is 'first',0
"otherwise, do a mean over all subwords in token",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
embed a dummy sentence to determine embedding_length,0
Avoid conflicts with flair's Token class,0
"<cls> token initially set to 1/D, so it attends to all image features equally",0
add positional encodings,0
reshape the pixels into the sequence,0
layer norm after convolution and positional encodings,0
add <cls> token,0
"transformer requires input in the shape [h*w+1, b, d]",0
the output is an embedding of <cls> token,0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
do not print transformer warnings as these are confusing in this case,0
load tokenizer and transformer model,0
model name,0
"when initializing, embeddings are in eval mode by default",0
embedding parameters,0
send mini-token through to check how many layers the model has,0
check whether CLS is at beginning or end,0
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial",0
gradients are enabled if fine-tuning is enabled,0
"first, subtokenize each sentence and find out into how many subtokens each token was divided",0
subtokenize sentences,0
tokenize and truncate to max subtokens (TODO: check better truncation strategies),1
find longest sentence in batch,0
initialize batch tensors and mask,0
put encoded batch through transformer model to get all hidden states of all encoder layers,0
iterate over all subtokenized sentences,0
use scalar mix of embeddings if so selected,0
set the extracted embedding for the token,0
special handling for serializing transformer models,0
serialize the transformer models and the constructor arguments (but nothing else),0
necessary for reverse compatibility with Flair <= 0.7,0
special handling for deserializing transformer models,0
load transformer model,0
constructor arguments,0
re-initialize transformer word embeddings with constructor arguments,0
for backward compatibility with previous models,0
"I have no idea why this is necessary, but otherwise it doesn't work",1
reload tokenizer to get around serialization issues,0
optional fine-tuning on top of embedding layer,0
"if only one sentence is passed, convert to list of sentence",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push through RNN,0
after-RNN dropout,0
extract embeddings from RNN,0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
serialize the language models and the constructor arguments (but nothing else),0
special handling for deserializing language models,0
re-initialize language model with constructor arguments,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
IMPORTANT: add embeddings as torch modules,0
iterate over sentences,0
"if its a forward LM, take last state",0
"convert to plain strings, embedded in a list for the encode function",0
CNN,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push CNN,0
after-CNN dropout,0
extract embeddings from CNN,0
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency",0
"if only one sentence is passed, convert to list of sentence",0
Expose base classses,0
Expose token embedding classes,0
Expose document embedding classes,0
Expose image embedding classes,0
Expose legacy embedding classes,0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNINOS embeddings,0
pubmed embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
gensim version 4,0
gensim version 3,0
fix serialized models,0
"this is required to force the module on the cpu,",0
"if a parent module is put to gpu, the _apply is called to each sub_module",0
self.to(..) actually sets the device properly,0
this ignores the get_cached_vec method when loading older versions,0
it is needed for compatibility reasons,0
gensim version 4,0
gensim version 3,0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
multilingual models,0
English models,0
Arabic,0
Bulgarian,0
Czech,0
Danish,0
German,0
Spanish,0
Basque,0
Persian,0
Finnish,0
French,0
Hebrew,0
Hindi,0
Croatian,0
Indonesian,0
Italian,0
Japanese,0
Malayalam,0
Dutch,0
Norwegian,0
Polish,0
Portuguese,0
Pubmed,0
Slovenian,0
Swedish,0
Tamil,0
Spanish clinical,0
CLEF HIPE Shared task,0
Amharic,0
load model if in pretrained model map,0
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir),0
CLEF HIPE models are lowercased,0
embeddings are static if we don't do finetuning,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
make compatible with serialized models (TODO: remove),1
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout",0
make compatible with serialized models (TODO: remove),1
gradients are enable if fine-tuning is enabled,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
offset mode that extracts at whitespace after last character,0
offset mode that extracts at last character,0
only clone if optimization mode is 'gpu',0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
check token.text is empty or not,0
set aggregation operation,0
add embeddings after updating,0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
do not print transformer warnings as these are confusing in this case,0
load tokenizer and transformer model,0
"in the end, these models don't need this configuration",0
model name,0
whether to detach gradients on overlong sentences,0
store whether to use context (and how much),0
dropout contexts,0
"if using context, can we cross document boundaries?",0
send self to flair-device,0
embedding parameters,0
send mini-token through to check how many layers the model has,0
calculate embedding length,0
return length,0
check if special tokens exist to circumvent error message,0
"most models have an intial BOS token, except for XLNet, T5 and GPT2",0
"when initializing, embeddings are in eval mode by default",0
remove special markup,0
"we require encoded subtokenized sentences, the mapping to original tokens and the number of",0
parts that each sentence produces,0
"if we also use context, first expand sentence to include context",0
set context if not set already,0
"in case of contextualization, we must remember non-expanded sentence",0
create expanded sentence and remember context offsets,0
overwrite sentence with expanded sentence,0
subtokenize the sentence,0
transformer specific tokenization,0
set zero embeddings for empty sentences and exclude,0
determine into how many subtokens each token is split,0
remember tokenized sentences and their subtokenization,0
encode inputs,0
Models such as FNet do not have an attention_mask,0
determine which sentence was split into how many parts,0
set language IDs for XLM-style transformers,0
put encoded batch through transformer model to get all hidden states of all encoder layers,0
make the tuple a tensor; makes working with it easier.,0
gradients are enabled if fine-tuning is enabled,0
iterate over all subtokenized sentences,0
"remove stride_size//2 at end of sentence_hidden_state, and half at beginning of remainder,",0
in order to get some context into the embeddings of these words.,0
also don't include the embedding of the extra [CLS] and [SEP] tokens.,0
"for each token, get embedding",0
some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector,0
"get states from all selected layers, aggregate with pooling operation",0
use layer mean of embeddings if so selected,0
set the extracted embedding for the token,0
move embeddings from context back to original sentence (if using context),0
remember original sentence,0
get left context,0
get right context,0
empty contexts should not introduce whitespace tokens,0
make expanded sentence,0
iterate over subtokens and reconstruct tokens,0
remove special markup,0
TODO check if this is necessary is this method is called before prepare_for_model,1
check if reconstructed token is special begin token ([CLS] or similar),0
some BERT tokenizers somehow omit words - in such cases skip to next token,0
append subtoken to reconstruct token,0
check if reconstructed token is the same as current token,0
"if so, add subtoken count",0
reset subtoken count and reconstructed token,0
break from loop if all tokens are accounted for,0
if tokens are unaccounted for,0
check if all tokens were matched to subtokens,0
"""""""Returns the length of the embedding vector.""""""",0
special handling for serializing transformer models,0
serialize the transformer models and the constructor arguments (but nothing else),0
necessary for reverse compatibility with Flair <= 0.7,0
special handling for deserializing transformer models,0
load transformer model,0
constructor arguments,0
re-initialize transformer word embeddings with constructor arguments,0
"I have no idea why this is necessary, but otherwise it doesn't work",1
reload tokenizer to get around serialization issues,0
model architecture,0
model architecture,0
"""pl"",",0
download if necessary,0
load the model,0
"TODO: keep for backwards compatibility, but remove in future",1
save the sentence piece model as binary file (not as path which may change),0
write out the binary sentence piece model into the expected directory,0
"if the model was saved as binary and it is not found on disk, write to appropriate path",0
"otherwise, use normal process and potentially trigger another download",0
"once the modes if there, load it with sentence piece",0
empty words get no embedding,0
all other words get embedded,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn,0
GLOVE embeddings,0
get train data,0
read in test file if exists,0
read in dev file if exists,0
"find train, dev and test files if not specified",0
special key for space after,0
"store either Sentence objects in memory, or only file offsets",0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
determine encoding of text file,0
skip first line if to selected,0
option 1: read only sentence boundaries as offset positions,0
option 2: keep everything in memory,0
pointer to previous,0
"if sentence ends, break",0
skip comments,0
"if sentence ends, convert and return",0
check if this sentence is a document boundary,0
"otherwise, this line is a token. parse and add to sentence",0
check if this sentence is a document boundary,0
"for example, transforming 'B-OBJ' to 'B-part-of-speech-object'",0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
set sentence context using partials,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
code-switch uses the same training data than multi but provides a different test set.,0
"as the test set is not published, those two tasks are the same.",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download files if not present locally,0
we need to slightly modify the original files by adding some new lines after document separators,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Remove CoNLL-U meta information in the last column,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
entity_mapping,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
data validation,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download files if not present locallys,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
# download zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"unzip the downloaded repo and merge the train, dev and test datasets",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
create folder,0
download dataset,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
create train test dev if not exist,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
If the extracted corpus file is not yet present in dir,0
download zip if necessary,0
"extracted corpus is not present , so unpacking it.",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download zip,0
unpacking the zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)",0
download files if not present locally,0
we need to modify the original files by adding new lines after after the end of each sentence,0
if only one language is given,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"use all languages if explicitly set to ""all""",0
download data if necessary,0
initialize comlumncorpus and add it to list,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
this list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
get google drive id from list,0
download from google drive,0
unzip,0
"tar.extractall(language_folder,members=[tar.getmember(file_name)])",0
transform data into required format,0
"the processed dataset has the additional ending ""_new""",0
remove the unprocessed dataset,0
initialize comlumncorpus and add it to list,0
if no languages are given as argument all languages used in XTREME will be loaded,0
if only one language is given,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
This list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
download from HU Server,0
unzip,0
transform data into required format,0
initialize comlumncorpus and add it to list,0
if only one language is given,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
initialize comlumncorpus and add it to list,0
download data if necessary,0
unpack and write out in CoNLL column-like format,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
data is not in IOB2 format. Thus we transform it to IOB2,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
rename according to train - test - dev - convention,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"TODO: Add a routine, that checks annotations for some widespread errors/inconsistencies??? (e.g. in AQUAINT corpus Iran-Iraq_War vs. Iran-Iraq_war)",1
Create the annotation dictionary,0
this fct removes every second unknown label,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
iterate over all html files,0
"get rid of html syntax, we only need the text",0
between all documents we write a separator symbol,0
skip empty strings,0
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)",0
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention",0
sentence splitting and tokenization,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out-file in column format,0
"in case something goes wrong, delete the dataset and raise error",0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
from qwikidata.linked_data_interface import get_entity_dict_from_api,0
generate qid wikiname dictionaries,0
merge dictionaries,0
ignore first line,0
commented and empty lines,0
read all Q-IDs,0
ignore first line,0
request,0
this dataset name,0
default dataset folder is the cache root,0
we use the wikiids in the data instead of directly utilizing the wikipedia urls.,0
like this we can quickly check if the corresponding page exists,0
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi,0
delete unprocessed file,0
collect all wikiids,0
create the dictionary,0
request,0
this dataset name,0
default dataset folder is the cache root,0
names of raw text documents,0
open output_file,0
iterate through all documents,0
split sentences and tokenize,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out file,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
First parse the post titles,0
Keep track of how many and which entity mentions does a given post title have,0
Check if the current post title has an entity link and parse accordingly,0
Post titles with entity mentions (if any) are handled via this function,0
Then parse the comments,0
"Iterate over the comments.tsv file, until the end is reached",0
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched.",0
Each comment thread is handled as one 'document'.,0
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.,0
This if-condition is needed to handle this problem.,0
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure",0
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above",0
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle.",0
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,",0
and not just single letters into single rows.,0
If there are annotated entity mentions for given post title or a comment thread,0
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence",0
Write the token with a corresponding tag to file,0
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed",0
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag",0
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized,0
"incorrectly, in order to keep the desired format (empty line as a sentence separator).",0
"Thrown when the second check above happens, but the last token of a sentence is reached.",0
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below.",0
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS",0
Check if further annotations belong to the current post title or comment thread as well,0
Stop when the end of an annotation file is reached,0
Check if further annotations belong to the current sentence as well,0
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)",0
Docstart,0
if there is more than one word in the chunk we write each in a separate line,0
print(chunks),0
empty line after each sentence,0
convert the file to CoNLL,0
this dataset name,0
default dataset folder is the cache root,0
"check if data there, if not, download the data",0
create folder,0
download data,0
transform data into column format if necessary,0
if no filenames are specified we use all the data,0
"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled",0
also we remove 'raganato_ALL' from filenames in case its in the list,0
generate the test file,0
make column file and save to data_folder,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
default dataset folder is the cache root,0
"We check if the the UFSAC data has already been downloaded. If not, we download it.",0
Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked),0
create folder,0
download data,0
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled.",0
generate the test file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
if True:,0
write CoNLL-U Plus header,0
"Some special cases (e.g., missing spaces before entity marker)",0
necessary if text should be whitespace tokenizeable,0
Handle case where tail may occur before the head,0
this dataset name,0
default dataset folder is the cache root,0
write CoNLL-U Plus header,0
this dataset name,0
default dataset folder is the cache root,0
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors,1
download data if necessary,0
write CoNLL-U Plus header,0
The span has ended.,0
We are entering a new span; reset indices,0
and active tag to new span.,0
We're inside a span.,0
Last token might have been a part of a valid span.,0
this dataset name,0
default dataset folder is the cache root,0
write CoNLL-U Plus header,0
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):",0
"with zip_file.open(source_file_path, mode=""r"") as source_file:",0
target_file_path = Path(data_folder) / target_filename,0
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:",0
# write CoNLL-U Plus header,0
"target_file.write(""# global.columns = id form ner\n"")",0
for example in json.load(source_file):,0
token_list = self._tacred_example_to_token_list(example),0
target_file.write(token_list.serialize()),0
check if first tag row is already occupied,0
"if first tag row is occupied, use second tag row",0
hardcoded mapping TODO: perhaps find nicer solution,1
"find train, dev and test files if not specified",0
use test_file to create test split if available,0
use dev_file to create test split if available,0
"if data point contains black-listed label, do not use",0
first check if valid sentence,0
"if so, add to indices",0
"find train, dev and test files if not specified",0
variables,0
different handling of in_memory data than streaming data,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
test if format is OK,0
test if at least one label given,0
make sentence from text (and filter for length),0
"if a pair column is defined, make a sentence pair object",0
noinspection PyDefaultArgument,0
dataset name includes the split size,0
default dataset folder is the cache root,0
download data if necessary,0
download each of the 28 splits,0
create dataset directory if necessary,0
download senteval datasets if necessary und unzip,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"by defaut, map point score to POSITIVE / NEGATIVE values",0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file from CSV,0
create test.txt file from CSV,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create train dev and test files in fasttext format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
convert to FastText format,0
if no base_path provided take cache root,0
download data if necessary,0
"if data is not downloaded yet, download it",0
get the zip file,0
move original .tsv files to another folder,0
create train and dev splits in fasttext format,0
create eval_dataset file with no labels,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download datasets if necessary,0
create dataset directory if necessary,0
create correctly formated txt files,0
multiple labels are possible,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
create a separate directory for different tasks,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
check if dataset is supported,0
set file names,0
set file names,0
download and unzip in file structure if necessary,0
instantiate corpus,0
"find train, dev and test files if not specified",0
"create DataPairDataset for train, test and dev file, if they are given",0
stop if file does not exist,0
create a DataPair object from strings,0
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings",0
if no base_path provided take cache root,0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
if no base_path provided take cache root,0
"if data is not downloaded yet, download it",0
get the zip file,0
"reorder dev datasets to have same columns as in train set: 8, 9, and 11",0
dev sets include 5 different annotations but we will only keep the gold label,0
"rename test file to eval_dataset, since it has no labels",0
if no base_path provided take cache root,0
"if data is not downloaded yet, download it",0
get test and dev sets,0
if no base_path provided take cache root,0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
if no base_path provided take cache root,0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
if no base_path provided take cache root,0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
if no base_path provided take cache root,0
"if data not downloaded yet, download it",0
get the zip file,0
"the downloaded files have json format, we transform them to tsv",0
Function to transform JSON file to tsv for Recognizing Textual Entailment Data,0
remove json file,0
noinspection PyProtectedMember,0
noinspection PyProtectedMember,0
"find train, dev and test files if not specified",0
get train data,0
get test data,0
get dev data,0
noinspection PyProtectedMember,0
"if no fields specified, check if the file is CoNLL plus formatted and get fields",0
Validate fields and token_annotation_fields,0
noinspection PyProtectedMember,0
option 1: read only sentence boundaries as offset positions,0
option 2: keep everything in memory,0
pointer to previous,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
Build the sentence tokens and add the annotations.,0
"For fields that contain key-value annotations,",0
we add the key as label type-name and the value as the label value.,0
head and tail span indices are 1-indexed and end index is inclusive,0
determine all NER label types in sentence and add all NER spans as sentence-level labels,0
Uses dynamic programming approach to calculate maximum independent set in interval graph,0
with sum of all entity lengths as secondary key,0
calculate offset without current text,0
because we stick all passages of a document together,0
TODO For split entities we also annotate everything inbetween which might be a bad idea?,1
Try to fix incorrect annotations,0
print(,0
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}""",0
),0
Ignore empty lines or relation annotations,0
FIX annotation of whitespaces (necessary for PDR),0
One token may contain multiple entities -> deque all of them,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Create tokenization-dependent CONLL files. This is necessary to prevent,0
from caching issues (e.g. loading the same corpus with different sentence splitters),0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Edge case: last token starts a new entity,0
Last document in file,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
Edge case: last token starts a new entity,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download file is huge => make default_dir visible so that derivative,0
corpora can all use the same download file,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Read texts,0
Read annotations,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
We need to apply a patch to correct the original training file,0
Articles title,0
Article abstract,0
Entity annotations,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Edge case: last token starts a new entity,0
Map all entities to chemicals,0
Map all entities to disease,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Incomplete article,0
Invalid XML syntax,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
if len(mid) != 3:,0
continue,0
Try to fix entity offsets,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
There is still one illegal annotation in the file ..,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"Abstract first, title second to prevent issues with sentence splitting",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"Filter for specific entity types, by default no entities will be filtered",0
Get original HUNER splits to retrieve a list of all document ids contained in V2,0
train and dev split of V2 will be train in V4,0
test split of V2 will be dev in V4,0
New documents in V4 will become test documents,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
cache Feidegger config file,0
cache Feidegger images,0
replace image URL with local cached file,0
append Sentence-Image data point,0
"in certain cases, multi-CPU data loading makes no sense and slows",0
"everything down. For this reason, we detect if a dataset is in-memory:",0
"if so, num_workers is set to 0 for faster processing",0
cast to list if necessary,0
cast to list if necessary,0
"first, check if pymongo is installed",0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
Expose base classses,0
Expose all sequence labeling datasets,0
standard NER datasets,0
other NER datasets,0
keyphrase detection datasets,0
universal proposition banks,0
Expose all entity linking datasets,0
word sense disambiguation,0
Expose all document classification datasets,0
Expose all treebanks,0
Expose all text-text datasets,0
Expose all text-image datasets,0
Expose all biomedical data sets,0
Expose all biomedical data sets using the HUNER splits,0
-,0
-,0
-,0
-,0
Expose all biomedical data sets used for the evaluation of BioBERT,0
Expose all relation extraction datasets,0
"find train, dev and test files if not specified",0
get train data,0
get test data,0
get dev data,0
option 1: read only sentence boundaries as offset positions,0
option 2: keep everything in memory,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
current token ID,0
handling for the awful UD multiword format,0
end of sentence,0
comments,0
ellipsis,0
if token is a multi-word,0
normal single-word tokens,0
"if we don't split multiwords, skip over component words",0
add token,0
add morphological tags,0
derive whitespace logic for multiwords,0
print(token),0
print(current_multiword_last_token),0
print(current_multiword_first_token),0
"if multi-word equals component tokens, there should be no whitespace",0
go through all tokens in subword and set whitespace_after information,0
print(i),0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"finally, print model card for information",0
test corpus,0
create a TARS classifier,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
test corpus,0
create a TARS classifier,0
switch to a new task (TARS can do multiple tasks so you must define one),0
initialize the text classifier trainer,0
start the training,0
"mini_batch_chunk_size=4,  # optionally set this if transformer is too much for your machine",0
clean up file,0
bioes tags,0
bio tags,0
broken tags,0
all tags,0
all weird tags,0
tags with confidence,0
bioes tags,0
bioes tags,0
"city single-token, person and company multi-token",0
increment for last token in sentence if not followed by whitespace,0
clean up directory,0
clean up directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
train model for 2 epochs,0
load the checkpoint model and train until epoch 4,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
from flair.trainers.trainer_regression import RegressorTrainer,0
def test_trainer_evaluation(tasks_base_path):,0
"corpus, model, trainer = init(tasks_base_path)",0
,0
expected = model.evaluate(corpus.dev),0
,0
assert expected is not None,0
def test_trainer_results(tasks_base_path):,0
"corpus, model, trainer = init(tasks_base_path)",0
"results = trainer.train(""regression_train/"", max_epochs=1)",0
"assert results[""test_score""] > 0",0
"assert len(results[""dev_loss_history""]) == 1",0
"assert len(results[""dev_score_history""]) == 1",0
"assert len(results[""train_loss_history""]) == 1",0
@pytest.mark.integration,0
initialize trainer,0
"loaded_model.predict([sentence, sentence_empty])",0
loaded_model.predict([sentence_empty]),0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
clean up results directory,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
"Here, we use the default token annotation fields.",0
load dataset,0
tagger without CRF,0
train,0
check if loaded model can predict,0
check if loaded model successfully fit the training data,0
clean up results directory,0
load dataset,0
tagger without CRF,0
train,0
check if loaded model can predict,0
check if loaded model successfully fit the training data,0
clean up results directory,0
load dataset,0
tagger without CRF,0
train,0
check if loaded model can predict,0
check if loaded model successfully fit the training data,0
clean up results directory,0
load dataset,0
tagger without CRF,0
train,0
check if loaded model can predict,0
check if loaded model successfully fit the training data,0
clean up results directory,0
check if model can predict,0
load model,0
chcek if model predicts correct label,0
check if loaded model successfully fit the training data,0
clean up results directory,0
check if model can predict,0
load model,0
chcek if model predicts correct label,0
check if loaded model successfully fit the training data,0
clean up results directory,0
check if model can predict,0
load model,0
chcek if model predicts correct label,0
check if loaded model successfully fit the training data,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
train model for 2 epochs,0
load the checkpoint model and train until epoch 4,0
clean up results directory,0
from allennlp.common.tqdm import Tqdm,0
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
Extract all the contents of zip file in current directory,0
Extract all the contents of zip file in current directory,0
get cache path to put the file,0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
first determine the distribution of classes in the dataset,0
weight for each sample,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
Maintains max of all exp. moving avg. of sq. grad. values,0
Decay the first and second moment running average coefficient,0
Maintains the maximum of all 2nd moment running avg. till now,0
Use the max. for normalizing running avg. of gradient,0
determine offsets for whitespace_after field,0
increment for last token in sentence if not followed by whitespace,0
determine offsets for whitespace_after field,0
conll 2000 column format,0
conll 03 NER column format,0
WNUT-17,0
-- WikiNER datasets,0
-- Universal Dependencies,0
Germanic,0
Romance,0
West-Slavic,0
South-Slavic,0
East-Slavic,0
Scandinavian,0
Asian,0
Language isolates,0
recent Universal Dependencies,0
other datasets,0
text classification format,0
text regression format,0
"first, try to fetch dataset online",0
default dataset folder is the cache root,0
get string value if enum is passed,0
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)",0
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag",0
the CoNLL 03 task for German has an additional lemma column,0
the CoNLL 03 task for Dutch has no NP column,0
the CoNLL 03 task for Spanish only has two columns,0
the GERMEVAL task only has two columns: text and ner,0
WSD tasks may be put into this column format,0
"the UD corpora follow the CoNLL-U format, for which we have a special reader",0
"for text classifiers, we use our own special format",0
NER corpus for Basque,0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
get train and test data,0
"read in test file if exists, otherwise sample 10% of train data as test dataset",0
"read in dev file if exists, otherwise sample 10% of train data as dev dataset",0
convert tag scheme to iobes,0
automatically identify train / test / dev files,0
automatically identify train / test / dev files,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
conll 2000 chunking task,0
Support both TREC-6 and TREC-50,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
Wikiner NER task,0
unpack and write out in CoNLL column-like format,0
CoNLL 02/03 NER,0
universal dependencies,0
--- UD Germanic,0
--- UD Romance,0
--- UD West-Slavic,0
--- UD Scandinavian,0
--- UD South-Slavic,0
--- UD Asian,0
this is the default init size of a lmdb database for embeddings,0
some non-used parameter to allow print,0
get db filename from embedding name,0
"In case initialization of cached version failed, just fallback to the original WordEmbeddings",0
SequenceTagger,0
TextClassifier,0
get db filename from embedding name,0
if embedding database already exists,0
"otherwise, push embedding to database",0
if embedding database already exists,0
open the database in read mode,0
we need to set self.k,0
create and load the database in write mode,0
"no idea why, but we need to close and reopen the environment to avoid",0
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot,0
when opening new transaction !,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
"if text is passed, instantiate sentence with tokens (words)",0
log a warning if the dataset is empty,0
some sentences represent a document boundary (but most do not),0
data with zero-width characters cannot be handled,0
set token idx if not set,0
non-set tags are OUT tags,0
anything that is not a BIOES tag is a SINGLE tag,0
anything that is not OUT is IN,0
single and begin tags start a new span,0
remember previous tag,0
"if label type is explicitly specified, get spans for this label type",0
else determine all label types in sentence and get all spans,0
move sentence embeddings to device,0
move token embeddings to device,0
clear sentence embeddings,0
clear token embeddings,0
infer whitespace after field,0
add Sentence labels to output if they exist,0
add Token labels to output if they exist,0
add Sentence labels to output if they exist,0
add Token labels to output if they exist,0
No character at the corresponding code point: remove it,0
TODO: crude hack - replace with something better,1
set name,0
sample test data if none is provided,0
sample dev data if none is provided,0
set train dev and test data,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
"if there are token labels of provided type, use these. Otherwise use sentence labels",0
if we are looking for sentence-level labels,0
check if sentence itself has labels,0
check for labels of words,0
"if this is not a token-level prediction problem, add sentence-level labels to dictionary",0
Make the tag dictionary,0
global variable: cache_root,0
global variable: device,0
global variable: embedding_storage_mode,0
# dummy return to fulfill trainer.train() needs,0
print(vec),0
Attach optimizer,0
"convert `metrics` to float, in case it's a zero-dim Tensor",0
if memory mode option 'none' delete everything,0
else delete only dynamic embeddings (otherwise autograd will keep everything in memory),0
find out which ones are dynamic embeddings,0
find out which ones are dynamic embeddings,0
memory management - option 1: send everything to CPU (pin to memory if we train on GPU),0
record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class),0
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups,1
see https://github.com/zalandoresearch/flair/issues/351,0
"read Dataset into data loader (if list of sentences passed, make Dataset first)",0
loss calculation,0
variables for printing,0
variables for computing scores,0
remove any previously predicted labels,0
predict for batch,0
get the gold labels,0
add to all_predicted_values,0
make printout lines,0
"if the model is span-level, transfer to word-level annotations for printout",0
"all labels default to ""O""",0
set gold token-level,0
set predicted token-level,0
now print labels in CoNLL format,0
check if there is a label mismatch,0
print info,0
write all_predicted_values to out_file if set,0
make the evaluation dictionary,0
"finally, compute numbers",0
"now, calculate evaluation numbers",0
there is at least one gold label or one prediction (default),0
issue error and default all evaluation numbers to 0.,0
line for log file,0
initialize the label dictionary,0
self.label_dictionary.add_item('O'),0
set up multi-label logic,0
loss weights and loss function,0
Initialize the weight tensor,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
remove previously predicted labels of this type,0
if anything could possibly be predicted,0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
plot i,0
save plots,0
save plots,0
plt.show(),0
save plot,0
take the average over the last three scores of training,0
take average over the scores from the different training runs,0
remove previous embeddings,0
clearing token embeddings to save memory,0
"read Dataset into data loader (if list of sentences passed, make Dataset first)",0
#TODO: not saving lines yet,1
== similarity measures ==,0
helper class for ModelSimilarity,0
-- works with binary cross entropy loss --,0
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}",0
-- works with ranking/triplet loss --,0
normalize the embeddings,0
== similarity losses ==,0
"we want that logits for corresponding pairs are high, and for non-corresponding low",0
TODO: this assumes eye matrix,0
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa",0
== similarity learner ==,0
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both",0
assumes that for each data pair there's at least one embedding per modality,0
pre-compute embeddings for all targets in evaluation dataset,0
compute the similarity,0
sort the similarity matrix across modality 1,0
"get the ranks, so +1 to start counting ranks from 1",0
The conversion from old model's constructor interface,0
auto-spawn on GPU if available,0
pad strings with whitespaces to longest sentence,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
serialize the language models and the constructor arguments (but nothing else),0
special handling for deserializing language models,0
re-initialize language model with constructor arguments,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
Transform input data into TARS format,0
print(all_labels),0
"if there are no labels, return a random sample as negatives",0
print(sample),0
"otherwise, go through all labels",0
make sure the probabilities always sum up to 1,0
get and embed all labels by making a Sentence object that contains only the label text,0
get each label embedding and scale between 0 and 1,0
compute similarity matrix,0
"the higher the similarity, the greater the chance that a label is",0
sampled as negative example,0
make label dictionary if no Dictionary object is passed,0
prepare dictionary of tags (without B- I- prefixes),0
check if candidate_label_set is empty,0
make list if only one candidate label is passed,0
"if list is passed, convert to set",0
note current task,0
create a temporary task,0
make zero shot predictions,0
switch to the pre-existing task,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
make a tars sentence where all labels are O by default,0
overwrite O labels with tags,0
init new TARS classifier,0
set all task information,0
linear layers of internal classifier,0
return,0
with torch.no_grad():,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
go through each sentence in the batch,0
always remove tags first,0
get the span and its label,0
determine whether tokens in this span already have a label,0
only add if all tokens have no label,0
clearing token embeddings to save memory,0
prepare TARS dictionary,0
initialize a bare-bones sequence tagger,0
transformer separator,0
Store task specific labels since TARS can handle multiple tasks,0
init new TARS classifier,0
set all task information,0
linear layers of internal classifier,0
with torch.no_grad():,0
set context if not set already,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
go through each sentence in the batch,0
always remove tags first,0
clearing token embeddings to save memory,0
if embed_separately == True the linear layer needs twice the length of the embeddings as input size,0
since we concatenate the embeddings of the two DataPoints in the DataPairs,0
representation for both sentences,0
set separator to concatenate two sentences,0
auto-spawn on GPU if available,0
linear layer,0
minimal return is scores and labels,0
"entity pairs could also be no relation at all, add default value for this case to dictionary",0
entity_pairs = [],0
super lame: make dictionary to find relation annotations for a given entity pair,0
get all entity spans,0
get embedding for each entity,0
"go through cross product of entities, for each pair concat embeddings",0
get gold label for this relation (if one exists),0
"if using gold spans only, skip all entity pairs that are not in gold data",0
"if no gold label exists, and all spans are used, label defaults to 'O' (no relation)",0
"if predicting, also remember sentences and label candidates",0
"return either scores and gold labels (for loss calculation), or include label candidates for prediction",0
if we concatenate the embeddings we need double input size in our linear layer,0
filter sentences with no candidates (no candidates means nothing can be linked anyway),0
fields to return,0
"if the entire batch has no sentence with candidates, return empty",0
"otherwise, embed sentence and send through prediction head",0
embed all tokens,0
get the embeddings of the entity mentions,0
minimal return is scores and labels,0
set the dictionaries,0
"if we use a CRF, we must add special START and STOP tags to the dictionary",0
Initialize the weight tensor,0
initialize the network architecture,0
dropouts,0
optional reprojection layer on top of word embeddings,0
bidirectional LSTM on top of embedding layer,0
Create initial hidden state and initialize it,0
TODO: Decide how to initialize the hidden state variables,1
self.hs_initializer(self.lstm_init_h),0
self.hs_initializer(self.lstm_init_c),0
final linear map to tag space,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
clearing token embeddings to save memory,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
"if initial hidden state is trainable, use this state",0
word dropout only before LSTM - TODO: more experimentation needed,1
if self.use_word_dropout > 0.0:,0
sentence_tensor = self.word_dropout(sentence_tensor),0
get the tags in this sentence,0
add tags as tensor,0
pad tags if using batch-CRF decoder,0
reduce raw values to avoid NaN during exp,0
broadcasting will do the job of reshaping and is more efficient than calling repeat,0
default value,0
core Flair models on Huggingface ModelHub,0
"Large NER models,",0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Language-specific NER models,0
English NER models,0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Danish models,0
German models,0
French models,0
Dutch models,0
Malayalam models,0
Portuguese models,0
Keyphase models,0
Biomedical models,0
check if model name is a valid local file,0
"check if model key is remapped to HF key - if so, print out information",0
get mapped name,0
output information,0
use mapped name instead,0
"if not, check if model key is remapped to direct download location. If so, download model",0
special handling for the taggers by the @redewiegergabe project (TODO: move to model hub),1
"for all other cases (not local file or special download location), use HF model hub",0
"if not a local file, get from model hub",0
use model name as subfolder,0
Lazy import,0
output information,0
"log.error(f"" - Error message: {e}"")",0
"all_tag_prob=all_tag_prob,",0
clear embeddings after predicting,0
load each model,0
check if the same embeddings were already loaded previously,0
"if the model uses StackedEmbedding, make a new stack with previous objects",0
sort embeddings by key alphabetically,0
check previous embeddings and add if found,0
only re-use static embeddings,0
"if not found, use existing embedding",0
initialize new stack,0
"of the model uses regular embedding, re-load if previous version found",0
auto-spawn on GPU if available,0
embed sentences,0
make tensor for all embedded sentences in batch,0
send through decoder to get logits,0
minimal return is scores and labels,0
English sentiment models,0
Communicative Functions Model,0
embeddings,0
dictionaries,0
linear layer,0
all parameters will be pushed internally to the specified device,0
get all tokens in this mini-batch,0
minimal return is scores and labels,0
weights for loss function,0
iput size is two times wordembedding size since we use pair of words as input,0
"the output size is max_distance + 1, i.e. we allow 0,1,...,max_distance words between pairs",0
regression,0
input size is two times word embedding size since we use pair of words as input,0
the output size is 1,0
auto-spawn on GPU if available,0
all input should be tensors,0
forward allows only a single sentcence!!,0
embed words of sentence,0
go through all pairs of words with a maximum number of max_distance in between,0
go through all pairs,0
2-dim matrix whose rows are the embeddings of word pairs of the sentence,0
So far only one sentence allowed,0
If list of sentences is handed the function works with the first sentence of the list,0
Assume data_points is a single sentence!!!,0
scores are the predictions for each word pair,0
"classification needs labels to be integers, regression needs labels to be float",0
this is due to the different loss functions,0
only single sentences as input,0
gold labels,0
for output text file,0
for buckets,0
for average prediction,0
add some statistics to the output,0
use scikit-learn to evaluate,0
"we iterate over each sentence, instead of batches",0
get single labels from scores,0
gold labels,0
for output text file,0
hot one vector of true value,0
hot one vector of predicted value,0
"speichert embeddings, falls embedding_storage!= 'None'",0
"make ""classification report""",0
get scores,0
"precision_score = round(metrics.precision_score(y_true, y_pred, average='macro', zero_division=0), 4)",0
"recall_score = round(metrics.recall_score(y_true, y_pred, average='macro', zero_division=0), 4)",0
line for log file,0
cast string to Path,0
check for previously saved best models in the current training folder and delete them,0
"determine what splits (train, dev, test) to evaluate and log",0
prepare loss logging file and set up header,0
"minimize training loss if training with dev data, else maximize dev score",0
"if we load a checkpoint, we have already trained for self.epoch",0
"if training also uses dev/train data, include in training set",0
initialize sampler if provided,0
init with default values if only class is provided,0
set dataset to sample from,0
At any point you can hit Ctrl + C to break out of training early.,0
get new learning rate,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
process mini-batches,0
zero the gradients on the model and optimizer,0
"if necessary, make batch_steps",0
forward and backward for batch,0
forward pass,0
Backward,0
do the optimizer step,0
do the scheduler step if one-cycle,0
get new learning rate,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
evaluate on train / dev / test split depending on training settings,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
calculate scores using dev data if available,0
append dev score to score history,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
determine if this is the best model or if we need to anneal,0
default mode: anneal against dev score,0
alternative: anneal against dev loss,0
alternative: anneal against train loss,0
determine bad epoch number,0
log bad epochs,0
output log file,0
make headers on first epoch,0
"if checkpoint is enabled, save model at each epoch",0
Check whether to save best model,0
"if we do not use dev data for model selection, save final model",0
test best model if test data is present,0
"if we are training over multiple datasets, do evaluation for each",0
get and return the final test score of best model,0
cast string to Path,0
forward pass,0
update optimizer and scheduler,0
"TextDataset returns a list. valid and test are only one file, so return the first element",0
cast string to Path,0
error message if the validation dataset is too small,0
Shuffle training files randomly after serially iterating through corpus one,0
"iterate through training data, starting at self.split (for checkpointing)",0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
Backward,0
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.,0
We detach the hidden state from how it was previously produced.,0
"If we didn't, the model would try backpropagating all the way to start of the dataset.",0
explicitly remove loss to clear up memory,0
##############################################################################,0
Save the model if the validation loss is the best we've seen so far.,0
##############################################################################,0
print info,0
##############################################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
1-camembert-base -> camembert-base,0
1-xlm-roberta-large -> xlm-roberta-large,0
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol,0
The mask has 1 for real tokens and 0 for padding tokens. Only real,0
tokens are attended to.,0
Zero-pad up to the sequence length.,0
"first, find longest sentence in batch",0
prepare id maps for BERT model,0
put encoded batch through BERT model to get all hidden states of all encoder layers,0
get aggregated embeddings for each BERT-subtoken in sentence,0
get the current sentence object,0
add concatenated embedding to sentence,0
use first subword embedding if pooling operation is 'first',0
"otherwise, do a mean over all subwords in token",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
embed a dummy sentence to determine embedding_length,0
Avoid conflicts with flair's Token class,0
"<cls> token initially set to 1/D, so it attends to all image features equally",0
add positional encodings,0
reshape the pixels into the sequence,0
layer norm after convolution and positional encodings,0
add <cls> token,0
"transformer requires input in the shape [h*w+1, b, d]",0
the output is an embedding of <cls> token,0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
do not print transformer warnings as these are confusing in this case,0
load tokenizer and transformer model,0
model name,0
"when initializing, embeddings are in eval mode by default",0
embedding parameters,0
send mini-token through to check how many layers the model has,0
check whether CLS is at beginning or end,0
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial",0
gradients are enabled if fine-tuning is enabled,0
"first, subtokenize each sentence and find out into how many subtokens each token was divided",0
subtokenize sentences,0
tokenize and truncate to max subtokens (TODO: check better truncation strategies),1
find longest sentence in batch,0
initialize batch tensors and mask,0
put encoded batch through transformer model to get all hidden states of all encoder layers,0
iterate over all subtokenized sentences,0
use scalar mix of embeddings if so selected,0
set the extracted embedding for the token,0
special handling for serializing transformer models,0
serialize the transformer models and the constructor arguments (but nothing else),0
necessary for reverse compatibility with Flair <= 0.7,0
special handling for deserializing transformer models,0
load transformer model,0
constructor arguments,0
re-initialize transformer word embeddings with constructor arguments,0
for backward compatibility with previous models,0
"I have no idea why this is necessary, but otherwise it doesn't work",1
reload tokenizer to get around serialization issues,0
optional fine-tuning on top of embedding layer,0
"if only one sentence is passed, convert to list of sentence",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push through RNN,0
after-RNN dropout,0
extract embeddings from RNN,0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
serialize the language models and the constructor arguments (but nothing else),0
special handling for deserializing language models,0
re-initialize language model with constructor arguments,0
copy over state dictionary to self,0
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM",0
"in their ""self.train()"" method)",0
IMPORTANT: add embeddings as torch modules,0
iterate over sentences,0
"if its a forward LM, take last state",0
"convert to plain strings, embedded in a list for the encode function",0
CNN,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push CNN,0
after-CNN dropout,0
extract embeddings from CNN,0
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency",0
"if only one sentence is passed, convert to list of sentence",0
Expose base classses,0
Expose token embedding classes,0
Expose document embedding classes,0
Expose image embedding classes,0
Expose legacy embedding classes,0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNINOS embeddings,0
pubmed embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
fix serialized models,0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
multilingual models,0
English models,0
Arabic,0
Bulgarian,0
Czech,0
Danish,0
German,0
Spanish,0
Basque,0
Persian,0
Finnish,0
French,0
Hebrew,0
Hindi,0
Croatian,0
Indonesian,0
Italian,0
Japanese,0
Malayalam,0
Dutch,0
Norwegian,0
Polish,0
Portuguese,0
Pubmed,0
Slovenian,0
Swedish,0
Tamil,0
Spanish clinical,0
CLEF HIPE Shared task,0
load model if in pretrained model map,0
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir),0
CLEF HIPE models are lowercased,0
embeddings are static if we don't do finetuning,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
make compatible with serialized models (TODO: remove),1
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout",0
make compatible with serialized models (TODO: remove),1
gradients are enable if fine-tuning is enabled,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
offset mode that extracts at whitespace after last character,0
offset mode that extracts at last character,0
only clone if optimization mode is 'gpu',0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
check token.text is empty or not,0
set aggregation operation,0
add embeddings after updating,0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
do not print transformer warnings as these are confusing in this case,0
load tokenizer and transformer model,0
"in the end, these models don't need this configuration",0
model name,0
whether to detach gradients on overlong sentences,0
store whether to use context (and how much),0
dropout contexts,0
"if using context, can we cross document boundaries?",0
send self to flair-device,0
embedding parameters,0
send mini-token through to check how many layers the model has,0
calculate embedding length,0
return length,0
check if special tokens exist to circumvent error message,0
"most models have an intial BOS token, except for XLNet, T5 and GPT2",0
"when initializing, embeddings are in eval mode by default",0
remove special markup,0
"we require encoded subtokenized sentences, the mapping to original tokens and the number of",0
parts that each sentence produces,0
"if we also use context, first expand sentence to include context",0
set context if not set already,0
"in case of contextualization, we must remember non-expanded sentence",0
create expanded sentence and remember context offsets,0
overwrite sentence with expanded sentence,0
subtokenize the sentence,0
transformer specific tokenization,0
set zero embeddings for empty sentences and return,0
determine into how many subtokens each token is split,0
encode inputs,0
overlong sentences are handled as multiple splits,0
find longest sentence in batch,0
initialize batch tensors and mask,0
put encoded batch through transformer model to get all hidden states of all encoder layers,0
make the tuple a tensor; makes working with it easier.,0
gradients are enabled if fine-tuning is enabled,0
iterate over all subtokenized sentences,0
"remove stride_size//2 at end of sentence_hidden_state, and half at beginning of remainder,",0
in order to get some context into the embeddings of these words.,0
also don't include the embedding of the extra [CLS] and [SEP] tokens.,0
"for each token, get embedding",0
some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector,0
"get states from all selected layers, aggregate with pooling operation",0
use layer mean of embeddings if so selected,0
set the extracted embedding for the token,0
move embeddings from context back to original sentence (if using context),0
remember original sentence,0
get left context,0
get right context,0
empty contexts should not introduce whitespace tokens,0
make expanded sentence,0
iterate over subtokens and reconstruct tokens,0
remove special markup,0
TODO check if this is necessary is this method is called before prepare_for_model,1
check if reconstructed token is special begin token ([CLS] or similar),0
some BERT tokenizers somehow omit words - in such cases skip to next token,0
append subtoken to reconstruct token,0
check if reconstructed token is the same as current token,0
"if so, add subtoken count",0
reset subtoken count and reconstructed token,0
break from loop if all tokens are accounted for,0
if tokens are unaccounted for,0
check if all tokens were matched to subtokens,0
"""""""Returns the length of the embedding vector.""""""",0
special handling for serializing transformer models,0
serialize the transformer models and the constructor arguments (but nothing else),0
necessary for reverse compatibility with Flair <= 0.7,0
special handling for deserializing transformer models,0
load transformer model,0
constructor arguments,0
re-initialize transformer word embeddings with constructor arguments,0
"I have no idea why this is necessary, but otherwise it doesn't work",1
reload tokenizer to get around serialization issues,0
max_tokens = 500,0
model architecture,0
model architecture,0
download if necessary,0
load the model,0
"TODO: keep for backwards compatibility, but remove in future",1
save the sentence piece model as binary file (not as path which may change),0
write out the binary sentence piece model into the expected directory,0
"if the model was saved as binary and it is not found on disk, write to appropriate path",0
"otherwise, use normal process and potentially trigger another download",0
"once the modes if there, load it with sentence piece",0
empty words get no embedding,0
all other words get embedded,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn,0
GLOVE embeddings,0
"find train, dev and test files if not specified",0
get train data,0
read in test file if exists,0
read in dev file if exists,0
special key for space after,0
"store either Sentence objects in memory, or only file offsets",0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
determine encoding of text file,0
skip first line if to selected,0
option 1: read only sentence boundaries as offset positions,0
option 2: keep everything in memory,0
pointer to previous,0
"if sentence ends, break",0
skip comments,0
"if sentence ends, convert and return",0
check if this sentence is a document boundary,0
"otherwise, this line is a token. parse and add to sentence",0
check if this sentence is a document boundary,0
"for example, transforming 'B-OBJ' to 'B-part-of-speech-object'",0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
set sentence context using partials,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download files if not present locally,0
we need to slightly modify the original files by adding some new lines after document separators,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Remove CoNLL-U meta information in the last column,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
entity_mapping,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
data validation,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download files if not present locallys,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
# download zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"unzip the downloaded repo and merge the train, dev and test datasets",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
create folder,0
download dataset,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
create train test dev if not exist,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
If the extracted corpus file is not yet present in dir,0
download zip if necessary,0
"extracted corpus is not present , so unpacking it.",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download zip,0
unpacking the zip,0
merge the files in one as the zip is containing multiples files,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)",0
download files if not present locally,0
we need to modify the original files by adding new lines after after the end of each sentence,0
if only one language is given,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"use all languages if explicitly set to ""all""",0
download data if necessary,0
initialize comlumncorpus and add it to list,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
this list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
get google drive id from list,0
download from google drive,0
unzip,0
"tar.extractall(language_folder,members=[tar.getmember(file_name)])",0
transform data into required format,0
"the processed dataset has the additional ending ""_new""",0
remove the unprocessed dataset,0
initialize comlumncorpus and add it to list,0
if no languages are given as argument all languages used in XTREME will be loaded,0
if only one language is given,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
This list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
download from HU Server,0
unzip,0
transform data into required format,0
initialize comlumncorpus and add it to list,0
if only one language is given,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
initialize comlumncorpus and add it to list,0
download data if necessary,0
unpack and write out in CoNLL column-like format,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
data is not in IOB2 format. Thus we transform it to IOB2,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
rename according to train - test - dev - convention,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
,0
since only the WordNet 3.0 version for senses is consistently available for all provided datasets we will,0
only consider this version,0
,0
also we ignore the id annotation used in datasets that were originally created for evaluation tasks,0
,0
if the other annotations should be needed simply add the columns in correct order according,0
to the chosen datasets here and respectively change the values of the blacklist array and,0
the range value of the else case in the token for loop in the from_ufsac_to_conll function,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
determine correct CoNLL files,0
tokens to ignore (edit here for variation),0
counter to keep track how many tags have been found in line,0
variable to count of how many words a chunk consists,0
indicates if surface form is chunk or not,0
array to save tags temporarily for handling chunks,0
cut token to get chunk,0
save single words of chunk,0
handle first word of chunk,0
edit here for variation,0
check if converted file exists,0
convert the file to CoNLL,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"TODO: Add a routine, that checks annotations for some widespread errors/inconsistencies??? (e.g. in AQUAINT corpus Iran-Iraq_War vs. Iran-Iraq_war)",1
Create the annotation dictionary,0
this fct removes every second unknown label,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
iterate over all html files,0
"get rid of html syntax, we only need the text",0
between all documents we write a separator symbol,0
skip empty strings,0
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)",0
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention",0
sentence splitting and tokenization,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out-file in column format,0
"in case something goes wrong, delete the dataset and raise error",0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
from qwikidata.linked_data_interface import get_entity_dict_from_api,0
generate qid wikiname dictionaries,0
merge dictionaries,0
ignore first line,0
commented and empty lines,0
read all Q-IDs,0
ignore first line,0
request,0
this dataset name,0
default dataset folder is the cache root,0
we use the wikiids in the data instead of directly utilizing the wikipedia urls.,0
like this we can quickly check if the corresponding page exists,0
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi,0
delete unprocessed file,0
collect all wikiids,0
create the dictionary,0
request,0
this dataset name,0
default dataset folder is the cache root,0
names of raw text documents,0
open output_file,0
iterate through all documents,0
split sentences and tokenize,0
iterate through all annotations and add to corresponding tokens,0
find sentence to which annotation belongs,0
position within corresponding sentence,0
set annotation for tokens of entity mention,0
write to out file,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download and parse data if necessary,0
First parse the post titles,0
Keep track of how many and which entity mentions does a given post title have,0
Check if the current post title has an entity link and parse accordingly,0
Post titles with entity mentions (if any) are handled via this function,0
Then parse the comments,0
"Iterate over the comments.tsv file, until the end is reached",0
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched.",0
Each comment thread is handled as one 'document'.,0
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.,0
This if-condition is needed to handle this problem.,0
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure",0
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above",0
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle.",0
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,",0
and not just single letters into single rows.,0
If there are annotated entity mentions for given post title or a comment thread,0
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence",0
Write the token with a corresponding tag to file,0
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed",0
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag",0
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized,0
"incorrectly, in order to keep the desired format (empty line as a sentence separator).",0
"Thrown when the second check above happens, but the last token of a sentence is reached.",0
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below.",0
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS",0
Check if further annotations belong to the current post title or comment thread as well,0
Stop when the end of an annotation file is reached,0
Check if further annotations belong to the current sentence as well,0
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)",0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
if True:,0
write CoNLL-U Plus header,0
"Some special cases (e.g., missing spaces before entity marker)",0
necessary if text should be whitespace tokenizeable,0
Handle case where tail may occur before the head,0
this dataset name,0
default dataset folder is the cache root,0
write CoNLL-U Plus header,0
this dataset name,0
default dataset folder is the cache root,0
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors,1
download data if necessary,0
write CoNLL-U Plus header,0
The span has ended.,0
We are entering a new span; reset indices,0
and active tag to new span.,0
We're inside a span.,0
Last token might have been a part of a valid span.,0
this dataset name,0
default dataset folder is the cache root,0
write CoNLL-U Plus header,0
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):",0
"with zip_file.open(source_file_path, mode=""r"") as source_file:",0
target_file_path = Path(data_folder) / target_filename,0
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:",0
# write CoNLL-U Plus header,0
"target_file.write(""# global.columns = id form ner\n"")",0
for example in json.load(source_file):,0
token_list = self._tacred_example_to_token_list(example),0
target_file.write(token_list.serialize()),0
hardcoded mapping TODO: perhaps find nicer solution,1
"find train, dev and test files if not specified",0
use test_file to create test split if available,0
use dev_file to create test split if available,0
"if data point contains black-listed label, do not use",0
first check if valid sentence,0
"if so, add to indices",0
"find train, dev and test files if not specified",0
variables,0
different handling of in_memory data than streaming data,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
test if format is OK,0
test if at least one label given,0
make sentence from text (and filter for length),0
"if a pair column is defined, make a sentence pair object",0
noinspection PyDefaultArgument,0
dataset name includes the split size,0
default dataset folder is the cache root,0
download data if necessary,0
download each of the 28 splits,0
create dataset directory if necessary,0
download senteval datasets if necessary und unzip,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"by defaut, map point score to POSITIVE / NEGATIVE values",0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file from CSV,0
create test.txt file from CSV,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create train dev and test files in fasttext format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
convert to FastText format,0
if no base_path provided take cache root,0
download data if necessary,0
"if data is not downloaded yet, download it",0
get the zip file,0
move original .tsv files to another folder,0
create train and dev splits in fasttext format,0
create eval_dataset file with no labels,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download datasets if necessary,0
create dataset directory if necessary,0
create correctly formated txt files,0
multiple labels are possible,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
create a separate directory for different tasks,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
check if dataset is supported,0
set file names,0
set file names,0
download and unzip in file structure if necessary,0
instantiate corpus,0
"find train, dev and test files if not specified",0
"create DataPairDataset for train, test and dev file, if they are given",0
stop if file does not exist,0
create a DataPair object from strings,0
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings",0
if no base_path provided take cache root,0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
if no base_path provided take cache root,0
"if data is not downloaded yet, download it",0
get the zip file,0
"reorder dev datasets to have same columns as in train set: 8, 9, and 11",0
dev sets include 5 different annotations but we will only keep the gold label,0
"rename test file to eval_dataset, since it has no labels",0
if no base_path provided take cache root,0
"if data is not downloaded yet, download it",0
get test and dev sets,0
if no base_path provided take cache root,0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
if no base_path provided take cache root,0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
if no base_path provided take cache root,0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
if no base_path provided take cache root,0
"if data not downloaded yet, download it",0
get the zip file,0
"the downloaded files have json format, we transform them to tsv",0
Function to transform JSON file to tsv for Recognizing Textual Entailment Data,0
remove json file,0
"find train, dev and test files if not specified",0
get train data,0
get test data,0
get dev data,0
"if no fields specified, check if the file is CoNLL plus formatted and get fields",0
option 1: read only sentence boundaries as offset positions,0
option 2: keep everything in memory,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
current token ID,0
relations: List[Relation] = [],0
head and tail span indices are 1-indexed and end index is inclusive,0
determine all NER label types in sentence and add all NER spans as sentence-level labels,0
Uses dynamic programming approach to calculate maximum independent set in interval graph,0
with sum of all entity lengths as secondary key,0
calculate offset without current text,0
because we stick all passages of a document together,0
TODO For split entities we also annotate everything inbetween which might be a bad idea?,1
Try to fix incorrect annotations,0
print(,0
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}""",0
),0
Ignore empty lines or relation annotations,0
FIX annotation of whitespaces (necessary for PDR),0
One token may contain multiple entities -> deque all of them,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Create tokenization-dependent CONLL files. This is necessary to prevent,0
from caching issues (e.g. loading the same corpus with different sentence splitters),0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Edge case: last token starts a new entity,0
Last document in file,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
Edge case: last token starts a new entity,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download file is huge => make default_dir visible so that derivative,0
corpora can all use the same download file,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Read texts,0
Read annotations,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
We need to apply a patch to correct the original training file,0
Articles title,0
Article abstract,0
Entity annotations,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Edge case: last token starts a new entity,0
Map all entities to chemicals,0
Map all entities to disease,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Incomplete article,0
Invalid XML syntax,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
if len(mid) != 3:,0
continue,0
Try to fix entity offsets,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
There is still one illegal annotation in the file ..,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"Abstract first, title second to prevent issues with sentence splitting",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"Filter for specific entity types, by default no entities will be filtered",0
Get original HUNER splits to retrieve a list of all document ids contained in V2,0
train and dev split of V2 will be train in V4,0
test split of V2 will be dev in V4,0
New documents in V4 will become test documents,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
cache Feidegger config file,0
cache Feidegger images,0
replace image URL with local cached file,0
append Sentence-Image data point,0
"in certain cases, multi-CPU data loading makes no sense and slows",0
"everything down. For this reason, we detect if a dataset is in-memory:",0
"if so, num_workers is set to 0 for faster processing",0
cast to list if necessary,0
cast to list if necessary,0
"first, check if pymongo is installed",0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
Expose base classses,0
Expose all sequence labeling datasets,0
standard NER datasets,0
other NER datasets,0
keyphrase detection datasets,0
word sense disambiugation,0
universal proposition banks,0
Expose all entity linking datasets,0
Expose all document classification datasets,0
Expose all treebanks,0
Expose all text-text datasets,0
Expose all text-image datasets,0
Expose all biomedical data sets,0
Expose all biomedical data sets using the HUNER splits,0
-,0
-,0
-,0
-,0
Expose all biomedical data sets used for the evaluation of BioBERT,0
Expose all relation extraction datasets,0
"find train, dev and test files if not specified",0
get train data,0
get test data,0
get dev data,0
option 1: read only sentence boundaries as offset positions,0
option 2: keep everything in memory,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
current token ID,0
handling for the awful UD multiword format,0
end of sentence,0
comments,0
ellipsis,0
if token is a multi-word,0
normal single-word tokens,0
"if we don't split multiwords, skip over component words",0
add token,0
add morphological tags,0
derive whitespace logic for multiwords,0
print(token),0
print(current_multiword_last_token),0
print(current_multiword_first_token),0
"if multi-word equals component tokens, there should be no whitespace",0
go through all tokens in subword and set whitespace_after information,0
print(i),0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
test corpus,0
create a TARS classifier,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
clean up file,0
bioes tags,0
bio tags,0
broken tags,0
all tags,0
all weird tags,0
tags with confidence,0
bioes tags,0
bioes tags,0
"city single-token, person and company multi-token",0
increment for last token in sentence if not followed by whitespace,0
clean up directory,0
clean up directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
from flair.trainers.trainer_regression import RegressorTrainer,0
def test_trainer_evaluation(tasks_base_path):,0
"corpus, model, trainer = init(tasks_base_path)",0
,0
expected = model.evaluate(corpus.dev),0
,0
assert expected is not None,0
def test_trainer_results(tasks_base_path):,0
"corpus, model, trainer = init(tasks_base_path)",0
"results = trainer.train(""regression_train/"", max_epochs=1)",0
"assert results[""test_score""] > 0",0
"assert len(results[""dev_loss_history""]) == 1",0
"assert len(results[""dev_score_history""]) == 1",0
"assert len(results[""train_loss_history""]) == 1",0
@pytest.mark.integration,0
initialize trainer,0
"loaded_model.predict([sentence, sentence_empty])",0
loaded_model.predict([sentence_empty]),0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
clean up results directory,0
define search space,0
sequence tagger parameter,0
model trainer parameter,0
training parameter,0
find best parameter settings,0
clean up results directory,0
document embeddings parameter,0
training parameter,0
clean up results directory,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
load dataset,0
tagger without CRF,0
train,0
check if loaded model can predict,0
check if loaded model successfully fit the training data,0
clean up results directory,0
load dataset,0
tagger without CRF,0
train,0
check if loaded model can predict,0
check if loaded model successfully fit the training data,0
clean up results directory,0
load dataset,0
tagger without CRF,0
train,0
check if loaded model can predict,0
check if loaded model successfully fit the training data,0
clean up results directory,0
load dataset,0
tagger without CRF,0
train,0
check if loaded model can predict,0
check if loaded model successfully fit the training data,0
clean up results directory,0
check if model can predict,0
load model,0
chcek if model predicts correct label,0
check if loaded model successfully fit the training data,0
clean up results directory,0
check if model can predict,0
load model,0
chcek if model predicts correct label,0
check if loaded model successfully fit the training data,0
clean up results directory,0
check if model can predict,0
load model,0
chcek if model predicts correct label,0
check if loaded model successfully fit the training data,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
def test_labels_to_indices(tasks_base_path):,0
"corpus = flair.datasets.ClassificationCorpus(tasks_base_path / ""ag_news"", label_type=""topic"")",0
label_dict = corpus.make_label_dictionary(),0
"model = TextClassifier(document_embeddings,",0
"label_dictionary=label_dict,",0
"label_type=""topic"",",0
multi_label=False),0
,0
result = model._labels_to_indices(corpus.train),0
,0
for i in range(len(corpus.train)):,0
expected = label_dict.get_idx_for_item(corpus.train[i].labels[0].value),0
actual = result[i].item(),0
,0
assert expected == actual,0
,0
,0
def test_labels_to_one_hot(tasks_base_path):,0
"corpus = flair.datasets.ClassificationCorpus(tasks_base_path / ""ag_news"", label_type=""topic"")",0
label_dict = corpus.make_label_dictionary(),0
"model = TextClassifier(document_embeddings,",0
"label_dictionary=label_dict,",0
"label_type=""topic"",",0
multi_label=False),0
,0
result = model._labels_to_one_hot(corpus.train),0
,0
for i in range(len(corpus.train)):,0
expected = label_dict.get_idx_for_item(corpus.train[i].labels[0].value),0
actual = result[i],0
,0
for idx in range(len(label_dict)):,0
if idx == expected:,0
assert actual[idx] == 1,0
else:,0
assert actual[idx] == 0,0
1. get the corpus,0
2. what tag do we want to predict?,0
3. make the tag dictionary from the corpus,0
initialize embeddings,0
comment in this line to use character embeddings,0
"CharacterEmbeddings(),",0
comment in these lines to use contextual string embeddings,0
,0
"FlairEmbeddings('news-forward'),",0
,0
"FlairEmbeddings('news-backward'),",0
initialize sequence tagger,0
initialize trainer,0
from allennlp.common.tqdm import Tqdm,0
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
Extract all the contents of zip file in current directory,0
Extract all the contents of zip file in current directory,0
get cache path to put the file,0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
first determine the distribution of classes in the dataset,0
weight for each sample,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups,1
see https://github.com/zalandoresearch/flair/issues/351,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
Maintains max of all exp. moving avg. of sq. grad. values,0
Decay the first and second moment running average coefficient,0
Maintains the maximum of all 2nd moment running avg. till now,0
Use the max. for normalizing running avg. of gradient,0
determine offsets for whitespace_after field,0
increment for last token in sentence if not followed by whitespace,0
determine offsets for whitespace_after field,0
conll 2000 column format,0
conll 03 NER column format,0
WNUT-17,0
-- WikiNER datasets,0
-- Universal Dependencies,0
Germanic,0
Romance,0
West-Slavic,0
South-Slavic,0
East-Slavic,0
Scandinavian,0
Asian,0
Language isolates,0
recent Universal Dependencies,0
other datasets,0
text classification format,0
text regression format,0
"first, try to fetch dataset online",0
default dataset folder is the cache root,0
get string value if enum is passed,0
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)",0
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag",0
the CoNLL 03 task for German has an additional lemma column,0
the CoNLL 03 task for Dutch has no NP column,0
the CoNLL 03 task for Spanish only has two columns,0
the GERMEVAL task only has two columns: text and ner,0
WSD tasks may be put into this column format,0
"the UD corpora follow the CoNLL-U format, for which we have a special reader",0
"for text classifiers, we use our own special format",0
NER corpus for Basque,0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
get train and test data,0
"read in test file if exists, otherwise sample 10% of train data as test dataset",0
"read in dev file if exists, otherwise sample 10% of train data as dev dataset",0
convert tag scheme to iobes,0
automatically identify train / test / dev files,0
automatically identify train / test / dev files,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
conll 2000 chunking task,0
Support both TREC-6 and TREC-50,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
Wikiner NER task,0
unpack and write out in CoNLL column-like format,0
CoNLL 02/03 NER,0
universal dependencies,0
--- UD Germanic,0
--- UD Romance,0
--- UD West-Slavic,0
--- UD Scandinavian,0
--- UD South-Slavic,0
--- UD Asian,0
this is the default init size of a lmdb database for embeddings,0
some non-used parameter to allow print,0
get db filename from embedding name,0
"In case initialization of cached version failed, just fallback to the original WordEmbeddings",0
SequenceTagger,0
TextClassifier,0
get db filename from embedding name,0
if embedding database already exists,0
"otherwise, push embedding to database",0
if embedding database already exists,0
open the database in read mode,0
we need to set self.k,0
create and load the database in write mode,0
"no idea why, but we need to close and reopen the environment to avoid",0
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot,0
when opening new transaction !,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
"We don't want to create a SpaceTokenizer object each time this function is called,",0
so delegate the call directly to the static run_tokenize method,0
"We don't want to create a SegtokTokenizer object each time this function is called,",0
so delegate the call directly to the static run_tokenize method,0
"if text is passed, instantiate sentence with tokens (words)",0
log a warning if the dataset is empty,0
some sentences represent a document boundary (but most do not),0
data with zero-width characters cannot be handled,0
set token idx if not set,0
non-set tags are OUT tags,0
anything that is not a BIOES tag is a SINGLE tag,0
anything that is not OUT is IN,0
single and begin tags start a new span,0
remember previous tag,0
"if label type is explicitly specified, get spans for this label type",0
else determine all label types in sentence and get all spans,0
move sentence embeddings to device,0
move token embeddings to device,0
clear sentence embeddings,0
clear token embeddings,0
infer whitespace after field,0
add Sentence labels to output if they exist,0
add Token labels to output if they exist,0
add Sentence labels to output if they exist,0
add Token labels to output if they exist,0
No character at the corresponding code point: remove it,0
set name,0
sample test data if none is provided,0
sample dev data if none is provided,0
set train dev and test data,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
check if sentence itself has labels,0
check for labels of words,0
Make the tag dictionary,0
global variable: cache_root,0
global variable: device,0
global variable: embedding_storage_mode,0
# dummy return to fulfill trainer.train() needs,0
print(vec),0
Attach optimizer,0
"convert `metrics` to float, in case it's a zero-dim Tensor",0
if memory mode option 'none' delete everything,0
else delete only dynamic embeddings (otherwise autograd will keep everything in memory),0
find out which ones are dynamic embeddings,0
find out which ones are dynamic embeddings,0
memory management - option 1: send everything to CPU (pin to memory if we train on GPU),0
record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class),0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
plot i,0
save plots,0
save plots,0
plt.show(),0
save plot,0
take the average over the last three scores of training,0
take average over the scores from the different training runs,0
remove previous embeddings,0
clearing token embeddings to save memory,0
"read Dataset into data loader (if list of sentences passed, make Dataset first)",0
#TODO: not saving lines yet,1
== similarity measures ==,0
helper class for ModelSimilarity,0
-- works with binary cross entropy loss --,0
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}",0
-- works with ranking/triplet loss --,0
normalize the embeddings,0
== similarity losses ==,0
"we want that logits for corresponding pairs are high, and for non-corresponding low",0
TODO: this assumes eye matrix,0
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa",0
== similarity learner ==,0
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both",0
assumes that for each data pair there's at least one embedding per modality,0
pre-compute embeddings for all targets in evaluation dataset,0
compute the similarity,0
sort the similarity matrix across modality 1,0
"get the ranks, so +1 to start counting ranks from 1",0
The conversion from old model's constructor interface,0
auto-spawn on GPU if available,0
pad strings with whitespaces to longest sentence,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
set the dictionaries,0
"if we use a CRF, we must add special START and STOP tags to the dictionary",0
Initialize the weight tensor,0
initialize the network architecture,0
dropouts,0
optional reprojection layer on top of word embeddings,0
bidirectional LSTM on top of embedding layer,0
Create initial hidden state and initialize it,0
TODO: Decide how to initialize the hidden state variables,1
self.hs_initializer(self.lstm_init_h),0
self.hs_initializer(self.lstm_init_c),0
final linear map to tag space,0
set context if not set already,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
clearing token embeddings to save memory,0
predict for batch,0
make list of gold tags,0
make list of predicted tags,0
"check for true positives, false positives and false negatives",0
also write to file in BIO format to use old conlleval script,0
check if in gold spans,0
check if in predicted spans,0
"read Dataset into data loader (if list of sentences passed, make Dataset first)",0
"if span F1 needs to be used, use separate eval method",0
"else, use scikit-learn to evaluate",0
predict for batch,0
add gold tag,0
add predicted tag,0
for file output,0
use sklearn,0
"make ""classification report""",0
report over all in case there are no labels,0
get scores,0
line for log file,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
"if initial hidden state is trainable, use this state",0
word dropout only before LSTM - TODO: more experimentation needed,1
if self.use_word_dropout > 0.0:,0
sentence_tensor = self.word_dropout(sentence_tensor),0
get the tags in this sentence,0
add tags as tensor,0
pad tags if using batch-CRF decoder,0
reduce raw values to avoid NaN during exp,0
broadcasting will do the job of reshaping and is more efficient than calling repeat,0
default value,0
core Flair models on Huggingface ModelHub,0
"Large NER models,",0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Language-specific NER models,0
English NER models,0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Danish models,0
German models,0
French models,0
Dutch models,0
Malayalam models,0
Portuguese models,0
Keyphase models,0
Biomedical models,0
check if model name is a valid local file,0
"check if model key is remapped to HF key - if so, print out information",0
get mapped name,0
output information,0
use mapped name instead,0
"if not, check if model key is remapped to direct download location. If so, download model",0
special handling for the taggers by the @redewiegergabe project (TODO: move to model hub),1
"for all other cases (not local file or special download location), use HF model hub",0
"if not a local file, get from model hub",0
use model name as subfolder,0
Lazy import,0
output information,0
"log.error(f"" - Error message: {e}"")",0
clear embeddings after predicting,0
load each model,0
check if the same embeddings were already loaded previously,0
"if the model uses StackedEmbedding, make a new stack with previous objects",0
sort embeddings by key alphabetically,0
check previous embeddings and add if found,0
only re-use static embeddings,0
"if not found, use existing embedding",0
initialize new stack,0
"of the model uses regular embedding, re-load if previous version found",0
Initialize the weight tensor,0
auto-spawn on GPU if available,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
"read Dataset into data loader (if list of sentences passed, make Dataset first)",0
use scikit-learn to evaluate,0
remove previously predicted labels,0
get the gold labels,0
predict for batch,0
get the predicted labels,0
remove predicted labels,0
"make ""classification report""",0
get scores,0
line for log file,0
English sentiment models,0
Communicative Functions Model,0
Initialize TextClassifier,0
if bi_mode == True the linear layer needs twice the length of the embeddings as input size,0
since we concatenate the embeddings of the two DataPoints in the DataPairs,0
TODO: Transformers use special separator symbols in the beginning and between elements,0
of datapair. Here should be a case dinstintion between the different transformers.,0
linear layer,0
Drop unnecessary attributes from Parent class,0
prepare binary label dictionary,0
Store task specific labels since TARS can handle multiple tasks,0
make label dictionary if no Dictionary object is passed,0
get and embed all labels by making a Sentence object that contains only the label text,0
get each label embedding and scale between 0 and 1,0
compute similarity matrix,0
"the higher the similarity, the greater the chance that a label is",0
sampled as negative example,0
make sure the probabilities always sum up to 1,0
init new TARS classifier,0
set all task information,0
linear layers of internal classifier,0
Transform input data into TARS format,0
"M: num_classes in task, N: num_samples",0
reshape scores MN x 2 -> N x M x 2,0
import torch,0
a = torch.arange(30),0
"b = torch.reshape(-1, 3, 2)",0
"c = b[:,:,1]",0
target shape N x M,0
Transform label_scores,0
Transform label_scores into current task's desired format,0
"TARS does not do a softmax, so confidence of the best predicted class might be very low.",0
Therefore enforce a min confidence of 0.5 for a match.,0
make list if only one candidate label is passed,0
"if list is passed, convert to set",0
check if candidate_label_set is empty,0
note current task,0
create a temporary task,0
make zero shot predictions,0
switch to the pre-existing task,0
remember current task,0
predict with each task model,0
switch to the pre-existing task,0
embeddings,0
dictionaries,0
linear layer,0
F-beta score,0
all parameters will be pushed internally to the specified device,0
"read Dataset into data loader (if list of sentences passed, make Dataset first)",0
"if span F1 needs to be used, use separate eval method",0
"else, use scikit-learn to evaluate",0
predict for batch,0
add gold tag,0
add predicted tag,0
for file output,0
use sklearn,0
"make ""classification report""",0
report over all in case there are no labels,0
get scores,0
line for log file,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
clearing token embeddings to save memory,0
get the tags in this sentence,0
add tags as tensor,0
predict for batch,0
make list of gold tags,0
make list of predicted tags,0
"check for true positives, false positives and false negatives",0
also write to file in BIO format to use old conlleval script,0
check if in gold spans,0
check if in predicted spans,0
weights for loss function,0
iput size is two times wordembedding size since we use pair of words as input,0
"the output size is max_distance + 1, i.e. we allow 0,1,...,max_distance words between pairs",0
regression,0
input size is two times word embedding size since we use pair of words as input,0
the output size is 1,0
auto-spawn on GPU if available,0
all input should be tensors,0
forward allows only a single sentcence!!,0
embed words of sentence,0
go through all pairs of words with a maximum number of max_distance in between,0
go through all pairs,0
2-dim matrix whose rows are the embeddings of word pairs of the sentence,0
So far only one sentence allowed,0
If list of sentences is handed the function works with the first sentence of the list,0
Assume data_points is a single sentence!!!,0
scores are the predictions for each word pair,0
"classification needs labels to be integers, regression needs labels to be float",0
this is due to the different loss functions,0
only single sentences as input,0
gold labels,0
for output text file,0
for buckets,0
for average prediction,0
add some statistics to the output,0
use scikit-learn to evaluate,0
"we iterate over each sentence, instead of batches",0
get single labels from scores,0
gold labels,0
for output text file,0
hot one vector of true value,0
hot one vector of predicted value,0
"speichert embeddings, falls embedding_storage!= 'None'",0
"make ""classification report""",0
get scores,0
"precision_score = round(metrics.precision_score(y_true, y_pred, average='macro', zero_division=0), 4)",0
"recall_score = round(metrics.recall_score(y_true, y_pred, average='macro', zero_division=0), 4)",0
line for log file,0
cast string to Path,0
"determine what splits (train, dev, test) to evaluate and log",0
prepare loss logging file and set up header,0
"minimize training loss if training with dev data, else maximize dev score",0
"if training also uses dev/train data, include in training set",0
initialize sampler if provided,0
init with default values if only class is provided,0
set dataset to sample from,0
At any point you can hit Ctrl + C to break out of training early.,0
get new learning rate,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
process mini-batches,0
zero the gradients on the model and optimizer,0
"if necessary, make batch_steps",0
forward and backward for batch,0
forward pass,0
Backward,0
do the optimizer step,0
do the scheduler step if one-cycle,0
get new learning rate,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"anneal against train loss if training with dev, otherwise anneal against dev score",0
evaluate on train / dev / test split depending on training settings,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
calculate scores using dev data if available,0
append dev score to score history,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
determine learning rate annealing through scheduler. Use auxiliary metric for AnnealOnPlateau,0
determine bad epoch number,0
log bad epochs,0
output log file,0
make headers on first epoch,0
"if checkpoint is enabled, save model at each epoch",0
"if we use dev data, remember best model based on dev evaluation score",0
"if we do not use dev data for model selection, save final model",0
test best model if test data is present,0
"if we are training over multiple datasets, do evaluation for each",0
get and return the final test score of best model,0
cast string to Path,0
forward pass,0
update optimizer and scheduler,0
Add chars to the dictionary,0
charsplit file content,0
charsplit file content,0
Add words to the dictionary,0
Tokenize file content,0
"TextDataset returns a list. valid and test are only one file, so return the first element",0
cast string to Path,0
error message if the validation dataset is too small,0
Shuffle training files randomly after serially iterating through corpus one,0
"iterate through training data, starting at self.split (for checkpointing)",0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
Backward,0
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.,0
We detach the hidden state from how it was previously produced.,0
"If we didn't, the model would try backpropagating all the way to start of the dataset.",0
explicitly remove loss to clear up memory,0
##############################################################################,0
Save the model if the validation loss is the best we've seen so far.,0
##############################################################################,0
print info,0
##############################################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
1-camembert-base -> camembert-base,0
1-xlm-roberta-large -> xlm-roberta-large,0
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol,0
The mask has 1 for real tokens and 0 for padding tokens. Only real,0
tokens are attended to.,0
Zero-pad up to the sequence length.,0
"first, find longest sentence in batch",0
prepare id maps for BERT model,0
put encoded batch through BERT model to get all hidden states of all encoder layers,0
get aggregated embeddings for each BERT-subtoken in sentence,0
get the current sentence object,0
add concatenated embedding to sentence,0
use first subword embedding if pooling operation is 'first',0
"otherwise, do a mean over all subwords in token",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
embed a dummy sentence to determine embedding_length,0
Avoid conflicts with flair's Token class,0
"<cls> token initially set to 1/D, so it attends to all image features equally",0
add positional encodings,0
reshape the pixels into the sequence,0
layer norm after convolution and positional encodings,0
add <cls> token,0
"transformer requires input in the shape [h*w+1, b, d]",0
the output is an embedding of <cls> token,0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
load tokenizer and transformer model,0
model name,0
"when initializing, embeddings are in eval mode by default",0
embedding parameters,0
send mini-token through to check how many layers the model has,0
check whether CLS is at beginning or end,0
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial",0
using list comprehension,0
gradients are enabled if fine-tuning is enabled,0
"first, subtokenize each sentence and find out into how many subtokens each token was divided",0
subtokenize sentences,0
tokenize and truncate to max subtokens (TODO: check better truncation strategies),1
find longest sentence in batch,0
initialize batch tensors and mask,0
put encoded batch through transformer model to get all hidden states of all encoder layers,0
iterate over all subtokenized sentences,0
use scalar mix of embeddings if so selected,0
set the extracted embedding for the token,0
special handling for serializing transformer models,0
serialize the transformer models and the constructor arguments (but nothing else),0
necessary for reverse compatibility with Flair <= 0.7,0
special handling for deserializing transformer models,0
load transformer model,0
constructor arguments,0
re-initialize transformer word embeddings with constructor arguments,0
"I have no idea why this is necessary, but otherwise it doesn't work",1
reload tokenizer to get around serialization issues,0
optional fine-tuning on top of embedding layer,0
"if only one sentence is passed, convert to list of sentence",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push through RNN,0
after-RNN dropout,0
extract embeddings from RNN,0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
IMPORTANT: add embeddings as torch modules,0
iterate over sentences,0
"if its a forward LM, take last state",0
"convert to plain strings, embedded in a list for the encode function",0
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency",0
"if only one sentence is passed, convert to list of sentence",0
Expose base classses,0
Expose token embedding classes,0
Expose document embedding classes,0
Expose image embedding classes,0
Expose legacy embedding classes,0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNINOS embeddings,0
pubmed embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
fix serialized models,0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
multilingual models,0
English models,0
Arabic,0
Bulgarian,0
Czech,0
Danish,0
German,0
Spanish,0
Basque,0
Persian,0
Finnish,0
French,0
Hebrew,0
Hindi,0
Croatian,0
Indonesian,0
Italian,0
Japanese,0
Malayalam,0
Dutch,0
Norwegian,0
Polish,0
Portuguese,0
Pubmed,0
Slovenian,0
Swedish,0
Tamil,0
CLEF HIPE Shared task,0
load model if in pretrained model map,0
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir),0
embeddings are static if we don't do finetuning,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
make compatible with serialized models (TODO: remove),1
make compatible with serialized models (TODO: remove),1
gradients are enable if fine-tuning is enabled,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
offset mode that extracts at whitespace after last character,0
offset mode that extracts at last character,0
only clone if optimization mode is 'gpu',0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
check token.text is empty or not,0
set aggregation operation,0
add embeddings after updating,0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
load tokenizer and transformer model,0
model name,0
whether to detach gradients on overlong sentences,0
store whether to use context (and how much),0
dropout contexts,0
"if using context, can we cross document boundaries?",0
"when initializing, embeddings are in eval mode by default",0
embedding parameters,0
send mini-token through to check how many layers the model has,0
calculate embedding length,0
return length,0
check if special tokens exist to circumvent error message,0
"most models have an intial BOS token, except for XLNet, T5 and GPT2",0
remove special markup,0
embed each sentence separately,0
"TODO: keep for backwards compatibility, but remove in future",1
"some pretrained models do not have this property, applying default settings now.",0
can be set manually after loading the model.,0
"if we also use context, first expand sentence to include context",0
"in case of contextualization, we must remember non-expanded sentence",0
create expanded sentence and remember context offsets,0
overwrite sentence with expanded sentence,0
subtokenize the sentence,0
method 1: subtokenize sentence,0
"subtokenized_sentence = self.tokenizer.encode(tokenized_string, add_special_tokens=True)",0
method 2:,0
transformer specific tokenization,0
set zero embeddings for empty sentences and return,0
determine into how many subtokens each token is split,0
"if sentence is too long, will be split into multiple parts",0
check if transformer version 3 is used - in this case use old handling,0
get sentence as list of subtoken ids,0
"else if current transformer is used, use default handling",0
overlong sentences are handled as multiple splits,0
embed each sentence split,0
initialize batch tensors and mask,0
propagate gradients if fine-tuning and only during training,0
increase memory effectiveness by skipping all but last sentence split,0
put encoded batch through transformer model to get all hidden states of all encoder layers,0
get hidden states as single tensor,0
put splits back together into one tensor using overlapping strides,0
"for each token, get embedding",0
some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector,0
"get states from all selected layers, aggregate with pooling operation",0
use layer mean of embeddings if so selected,0
set the extracted embedding for the token,0
move embeddings from context back to original sentence (if using context),0
remember original sentence,0
get left context,0
get right context,0
make expanded sentence,0
iterate over subtokens and reconstruct tokens,0
remove special markup,0
TODO check if this is necessary is this method is called before prepare_for_model,1
check if reconstructed token is special begin token ([CLS] or similar),0
some BERT tokenizers somehow omit words - in such cases skip to next token,0
append subtoken to reconstruct token,0
check if reconstructed token is the same as current token,0
"if so, add subtoken count",0
reset subtoken count and reconstructed token,0
break from loop if all tokens are accounted for,0
if tokens are unaccounted for,0
check if all tokens were matched to subtokens,0
"""""""Returns the length of the embedding vector.""""""",0
special handling for serializing transformer models,0
serialize the transformer models and the constructor arguments (but nothing else),0
necessary for reverse compatibility with Flair <= 0.7,0
special handling for deserializing transformer models,0
load transformer model,0
constructor arguments,0
re-initialize transformer word embeddings with constructor arguments,0
"I have no idea why this is necessary, but otherwise it doesn't work",1
reload tokenizer to get around serialization issues,0
max_tokens = 500,0
model architecture,0
model architecture,0
download if necessary,0
load the model,0
"TODO: keep for backwards compatibility, but remove in future",1
save the sentence piece model as binary file (not as path which may change),0
write out the binary sentence piece model into the expected directory,0
"if the model was saved as binary and it is not found on disk, write to appropriate path",0
"otherwise, use normal process and potentially trigger another download",0
"once the modes if there, load it with sentence piece",0
empty words get no embedding,0
all other words get embedded,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn,0
GLOVE embeddings,0
"find train, dev and test files if not specified",0
get train data,0
read in test file if exists,0
read in dev file if exists,0
special key for space after,0
"store either Sentence objects in memory, or only file offsets",0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
determine encoding of text file,0
skip first line if to selected,0
option 1: read only sentence boundaries as offset positions,0
option 2: keep everything in memory,0
pointer to previous,0
"if sentence ends, break",0
skip comments,0
"if sentence ends, convert and return",0
check if this sentence is a document boundary,0
"otherwise, this line is a token. parse and add to sentence",0
check if this sentence is a document boundary,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
set sentence context using partials,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
"tag_to_bioes=tag_to_bioes,",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download files if not present locally,0
we need to slightly modify the original files by adding some new lines after document separators,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{STACKOVERFLOW_NER_path}train_merged_labels.txt"", Path(""datasets"") / dataset_name) # TODO: what is this?",1
column format,0
this dataset name,0
default dataset folder is the cache root,0
If the extracted corpus file is not yet present in dir,0
download zip if necessary,0
"extracted corpus is not present , so unpacking it.",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Remove CoNLL-U meta information in the last column,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
rename according to train - test - dev - convention,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
data is not in IOB2 format. Thus we transform it to IOB2,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
tokens to ignore (edit here for variation),0
counter to keep track how many tags have been found in line,0
variable to count of how many words a chunk consists,0
indicates if surface form is chunk or not,0
array to save tags temporarily for handling chunks,0
cut token to get chunk,0
save single words of chunk,0
handle first word of chunk,0
edit here for variation,0
check if converted file exists,0
convert the file to CoNLL,0
column format,0
,0
since only the WordNet 3.0 version for senses is consistently available for all provided datasets we will,0
only consider this version,0
,0
also we ignore the id annotation used in datasets that were originally created for evaluation tasks,0
,0
if the other annotations should be needed simply add the columns in correct order according,0
to the chosen datasets here and respectively change the values of the blacklist array and,0
the range value of the else case in the token for loop in the from_ufsac_to_conll function,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
determine correct CoNLL files,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
this list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
get google drive id from list,0
download from google drive,0
unzip,0
"tar.extractall(language_folder,members=[tar.getmember(file_name)])",0
transform data into required format,0
"the processed dataset has the additional ending ""_new""",0
remove the unprocessed dataset,0
initialize comlumncorpus and add it to list,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"unzip the downloaded repo and merge the train, dev and test datasets",0
download data if necessary,0
unpack and write out in CoNLL column-like format,0
if no languages are given as argument all languages used in XTREME will be loaded,0
if only one language is given,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
This list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
download from HU Server,0
unzip,0
transform data into required format,0
initialize comlumncorpus and add it to list,0
"find train, dev and test files if not specified",0
use test_file to create test split if available,0
use dev_file to create test split if available,0
"if data point contains black-listed label, do not use",0
first check if valid sentence,0
"if so, add to indices",0
"find train, dev and test files if not specified",0
variables,0
different handling of in_memory data than streaming data,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
test if format is OK,0
test if at least one label given,0
noinspection PyDefaultArgument,0
dataset name includes the split size,0
default dataset folder is the cache root,0
download data if necessary,0
download each of the 28 splits,0
create dataset directory if necessary,0
download senteval datasets if necessary und unzip,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"by defaut, map point score to POSITIVE / NEGATIVE values",0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file from CSV,0
create test.txt file from CSV,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
convert to FastText format,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download datasets if necessary,0
create dataset directory if necessary,0
create correctly formated txt files,0
multiple labels are possible,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
create a separate directory for different tasks,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
check if dataset is supported,0
set file names,0
download and unzip in file structure if necessary,0
instantiate corpus,0
"find train, dev and test files if not specified",0
"create DataPairDataset for train, test and dev file, if they are given",0
stop if file does not exist,0
create a DataPair object from strings,0
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings",0
if no base_path provided take cache root,0
"if data is not downloaded yet, download it",0
get the zip file,0
"rename test file to eval_dataset, since it has no labels",0
if no base_path provided take cache root,0
"if data not downloaded yet, download it",0
get the zip file,0
"the downloaded files have json format, we transform them to tsv",0
Function to transform JSON file to tsv for Recognizing Textual Entailment Data,0
remove json file,0
Uses dynamic programming approach to calculate maximum independent set in interval graph,0
with sum of all entity lengths as secondary key,0
calculate offset without current text,0
because we stick all passages of a document together,0
TODO For split entities we also annotate everything inbetween which might be a bad idea?,1
Try to fix incorrect annotations,0
print(,0
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}""",0
),0
Ignore empty lines or relation annotations,0
FIX annotation of whitespaces (necessary for PDR),0
One token may contain multiple entities -> deque all of them,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Create tokenization-dependent CONLL files. This is necessary to prevent,0
from caching issues (e.g. loading the same corpus with different sentence splitters),0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Edge case: last token starts a new entity,0
Last document in file,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
Edge case: last token starts a new entity,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download file is huge => make default_dir visible so that derivative,0
corpora can all use the same download file,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Read texts,0
Read annotations,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
We need to apply a patch to correct the original training file,0
Articles title,0
Article abstract,0
Entity annotations,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Edge case: last token starts a new entity,0
Map all entities to chemicals,0
Map all entities to disease,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Incomplete article,0
Invalid XML syntax,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
if len(mid) != 3:,0
continue,0
Try to fix entity offsets,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
There is still one illegal annotation in the file ..,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"Abstract first, title second to prevent issues with sentence splitting",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"Filter for specific entity types, by default no entities will be filtered",0
Get original HUNER splits to retrieve a list of all document ids contained in V2,0
train and dev split of V2 will be train in V4,0
test split of V2 will be dev in V4,0
New documents in V4 will become test documents,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
cache Feidegger config file,0
cache Feidegger images,0
replace image URL with local cached file,0
append Sentence-Image data point,0
"in certain cases, multi-CPU data loading makes no sense and slows",0
"everything down. For this reason, we detect if a dataset is in-memory:",0
"if so, num_workers is set to 0 for faster processing",0
cast to list if necessary,0
cast to list if necessary,0
"first, check if pymongo is installed",0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
Expose base classses,0
Expose all sequence labeling datasets,0
Expose all document classification datasets,0
Expose all treebanks,0
Expose all text-text datasets,0
Expose all text-image datasets,0
Expose all biomedical data sets,0
Expose all biomedical data sets using the HUNER splits,0
-,0
-,0
-,0
-,0
Expose all biomedical data sets used for the evaluation of BioBERT,0
"find train, dev and test files if not specified",0
get train data,0
get test data,0
get dev data,0
option 1: read only sentence boundaries as offset positions,0
option 2: keep everything in memory,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
current token ID,0
handling for the awful UD multiword format,0
end of sentence,0
comments,0
ellipsis,0
if token is a multi-word,0
normal single-word tokens,0
"if we don't split multiwords, skip over component words",0
add token,0
add morphological tags,0
derive whitespace logic for multiwords,0
print(token),0
print(current_multiword_last_token),0
print(current_multiword_first_token),0
"if multi-word equals component tokens, there should be no whitespace",0
go through all tokens in subword and set whitespace_after information,0
print(i),0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
test corpus,0
create a TARS classifier,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
clean up file,0
bioes tags,0
bio tags,0
broken tags,0
all tags,0
all weird tags,0
tags with confidence,0
bioes tags,0
bioes tags,0
increment for last token in sentence if not followed by whitespace,0
clean up directory,0
clean up directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
def test_multiclass_metrics():,0
,0
"metric = Metric(""Test"")",0
"available_labels = [""A"", ""B"", ""C""]",0
,0
"predictions = [""A"", ""B""]",0
"true_values = [""A""]",0
TextClassifier._evaluate_sentence_for_text_classification(,0
"metric, available_labels, predictions, true_values",0
),0
,0
"predictions = [""C"", ""B""]",0
"true_values = [""A"", ""B""]",0
TextClassifier._evaluate_sentence_for_text_classification(,0
"metric, available_labels, predictions, true_values",0
),0
,0
print(metric),0
from flair.trainers.trainer_regression import RegressorTrainer,0
def test_trainer_results(tasks_base_path):,0
"corpus, model, trainer = init(tasks_base_path)",0
"results = trainer.train(""regression_train/"", max_epochs=1)",0
"assert results[""test_score""] > 0",0
"assert len(results[""dev_loss_history""]) == 1",0
"assert len(results[""dev_score_history""]) == 1",0
"assert len(results[""train_loss_history""]) == 1",0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
clean up results directory,0
define search space,0
sequence tagger parameter,0
model trainer parameter,0
training parameter,0
find best parameter settings,0
clean up results directory,0
document embeddings parameter,0
training parameter,0
clean up results directory,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
1. get the corpus,0
2. what tag do we want to predict?,0
3. make the tag dictionary from the corpus,0
initialize embeddings,0
comment in this line to use character embeddings,0
"CharacterEmbeddings(),",0
comment in these lines to use contextual string embeddings,0
,0
"FlairEmbeddings('news-forward'),",0
,0
"FlairEmbeddings('news-backward'),",0
initialize sequence tagger,0
initialize trainer,0
from allennlp.common.tqdm import Tqdm,0
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
Extract all the contents of zip file in current directory,0
Extract all the contents of zip file in current directory,0
get cache path to put the file,0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
first determine the distribution of classes in the dataset,0
weight for each sample,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups,1
see https://github.com/zalandoresearch/flair/issues/351,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
Maintains max of all exp. moving avg. of sq. grad. values,0
Decay the first and second moment running average coefficient,0
Maintains the maximum of all 2nd moment running avg. till now,0
Use the max. for normalizing running avg. of gradient,0
determine offsets for whitespace_after field,0
increment for last token in sentence if not followed by whitespace,0
determine offsets for whitespace_after field,0
conll 2000 column format,0
conll 03 NER column format,0
WNUT-17,0
-- WikiNER datasets,0
-- Universal Dependencies,0
Germanic,0
Romance,0
West-Slavic,0
South-Slavic,0
East-Slavic,0
Scandinavian,0
Asian,0
Language isolates,0
recent Universal Dependencies,0
other datasets,0
text classification format,0
text regression format,0
"first, try to fetch dataset online",0
default dataset folder is the cache root,0
get string value if enum is passed,0
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)",0
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag",0
the CoNLL 03 task for German has an additional lemma column,0
the CoNLL 03 task for Dutch has no NP column,0
the CoNLL 03 task for Spanish only has two columns,0
the GERMEVAL task only has two columns: text and ner,0
WSD tasks may be put into this column format,0
"the UD corpora follow the CoNLL-U format, for which we have a special reader",0
"for text classifiers, we use our own special format",0
NER corpus for Basque,0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
get train and test data,0
"read in test file if exists, otherwise sample 10% of train data as test dataset",0
"read in dev file if exists, otherwise sample 10% of train data as dev dataset",0
convert tag scheme to iobes,0
automatically identify train / test / dev files,0
automatically identify train / test / dev files,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
conll 2000 chunking task,0
Support both TREC-6 and TREC-50,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
Wikiner NER task,0
unpack and write out in CoNLL column-like format,0
CoNLL 02/03 NER,0
universal dependencies,0
--- UD Germanic,0
--- UD Romance,0
--- UD West-Slavic,0
--- UD Scandinavian,0
--- UD South-Slavic,0
--- UD Asian,0
this is the default init size of a lmdb database for embeddings,0
some non-used parameter to allow print,0
get db filename from embedding name,0
"In case initialization of cached version failed, just fallback to the original WordEmbeddings",0
SequenceTagger,0
TextClassifier,0
get db filename from embedding name,0
if embedding database already exists,0
"otherwise, push embedding to database",0
if embedding database already exists,0
open the database in read mode,0
we need to set self.k,0
create and load the database in write mode,0
"no idea why, but we need to close and reopen the environment to avoid",0
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot,0
when opening new transaction !,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
"We don't want to create a SpaceTokenizer object each time this function is called,",0
so delegate the call directly to the static run_tokenize method,0
"We don't want to create a SegtokTokenizer object each time this function is called,",0
so delegate the call directly to the static run_tokenize method,0
"if text is passed, instantiate sentence with tokens (words)",0
log a warning if the dataset is empty,0
data with zero-width characters cannot be handled,0
set token idx if not set,0
non-set tags are OUT tags,0
anything that is not a BIOES tag is a SINGLE tag,0
anything that is not OUT is IN,0
single and begin tags start a new span,0
remember previous tag,0
"if label type is explicitly specified, get spans for this label type",0
else determine all label types in sentence and get all spans,0
move sentence embeddings to device,0
move token embeddings to device,0
clear sentence embeddings,0
clear token embeddings,0
infer whitespace after field,0
add Sentence labels to output if they exist,0
add Token labels to output if they exist,0
add Sentence labels to output if they exist,0
add Token labels to output if they exist,0
No character at the corresponding code point: remove it,0
set name,0
sample test data if none is provided,0
sample dev data if none is provided,0
set train dev and test data,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
check if sentence itself has labels,0
check for labels of words,0
Make the tag dictionary,0
global variable: cache_root,0
global variable: device,0
global variable: embedding_storage_mode,0
# dummy return to fulfill trainer.train() needs,0
print(vec),0
Attach optimizer,0
"convert `metrics` to float, in case it's a zero-dim Tensor",0
if memory mode option 'none' delete everything,0
else delete only dynamic embeddings (otherwise autograd will keep everything in memory),0
find out which ones are dynamic embeddings,0
find out which ones are dynamic embeddings,0
memory management - option 1: send everything to CPU (pin to memory if we train on GPU),0
record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class),0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
plot i,0
save plots,0
save plots,0
plt.show(),0
save plot,0
take the average over the last three scores of training,0
take average over the scores from the different training runs,0
remove previous embeddings,0
clearing token embeddings to save memory,0
"read Dataset into data loader (if list of sentences passed, make Dataset first)",0
#TODO: not saving lines yet,1
== similarity measures ==,0
helper class for ModelSimilarity,0
-- works with binary cross entropy loss --,0
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}",0
-- works with ranking/triplet loss --,0
normalize the embeddings,0
== similarity losses ==,0
"we want that logits for corresponding pairs are high, and for non-corresponding low",0
TODO: this assumes eye matrix,0
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa",0
== similarity learner ==,0
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both",0
assumes that for each data pair there's at least one embedding per modality,0
pre-compute embeddings for all targets in evaluation dataset,0
compute the similarity,0
sort the similarity matrix across modality 1,0
"get the ranks, so +1 to start counting ranks from 1",0
The conversion from old model's constructor interface,0
auto-spawn on GPU if available,0
pad strings with whitespaces to longest sentence,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
set the dictionaries,0
"if we use a CRF, we must add special START and STOP tags to the dictionary",0
Initialize the weight tensor,0
initialize the network architecture,0
dropouts,0
optional reprojection layer on top of word embeddings,0
bidirectional LSTM on top of embedding layer,0
Create initial hidden state and initialize it,0
TODO: Decide how to initialize the hidden state variables,1
self.hs_initializer(self.lstm_init_h),0
self.hs_initializer(self.lstm_init_c),0
final linear map to tag space,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
clearing token embeddings to save memory,0
predict for batch,0
make list of gold tags,0
make list of predicted tags,0
"check for true positives, false positives and false negatives",0
also write to file in BIO format to use old conlleval script,0
check if in gold spans,0
check if in predicted spans,0
"read Dataset into data loader (if list of sentences passed, make Dataset first)",0
"if span F1 needs to be used, use separate eval method",0
"else, use scikit-learn to evaluate",0
predict for batch,0
add gold tag,0
add predicted tag,0
for file output,0
use sklearn,0
"make ""classification report""",0
report over all in case there are no labels,0
get scores,0
line for log file,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
"if initial hidden state is trainable, use this state",0
word dropout only before LSTM - TODO: more experimentation needed,1
if self.use_word_dropout > 0.0:,0
sentence_tensor = self.word_dropout(sentence_tensor),0
get the tags in this sentence,0
add tags as tensor,0
pad tags if using batch-CRF decoder,0
reduce raw values to avoid NaN during exp,0
broadcasting will do the job of reshaping and is more efficient than calling repeat,0
default value,0
English NER models,0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Danish models,0
German models,0
French models,0
Dutch models,0
Malayalam models,0
Portuguese models,0
Keyphase models,0
Biomedical models,0
the historical German taggers by the @redewiegergabe project,0
Fallback to Hugging Face model hub,0
e.g. stefan-it/flair-ner-conll03 is a valid namespace,0
and  stefan-it/flair-ner-conll03@main supports specifying a commit/branch name,0
Lazy import,0
clear embeddings after predicting,0
load each model,0
check if the same embeddings were already loaded previously,0
"if the model uses StackedEmbedding, make a new stack with previous objects",0
sort embeddings by key alphabetically,0
check previous embeddings and add if found,0
only re-use static embeddings,0
"if not found, use existing embedding",0
initialize new stack,0
"of the model uses regular embedding, re-load if previous version found",0
Initialize the weight tensor,0
auto-spawn on GPU if available,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
"read Dataset into data loader (if list of sentences passed, make Dataset first)",0
use scikit-learn to evaluate,0
remove previously predicted labels,0
get the gold labels,0
predict for batch,0
get the predicted labels,0
remove predicted labels,0
"make ""classification report""",0
get scores,0
line for log file,0
English sentiment models,0
Communicative Functions Model,0
Drop unnecessary attributes from Parent class,0
prepare binary label dictionary,0
Store task specific labels since TARS can handle multiple tasks,0
make label dictionary if no Dictionary object is passed,0
get and embed all labels by making a Sentence object that contains only the label text,0
get each label embedding and scale between 0 and 1,0
compute similarity matrix,0
"the higher the similarity, the greater the chance that a label is",0
sampled as negative example,0
make sure the probabilities always sum up to 1,0
Transform input data into TARS format,0
"M: num_classes in task, N: num_samples",0
reshape scores MN x 2 -> N x M x 2,0
import torch,0
a = torch.arange(30),0
"b = torch.reshape(-1, 3, 2)",0
"c = b[:,:,1]",0
target shape N x M,0
Transform label_scores,0
Transform label_scores into current task's desired format,0
"TARS does not do a softmax, so confidence of the best predicted class might be very low.",0
Therefore enforce a min confidence of 0.5 for a match.,0
make list if only one candidate label is passed,0
"if list is passed, convert to set",0
check if candidate_label_set is empty,0
note current task,0
create a temporary task,0
make zero shot predictions,0
switch to the pre-existing task,0
remember current task,0
predict with each task model,0
switch to the pre-existing task,0
embeddings,0
dictionaries,0
linear layer,0
F-beta score,0
all parameters will be pushed internally to the specified device,0
"read Dataset into data loader (if list of sentences passed, make Dataset first)",0
"if span F1 needs to be used, use separate eval method",0
"else, use scikit-learn to evaluate",0
predict for batch,0
add gold tag,0
add predicted tag,0
for file output,0
use sklearn,0
"make ""classification report""",0
report over all in case there are no labels,0
get scores,0
line for log file,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
clearing token embeddings to save memory,0
get the tags in this sentence,0
add tags as tensor,0
predict for batch,0
make list of gold tags,0
make list of predicted tags,0
"check for true positives, false positives and false negatives",0
also write to file in BIO format to use old conlleval script,0
check if in gold spans,0
check if in predicted spans,0
weights for loss function,0
iput size is two times wordembedding size since we use pair of words as input,0
"the output size is max_distance + 1, i.e. we allow 0,1,...,max_distance words between pairs",0
regression,0
input size is two times word embedding size since we use pair of words as input,0
the output size is 1,0
auto-spawn on GPU if available,0
all input should be tensors,0
forward allows only a single sentcence!!,0
embed words of sentence,0
go through all pairs of words with a maximum number of max_distance in between,0
go through all pairs,0
2-dim matrix whose rows are the embeddings of word pairs of the sentence,0
So far only one sentence allowed,0
If list of sentences is handed the function works with the first sentence of the list,0
Assume data_points is a single sentence!!!,0
scores are the predictions for each word pair,0
"classification needs labels to be integers, regression needs labels to be float",0
this is due to the different loss functions,0
only single sentences as input,0
gold labels,0
for output text file,0
for buckets,0
for average prediction,0
add some statistics to the output,0
use scikit-learn to evaluate,0
"we iterate over each sentence, instead of batches",0
get single labels from scores,0
gold labels,0
for output text file,0
hot one vector of true value,0
hot one vector of predicted value,0
"speichert embeddings, falls embedding_storage!= 'None'",0
"make ""classification report""",0
get scores,0
"precision_score = round(metrics.precision_score(y_true, y_pred, average='macro', zero_division=0), 4)",0
"recall_score = round(metrics.recall_score(y_true, y_pred, average='macro', zero_division=0), 4)",0
line for log file,0
cast string to Path,0
"determine what splits (train, dev, test) to evaluate and log",0
prepare loss logging file and set up header,0
"minimize training loss if training with dev data, else maximize dev score",0
"if training also uses dev data, include in training set",0
initialize sampler if provided,0
init with default values if only class is provided,0
set dataset to sample from,0
At any point you can hit Ctrl + C to break out of training early.,0
get new learning rate,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
process mini-batches,0
zero the gradients on the model and optimizer,0
"if necessary, make batch_steps",0
forward and backward for batch,0
forward pass,0
Backward,0
do the optimizer step,0
do the scheduler step if one-cycle,0
get new learning rate,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"anneal against train loss if training with dev, otherwise anneal against dev score",0
evaluate on train / dev / test split depending on training settings,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
calculate scores using dev data if available,0
append dev score to score history,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
determine learning rate annealing through scheduler. Use auxiliary metric for AnnealOnPlateau,0
determine bad epoch number,0
log bad epochs,0
output log file,0
make headers on first epoch,0
"if checkpoint is enabled, save model at each epoch",0
"if we use dev data, remember best model based on dev evaluation score",0
"if we do not use dev data for model selection, save final model",0
test best model if test data is present,0
"if we are training over multiple datasets, do evaluation for each",0
get and return the final test score of best model,0
cast string to Path,0
forward pass,0
update optimizer and scheduler,0
Add chars to the dictionary,0
charsplit file content,0
charsplit file content,0
Add words to the dictionary,0
Tokenize file content,0
"TextDataset returns a list. valid and test are only one file, so return the first element",0
cast string to Path,0
error message if the validation dataset is too small,0
Shuffle training files randomly after serially iterating through corpus one,0
"iterate through training data, starting at self.split (for checkpointing)",0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
Backward,0
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.,0
We detach the hidden state from how it was previously produced.,0
"If we didn't, the model would try backpropagating all the way to start of the dataset.",0
explicitly remove loss to clear up memory,0
##############################################################################,0
Save the model if the validation loss is the best we've seen so far.,0
##############################################################################,0
print info,0
##############################################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
1-camembert-base -> camembert-base,0
1-xlm-roberta-large -> xlm-roberta-large,0
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol,0
The mask has 1 for real tokens and 0 for padding tokens. Only real,0
tokens are attended to.,0
Zero-pad up to the sequence length.,0
"first, find longest sentence in batch",0
prepare id maps for BERT model,0
put encoded batch through BERT model to get all hidden states of all encoder layers,0
get aggregated embeddings for each BERT-subtoken in sentence,0
get the current sentence object,0
add concatenated embedding to sentence,0
use first subword embedding if pooling operation is 'first',0
"otherwise, do a mean over all subwords in token",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
embed a dummy sentence to determine embedding_length,0
Avoid conflicts with flair's Token class,0
"<cls> token initially set to 1/D, so it attends to all image features equally",0
add positional encodings,0
reshape the pixels into the sequence,0
layer norm after convolution and positional encodings,0
add <cls> token,0
"transformer requires input in the shape [h*w+1, b, d]",0
the output is an embedding of <cls> token,0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
load tokenizer and transformer model,0
model name,0
"when initializing, embeddings are in eval mode by default",0
embedding parameters,0
send mini-token through to check how many layers the model has,0
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial",0
using list comprehension,0
gradients are enabled if fine-tuning is enabled,0
"first, subtokenize each sentence and find out into how many subtokens each token was divided",0
subtokenize sentences,0
tokenize and truncate to max subtokens (TODO: check better truncation strategies),1
find longest sentence in batch,0
initialize batch tensors and mask,0
put encoded batch through transformer model to get all hidden states of all encoder layers,0
iterate over all subtokenized sentences,0
use scalar mix of embeddings if so selected,0
set the extracted embedding for the token,0
reload tokenizer to get around serialization issues,0
optional fine-tuning on top of embedding layer,0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push through RNN,0
after-RNN dropout,0
extract embeddings from RNN,0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
IMPORTANT: add embeddings as torch modules,0
iterate over sentences,0
"if its a forward LM, take last state",0
"convert to plain strings, embedded in a list for the encode function",0
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency",0
"if only one sentence is passed, convert to list of sentence",0
Expose base classses,0
Expose token embedding classes,0
Expose document embedding classes,0
Expose image embedding classes,0
Expose legacy embedding classes,0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNINOS embeddings,0
pubmed embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
fix serialized models,0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
multilingual models,0
English models,0
Arabic,0
Bulgarian,0
Czech,0
Danish,0
German,0
Spanish,0
Basque,0
Persian,0
Finnish,0
French,0
Hebrew,0
Hindi,0
Croatian,0
Indonesian,0
Italian,0
Japanese,0
Malayalam,0
Dutch,0
Norwegian,0
Polish,0
Portuguese,0
Pubmed,0
Slovenian,0
Swedish,0
Tamil,0
CLEF HIPE Shared task,0
load model if in pretrained model map,0
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir),0
embeddings are static if we don't do finetuning,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
make compatible with serialized models (TODO: remove),1
make compatible with serialized models (TODO: remove),1
gradients are enable if fine-tuning is enabled,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
offset mode that extracts at whitespace after last character,0
offset mode that extracts at last character,0
only clone if optimization mode is 'gpu',0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
check token.text is empty or not,0
set aggregation operation,0
add embeddings after updating,0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
load tokenizer and transformer model,0
model name,0
"when initializing, embeddings are in eval mode by default",0
embedding parameters,0
send mini-token through to check how many layers the model has,0
"self.mix = ScalarMix(mixture_size=len(self.layer_indexes), trainable=False)",0
check if special tokens exist to circumvent error message,0
"most models have an intial BOS token, except for XLNet, T5 and GPT2",0
split into micro batches of size self.batch_size before pushing through transformer,0
embed each micro-batch,0
remove special markup,0
"first, subtokenize each sentence and find out into how many subtokens each token was divided",0
"TODO: keep for backwards compatibility, but remove in future",1
"some pretrained models do not have this property, applying default settings now.",0
can be set manually after loading the model.,0
method 1: subtokenize sentence,0
"subtokenized_sentence = self.tokenizer.encode(tokenized_string, add_special_tokens=True)",0
method 2:,0
transformer specific tokenization,0
empty sentences get zero embeddings,0
only embed non-empty sentences and if there is at least one,0
find longest sentence in batch,0
initialize batch tensors and mask,0
put encoded batch through transformer model to get all hidden states of all encoder layers,0
make the tuple a tensor; makes working with it easier.,0
gradients are enabled if fine-tuning is enabled,0
iterate over all subtokenized sentences,0
"remove stride_size//2 at end of sentence_hidden_state, and half at beginning of remainder,",0
in order to get some context into the embeddings of these words.,0
also don't include the embedding of the extra [CLS] and [SEP] tokens.,0
"for each token, get embedding",0
some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector,0
"get states from all selected layers, aggregate with pooling operation",0
use scalar mix of embeddings if so selected,0
sm_embeddings = self.mix(subtoken_embeddings),0
set the extracted embedding for the token,0
iterate over subtokens and reconstruct tokens,0
remove special markup,0
TODO check if this is necessary is this method is called before prepare_for_model,1
check if reconstructed token is special begin token ([CLS] or similar),0
some BERT tokenizers somehow omit words - in such cases skip to next token,0
append subtoken to reconstruct token,0
check if reconstructed token is the same as current token,0
"if so, add subtoken count",0
reset subtoken count and reconstructed token,0
break from loop if all tokens are accounted for,0
if tokens are unaccounted for,0
check if all tokens were matched to subtokens,0
"if fine-tuning is not enabled (i.e. a ""feature-based approach"" used), this",0
module should never be in training mode,0
reload tokenizer to get around serialization issues,0
max_tokens = 500,0
model architecture,0
model architecture,0
download if necessary,0
load the model,0
"TODO: keep for backwards compatibility, but remove in future",1
save the sentence piece model as binary file (not as path which may change),0
write out the binary sentence piece model into the expected directory,0
"if the model was saved as binary and it is not found on disk, write to appropriate path",0
"otherwise, use normal process and potentially trigger another download",0
"once the modes if there, load it with sentence piece",0
empty words get no embedding,0
all other words get embedded,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn,0
GLOVE embeddings,0
"find train, dev and test files if not specified",0
get train data,0
read in test file if exists,0
read in dev file if exists,0
special key for space after,0
"store either Sentence objects in memory, or only file offsets",0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
determine encoding of text file,0
skip first line if to selected,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)",0
"tag_to_bioes=tag_to_bioes,",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Remove CoNLL-U meta information in the last column,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
rename according to train - test - dev - convention,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
dataset name,0
data folder: default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
data is not in IOB2 format. Thus we transform it to IOB2,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
this list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
get google drive id from list,0
download from google drive,0
unzip,0
"tar.extractall(language_folder,members=[tar.getmember(file_name)])",0
transform data into required format,0
"the processed dataset has the additional ending ""_new""",0
remove the unprocessed dataset,0
initialize comlumncorpus and add it to list,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"unzip the downloaded repo and merge the train, dev and test datasets",0
download data if necessary,0
unpack and write out in CoNLL column-like format,0
if no languages are given as argument all languages used in XTREME will be loaded,0
if only one language is given,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
This list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
download from HU Server,0
unzip,0
transform data into required format,0
initialize comlumncorpus and add it to list,0
"find train, dev and test files if not specified",0
use test_file to create test split if available,0
use dev_file to create test split if available,0
"if data point contains black-listed label, do not use",0
first check if valid sentence,0
"if so, add to indices",0
"find train, dev and test files if not specified",0
variables,0
different handling of in_memory data than streaming data,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
test if format is OK,0
test if at least one label given,0
noinspection PyDefaultArgument,0
dataset name includes the split size,0
default dataset folder is the cache root,0
download data if necessary,0
download each of the 28 splits,0
create dataset directory if necessary,0
download senteval datasets if necessary und unzip,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"by defaut, map point score to POSITIVE / NEGATIVE values",0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file from CSV,0
create test.txt file from CSV,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
convert to FastText format,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download datasets if necessary,0
create dataset directory if necessary,0
create correctly formated txt files,0
multiple labels are possible,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
check if dataset is supported,0
set file names,0
download and unzip in file structure if necessary,0
instantiate corpus,0
Uses dynamic programming approach to calculate maximum independent set in interval graph,0
with sum of all entity lengths as secondary key,0
calculate offset without current text,0
because we stick all passages of a document together,0
TODO For split entities we also annotate everything inbetween which might be a bad idea?,1
Try to fix incorrect annotations,0
print(,0
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}""",0
),0
Ignore empty lines or relation annotations,0
FIX annotation of whitespaces (necessary for PDR),0
One token may contain multiple entities -> deque all of them,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Create tokenization-dependent CONLL files. This is necessary to prevent,0
from caching issues (e.g. loading the same corpus with different sentence splitters),0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Edge case: last token starts a new entity,0
Last document in file,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
Edge case: last token starts a new entity,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download file is huge => make default_dir visible so that derivative,0
corpora can all use the same download file,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Read texts,0
Read annotations,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
We need to apply a patch to correct the original training file,0
Articles title,0
Article abstract,0
Entity annotations,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Edge case: last token starts a new entity,0
Map all entities to chemicals,0
Map all entities to disease,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Incomplete article,0
Invalid XML syntax,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
if len(mid) != 3:,0
continue,0
Try to fix entity offsets,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
There is still one illegal annotation in the file ..,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"Abstract first, title second to prevent issues with sentence splitting",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"Filter for specific entity types, by default no entities will be filtered",0
Get original HUNER splits to retrieve a list of all document ids contained in V2,0
train and dev split of V2 will be train in V4,0
test split of V2 will be dev in V4,0
New documents in V4 will become test documents,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
cache Feidegger config file,0
cache Feidegger images,0
replace image URL with local cached file,0
append Sentence-Image data point,0
"in certain cases, multi-CPU data loading makes no sense and slows",0
"everything down. For this reason, we detect if a dataset is in-memory:",0
"if so, num_workers is set to 0 for faster processing",0
cast to list if necessary,0
cast to list if necessary,0
"first, check if pymongo is installed",0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
Expose base classses,0
Expose all sequence labeling datasets,0
Expose all document classification datasets,0
Expose all treebanks,0
Expose all text-text datasets,0
Expose all text-image datasets,0
Expose all biomedical data sets,0
Expose all biomedical data sets using the HUNER splits,0
-,0
-,0
-,0
-,0
Expose all biomedical data sets used for the evaluation of BioBERT,0
"find train, dev and test files if not specified",0
get train data,0
get test data,0
get dev data,0
option 1: read only sentence boundaries as offset positions,0
option 2: keep everything in memory,0
"if in memory, retrieve parsed sentence",0
else skip to position in file where sentence begins,0
current token ID,0
handling for the awful UD multiword format,0
end of sentence,0
comments,0
ellipsis,0
if token is a multi-word,0
normal single-word tokens,0
"if we don't split multiwords, skip over component words",0
add token,0
add morphological tags,0
derive whitespace logic for multiwords,0
print(token),0
print(current_multiword_last_token),0
print(current_multiword_first_token),0
"if multi-word equals component tokens, there should be no whitespace",0
go through all tokens in subword and set whitespace_after information,0
print(i),0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
test corpus,0
create a TARS classifier,0
check if right number of classes,0
switch to task with only one label,0
check if right number of classes,0
switch to task with three labels provided as list,0
check if right number of classes,0
switch to task with four labels provided as set,0
check if right number of classes,0
switch to task with two labels provided as Dictionary,0
check if right number of classes,0
clean up file,0
bioes tags,0
bio tags,0
broken tags,0
all tags,0
all weird tags,0
tags with confidence,0
bioes tags,0
bioes tags,0
increment for last token in sentence if not followed by whitespace,0
clean up directory,0
clean up directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
def test_multiclass_metrics():,0
,0
"metric = Metric(""Test"")",0
"available_labels = [""A"", ""B"", ""C""]",0
,0
"predictions = [""A"", ""B""]",0
"true_values = [""A""]",0
TextClassifier._evaluate_sentence_for_text_classification(,0
"metric, available_labels, predictions, true_values",0
),0
,0
"predictions = [""C"", ""B""]",0
"true_values = [""A"", ""B""]",0
TextClassifier._evaluate_sentence_for_text_classification(,0
"metric, available_labels, predictions, true_values",0
),0
,0
print(metric),0
from flair.trainers.trainer_regression import RegressorTrainer,0
def test_trainer_results(tasks_base_path):,0
"corpus, model, trainer = init(tasks_base_path)",0
"results = trainer.train(""regression_train/"", max_epochs=1)",0
"assert results[""test_score""] > 0",0
"assert len(results[""dev_loss_history""]) == 1",0
"assert len(results[""dev_score_history""]) == 1",0
"assert len(results[""train_loss_history""]) == 1",0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
clean up results directory,0
define search space,0
sequence tagger parameter,0
model trainer parameter,0
training parameter,0
find best parameter settings,0
clean up results directory,0
document embeddings parameter,0
training parameter,0
clean up results directory,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
1. get the corpus,0
2. what tag do we want to predict?,0
3. make the tag dictionary from the corpus,0
initialize embeddings,0
comment in this line to use character embeddings,0
"CharacterEmbeddings(),",0
comment in these lines to use contextual string embeddings,0
,0
"FlairEmbeddings('news-forward'),",0
,0
"FlairEmbeddings('news-backward'),",0
initialize sequence tagger,0
initialize trainer,0
from allennlp.common.tqdm import Tqdm,0
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
Extract all the contents of zip file in current directory,0
Extract all the contents of zip file in current directory,0
get cache path to put the file,0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
first determine the distribution of classes in the dataset,0
weight for each sample,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups,1
see https://github.com/zalandoresearch/flair/issues/351,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
Maintains max of all exp. moving avg. of sq. grad. values,0
Decay the first and second moment running average coefficient,0
Maintains the maximum of all 2nd moment running avg. till now,0
Use the max. for normalizing running avg. of gradient,0
determine offsets for whitespace_after field,0
increment for last token in sentence if not followed by whitespace,0
determine offsets for whitespace_after field,0
conll 2000 column format,0
conll 03 NER column format,0
WNUT-17,0
-- WikiNER datasets,0
-- Universal Dependencies,0
Germanic,0
Romance,0
West-Slavic,0
South-Slavic,0
East-Slavic,0
Scandinavian,0
Asian,0
Language isolates,0
recent Universal Dependencies,0
other datasets,0
text classification format,0
text regression format,0
"first, try to fetch dataset online",0
default dataset folder is the cache root,0
get string value if enum is passed,0
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)",0
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag",0
the CoNLL 03 task for German has an additional lemma column,0
the CoNLL 03 task for Dutch has no NP column,0
the CoNLL 03 task for Spanish only has two columns,0
the GERMEVAL task only has two columns: text and ner,0
WSD tasks may be put into this column format,0
"the UD corpora follow the CoNLL-U format, for which we have a special reader",0
"for text classifiers, we use our own special format",0
NER corpus for Basque,0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
get train and test data,0
"read in test file if exists, otherwise sample 10% of train data as test dataset",0
"read in dev file if exists, otherwise sample 10% of train data as dev dataset",0
convert tag scheme to iobes,0
automatically identify train / test / dev files,0
automatically identify train / test / dev files,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
conll 2000 chunking task,0
Support both TREC-6 and TREC-50,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
Wikiner NER task,0
unpack and write out in CoNLL column-like format,0
CoNLL 02/03 NER,0
universal dependencies,0
--- UD Germanic,0
--- UD Romance,0
--- UD West-Slavic,0
--- UD Scandinavian,0
--- UD South-Slavic,0
--- UD Asian,0
this is the default init size of a lmdb database for embeddings,0
some non-used parameter to allow print,0
get db filename from embedding name,0
"In case initialization of cached version failed, just fallback to the original WordEmbeddings",0
SequenceTagger,0
TextClassifier,0
get db filename from embedding name,0
if embedding database already exists,0
"otherwise, push embedding to database",0
if embedding database already exists,0
open the database in read mode,0
we need to set self.k,0
create and load the database in write mode,0
"no idea why, but we need to close and reopen the environment to avoid",0
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot,0
when opening new transaction !,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
"We don't want to create a SpaceTokenizer object each time this function is called,",0
so delegate the call directly to the static run_tokenize method,0
"We don't want to create a SegtokTokenizer object each time this function is called,",0
so delegate the call directly to the static run_tokenize method,0
"if text is passed, instantiate sentence with tokens (words)",0
log a warning if the dataset is empty,0
data with zero-width characters cannot be handled,0
set token idx if not set,0
non-set tags are OUT tags,0
anything that is not a BIOES tag is a SINGLE tag,0
anything that is not OUT is IN,0
single and begin tags start a new span,0
remember previous tag,0
"if label type is explicitly specified, get spans for this label type",0
else determine all label types in sentence and get all spans,0
move sentence embeddings to device,0
move token embeddings to device,0
clear sentence embeddings,0
clear token embeddings,0
infer whitespace after field,0
add Sentence labels to output if they exist,0
add Token labels to output if they exist,0
add Sentence labels to output if they exist,0
add Token labels to output if they exist,0
No character at the corresponding code point: remove it,0
set name,0
sample test data if none is provided,0
sample dev data if none is provided,0
set train dev and test data,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
check if sentence itself has labels,0
check for labels of words,0
Make the tag dictionary,0
global variable: cache_root,0
global variable: device,0
global variable: embedding_storage_mode,0
# dummy return to fulfill trainer.train() needs,0
print(vec),0
Attach optimizer,0
"convert `metrics` to float, in case it's a zero-dim Tensor",0
if memory mode option 'none' delete everything,0
else delete only dynamic embeddings (otherwise autograd will keep everything in memory),0
find out which ones are dynamic embeddings,0
find out which ones are dynamic embeddings,0
memory management - option 1: send everything to CPU (pin to memory if we train on GPU),0
record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class),0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
plot i,0
save plots,0
save plots,0
plt.show(),0
save plot,0
take the average over the last three scores of training,0
take average over the scores from the different training runs,0
remove previous embeddings,0
clearing token embeddings to save memory,0
"read Dataset into data loader (if list of sentences passed, make Dataset first)",0
#TODO: not saving lines yet,1
== similarity measures ==,0
helper class for ModelSimilarity,0
-- works with binary cross entropy loss --,0
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}",0
-- works with ranking/triplet loss --,0
normalize the embeddings,0
== similarity losses ==,0
"we want that logits for corresponding pairs are high, and for non-corresponding low",0
TODO: this assumes eye matrix,0
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa",0
== similarity learner ==,0
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both",0
assumes that for each data pair there's at least one embedding per modality,0
pre-compute embeddings for all targets in evaluation dataset,0
compute the similarity,0
sort the similarity matrix across modality 1,0
"get the ranks, so +1 to start counting ranks from 1",0
The conversion from old model's constructor interface,0
auto-spawn on GPU if available,0
pad strings with whitespaces to longest sentence,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
set the dictionaries,0
"if we use a CRF, we must add special START and STOP tags to the dictionary",0
Initialize the weight tensor,0
initialize the network architecture,0
dropouts,0
optional reprojection layer on top of word embeddings,0
bidirectional LSTM on top of embedding layer,0
Create initial hidden state and initialize it,0
TODO: Decide how to initialize the hidden state variables,1
self.hs_initializer(self.lstm_init_h),0
self.hs_initializer(self.lstm_init_c),0
final linear map to tag space,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
clearing token embeddings to save memory,0
predict for batch,0
make list of gold tags,0
make list of predicted tags,0
"check for true positives, false positives and false negatives",0
also write to file in BIO format to use old conlleval script,0
check if in gold spans,0
check if in predicted spans,0
"read Dataset into data loader (if list of sentences passed, make Dataset first)",0
"if span F1 needs to be used, use separate eval method",0
"else, use scikit-learn to evaluate",0
predict for batch,0
add gold tag,0
add predicted tag,0
for file output,0
use sklearn,0
"make ""classification report""",0
get scores,0
line for log file,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
"if initial hidden state is trainable, use this state",0
word dropout only before LSTM - TODO: more experimentation needed,1
if self.use_word_dropout > 0.0:,0
sentence_tensor = self.word_dropout(sentence_tensor),0
get the tags in this sentence,0
add tags as tensor,0
pad tags if using batch-CRF decoder,0
reduce raw values to avoid NaN during exp,0
broadcasting will do the job of reshaping and is more efficient than calling repeat,0
default value,0
English NER models,0
Multilingual NER models,0
English POS models,0
Multilingual POS models,0
English SRL models,0
English chunking models,0
Danish models,0
German models,0
French models,0
Dutch models,0
Malayalam models,0
Portuguese models,0
Keyphase models,0
Biomedical models,0
the historical German taggers by the @redewiegergabe project,0
clear embeddings after predicting,0
load each model,0
check if the same embeddings were already loaded previously,0
"if the model uses StackedEmbedding, make a new stack with previous objects",0
sort embeddings by key alphabetically,0
check previous embeddings and add if found,0
only re-use static embeddings,0
"if not found, use existing embedding",0
initialize new stack,0
"of the model uses regular embedding, re-load if previous version found",0
Initialize the weight tensor,0
auto-spawn on GPU if available,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
"read Dataset into data loader (if list of sentences passed, make Dataset first)",0
use scikit-learn to evaluate,0
remove previously predicted labels,0
get the gold labels,0
predict for batch,0
get the predicted labels,0
remove predicted labels,0
"make ""classification report""",0
get scores,0
line for log file,0
English sentiment models,0
Communicative Functions Model,0
cast string to Path,0
"determine what splits (train, dev, test) to evaluate and log",0
prepare loss logging file and set up header,0
"minimize training loss if training with dev data, else maximize dev score",0
"if training also uses dev data, include in training set",0
initialize sampler if provided,0
init with default values if only class is provided,0
set dataset to sample from,0
At any point you can hit Ctrl + C to break out of training early.,0
get new learning rate,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
process mini-batches,0
zero the gradients on the model and optimizer,0
"if necessary, make batch_steps",0
forward and backward for batch,0
forward pass,0
Backward,0
do the optimizer step,0
do the scheduler step if one-cycle,0
get new learning rate,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"anneal against train loss if training with dev, otherwise anneal against dev score",0
evaluate on train / dev / test split depending on training settings,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
calculate scores using dev data if available,0
append dev score to score history,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
determine learning rate annealing through scheduler. Use auxiliary metric for AnnealOnPlateau,0
determine bad epoch number,0
log bad epochs,0
output log file,0
make headers on first epoch,0
"if checkpoint is enabled, save model at each epoch",0
"if we use dev data, remember best model based on dev evaluation score",0
"if we do not use dev data for model selection, save final model",0
test best model if test data is present,0
"if we are training over multiple datasets, do evaluation for each",0
get and return the final test score of best model,0
cast string to Path,0
forward pass,0
update optimizer and scheduler,0
Add chars to the dictionary,0
charsplit file content,0
charsplit file content,0
Add words to the dictionary,0
Tokenize file content,0
"TextDataset returns a list. valid and test are only one file, so return the first element",0
cast string to Path,0
error message if the validation dataset is too small,0
Shuffle training files randomly after serially iterating through corpus one,0
"iterate through training data, starting at self.split (for checkpointing)",0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
Backward,0
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.,0
We detach the hidden state from how it was previously produced.,0
"If we didn't, the model would try backpropagating all the way to start of the dataset.",0
explicitly remove loss to clear up memory,0
##############################################################################,0
Save the model if the validation loss is the best we've seen so far.,0
##############################################################################,0
print info,0
##############################################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
1-camembert-base -> camembert-base,0
1-xlm-roberta-large -> xlm-roberta-large,0
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol,0
The mask has 1 for real tokens and 0 for padding tokens. Only real,0
tokens are attended to.,0
Zero-pad up to the sequence length.,0
"first, find longest sentence in batch",0
prepare id maps for BERT model,0
put encoded batch through BERT model to get all hidden states of all encoder layers,0
get aggregated embeddings for each BERT-subtoken in sentence,0
get the current sentence object,0
add concatenated embedding to sentence,0
use first subword embedding if pooling operation is 'first',0
"otherwise, do a mean over all subwords in token",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
embed a dummy sentence to determine embedding_length,0
Avoid conflicts with flair's Token class,0
"<cls> token initially set to 1/D, so it attends to all image features equally",0
add positional encodings,0
reshape the pixels into the sequence,0
layer norm after convolution and positional encodings,0
add <cls> token,0
"transformer requires input in the shape [h*w+1, b, d]",0
the output is an embedding of <cls> token,0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
load tokenizer and transformer model,0
model name,0
"when initializing, embeddings are in eval mode by default",0
embedding parameters,0
send mini-token through to check how many layers the model has,0
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial",0
using list comprehension,0
gradients are enabled if fine-tuning is enabled,0
"first, subtokenize each sentence and find out into how many subtokens each token was divided",0
subtokenize sentences,0
tokenize and truncate to max subtokens (TODO: check better truncation strategies),1
find longest sentence in batch,0
initialize batch tensors and mask,0
put encoded batch through transformer model to get all hidden states of all encoder layers,0
iterate over all subtokenized sentences,0
use scalar mix of embeddings if so selected,0
set the extracted embedding for the token,0
reload tokenizer to get around serialization issues,0
optional fine-tuning on top of embedding layer,0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push through RNN,0
after-RNN dropout,0
extract embeddings from RNN,0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
IMPORTANT: add embeddings as torch modules,0
iterate over sentences,0
"if its a forward LM, take last state",0
"convert to plain strings, embedded in a list for the encode function",0
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency",0
"if only one sentence is passed, convert to list of sentence",0
Expose base classses,0
Expose token embedding classes,0
Expose document embedding classes,0
Expose image embedding classes,0
Expose legacy embedding classes,0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNINOS embeddings,0
pubmed embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
fix serialized models,0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
multilingual models,0
English models,0
Arabic,0
Bulgarian,0
Czech,0
Danish,0
German,0
Spanish,0
Basque,0
Persian,0
Finnish,0
French,0
Hebrew,0
Hindi,0
Croatian,0
Indonesian,0
Italian,0
Japanese,0
Malayalam,0
Dutch,0
Norwegian,0
Polish,0
Portuguese,0
Pubmed,0
Slovenian,0
Swedish,0
Tamil,0
CLEF HIPE Shared task,0
load model if in pretrained model map,0
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir),0
embeddings are static if we don't do finetuning,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
make compatible with serialized models (TODO: remove),1
make compatible with serialized models (TODO: remove),1
gradients are enable if fine-tuning is enabled,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
offset mode that extracts at whitespace after last character,0
offset mode that extracts at last character,0
only clone if optimization mode is 'gpu',0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
check token.text is empty or not,0
set aggregation operation,0
add embeddings after updating,0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
load tokenizer and transformer model,0
model name,0
"when initializing, embeddings are in eval mode by default",0
embedding parameters,0
send mini-token through to check how many layers the model has,0
"self.mix = ScalarMix(mixture_size=len(self.layer_indexes), trainable=False)",0
check if special tokens exist to circumvent error message,0
"most models have an intial BOS token, except for XLNet, T5 and GPT2",0
split into micro batches of size self.batch_size before pushing through transformer,0
embed each micro-batch,0
remove special markup,0
"first, subtokenize each sentence and find out into how many subtokens each token was divided",0
"TODO: keep for backwards compatibility, but remove in future",1
"some pretrained models do not have this property, applying default settings now.",0
can be set manually after loading the model.,0
method 1: subtokenize sentence,0
"subtokenized_sentence = self.tokenizer.encode(tokenized_string, add_special_tokens=True)",0
method 2:,0
transformer specific tokenization,0
empty sentences get zero embeddings,0
only embed non-empty sentences and if there is at least one,0
find longest sentence in batch,0
initialize batch tensors and mask,0
put encoded batch through transformer model to get all hidden states of all encoder layers,0
make the tuple a tensor; makes working with it easier.,0
gradients are enabled if fine-tuning is enabled,0
iterate over all subtokenized sentences,0
"remove stride_size//2 at end of sentence_hidden_state, and half at beginning of remainder,",0
in order to get some context into the embeddings of these words.,0
also don't include the embedding of the extra [CLS] and [SEP] tokens.,0
"for each token, get embedding",0
some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector,0
"get states from all selected layers, aggregate with pooling operation",0
use scalar mix of embeddings if so selected,0
sm_embeddings = self.mix(subtoken_embeddings),0
set the extracted embedding for the token,0
iterate over subtokens and reconstruct tokens,0
remove special markup,0
TODO check if this is necessary is this method is called before prepare_for_model,1
check if reconstructed token is special begin token ([CLS] or similar),0
some BERT tokenizers somehow omit words - in such cases skip to next token,0
append subtoken to reconstruct token,0
check if reconstructed token is the same as current token,0
"if so, add subtoken count",0
reset subtoken count and reconstructed token,0
break from loop if all tokens are accounted for,0
if tokens are unaccounted for,0
check if all tokens were matched to subtokens,0
"if fine-tuning is not enabled (i.e. a ""feature-based approach"" used), this",0
module should never be in training mode,0
reload tokenizer to get around serialization issues,0
max_tokens = 500,0
model architecture,0
model architecture,0
download if necessary,0
load the model,0
"TODO: keep for backwards compatibility, but remove in future",1
save the sentence piece model as binary file (not as path which may change),0
write out the binary sentence piece model into the expected directory,0
"if the model was saved as binary and it is not found on disk, write to appropriate path",0
"otherwise, use normal process and potentially trigger another download",0
"once the modes if there, load it with sentence piece",0
empty words get no embedding,0
all other words get embedded,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn,0
GLOVE embeddings,0
"find train, dev and test files if not specified",0
get train data,0
read in test file if exists,0
read in dev file if exists,0
special key for space after,0
"store either Sentence objects in memory, or only file offsets",0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
determine encoding of text file,0
skip first line if to selected,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
if no languages are given as argument all languages used in XTREME will be loaded,0
if only one language is given,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
This list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
download from HU Server,0
unzip,0
transform data into required format,0
initialize comlumncorpus and add it to list,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"For each language in languages, the file is downloaded if not existent",0
Then a comlumncorpus of that data is created and saved in a list,0
this list is handed to the multicorpus,0
list that contains the columncopora,0
download data if necessary,0
"if language not downloaded yet, download it",0
create folder,0
get google drive id from list,0
download from google drive,0
unzip,0
"tar.extractall(language_folder,members=[tar.getmember(file_name)])",0
transform data into required format,0
"the processed dataset has the additional ending ""_new""",0
remove the unprocessed dataset,0
initialize comlumncorpus and add it to list,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Remove CoNLL-U meta information in the last column,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
rename according to train - test - dev - convention,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
data is not in IOB2 format. Thus we transform it to IOB2,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download data if necessary,0
unpack and write out in CoNLL column-like format,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"find train, dev and test files if not specified",0
use test_file to create test split if available,0
use dev_file to create test split if available,0
"if data point contains black-listed label, do not use",0
first check if valid sentence,0
"if so, add to indices",0
"find train, dev and test files if not specified",0
variables,0
different handling of in_memory data than streaming data,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
test if format is OK,0
test if at least one label given,0
noinspection PyDefaultArgument,0
dataset name includes the split size,0
default dataset folder is the cache root,0
download data if necessary,0
download each of the 28 splits,0
create dataset directory if necessary,0
download senteval datasets if necessary und unzip,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"by defaut, map point score to POSITIVE / NEGATIVE values",0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file from CSV,0
create test.txt file from CSV,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
convert to FastText format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
check if dataset is supported,0
set file names,0
download and unzip in file structure if necessary,0
instantiate corpus,0
Uses dynamic programming approach to calculate maximum independent set in interval graph,0
with sum of all entity lengths as secondary key,0
calculate offset without current text,0
because we stick all passages of a document together,0
TODO For split entities we also annotate everything inbetween which might be a bad idea?,1
Try to fix incorrect annotations,0
print(,0
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}""",0
),0
Ignore empty lines or relation annotations,0
FIX annotation of whitespaces (necessary for PDR),0
One token may contain multiple entities -> deque all of them,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Create tokenization-dependent CONLL files. This is necessary to prevent,0
from caching issues (e.g. loading the same corpus with different sentence splitters),0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Edge case: last token starts a new entity,0
Last document in file,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
Edge case: last token starts a new entity,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download file is huge => make default_dir visible so that derivative,0
corpora can all use the same download file,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Read texts,0
Read annotations,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
We need to apply a patch to correct the original training file,0
Articles title,0
Article abstract,0
Entity annotations,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Edge case: last token starts a new entity,0
Map all entities to chemicals,0
Map all entities to disease,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Incomplete article,0
Invalid XML syntax,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
if len(mid) != 3:,0
continue,0
Try to fix entity offsets,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
There is still one illegal annotation in the file ..,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"Abstract first, title second to prevent issues with sentence splitting",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"Filter for specific entity types, by default no entities will be filtered",0
Get original HUNER splits to retrieve a list of all document ids contained in V2,0
train and dev split of V2 will be train in V4,0
test split of V2 will be dev in V4,0
New documents in V4 will become test documents,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
cache Feidegger config file,0
cache Feidegger images,0
replace image URL with local cached file,0
append Sentence-Image data point,0
"in certain cases, multi-CPU data loading makes no sense and slows",0
"everything down. For this reason, we detect if a dataset is in-memory:",0
"if so, num_workers is set to 0 for faster processing",0
cast to list if necessary,0
cast to list if necessary,0
"first, check if pymongo is installed",0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
Expose base classses,0
Expose all sequence labeling datasets,0
Expose all document classification datasets,0
Expose all treebanks,0
Expose all text-text datasets,0
Expose all text-image datasets,0
Expose all biomedical data sets,0
Expose all biomedical data sets using the HUNER splits,0
-,0
-,0
-,0
-,0
Expose all biomedical data sets used for the evaluation of BioBERT,0
"find train, dev and test files if not specified",0
get train data,0
get test data,0
get dev data,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
clean up file,0
bioes tags,0
bio tags,0
broken tags,0
all tags,0
all weird tags,0
tags with confidence,0
bioes tags,0
bioes tags,0
increment for last token in sentence if not followed by whitespace,0
clean up directory,0
clean up directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
def test_multiclass_metrics():,0
,0
"metric = Metric(""Test"")",0
"available_labels = [""A"", ""B"", ""C""]",0
,0
"predictions = [""A"", ""B""]",0
"true_values = [""A""]",0
TextClassifier._evaluate_sentence_for_text_classification(,0
"metric, available_labels, predictions, true_values",0
),0
,0
"predictions = [""C"", ""B""]",0
"true_values = [""A"", ""B""]",0
TextClassifier._evaluate_sentence_for_text_classification(,0
"metric, available_labels, predictions, true_values",0
),0
,0
print(metric),0
from flair.trainers.trainer_regression import RegressorTrainer,0
def test_trainer_results(tasks_base_path):,0
"corpus, model, trainer = init(tasks_base_path)",0
"results = trainer.train(""regression_train/"", max_epochs=1)",0
"assert results[""test_score""] > 0",0
"assert len(results[""dev_loss_history""]) == 1",0
"assert len(results[""dev_score_history""]) == 1",0
"assert len(results[""train_loss_history""]) == 1",0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
clean up results directory,0
define search space,0
sequence tagger parameter,0
model trainer parameter,0
training parameter,0
find best parameter settings,0
clean up results directory,0
document embeddings parameter,0
training parameter,0
clean up results directory,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
1. get the corpus,0
2. what tag do we want to predict?,0
3. make the tag dictionary from the corpus,0
initialize embeddings,0
comment in this line to use character embeddings,0
"CharacterEmbeddings(),",0
comment in these lines to use contextual string embeddings,0
,0
"FlairEmbeddings('news-forward'),",0
,0
"FlairEmbeddings('news-backward'),",0
initialize sequence tagger,0
initialize trainer,0
from allennlp.common.tqdm import Tqdm,0
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
Extract all the contents of zip file in current directory,0
Extract all the contents of zip file in current directory,0
get cache path to put the file,0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
first determine the distribution of classes in the dataset,0
weight for each sample,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups,1
see https://github.com/zalandoresearch/flair/issues/351,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
Maintains max of all exp. moving avg. of sq. grad. values,0
Decay the first and second moment running average coefficient,0
Maintains the maximum of all 2nd moment running avg. till now,0
Use the max. for normalizing running avg. of gradient,0
determine offsets for whitespace_after field,0
increment for last token in sentence if not followed by whitespace,0
determine offsets for whitespace_after field,0
conll 2000 column format,0
conll 03 NER column format,0
WNUT-17,0
-- WikiNER datasets,0
-- Universal Dependencies,0
Germanic,0
Romance,0
West-Slavic,0
South-Slavic,0
East-Slavic,0
Scandinavian,0
Asian,0
Language isolates,0
recent Universal Dependencies,0
other datasets,0
text classification format,0
text regression format,0
"first, try to fetch dataset online",0
default dataset folder is the cache root,0
get string value if enum is passed,0
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)",0
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag",0
the CoNLL 03 task for German has an additional lemma column,0
the CoNLL 03 task for Dutch has no NP column,0
the CoNLL 03 task for Spanish only has two columns,0
the GERMEVAL task only has two columns: text and ner,0
WSD tasks may be put into this column format,0
"the UD corpora follow the CoNLL-U format, for which we have a special reader",0
"for text classifiers, we use our own special format",0
NER corpus for Basque,0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
get train and test data,0
"read in test file if exists, otherwise sample 10% of train data as test dataset",0
"read in dev file if exists, otherwise sample 10% of train data as dev dataset",0
convert tag scheme to iobes,0
automatically identify train / test / dev files,0
automatically identify train / test / dev files,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
conll 2000 chunking task,0
Support both TREC-6 and TREC-50,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
Wikiner NER task,0
unpack and write out in CoNLL column-like format,0
CoNLL 02/03 NER,0
universal dependencies,0
--- UD Germanic,0
--- UD Romance,0
--- UD West-Slavic,0
--- UD Scandinavian,0
--- UD South-Slavic,0
--- UD Asian,0
this is the default init size of a lmdb database for embeddings,0
some non-used parameter to allow print,0
get db filename from embedding name,0
"In case initialization of cached version failed, just fallback to the original WordEmbeddings",0
SequenceTagger,0
TextClassifier,0
get db filename from embedding name,0
if embedding database already exists,0
"otherwise, push embedding to database",0
if embedding database already exists,0
open the database in read mode,0
we need to set self.k,0
create and load the database in write mode,0
"no idea why, but we need to close and reopen the environment to avoid",0
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot,0
when opening new transaction !,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
"We don't want to create a SpaceTokenizer object each time this function is called,",0
so delegate the call directly to the static run_tokenize method,0
"We don't want to create a SegtokTokenizer object each time this function is called,",0
so delegate the call directly to the static run_tokenize method,0
"if text is passed, instantiate sentence with tokens (words)",0
log a warning if the dataset is empty,0
data with zero-width characters cannot be handled,0
set token idx if not set,0
non-set tags are OUT tags,0
anything that is not a BIOES tag is a SINGLE tag,0
anything that is not OUT is IN,0
single and begin tags start a new span,0
remember previous tag,0
"if label type is explicitly specified, get spans for this label type",0
else determine all label types in sentence and get all spans,0
move sentence embeddings to device,0
move token embeddings to device,0
clear sentence embeddings,0
clear token embeddings,0
infer whitespace after field,0
add Sentence labels to output if they exist,0
add Token labels to output if they exist,0
add Sentence labels to output if they exist,0
add Token labels to output if they exist,0
No character at the corresponding code point: remove it,0
set name,0
sample test data if none is provided,0
sample dev data if none is provided,0
set train dev and test data,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
check if sentence itself has labels,0
check for labels of words,0
Make the tag dictionary,0
global variable: cache_root,0
global variable: device,0
global variable: embedding_storage_mode,0
# dummy return to fulfill trainer.train() needs,0
print(vec),0
Attach optimizer,0
"convert `metrics` to float, in case it's a zero-dim Tensor",0
if memory mode option 'none' delete everything,0
else delete only dynamic embeddings (otherwise autograd will keep everything in memory),0
find out which ones are dynamic embeddings,0
find out which ones are dynamic embeddings,0
memory management - option 1: send everything to CPU (pin to memory if we train on GPU),0
record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class),0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
plot i,0
save plots,0
save plots,0
plt.show(),0
save plot,0
take the average over the last three scores of training,0
take average over the scores from the different training runs,0
remove previous embeddings,0
clearing token embeddings to save memory,0
"read Dataset into data loader (if list of sentences passed, make Dataset first)",0
#TODO: not saving lines yet,1
== similarity measures ==,0
helper class for ModelSimilarity,0
-- works with binary cross entropy loss --,0
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}",0
-- works with ranking/triplet loss --,0
normalize the embeddings,0
== similarity losses ==,0
"we want that logits for corresponding pairs are high, and for non-corresponding low",0
TODO: this assumes eye matrix,0
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa",0
== similarity learner ==,0
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both",0
assumes that for each data pair there's at least one embedding per modality,0
pre-compute embeddings for all targets in evaluation dataset,0
compute the similarity,0
sort the similarity matrix across modality 1,0
"get the ranks, so +1 to start counting ranks from 1",0
The conversion from old model's constructor interface,0
auto-spawn on GPU if available,0
pad strings with whitespaces to longest sentence,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
set the dictionaries,0
"if we use a CRF, we must add special START and STOP tags to the dictionary",0
Initialize the weight tensor,0
initialize the network architecture,0
dropouts,0
optional reprojection layer on top of word embeddings,0
bidirectional LSTM on top of embedding layer,0
Create initial hidden state and initialize it,0
TODO: Decide how to initialize the hidden state variables,1
self.hs_initializer(self.lstm_init_h),0
self.hs_initializer(self.lstm_init_c),0
final linear map to tag space,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
clearing token embeddings to save memory,0
predict for batch,0
make list of gold tags,0
make list of predicted tags,0
"check for true positives, false positives and false negatives",0
also write to file in BIO format to use old conlleval script,0
check if in gold spans,0
check if in predicted spans,0
"read Dataset into data loader (if list of sentences passed, make Dataset first)",0
"if span F1 needs to be used, use separate eval method",0
"else, use scikit-learn to evaluate",0
predict for batch,0
add gold tag,0
add predicted tag,0
for file output,0
use sklearn,0
"make ""classification report""",0
get scores,0
line for log file,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
"if initial hidden state is trainable, use this state",0
word dropout only before LSTM - TODO: more experimentation needed,1
if self.use_word_dropout > 0.0:,0
sentence_tensor = self.word_dropout(sentence_tensor),0
get the tags in this sentence,0
add tags as tensor,0
pad tags if using batch-CRF decoder,0
reduce raw values to avoid NaN during exp,0
broadcasting will do the job of reshaping and is more efficient than calling repeat,0
default value,0
the historical German taggers by the @redewiegergabe project,0
clear embeddings after predicting,0
load each model,0
check if the same embeddings were already loaded previously,0
"if the model uses StackedEmbedding, make a new stack with previous objects",0
sort embeddings by key alphabetically,0
check previous embeddings and add if found,0
only re-use static embeddings,0
"if not found, use existing embedding",0
initialize new stack,0
"of the model uses regular embedding, re-load if previous version found",0
Initialize the weight tensor,0
auto-spawn on GPU if available,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
"read Dataset into data loader (if list of sentences passed, make Dataset first)",0
use scikit-learn to evaluate,0
remove previously predicted labels,0
get the gold labels,0
predict for batch,0
get the predicted labels,0
remove predicted labels,0
"make ""classification report""",0
get scores,0
line for log file,0
English sentiment models,0
Communicative Functions Model,0
cast string to Path,0
"determine what splits (train, dev, test) to evaluate and log",0
prepare loss logging file and set up header,0
"minimize training loss if training with dev data, else maximize dev score",0
"if training also uses dev data, include in training set",0
initialize sampler if provided,0
init with default values if only class is provided,0
set dataset to sample from,0
At any point you can hit Ctrl + C to break out of training early.,0
get new learning rate,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
process mini-batches,0
zero the gradients on the model and optimizer,0
"if necessary, make batch_steps",0
forward and backward for batch,0
forward pass,0
Backward,0
do the optimizer step,0
do the scheduler step if one-cycle,0
get new learning rate,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"anneal against train loss if training with dev, otherwise anneal against dev score",0
evaluate on train / dev / test split depending on training settings,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
calculate scores using dev data if available,0
append dev score to score history,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
determine learning rate annealing through scheduler. Use auxiliary metric for AnnealOnPlateau,0
determine bad epoch number,0
log bad epochs,0
output log file,0
make headers on first epoch,0
"if checkpoint is enabled, save model at each epoch",0
"if we use dev data, remember best model based on dev evaluation score",0
"if we do not use dev data for model selection, save final model",0
test best model if test data is present,0
"if we are training over multiple datasets, do evaluation for each",0
get and return the final test score of best model,0
cast string to Path,0
forward pass,0
update optimizer and scheduler,0
Add chars to the dictionary,0
charsplit file content,0
charsplit file content,0
Add words to the dictionary,0
Tokenize file content,0
"TextDataset returns a list. valid and test are only one file, so return the first element",0
cast string to Path,0
error message if the validation dataset is too small,0
Shuffle training files randomly after serially iterating through corpus one,0
"iterate through training data, starting at self.split (for checkpointing)",0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
Backward,0
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.,0
We detach the hidden state from how it was previously produced.,0
"If we didn't, the model would try backpropagating all the way to start of the dataset.",0
explicitly remove loss to clear up memory,0
##############################################################################,0
Save the model if the validation loss is the best we've seen so far.,0
##############################################################################,0
print info,0
##############################################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
1-camembert-base -> camembert-base,0
1-xlm-roberta-large -> xlm-roberta-large,0
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol,0
The mask has 1 for real tokens and 0 for padding tokens. Only real,0
tokens are attended to.,0
Zero-pad up to the sequence length.,0
"first, find longest sentence in batch",0
prepare id maps for BERT model,0
put encoded batch through BERT model to get all hidden states of all encoder layers,0
get aggregated embeddings for each BERT-subtoken in sentence,0
get the current sentence object,0
add concatenated embedding to sentence,0
use first subword embedding if pooling operation is 'first',0
"otherwise, do a mean over all subwords in token",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
embed a dummy sentence to determine embedding_length,0
Avoid conflicts with flair's Token class,0
"<cls> token initially set to 1/D, so it attends to all image features equally",0
add positional encodings,0
reshape the pixels into the sequence,0
layer norm after convolution and positional encodings,0
add <cls> token,0
"transformer requires input in the shape [h*w+1, b, d]",0
the output is an embedding of <cls> token,0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
load tokenizer and transformer model,0
model name,0
"when initializing, embeddings are in eval mode by default",0
embedding parameters,0
send mini-token through to check how many layers the model has,0
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial",0
using list comprehension,0
gradients are enabled if fine-tuning is enabled,0
"first, subtokenize each sentence and find out into how many subtokens each token was divided",0
subtokenize sentences,0
tokenize and truncate to max subtokens (TODO: check better truncation strategies),1
find longest sentence in batch,0
initialize batch tensors and mask,0
put encoded batch through transformer model to get all hidden states of all encoder layers,0
iterate over all subtokenized sentences,0
use scalar mix of embeddings if so selected,0
set the extracted embedding for the token,0
reload tokenizer to get around serialization issues,0
optional fine-tuning on top of embedding layer,0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push through RNN,0
after-RNN dropout,0
extract embeddings from RNN,0
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute,0
"check if this is the case and if so, set it",0
IMPORTANT: add embeddings as torch modules,0
iterate over sentences,0
"if its a forward LM, take last state",0
"convert to plain strings, embedded in a list for the encode function",0
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency",0
"if only one sentence is passed, convert to list of sentence",0
Expose base classses,0
Expose token embedding classes,0
Expose document embedding classes,0
Expose image embedding classes,0
Expose legacy embedding classes,0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNINOS embeddings,0
pubmed embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
fix serialized models,0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
multilingual models,0
English models,0
Arabic,0
Bulgarian,0
Czech,0
Danish,0
German,0
Spanish,0
Basque,0
Persian,0
Finnish,0
French,0
Hebrew,0
Hindi,0
Croatian,0
Indonesian,0
Italian,0
Japanese,0
Malayalam,0
Dutch,0
Norwegian,0
Polish,0
Portuguese,0
Pubmed,0
Slovenian,0
Swedish,0
Tamil,0
CLEF HIPE Shared task,0
load model if in pretrained model map,0
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir),0
embeddings are static if we don't do finetuning,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
make compatible with serialized models (TODO: remove),1
make compatible with serialized models (TODO: remove),1
gradients are enable if fine-tuning is enabled,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
offset mode that extracts at whitespace after last character,0
offset mode that extracts at last character,0
only clone if optimization mode is 'gpu',0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
check token.text is empty or not,0
set aggregation operation,0
add embeddings after updating,0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
load tokenizer and transformer model,0
model name,0
"when initializing, embeddings are in eval mode by default",0
embedding parameters,0
send mini-token through to check how many layers the model has,0
"self.mix = ScalarMix(mixture_size=len(self.layer_indexes), trainable=False)",0
check if special tokens exist to circumvent error message,0
"most models have an intial BOS token, except for XLNet, T5 and GPT2",0
split into micro batches of size self.batch_size before pushing through transformer,0
embed each micro-batch,0
remove special markup,0
"first, subtokenize each sentence and find out into how many subtokens each token was divided",0
"TODO: keep for backwards compatibility, but remove in future",1
"some pretrained models do not have this property, applying default settings now.",0
can be set manually after loading the model.,0
method 1: subtokenize sentence,0
"subtokenized_sentence = self.tokenizer.encode(tokenized_string, add_special_tokens=True)",0
method 2:,0
transformer specific tokenization,0
empty sentences get zero embeddings,0
only embed non-empty sentences and if there is at least one,0
find longest sentence in batch,0
initialize batch tensors and mask,0
put encoded batch through transformer model to get all hidden states of all encoder layers,0
make the tuple a tensor; makes working with it easier.,0
gradients are enabled if fine-tuning is enabled,0
iterate over all subtokenized sentences,0
"remove stride_size//2 at end of sentence_hidden_state, and half at beginning of remainder,",0
in order to get some context into the embeddings of these words.,0
also don't include the embedding of the extra [CLS] and [SEP] tokens.,0
"for each token, get embedding",0
some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector,0
"get states from all selected layers, aggregate with pooling operation",0
use scalar mix of embeddings if so selected,0
sm_embeddings = self.mix(subtoken_embeddings),0
set the extracted embedding for the token,0
iterate over subtokens and reconstruct tokens,0
remove special markup,0
TODO check if this is necessary is this method is called before prepare_for_model,1
check if reconstructed token is special begin token ([CLS] or similar),0
some BERT tokenizers somehow omit words - in such cases skip to next token,0
append subtoken to reconstruct token,0
check if reconstructed token is the same as current token,0
"if so, add subtoken count",0
reset subtoken count and reconstructed token,0
break from loop if all tokens are accounted for,0
if tokens are unaccounted for,0
check if all tokens were matched to subtokens,0
"if fine-tuning is not enabled (i.e. a ""feature-based approach"" used), this",0
module should never be in training mode,0
reload tokenizer to get around serialization issues,0
max_tokens = 500,0
model architecture,0
model architecture,0
download if necessary,0
load the model,0
"TODO: keep for backwards compatibility, but remove in future",1
save the sentence piece model as binary file (not as path which may change),0
write out the binary sentence piece model into the expected directory,0
"if the model was saved as binary and it is not found on disk, write to appropriate path",0
"otherwise, use normal process and potentially trigger another download",0
"once the modes if there, load it with sentence piece",0
empty words get no embedding,0
all other words get embedded,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn,0
GLOVE embeddings,0
"find train, dev and test files if not specified",0
get train data,0
read in test file if exists,0
read in dev file if exists,0
special key for space after,0
"store either Sentence objects in memory, or only file offsets",0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
determine encoding of text file,0
skip first line if to selected,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Remove CoNLL-U meta information in the last column,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
rename according to train - test - dev - convention,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
data is not in IOB2 format. Thus we transform it to IOB2,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download data if necessary,0
unpack and write out in CoNLL column-like format,0
"find train, dev and test files if not specified",0
use test_file to create test split if available,0
use dev_file to create test split if available,0
"if data point contains black-listed label, do not use",0
first check if valid sentence,0
"if so, add to indices",0
"find train, dev and test files if not specified",0
variables,0
different handling of in_memory data than streaming data,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
test if format is OK,0
test if at least one label given,0
noinspection PyDefaultArgument,0
dataset name includes the split size,0
default dataset folder is the cache root,0
download data if necessary,0
download each of the 28 splits,0
create dataset directory if necessary,0
download senteval datasets if necessary und unzip,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"by defaut, map point score to POSITIVE / NEGATIVE values",0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file from CSV,0
create test.txt file from CSV,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
convert to FastText format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
check if dataset is supported,0
set file names,0
download and unzip in file structure if necessary,0
instantiate corpus,0
Uses dynamic programming approach to calculate maximum independent set in interval graph,0
with sum of all entity lengths as secondary key,0
calculate offset without current text,0
because we stick all passages of a document together,0
TODO For split entities we also annotate everything inbetween which might be a bad idea?,1
Try to fix incorrect annotations,0
print(,0
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}""",0
),0
Ignore empty lines or relation annotations,0
FIX annotation of whitespaces (necessary for PDR),0
One token may contain multiple entities -> deque all of them,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Create tokenization-dependent CONLL files. This is necessary to prevent,0
from caching issues (e.g. loading the same corpus with different sentence splitters),0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Edge case: last token starts a new entity,0
Last document in file,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
In the huner split files there is no information whether a given id originates,0
from the train or test file of the original corpus - so we have to adapt corpus,0
splitting here,0
Edge case: last token starts a new entity,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download file is huge => make default_dir visible so that derivative,0
corpora can all use the same download file,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Read texts,0
Read annotations,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
We need to apply a patch to correct the original training file,0
Articles title,0
Article abstract,0
Entity annotations,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Edge case: last token starts a new entity,0
Map all entities to chemicals,0
Map all entities to disease,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
Incomplete article,0
Invalid XML syntax,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
if len(mid) != 3:,0
continue,0
Try to fix entity offsets,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
There is still one illegal annotation in the file ..,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"Abstract first, title second to prevent issues with sentence splitting",0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
"Filter for specific entity types, by default no entities will be filtered",0
Get original HUNER splits to retrieve a list of all document ids contained in V2,0
train and dev split of V2 will be train in V4,0
test split of V2 will be dev in V4,0
New documents in V4 will become test documents,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
cache Feidegger config file,0
cache Feidegger images,0
replace image URL with local cached file,0
append Sentence-Image data point,0
"in certain cases, multi-CPU data loading makes no sense and slows",0
"everything down. For this reason, we detect if a dataset is in-memory:",0
"if so, num_workers is set to 0 for faster processing",0
cast to list if necessary,0
cast to list if necessary,0
"first, check if pymongo is installed",0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
Expose base classses,0
Expose all sequence labeling datasets,0
Expose all document classification datasets,0
Expose all treebanks,0
Expose all text-text datasets,0
Expose all text-image datasets,0
Expose all biomedical data sets,0
Expose all biomedical data sets using the HUNER splits,0
-,0
-,0
-,0
-,0
Expose all biomedical data sets used for the evaluation of BioBERT,0
"find train, dev and test files if not specified",0
get train data,0
get test data,0
get dev data,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
clean up file,0
bioes tags,0
bio tags,0
broken tags,0
all tags,0
all weird tags,0
tags with confidence,0
bioes tags,0
bioes tags,0
increment for last token in sentence if not followed by whitespace,0
clean up directory,0
clean up directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
def test_multiclass_metrics():,0
,0
"metric = Metric(""Test"")",0
"available_labels = [""A"", ""B"", ""C""]",0
,0
"predictions = [""A"", ""B""]",0
"true_values = [""A""]",0
TextClassifier._evaluate_sentence_for_text_classification(,0
"metric, available_labels, predictions, true_values",0
),0
,0
"predictions = [""C"", ""B""]",0
"true_values = [""A"", ""B""]",0
TextClassifier._evaluate_sentence_for_text_classification(,0
"metric, available_labels, predictions, true_values",0
),0
,0
print(metric),0
from flair.trainers.trainer_regression import RegressorTrainer,0
def test_trainer_results(tasks_base_path):,0
"corpus, model, trainer = init(tasks_base_path)",0
"results = trainer.train(""regression_train/"", max_epochs=1)",0
"assert results[""test_score""] > 0",0
"assert len(results[""dev_loss_history""]) == 1",0
"assert len(results[""dev_score_history""]) == 1",0
"assert len(results[""train_loss_history""]) == 1",0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
clean up results directory,0
define search space,0
sequence tagger parameter,0
model trainer parameter,0
training parameter,0
find best parameter settings,0
clean up results directory,0
document embeddings parameter,0
training parameter,0
clean up results directory,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
1. get the corpus,0
2. what tag do we want to predict?,0
3. make the tag dictionary from the corpus,0
initialize embeddings,0
comment in this line to use character embeddings,0
"CharacterEmbeddings(),",0
comment in these lines to use contextual string embeddings,0
,0
"FlairEmbeddings('news-forward'),",0
,0
"FlairEmbeddings('news-backward'),",0
initialize sequence tagger,0
initialize trainer,0
from allennlp.common.tqdm import Tqdm,0
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
Extract all the contents of zip file in current directory,0
get cache path to put the file,0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
first determine the distribution of classes in the dataset,0
weight for each sample,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups,1
see https://github.com/zalandoresearch/flair/issues/351,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
Maintains max of all exp. moving avg. of sq. grad. values,0
Decay the first and second moment running average coefficient,0
Maintains the maximum of all 2nd moment running avg. till now,0
Use the max. for normalizing running avg. of gradient,0
determine offsets for whitespace_after field,0
increment for last token in sentence if not followed by whitespace,0
determine offsets for whitespace_after field,0
conll 2000 column format,0
conll 03 NER column format,0
WNUT-17,0
-- WikiNER datasets,0
-- Universal Dependencies,0
Germanic,0
Romance,0
West-Slavic,0
South-Slavic,0
East-Slavic,0
Scandinavian,0
Asian,0
Language isolates,0
recent Universal Dependencies,0
other datasets,0
text classification format,0
text regression format,0
"first, try to fetch dataset online",0
default dataset folder is the cache root,0
get string value if enum is passed,0
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)",0
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag",0
the CoNLL 03 task for German has an additional lemma column,0
the CoNLL 03 task for Dutch has no NP column,0
the CoNLL 03 task for Spanish only has two columns,0
the GERMEVAL task only has two columns: text and ner,0
WSD tasks may be put into this column format,0
"the UD corpora follow the CoNLL-U format, for which we have a special reader",0
"for text classifiers, we use our own special format",0
NER corpus for Basque,0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
get train and test data,0
"read in test file if exists, otherwise sample 10% of train data as test dataset",0
"read in dev file if exists, otherwise sample 10% of train data as dev dataset",0
convert tag scheme to iobes,0
automatically identify train / test / dev files,0
automatically identify train / test / dev files,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
conll 2000 chunking task,0
Support both TREC-6 and TREC-50,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
Wikiner NER task,0
unpack and write out in CoNLL column-like format,0
CoNLL 02/03 NER,0
universal dependencies,0
--- UD Germanic,0
--- UD Romance,0
--- UD West-Slavic,0
--- UD Scandinavian,0
--- UD South-Slavic,0
--- UD Asian,0
this is the default init size of a lmdb database for embeddings,0
some non-used parameter to allow print,0
get db filename from embedding name,0
"In case initialization of cached version failed, just fallback to the original WordEmbeddings",0
SequenceTagger,0
TextClassifier,0
get db filename from embedding name,0
if embedding database already exists,0
"otherwise, push embedding to database",0
if embedding database already exists,0
open the database in read mode,0
we need to set self.k,0
create and load the database in write mode,0
"no idea why, but we need to close and reopen the environment to avoid",0
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot,0
when opening new transaction !,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
"We don't want to create a SpaceTokenizer object each time this function is called,",0
so delegate the call directly to the static run_tokenize method,0
"We don't want to create a SegtokTokenizer object each time this function is called,",0
so delegate the call directly to the static run_tokenize method,0
"if text is passed, instantiate sentence with tokens (words)",0
log a warning if the dataset is empty,0
data with zero-width characters cannot be handled,0
set token idx if not set,0
non-set tags are OUT tags,0
anything that is not a BIOES tag is a SINGLE tag,0
anything that is not OUT is IN,0
single and begin tags start a new span,0
remember previous tag,0
move sentence embeddings to device,0
move token embeddings to device,0
clear sentence embeddings,0
clear token embeddings,0
infer whitespace after field,0
add Sentence labels to output if they exist,0
add Token labels to output if they exist,0
add Sentence labels to output if they exist,0
add Token labels to output if they exist,0
No character at the corresponding code point: remove it,0
set name,0
sample test data if none is provided,0
sample dev data if none is provided,0
set train dev and test data,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
check if sentence itself has labels,0
check for labels of words,0
Make the tag dictionary,0
global variable: cache_root,0
global variable: device,0
global variable: embedding_storage_mode,0
# dummy return to fulfill trainer.train() needs,0
Attach optimizer,0
"convert `metrics` to float, in case it's a zero-dim Tensor",0
if memory mode option 'none' delete everything,0
else delete only dynamic embeddings (otherwise autograd will keep everything in memory),0
find out which ones are dynamic embeddings,0
find out which ones are dynamic embeddings,0
memory management - option 1: send everything to CPU (pin to memory if we train on GPU),0
record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class),0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
plot i,0
save plots,0
save plots,0
plt.show(),0
save plot,0
take the average over the last three scores of training,0
take average over the scores from the different training runs,0
remove previous embeddings,0
clearing token embeddings to save memory,0
"read Dataset into data loader (if list of sentences passed, make Dataset first)",0
#TODO: not saving lines yet,1
== similarity measures ==,0
helper class for ModelSimilarity,0
-- works with binary cross entropy loss --,0
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}",0
-- works with ranking/triplet loss --,0
normalize the embeddings,0
== similarity losses ==,0
"we want that logits for corresponding pairs are high, and for non-corresponding low",0
TODO: this assumes eye matrix,0
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa",0
== similarity learner ==,0
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both",0
assumes that for each data pair there's at least one embedding per modality,0
pre-compute embeddings for all targets in evaluation dataset,0
compute the similarity,0
sort the similarity matrix across modality 1,0
"get the ranks, so +1 to start counting ranks from 1",0
The conversion from old model's constructor interface,0
auto-spawn on GPU if available,0
pad strings with whitespaces to longest sentence,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
fixed RNN change format for torch 1.4.0,0
set the dictionaries,0
"if we use a CRF, we must add special START and STOP tags to the dictionary",0
Initialize the weight tensor,0
initialize the network architecture,0
dropouts,0
optional reprojection layer on top of word embeddings,0
bidirectional LSTM on top of embedding layer,0
Create initial hidden state and initialize it,0
TODO: Decide how to initialize the hidden state variables,1
self.hs_initializer(self.lstm_init_h),0
self.hs_initializer(self.lstm_init_c),0
final linear map to tag space,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
clearing token embeddings to save memory,0
predict for batch,0
make list of gold tags,0
make list of predicted tags,0
"check for true positives, false positives and false negatives",0
also write to file in BIO format to use old conlleval script,0
check if in gold spans,0
check if in predicted spans,0
"read Dataset into data loader (if list of sentences passed, make Dataset first)",0
"if span F1 needs to be used, use separate eval method",0
"else, use scikit-learn to evaluate",0
predict for batch,0
add gold tag,0
add predicted tag,0
for file output,0
use sklearn,0
"make ""classification report""",0
get scores,0
line for log file,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
"if initial hidden state is trainable, use this state",0
word dropout only before LSTM - TODO: more experimentation needed,1
if self.use_word_dropout > 0.0:,0
sentence_tensor = self.word_dropout(sentence_tensor),0
get the tags in this sentence,0
add tags as tensor,0
pad tags if using batch-CRF decoder,0
reduce raw values to avoid NaN during exp,0
broadcasting will do the job of reshaping and is more efficient than calling repeat,0
default value,0
the historical German taggers by the @redewiegergabe project,0
Initialize the weight tensor,0
auto-spawn on GPU if available,0
filter empty sentences,0
reverse sort all sequences by their length,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
"read Dataset into data loader (if list of sentences passed, make Dataset first)",0
use scikit-learn to evaluate,0
predict for batch,0
remove predicted labels,0
"make ""classification report""",0
get scores,0
line for log file,0
English sentiment models,0
Communicative Functions Model,0
cast string to Path,0
"determine what splits (train, dev, test) to evaluate and log",0
prepare loss logging file and set up header,0
"minimize training loss if training with dev data, else maximize dev score",0
"if training also uses dev data, include in training set",0
initialize sampler if provided,0
init with default values if only class is provided,0
set dataset to sample from,0
At any point you can hit Ctrl + C to break out of training early.,0
get new learning rate,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
process mini-batches,0
zero the gradients on the model and optimizer,0
"if necessary, make batch_steps",0
forward and backward for batch,0
forward pass,0
Backward,0
do the optimizer step,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"anneal against train loss if training with dev, otherwise anneal against dev score",0
evaluate on train / dev / test split depending on training settings,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
calculate scores using dev data if available,0
append dev score to score history,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
determine learning rate annealing through scheduler. Use auxiliary metric for AnnealOnPlateau,0
determine bad epoch number,0
log bad epochs,0
output log file,0
make headers on first epoch,0
"if checkpoint is enabled, save model at each epoch",0
"if we use dev data, remember best model based on dev evaluation score",0
"if we do not use dev data for model selection, save final model",0
test best model if test data is present,0
"if we are training over multiple datasets, do evaluation for each",0
get and return the final test score of best model,0
cast string to Path,0
forward pass,0
update optimizer and scheduler,0
Add chars to the dictionary,0
charsplit file content,0
charsplit file content,0
Add words to the dictionary,0
Tokenize file content,0
"TextDataset returns a list. valid and test are only one file, so return the first element",0
cast string to Path,0
error message if the validation dataset is too small,0
Shuffle training files randomly after serially iterating through corpus one,0
"iterate through training data, starting at self.split (for checkpointing)",0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
Backward,0
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.,0
We detach the hidden state from how it was previously produced.,0
"If we didn't, the model would try backpropagating all the way to start of the dataset.",0
explicitly remove loss to clear up memory,0
##############################################################################,0
Save the model if the validation loss is the best we've seen so far.,0
##############################################################################,0
print info,0
##############################################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
1-camembert-base -> camembert-base,0
1-xlm-roberta-large -> xlm-roberta-large,0
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol,0
The mask has 1 for real tokens and 0 for padding tokens. Only real,0
tokens are attended to.,0
Zero-pad up to the sequence length.,0
"first, find longest sentence in batch",0
prepare id maps for BERT model,0
put encoded batch through BERT model to get all hidden states of all encoder layers,0
get aggregated embeddings for each BERT-subtoken in sentence,0
get the current sentence object,0
add concatenated embedding to sentence,0
use first subword embedding if pooling operation is 'first',0
"otherwise, do a mean over all subwords in token",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
embed a dummy sentence to determine embedding_length,0
Avoid conflicts with flair's Token class,0
"<cls> token initially set to 1/D, so it attends to all image features equally",0
add positional encodings,0
reshape the pixels into the sequence,0
layer norm after convolution and positional encodings,0
add <cls> token,0
"transformer requires input in the shape [h*w+1, b, d]",0
the output is an embedding of <cls> token,0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
load tokenizer and transformer model,0
model name,0
"when initializing, embeddings are in eval mode by default",0
embedding parameters,0
send mini-token through to check how many layers the model has,0
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial",0
using list comprehension,0
gradients are enabled if fine-tuning is enabled,0
"first, subtokenize each sentence and find out into how many subtokens each token was divided",0
subtokenize sentences,0
tokenize and truncate to 512 subtokens (TODO: check better truncation strategies),1
find longest sentence in batch,0
initialize batch tensors and mask,0
put encoded batch through transformer model to get all hidden states of all encoder layers,0
iterate over all subtokenized sentences,0
use scalar mix of embeddings if so selected,0
set the extracted embedding for the token,0
reload tokenizer to get around serialization issues,0
optional fine-tuning on top of embedding layer,0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push through RNN,0
after-RNN dropout,0
extract embeddings from RNN,0
fixed RNN change format for torch 1.4.0,0
IMPORTANT: add embeddings as torch modules,0
iterate over sentences,0
"if its a forward LM, take last state",0
"convert to plain strings, embedded in a list for the encode function",0
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency",0
"if only one sentence is passed, convert to list of sentence",0
Expose base classses,0
Expose token embedding classes,0
Expose document embedding classes,0
Expose image embedding classes,0
Expose legacy embedding classes,0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNINOS embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
fix serialized models,0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
multilingual models,0
English models,0
Arabic,0
Bulgarian,0
Czech,0
Danish,0
German,0
Spanish,0
Basque,0
Persian,0
Finnish,0
French,0
Hebrew,0
Hindi,0
Croatian,0
Indonesian,0
Italian,0
Japanese,0
Malayalam,0
Dutch,0
Norwegian,0
Polish,0
Portuguese,0
Pubmed,0
Slovenian,0
Swedish,0
Tamil,0
CLEF HIPE Shared task,0
load model if in pretrained model map,0
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir),0
embeddings are static if we don't do finetuning,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
make compatible with serialized models (TODO: remove),1
make compatible with serialized models (TODO: remove),1
gradients are enable if fine-tuning is enabled,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
offset mode that extracts at whitespace after last character,0
offset mode that extracts at last character,0
only clone if optimization mode is 'gpu',0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
check token.text is empty or not,0
set aggregation operation,0
add embeddings after updating,0
temporary fix to disable tokenizer parallelism warning,1
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning),0
load tokenizer and transformer model,0
model name,0
"when initializing, embeddings are in eval mode by default",0
embedding parameters,0
send mini-token through to check how many layers the model has,0
"self.mix = ScalarMix(mixture_size=len(self.layer_indexes), trainable=False)",0
check if special tokens exist to circumvent error message,0
"most models have an intial BOS token, except for XLNet, T5 and GPT2",0
split into micro batches of size self.batch_size before pushing through transformer,0
embed each micro-batch,0
remove special markup,0
"first, subtokenize each sentence and find out into how many subtokens each token was divided",0
"TODO: keep for backwards compatibility, but remove in future",1
"some pretrained models do not have this property, applying default settings now.",0
can be set manually after loading the model.,0
method 1: subtokenize sentence,0
"subtokenized_sentence = self.tokenizer.encode(tokenized_string, add_special_tokens=True)",0
method 2:,0
transformer specific tokenization,0
empty sentences get zero embeddings,0
only embed non-empty sentences and if there is at least one,0
find longest sentence in batch,0
initialize batch tensors and mask,0
put encoded batch through transformer model to get all hidden states of all encoder layers,0
make the tuple a tensor; makes working with it easier.,0
gradients are enabled if fine-tuning is enabled,0
iterate over all subtokenized sentences,0
"remove stride_size//2 at end of sentence_hidden_state, and half at beginning of remainder,",0
in order to get some context into the embeddings of these words.,0
also don't include the embedding of the extra [CLS] and [SEP] tokens.,0
"for each token, get embedding",0
some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector,0
"get states from all selected layers, aggregate with pooling operation",0
use scalar mix of embeddings if so selected,0
sm_embeddings = self.mix(subtoken_embeddings),0
set the extracted embedding for the token,0
iterate over subtokens and reconstruct tokens,0
remove special markup,0
TODO check if this is necessary is this method is called before prepare_for_model,1
check if reconstructed token is special begin token ([CLS] or similar),0
some BERT tokenizers somehow omit words - in such cases skip to next token,0
append subtoken to reconstruct token,0
check if reconstructed token is the same as current token,0
"if so, add subtoken count",0
reset subtoken count and reconstructed token,0
break from loop if all tokens are accounted for,0
if tokens are unaccounted for,0
check if all tokens were matched to subtokens,0
"if fine-tuning is not enabled (i.e. a ""feature-based approach"" used), this",0
module should never be in training mode,0
reload tokenizer to get around serialization issues,0
max_tokens = 500,0
model architecture,0
model architecture,0
download if necessary,0
load the model,0
"TODO: keep for backwards compatibility, but remove in future",1
save the sentence piece model as binary file (not as path which may change),0
write out the binary sentence piece model into the expected directory,0
"if the model was saved as binary and it is not found on disk, write to appropriate path",0
"otherwise, use normal process and potentially trigger another download",0
"once the modes if there, load it with sentence piece",0
empty words get no embedding,0
all other words get embedded,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn,0
GLOVE embeddings,0
"find train, dev and test files if not specified",0
get train data,0
read in test file if exists,0
read in dev file if exists,0
special key for space after,0
"store either Sentence objects in memory, or only file offsets",0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
determine encoding of text file,0
skip first line if to selected,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Remove CoNLL-U meta information in the last column,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
rename according to train - test - dev - convention,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
data is not in IOB2 format. Thus we transform it to IOB2,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download data if necessary,0
unpack and write out in CoNLL column-like format,0
"find train, dev and test files if not specified",0
use test_file to create test split if available,0
use dev_file to create test split if available,0
"if data point contains black-listed label, do not use",0
first check if valid sentence,0
"if so, add to indices",0
"find train, dev and test files if not specified",0
variables,0
different handling of in_memory data than streaming data,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
test if format is OK,0
test if at least one label given,0
noinspection PyDefaultArgument,0
dataset name includes the split size,0
default dataset folder is the cache root,0
download data if necessary,0
download each of the 28 splits,0
create dataset directory if necessary,0
download senteval datasets if necessary und unzip,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"by defaut, map point score to POSITIVE / NEGATIVE values",0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file from CSV,0
create test.txt file from CSV,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
convert to FastText format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
check if dataset is supported,0
set file names,0
download and unzip in file structure if necessary,0
instantiate corpus,0
cache Feidegger config file,0
cache Feidegger images,0
replace image URL with local cached file,0
append Sentence-Image data point,0
"in certain cases, multi-CPU data loading makes no sense and slows",0
"everything down. For this reason, we detect if a dataset is in-memory:",0
"if so, num_workers is set to 0 for faster processing",0
cast to list if necessary,0
cast to list if necessary,0
"first, check if pymongo is installed",0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
Expose base classses,0
Expose all sequence labeling datasets,0
Expose all document classification datasets,0
Expose all treebanks,0
Expose all text-text datasets,0
Expose all text-image datasets,0
"find train, dev and test files if not specified",0
get train data,0
get test data,0
get dev data,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
skip because it is optional https://github.com/flairNLP/flair/pull/1296,0
clean up file,0
bioes tags,0
bio tags,0
broken tags,0
all tags,0
all weird tags,0
tags with confidence,0
bioes tags,0
bioes tags,0
clean up directory,0
clean up directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
def test_multiclass_metrics():,0
,0
"metric = Metric(""Test"")",0
"available_labels = [""A"", ""B"", ""C""]",0
,0
"predictions = [""A"", ""B""]",0
"true_values = [""A""]",0
TextClassifier._evaluate_sentence_for_text_classification(,0
"metric, available_labels, predictions, true_values",0
),0
,0
"predictions = [""C"", ""B""]",0
"true_values = [""A"", ""B""]",0
TextClassifier._evaluate_sentence_for_text_classification(,0
"metric, available_labels, predictions, true_values",0
),0
,0
print(metric),0
from flair.trainers.trainer_regression import RegressorTrainer,0
def test_trainer_results(tasks_base_path):,0
"corpus, model, trainer = init(tasks_base_path)",0
"results = trainer.train(""regression_train/"", max_epochs=1)",0
"assert results[""test_score""] > 0",0
"assert len(results[""dev_loss_history""]) == 1",0
"assert len(results[""dev_score_history""]) == 1",0
"assert len(results[""train_loss_history""]) == 1",0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
clean up results directory,0
define search space,0
sequence tagger parameter,0
model trainer parameter,0
training parameter,0
find best parameter settings,0
clean up results directory,0
document embeddings parameter,0
training parameter,0
clean up results directory,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
1. get the corpus,0
2. what tag do we want to predict?,0
3. make the tag dictionary from the corpus,0
initialize embeddings,0
comment in this line to use character embeddings,0
"CharacterEmbeddings(),",0
comment in these lines to use contextual string embeddings,0
,0
"FlairEmbeddings('news-forward'),",0
,0
"FlairEmbeddings('news-backward'),",0
initialize sequence tagger,0
initialize trainer,0
from allennlp.common.tqdm import Tqdm,0
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
Extract all the contents of zip file in current directory,0
get cache path to put the file,0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
first determine the distribution of classes in the dataset,0
weight for each sample,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups,1
see https://github.com/zalandoresearch/flair/issues/351,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
Maintains max of all exp. moving avg. of sq. grad. values,0
Decay the first and second moment running average coefficient,0
Maintains the maximum of all 2nd moment running avg. till now,0
Use the max. for normalizing running avg. of gradient,0
conll 2000 column format,0
conll 03 NER column format,0
WNUT-17,0
-- WikiNER datasets,0
-- Universal Dependencies,0
Germanic,0
Romance,0
West-Slavic,0
South-Slavic,0
East-Slavic,0
Scandinavian,0
Asian,0
Language isolates,0
recent Universal Dependencies,0
other datasets,0
text classification format,0
text regression format,0
"first, try to fetch dataset online",0
default dataset folder is the cache root,0
get string value if enum is passed,0
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)",0
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag",0
the CoNLL 03 task for German has an additional lemma column,0
the CoNLL 03 task for Dutch has no NP column,0
the CoNLL 03 task for Spanish only has two columns,0
the GERMEVAL task only has two columns: text and ner,0
WSD tasks may be put into this column format,0
"the UD corpora follow the CoNLL-U format, for which we have a special reader",0
"for text classifiers, we use our own special format",0
NER corpus for Basque,0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
get train and test data,0
"read in test file if exists, otherwise sample 10% of train data as test dataset",0
"read in dev file if exists, otherwise sample 10% of train data as dev dataset",0
convert tag scheme to iobes,0
automatically identify train / test / dev files,0
automatically identify train / test / dev files,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
conll 2000 chunking task,0
Support both TREC-6 and TREC-50,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
Wikiner NER task,0
unpack and write out in CoNLL column-like format,0
CoNLL 02/03 NER,0
universal dependencies,0
--- UD Germanic,0
--- UD Romance,0
--- UD West-Slavic,0
--- UD Scandinavian,0
--- UD South-Slavic,0
--- UD Asian,0
this is the default init size of a lmdb database for embeddings,0
some non-used parameter to allow print,0
get db filename from embedding name,0
"In case initialization of cached version failed, just fallback to the original WordEmbeddings",0
SequenceTagger,0
TextClassifier,0
get db filename from embedding name,0
if embedding database already exists,0
"otherwise, push embedding to database",0
if embedding database already exists,0
open the database in read mode,0
we need to set self.k,0
create and load the database in write mode,0
"no idea why, but we need to close and reopen the environment to avoid",0
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot,0
when opening new transaction !,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
"if text is passed, instantiate sentence with tokens (words)",0
log a warning if the dataset is empty,0
data with zero-width characters cannot be handled,0
set token idx if not set,0
non-set tags are OUT tags,0
anything that is not a BIOES tag is a SINGLE tag,0
anything that is not OUT is IN,0
single and begin tags start a new span,0
remember previous tag,0
move sentence embeddings to device,0
move token embeddings to device,0
clear sentence embeddings,0
clear token embeddings,0
infer whitespace after field,0
add Sentence labels to output if they exist,0
add Token labels to output if they exist,0
add Sentence labels to output if they exist,0
add Token labels to output if they exist,0
No character at the corresponding code point: remove it,0
set name,0
sample test data if none is provided,0
sample dev data if none is provided,0
set train dev and test data,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
check if sentence itself has labels,0
check for labels of words,0
Make the tag dictionary,0
increment for last token in sentence if not followed by whitespace,0
determine offsets for whitespace_after field,0
determine offsets for whitespace_after field,0
global variable: cache_root,0
global variable: device,0
global variable: embedding_storage_mode,0
# dummy return to fulfill trainer.train() needs,0
Attach optimizer,0
"convert `metrics` to float, in case it's a zero-dim Tensor",0
if memory mode option 'none' delete everything,0
else delete only dynamic embeddings (otherwise autograd will keep everything in memory),0
find out which ones are dynamic embeddings,0
find out which ones are dynamic embeddings,0
memory management - option 1: send everything to CPU (pin to memory if we train on GPU),0
record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class),0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
print(rows),0
"figsize = (16, 16)",0
plot i,0
save plots,0
save plots,0
plt.show(),0
save plot,0
take the average over the last three scores of training,0
take average over the scores from the different training runs,0
remove previous embeddings,0
clearing token embeddings to save memory,0
#TODO: not saving lines yet,1
== similarity measures ==,0
helper class for ModelSimilarity,0
-- works with binary cross entropy loss --,0
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}",0
-- works with ranking/triplet loss --,0
normalize the embeddings,0
== similarity losses ==,0
"we want that logits for corresponding pairs are high, and for non-corresponding low",0
TODO: this assumes eye matrix,0
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa",0
== similarity learner ==,0
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both",0
assumes that for each data pair there's at least one embedding per modality,0
pre-compute embeddings for all targets in evaluation dataset,0
compute the similarity,0
sort the similarity matrix across modality 1,0
"get the ranks, so +1 to start counting ranks from 1",0
The conversion from old model's constructor interface,0
auto-spawn on GPU if available,0
pad strings with whitespaces to longest sentence,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
fixed RNN change format for torch 1.4.0,0
set the dictionaries,0
"if we use a CRF, we must add special START and STOP tags to the dictionary",0
Initialize the weight tensor,0
initialize the network architecture,0
dropouts,0
"if no dimensionality for reprojection layer is set, reproject to equal dimension",0
bidirectional LSTM on top of embedding layer,0
Create initial hidden state and initialize it,0
TODO: Decide how to initialize the hidden state variables,1
self.hs_initializer(self.lstm_init_h),0
self.hs_initializer(self.lstm_init_c),0
final linear map to tag space,0
reverse sort all sequences by their length,0
remove previous embeddings,0
progress bar for verbosity,0
stop if all sentences are empty,0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
clearing token embeddings to save memory,0
append both to file for evaluation,0
make list of gold tags,0
make list of predicted tags,0
"check for true positives, false positives and false negatives",0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
"if initial hidden state is trainable, use this state",0
word dropout only before LSTM - TODO: more experimentation needed,1
if self.use_word_dropout > 0.0:,0
sentence_tensor = self.word_dropout(sentence_tensor),0
get the tags in this sentence,0
add tags as tensor,0
pad tags if using batch-CRF decoder,0
reduce raw values to avoid NaN during exp,0
broadcasting will do the job of reshaping and is more efficient than calling repeat,0
default value,0
the historical German taggers by the @redewiegergabe project,0
Initialize the weight tensor,0
auto-spawn on GPU if available,0
filter empty sentences,0
reverse sort all sequences by their length,0
remove previous embeddings,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
English sentiment models,0
cast string to Path,0
"determine what splits (train, dev, test) to evaluate and log",0
prepare loss logging file and set up header,0
"minimize training loss if training with dev data, else maximize dev score",0
"if training also uses dev data, include in training set",0
initialize sampler if provided,0
init with default values if only class is provided,0
set dataset to sample from,0
At any point you can hit Ctrl + C to break out of training early.,0
get new learning rate,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
process mini-batches,0
zero the gradients on the model and optimizer,0
"if necessary, make batch_steps",0
forward and backward for batch,0
forward pass,0
Backward,0
do the optimizer step,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"anneal against train loss if training with dev, otherwise anneal against dev score",0
evaluate on train / dev / test split depending on training settings,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
calculate scores using dev data if available,0
append dev score to score history,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
determine learning rate annealing through scheduler. Use auxiliary metric for AnnealOnPlateau,0
determine bad epoch number,0
log bad epochs,0
output log file,0
make headers on first epoch,0
"if checkpoint is enabled, save model at each epoch",0
"if we use dev data, remember best model based on dev evaluation score",0
"if we do not use dev data for model selection, save final model",0
test best model if test data is present,0
"if we are training over multiple datasets, do evaluation for each",0
get and return the final test score of best model,0
cast string to Path,0
forward pass,0
update optimizer and scheduler,0
Add chars to the dictionary,0
charsplit file content,0
charsplit file content,0
Add words to the dictionary,0
Tokenize file content,0
"TextDataset returns a list. valid and test are only one file, so return the first element",0
cast string to Path,0
error message if the validation dataset is too small,0
Shuffle training files randomly after serially iterating through corpus one,0
"iterate through training data, starting at self.split (for checkpointing)",0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
Backward,0
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.,0
We detach the hidden state from how it was previously produced.,0
"If we didn't, the model would try backpropagating all the way to start of the dataset.",0
explicitly remove loss to clear up memory,0
##############################################################################,0
Save the model if the validation loss is the best we've seen so far.,0
##############################################################################,0
print info,0
##############################################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
1-camembert-base -> camembert-base,0
1-xlm-roberta-large -> xlm-roberta-large,0
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol,0
The mask has 1 for real tokens and 0 for padding tokens. Only real,0
tokens are attended to.,0
Zero-pad up to the sequence length.,0
"first, find longest sentence in batch",0
prepare id maps for BERT model,0
put encoded batch through BERT model to get all hidden states of all encoder layers,0
get aggregated embeddings for each BERT-subtoken in sentence,0
get the current sentence object,0
add concatenated embedding to sentence,0
use first subword embedding if pooling operation is 'first',0
"otherwise, do a mean over all subwords in token",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
embed a dummy sentence to determine embedding_length,0
Avoid conflicts with flair's Token class,0
"<cls> token initially set to 1/D, so it attends to all image features equally",0
add positional encodings,0
reshape the pixels into the sequence,0
layer norm after convolution and positional encodings,0
add <cls> token,0
"transformer requires input in the shape [h*w+1, b, d]",0
the output is an embedding of <cls> token,0
load tokenizer and transformer model,0
model name,0
"when initializing, embeddings are in eval mode by default",0
embedding parameters,0
send mini-token through to check how many layers the model has,0
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial",0
using list comprehension,0
gradients are enabled if fine-tuning is enabled,0
"first, subtokenize each sentence and find out into how many subtokens each token was divided",0
subtokenize sentences,0
tokenize and truncate to 512 subtokens (TODO: check better truncation strategies),1
find longest sentence in batch,0
initialize batch tensors and mask,0
put encoded batch through transformer model to get all hidden states of all encoder layers,0
iterate over all subtokenized sentences,0
use scalar mix of embeddings if so selected,0
set the extracted embedding for the token,0
reload tokenizer to get around serialization issues,0
optional fine-tuning on top of embedding layer,0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push through RNN,0
after-RNN dropout,0
extract embeddings from RNN,0
fixed RNN change format for torch 1.4.0,0
IMPORTANT: add embeddings as torch modules,0
iterate over sentences,0
"if its a forward LM, take last state",0
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency",0
"if only one sentence is passed, convert to list of sentence",0
Expose base classses,0
Expose token embedding classes,0
Expose document embedding classes,0
Expose image embedding classes,0
Expose legacy embedding classes,0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNINOS embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
fix serialized models,0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
multilingual models,0
English models,0
Arabic,0
Bulgarian,0
Czech,0
Danish,0
German,0
Spanish,0
Basque,0
Persian,0
Finnish,0
French,0
Hebrew,0
Hindi,0
Croatian,0
Indonesian,0
Italian,0
Japanese,0
Malayalam,0
Dutch,0
Norwegian,0
Polish,0
Portuguese,0
Pubmed,0
Slovenian,0
Swedish,0
Tamil,0
CLEF HIPE Shared task,0
load model if in pretrained model map,0
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir),0
embeddings are static if we don't do finetuning,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
make compatible with serialized models (TODO: remove),1
make compatible with serialized models (TODO: remove),1
gradients are enable if fine-tuning is enabled,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
offset mode that extracts at whitespace after last character,0
offset mode that extracts at last character,0
only clone if optimization mode is 'gpu',0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
check token.text is empty or not,0
add embeddings after updating,0
load tokenizer and transformer model,0
model name,0
"when initializing, embeddings are in eval mode by default",0
embedding parameters,0
send mini-token through to check how many layers the model has,0
"self.mix = ScalarMix(mixture_size=len(self.layer_indexes), trainable=False)",0
check if special tokens exist to circumvent error message,0
"most models have an intial BOS token, except for XLNet, T5 and GPT2",0
split into micro batches of size self.batch_size before pushing through transformer,0
embed each micro-batch,0
remove special markup,0
"first, subtokenize each sentence and find out into how many subtokens each token was divided",0
method 1: subtokenize sentence,0
"subtokenized_sentence = self.tokenizer.encode(tokenized_string, add_special_tokens=True)",0
method 2:,0
print(subtokens),0
iterate over subtokens and reconstruct tokens,0
remove special markup,0
append subtoken to reconstruct token,0
check if reconstructed token is special begin token ([CLS] or similar),0
check if reconstructed token is the same as current token,0
"if so, add subtoken count",0
reset subtoken count and reconstructed token,0
break from loop if all tokens are accounted for,0
check if all tokens were matched to subtokens,0
find longest sentence in batch,0
initialize batch tensors and mask,0
put encoded batch through transformer model to get all hidden states of all encoder layers,0
gradients are enabled if fine-tuning is enabled,0
iterate over all subtokenized sentences,0
"for each token, get embedding",0
"get states from all selected layers, aggregate with pooling operation",0
use scalar mix of embeddings if so selected,0
sm_embeddings = self.mix(subtoken_embeddings),0
set the extracted embedding for the token,0
"if fine-tuning is not enabled (i.e. a ""feature-based approach"" used), this",0
module should never be in training mode,0
reload tokenizer to get around serialization issues,0
max_tokens = 500,0
model architecture,0
model architecture,0
download if necessary,0
load the model,0
"TODO: keep for backwards compatibility, but remove in future",1
save the sentence piece model as binary file (not as path which may change),0
write out the binary sentence piece model into the expected directory,0
"if the model was saved as binary and it is not found on disk, write to appropriate path",0
"otherwise, use normal process and potentially trigger another download",0
"once the modes if there, load it with sentence piece",0
empty words get no embedding,0
all other words get embedded,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
GLOVE embeddings,0
"find train, dev and test files if not specified",0
get train data,0
read in test file if exists,0
read in dev file if exists,0
special key for space after,0
"store either Sentence objects in memory, or only file offsets",0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
determine encoding of text file,0
skip first line if to selected,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Remove CoNLL-U meta information in the last column,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download data if necessary,0
unpack and write out in CoNLL column-like format,0
"find train, dev and test files if not specified",0
use test_file to create test split if available,0
use dev_file to create test split if available,0
"if data point contains black-listed label, do not use",0
first check if valid sentence,0
"if so, add to indices",0
"find train, dev and test files if not specified",0
variables,0
different handling of in_memory data than streaming data,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
test if format is OK,0
test if at least one label given,0
noinspection PyDefaultArgument,0
dataset name includes the split size,0
default dataset folder is the cache root,0
download data if necessary,0
download each of the 28 splits,0
create dataset directory if necessary,0
download senteval datasets if necessary und unzip,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"by defaut, map point score to POSITIVE / NEGATIVE values",0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file from CSV,0
create test.txt file from CSV,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create dataset directory if necessary,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
create train.txt file by iterating over pos and neg file,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download senteval datasets if necessary und unzip,0
convert to FastText format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
check if dataset is supported,0
set file names,0
download and unzip in file structure if necessary,0
instantiate corpus,0
cache Feidegger config file,0
cache Feidegger images,0
replace image URL with local cached file,0
append Sentence-Image data point,0
"in certain cases, multi-CPU data loading makes no sense and slows",0
"everything down. For this reason, we detect if a dataset is in-memory:",0
"if so, num_workers is set to 0 for faster processing",0
cast to list if necessary,0
cast to list if necessary,0
"first, check if pymongo is installed",0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
Expose base classses,0
Expose all sequence labeling datasets,0
Expose all document classification datasets,0
Expose all treebanks,0
Expose all text-text datasets,0
Expose all text-image datasets,0
"find train, dev and test files if not specified",0
get train data,0
get test data,0
get dev data,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
skip because it is optional https://github.com/flairNLP/flair/pull/1296,0
def test_create_sentence_using_japanese_tokenizer():,0
"sentence: Sentence = Sentence("""", use_tokenizer=build_japanese_tokenizer())",0
,0
assert 5 == len(sentence.tokens),0
"assert """" == sentence.tokens[0].text",0
"assert """" == sentence.tokens[1].text",0
"assert """" == sentence.tokens[2].text",0
"assert """" == sentence.tokens[3].text",0
"assert """" == sentence.tokens[4].text",0
clean up file,0
bioes tags,0
bio tags,0
broken tags,0
all tags,0
all weird tags,0
tags with confidence,0
bioes tags,0
bioes tags,0
clean up directory,0
clean up directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
def test_multiclass_metrics():,0
,0
"metric = Metric(""Test"")",0
"available_labels = [""A"", ""B"", ""C""]",0
,0
"predictions = [""A"", ""B""]",0
"true_values = [""A""]",0
TextClassifier._evaluate_sentence_for_text_classification(,0
"metric, available_labels, predictions, true_values",0
),0
,0
"predictions = [""C"", ""B""]",0
"true_values = [""A"", ""B""]",0
TextClassifier._evaluate_sentence_for_text_classification(,0
"metric, available_labels, predictions, true_values",0
),0
,0
print(metric),0
from flair.trainers.trainer_regression import RegressorTrainer,0
def test_trainer_results(tasks_base_path):,0
"corpus, model, trainer = init(tasks_base_path)",0
"results = trainer.train(""regression_train/"", max_epochs=1)",0
"assert results[""test_score""] > 0",0
"assert len(results[""dev_loss_history""]) == 1",0
"assert len(results[""dev_score_history""]) == 1",0
"assert len(results[""train_loss_history""]) == 1",0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
clean up results directory,0
define search space,0
sequence tagger parameter,0
model trainer parameter,0
training parameter,0
find best parameter settings,0
clean up results directory,0
document embeddings parameter,0
training parameter,0
clean up results directory,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
1. get the corpus,0
2. what tag do we want to predict?,0
3. make the tag dictionary from the corpus,0
initialize embeddings,0
comment in this line to use character embeddings,0
"CharacterEmbeddings(),",0
comment in these lines to use contextual string embeddings,0
,0
"FlairEmbeddings('news-forward'),",0
,0
"FlairEmbeddings('news-backward'),",0
initialize sequence tagger,0
initialize trainer,0
"if only one sentence is passed, convert to list of sentence",0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNINOS embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
fix serialized models,0
max_tokens = 500,0
model architecture,0
model architecture,0
download if necessary,0
load the model,0
empty words get no embedding,0
all other words get embedded,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
embed a dummy sentence to determine embedding_length,0
Avoid conflicts with flair's Token class,0
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol,0
1-camembert-base -> camembert-base,0
1-xlm-roberta-large -> xlm-roberta-large,0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
multilingual models,0
English models,0
Arabic,0
Bulgarian,0
Czech,0
Danish,0
German,0
Spanish,0
Basque,0
Persian,0
Finnish,0
French,0
Hebrew,0
Hindi,0
Croatian,0
Indonesian,0
Italian,0
Japanese,0
Dutch,0
Norwegian,0
Polish,0
Portuguese,0
Pubmed,0
Slovenian,0
Swedish,0
Tamil,0
load model if in pretrained model map,0
embeddings are static if we don't do finetuning,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
make compatible with serialized models (TODO: remove),1
gradients are enable if fine-tuning is enabled,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
only clone if optimization mode is 'gpu',0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
check token.text is empty or not,0
add embeddings after updating,0
The mask has 1 for real tokens and 0 for padding tokens. Only real,0
tokens are attended to.,0
Zero-pad up to the sequence length.,0
"first, find longest sentence in batch",0
prepare id maps for BERT model,0
put encoded batch through BERT model to get all hidden states of all encoder layers,0
get aggregated embeddings for each BERT-subtoken in sentence,0
get the current sentence object,0
add concatenated embedding to sentence,0
use first subword embedding if pooling operation is 'first',0
"otherwise, do a mean over all subwords in token",0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
"if only one sentence is passed, convert to list of sentence",0
optional fine-tuning on top of embedding layer,0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
before-RNN dropout,0
reproject if set,0
push through RNN,0
after-RNN dropout,0
extract embeddings from RNN,0
fixed RNN change format for torch 1.4.0,0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
IMPORTANT: add embeddings as torch modules,0
iterate over sentences,0
"if its a forward LM, take last state",0
GLOVE embeddings,0
"<cls> token initially set to 1/D, so it attends to all image features equally",0
add positional encodings,0
reshape the pixels into the sequence,0
layer norm after convolution and positional encodings,0
add <cls> token,0
"transformer requires input in the shape [h*w+1, b, d]",0
the output is an embedding of <cls> token,0
"TODO: keep for backwards compatibility, but remove in future",1
save the sentence piece model as binary file (not as path which may change),0
write out the binary sentence piece model into the expected directory,0
"if the model was saved as binary and it is not found on disk, write to appropriate path",0
"otherwise, use normal process and potentially trigger another download",0
"once the modes if there, load it with sentence piece",0
from allennlp.common.tqdm import Tqdm,0
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
Extract all the contents of zip file in current directory,0
get cache path to put the file,0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
first determine the distribution of classes in the dataset,0
weight for each sample,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups,1
see https://github.com/zalandoresearch/flair/issues/351,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
Maintains max of all exp. moving avg. of sq. grad. values,0
Decay the first and second moment running average coefficient,0
Maintains the maximum of all 2nd moment running avg. till now,0
Use the max. for normalizing running avg. of gradient,0
conll 2000 column format,0
conll 03 NER column format,0
WNUT-17,0
-- WikiNER datasets,0
-- Universal Dependencies,0
Germanic,0
Romance,0
West-Slavic,0
South-Slavic,0
East-Slavic,0
Scandinavian,0
Asian,0
Language isolates,0
recent Universal Dependencies,0
other datasets,0
text classification format,0
text regression format,0
"first, try to fetch dataset online",0
default dataset folder is the cache root,0
get string value if enum is passed,0
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)",0
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag",0
the CoNLL 03 task for German has an additional lemma column,0
the CoNLL 03 task for Dutch has no NP column,0
the CoNLL 03 task for Spanish only has two columns,0
the GERMEVAL task only has two columns: text and ner,0
WSD tasks may be put into this column format,0
"the UD corpora follow the CoNLL-U format, for which we have a special reader",0
"for text classifiers, we use our own special format",0
NER corpus for Basque,0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
get train and test data,0
"read in test file if exists, otherwise sample 10% of train data as test dataset",0
"read in dev file if exists, otherwise sample 10% of train data as dev dataset",0
convert tag scheme to iobes,0
automatically identify train / test / dev files,0
automatically identify train / test / dev files,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
conll 2000 chunking task,0
Support both TREC-6 and TREC-50,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
Wikiner NER task,0
unpack and write out in CoNLL column-like format,0
CoNLL 02/03 NER,0
universal dependencies,0
--- UD Germanic,0
--- UD Romance,0
--- UD West-Slavic,0
--- UD Scandinavian,0
--- UD South-Slavic,0
--- UD Asian,0
some non-used parameter to allow print,0
get db filename from embedding name,0
if embedding database already exists,0
"otherwise, push embedding to database",0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
increment for last token in sentence if not followed by whitespace,0
determine offsets for whitespace_after field,0
determine offsets for whitespace_after field,0
"if text is passed, instantiate sentence with tokens (words)",0
log a warning if the dataset is empty,0
set token idx if not set,0
non-set tags are OUT tags,0
anything that is not a BIOES tag is a SINGLE tag,0
anything that is not OUT is IN,0
single and begin tags start a new span,0
remember previous tag,0
move sentence embeddings to device,0
move token embeddings to device,0
clear sentence embeddings,0
clear token embeddings,0
infer whitespace after field,0
No character at the corresponding code point: remove it,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
Make the tag dictionary,0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
get train data,0
"read in test file if exists, otherwise sample 10% of train data as test dataset",0
"read in dev file if exists, otherwise sample 10% of train data as dev dataset",0
automatically identify train / test / dev files,0
get train data,0
get test data,0
get dev data,0
automatically identify train / test / dev files,0
use test_file to create test split if available,0
"otherwise, sample test data from train data",0
use dev_file to create test split if available,0
"otherwise, sample dev data from dev data",0
cache Feidegger config file,0
cache Feidegger images,0
replace image URL with local cached file,0
automatically identify train / test / dev files,0
check if dataset is supported,0
set file names,0
download and unzip in file structure if necessary,0
instantiate corpus,0
cast to list if necessary,0
cast to list if necessary,0
"store either Sentence objects in memory, or only file offsets",0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
determine encoding of text file,0
variables,0
different handling of in_memory data than streaming data,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
test if format is OK,0
test if at least one label given,0
"first, check if pymongo is installed",0
append Sentence-Image data point,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download data if necessary,0
unpack and write out in CoNLL column-like format,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"in certain cases, multi-CPU data loading makes no sense and slows",0
"everything down. For this reason, we detect if a dataset is in-memory:",0
"if so, num_workers is set to 0 for faster processing",0
global variable: cache_root,0
global variable: device,0
global variable: embedding_storage_mode,0
# dummy return to fulfill trainer.train() needs,0
if memory mode option 'none' delete everything,0
else delete only dynamic embeddings (otherwise autograd will keep everything in memory),0
find out which ones are dynamic embeddings,0
find out which ones are dynamic embeddings,0
memory management - option 1: send everything to CPU (pin to memory if we train on GPU),0
record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class),0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
print(rows),0
"figsize = (16, 16)",0
plot i,0
save plots,0
save plots,0
plt.show(),0
save plot,0
take the average over the last three scores of training,0
take average over the scores from the different training runs,0
remove previous embeddings,0
clearing token embeddings to save memory,0
#TODO: not saving lines yet,1
== similarity measures ==,0
helper class for ModelSimilarity,0
-- works with binary cross entropy loss --,0
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}",0
-- works with ranking/triplet loss --,0
normalize the embeddings,0
== similarity losses ==,0
"we want that logits for corresponding pairs are high, and for non-corresponding low",0
TODO: this assumes eye matrix,0
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa",0
== similarity learner ==,0
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both",0
assumes that for each data pair there's at least one embedding per modality,0
pre-compute embeddings for all targets in evaluation dataset,0
compute the similarity,0
sort the similarity matrix across modality 1,0
"get the ranks, so +1 to start counting ranks from 1",0
The conversion from old model's constructor interface,0
auto-spawn on GPU if available,0
pad strings with whitespaces to longest sentence,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
fixed RNN change format for torch 1.4.0,0
set the dictionaries,0
Initialize the weight tensor,0
initialize the network architecture,0
dropouts,0
bidirectional LSTM on top of embedding layer,0
Create initial hidden state and initialize it,0
TODO: Decide how to initialize the hidden state variables,1
self.hs_initializer(self.lstm_init_h),0
self.hs_initializer(self.lstm_init_c),0
final linear map to tag space,0
reverse sort all sequences by their length,0
remove previous embeddings,0
progress bar for verbosity,0
stop if all sentences are empty,0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
clearing token embeddings to save memory,0
append both to file for evaluation,0
make list of gold tags,0
make list of predicted tags,0
"check for true positives, false positives and false negatives",0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
"if initial hidden state is trainable, use this state",0
word dropout only before LSTM - TODO: more experimentation needed,1
if self.use_word_dropout > 0.0:,0
sentence_tensor = self.word_dropout(sentence_tensor),0
get the tags in this sentence,0
add tags as tensor,0
pad tags if using batch-CRF decoder,0
reduce raw values to avoid NaN during exp,0
broadcasting will do the job of reshaping and is more efficient than calling repeat,0
default value,0
Initialize the weight tensor,0
auto-spawn on GPU if available,0
reverse sort all sequences by their length,0
remove previous embeddings,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
cast string to Path,0
"determine what splits (train, dev, test) to evaluate and log",0
prepare loss logging file and set up header,0
"minimize training loss if training with dev data, else maximize dev score",0
"if training also uses dev data, include in training set",0
initialize sampler if provided,0
init with default values if only class is provided,0
set dataset to sample from,0
At any point you can hit Ctrl + C to break out of training early.,0
get new learning rate,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
process mini-batches,0
zero the gradients on the model and optimizer,0
"if necessary, make batch_steps",0
forward and backward for batch,0
forward pass,0
Backward,0
do the optimizer step,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"anneal against train loss if training with dev, otherwise anneal against dev score",0
evaluate on train / dev / test split depending on training settings,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
calculate scores using dev data if available,0
append dev score to score history,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
determine learning rate annealing through scheduler,0
determine bad epoch number,0
log bad epochs,0
output log file,0
make headers on first epoch,0
"if checkpoint is enabled, save model at each epoch",0
"if we use dev data, remember best model based on dev evaluation score",0
"if we do not use dev data for model selection, save final model",0
test best model if test data is present,0
"if we are training over multiple datasets, do evaluation for each",0
get and return the final test score of best model,0
cast string to Path,0
forward pass,0
update optimizer and scheduler,0
Add chars to the dictionary,0
charsplit file content,0
charsplit file content,0
Add words to the dictionary,0
Tokenize file content,0
"TextDataset returns a list. valid and test are only one file, so return the first element",0
cast string to Path,0
error message if the validation dataset is too small,0
Shuffle training files randomly after serially iterating through corpus one,0
"iterate through training data, starting at self.split (for checkpointing)",0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
Backward,0
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.,0
We detach the hidden state from how it was previously produced.,0
"If we didn't, the model would try backpropagating all the way to start of the dataset.",0
explicitly remove loss to clear up memory,0
##############################################################################,0
Save the model if the validation loss is the best we've seen so far.,0
##############################################################################,0
print info,0
##############################################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
skip because it is optional https://github.com/flairNLP/flair/pull/1296,0
def test_create_sentence_using_japanese_tokenizer():,0
"sentence: Sentence = Sentence("""", use_tokenizer=build_japanese_tokenizer())",0
,0
assert 5 == len(sentence.tokens),0
"assert """" == sentence.tokens[0].text",0
"assert """" == sentence.tokens[1].text",0
"assert """" == sentence.tokens[2].text",0
"assert """" == sentence.tokens[3].text",0
"assert """" == sentence.tokens[4].text",0
clean up file,0
bioes tags,0
bio tags,0
broken tags,0
all tags,0
all weird tags,0
tags with confidence,0
bioes tags,0
bioes tags,0
clean up directory,0
clean up directory,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
def test_multiclass_metrics():,0
,0
"metric = Metric(""Test"")",0
"available_labels = [""A"", ""B"", ""C""]",0
,0
"predictions = [""A"", ""B""]",0
"true_values = [""A""]",0
TextClassifier._evaluate_sentence_for_text_classification(,0
"metric, available_labels, predictions, true_values",0
),0
,0
"predictions = [""C"", ""B""]",0
"true_values = [""A"", ""B""]",0
TextClassifier._evaluate_sentence_for_text_classification(,0
"metric, available_labels, predictions, true_values",0
),0
,0
print(metric),0
from flair.trainers.trainer_regression import RegressorTrainer,0
def test_trainer_results(tasks_base_path):,0
"corpus, model, trainer = init(tasks_base_path)",0
"results = trainer.train(""regression_train/"", max_epochs=1)",0
"assert results[""test_score""] > 0",0
"assert len(results[""dev_loss_history""]) == 1",0
"assert len(results[""dev_score_history""]) == 1",0
"assert len(results[""train_loss_history""]) == 1",0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
initialize trainer,0
clean up results directory,0
document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(,0
"[flair_embeddings], 128, 1, False",0
),0
clean up results directory,0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
clean up results directory,0
get default dictionary,0
get the example corpus and process at character level in forward direction,0
define search space,0
sequence tagger parameter,0
model trainer parameter,0
training parameter,0
find best parameter settings,0
clean up results directory,0
document embeddings parameter,0
training parameter,0
clean up results directory,0
0           1      2       3        4         5       6      7      8       9      10     11     12     13    14      15,0
,0
"'<s>',      'Ber', 'lin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'pupp', 'ete', 'er', 'to', 'see', '.',  '</s>'",0
\     /       |        |         |       |      |      |         \      |      /     |      |      |,0
Berlin      and    Munich     have      a     lot     of           puppeteer        to    see     .,0
,0
0          1        2         3       4      5       6               7             8     9      10,0
First subword embedding,0
Last subword embedding,0
First token is splitted into two subwords.,0
"As we use ""last"" as pooling operation, we consider the last subword as ""first token"" here",0
First and last subword embedding,0
Mean of all subword embeddings,0
Check embedding dimension when using multiple layers,0
Check embedding dimension when using multiple layers and scalar mix,0
0             1           2            3          4         5         6        7       8       9        10        11         12,0
,0
"'berlin</w>', 'and</w>', 'munich</w>', 'have</w>', 'a</w>', 'lot</w>', 'of</w>', 'pupp', 'ete', 'er</w>', 'to</w>', 'see</w>', '.</w>'",0
|             |           |            |          |         |         |         \      |      /          |         |          |,0
Berlin         and        Munich        have        a        lot        of           puppeteer             to       see         .,0
,0
0             1           2            3          4         5         6                7                  8        9          10,0
First subword embedding,0
Last subword embedding,0
First and last subword embedding,0
Mean of all subword embeddings,0
Check embedding dimension when using multiple layers,0
Check embedding dimension when using multiple layers and scalar mix,0
0           1      2       3        4         5       6      7      8       9      10     11     12     13    14          15,0
,0
"'<|endoftext|>', 'Ber', 'lin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'pupp', 'ete', 'er', 'to', 'see', '.', '<|endoftext|>'",0
\     /       |        |         |       |      |      |         \      |      /     |      |      |,0
Berlin      and    Munich     have      a     lot     of           puppeteer        to    see     .,0
,0
0          1        2         3       4      5       6               7             8     9      10,0
First subword embedding,0
Last subword embedding,0
First token is splitted into two subwords.,0
"As we use ""last"" as pooling operation, we consider the last subword as ""first token"" here",0
First and last subword embedding,0
Mean of all subword embeddings,0
Check embedding dimension when using multiple layers,0
Check embedding dimension when using multiple layers and scalar mix,0
0        1         2         3         4      5      6      7        8        9      10      11   12   13     14,0
,0
"'<s>', 'Berlin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'puppet', 'eer', 'to', 'see', '', '.', '</s>'",0
|          |         |         |      |      |      |         \      /       |       |     \    /,0
Berlin      and     Munich     have     a     lot     of       puppeteer       to     see       .,0
,0
0          1         2         3      4      5       6           7           8       9        10,0
First subword embedding,0
Last subword embedding,0
First and last subword embedding,0
Mean of all subword embeddings,0
Check embedding dimension when using multiple layers,0
Check embedding dimension when using multiple layers and scalar mix,0
0       1        2        3     4     5      6        7        8      9     10     11,0
,0
"'Berlin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'puppeteer', 'to', 'see', '.', '<eos>'",0
|       |        |        |     |     |      |        |        |      |     |,0
Berlin    and    Munich    have    a    lot    of    puppeteer    to    see    .,0
,0
0       1        2        3     4     5      6        7        8      9     10,0
Check embedding dimension when using multiple layers,0
Check embedding dimension when using multiple layers and scalar mix,0
0      1             2           3            4          5         6         7         8       9      10        11       12         13        14,0
,0
"<s>  'berlin</w>', 'and</w>', 'munich</w>', 'have</w>', 'a</w>', 'lot</w>', 'of</w>', 'pupp', 'ete', 'er</w>', 'to</w>', 'see</w>', '.</w>', '</s>",0
|             |           |            |          |         |         |         \      |      /          |         |          |,0
Berlin         and        Munich        have        a        lot        of           puppeteer             to       see         .,0
,0
0             1           2            3          4         5          6               7                  8        9          10,0
First subword embedding,0
Last subword embedding,0
First and last subword embedding,0
Mean of all subword embeddings,0
Check embedding dimension when using multiple layers,0
Check embedding dimension when using multiple layers and scalar mix,0
0       1          2         3         4      5       6       7    8      9,0
,0
"'<s>',   'J',      ""'"",     'aime',   'le', 'ca', 'member', 't', '!', '</s>'",0
\          |         /         |      \       |        /   |,0
J'aime                le         camembert        !,0
,0
0                   1              2            3,0
First subword embedding,0
Last subword embedding,0
First and last subword embedding,0
Mean of all subword embeddings,0
Check embedding dimension when using multiple layers,0
Check embedding dimension when using multiple layers and scalar mix,0
"get training, test and dev data",0
1. get the corpus,0
2. what tag do we want to predict?,0
3. make the tag dictionary from the corpus,0
initialize embeddings,0
comment in this line to use character embeddings,0
"CharacterEmbeddings(),",0
comment in these lines to use contextual string embeddings,0
,0
"FlairEmbeddings('news-forward'),",0
,0
"FlairEmbeddings('news-backward'),",0
initialize sequence tagger,0
initialize trainer,0
"if only one sentence is passed, convert to list of sentence",0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNINOS embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
fix serialized models,0
max_tokens = 500,0
model architecture,0
download if necessary,0
load the model,0
empty words get no embedding,0
all other words get embedded,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
embed a dummy sentence to determine embedding_length,0
Avoid conflicts with flair's Token class,0
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol,0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
multilingual models,0
English models,0
Arabic,0
Bulgarian,0
Czech,0
Danish,0
German,0
Spanish,0
Basque,0
Persian,0
Finnish,0
French,0
Hebrew,0
Hindi,0
Croatian,0
Indonesian,0
Italian,0
Japanese,0
Dutch,0
Norwegian,0
Polish,0
Portuguese,0
Pubmed,0
Slovenian,0
Swedish,0
Tamil,0
load model if in pretrained model map,0
embeddings are static if we don't do finetuning,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
make compatible with serialized models (TODO: remove),1
gradients are enable if fine-tuning is enabled,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
only clone if optimization mode is 'gpu',0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
check token.text is empty or not,0
add embeddings after updating,0
The mask has 1 for real tokens and 0 for padding tokens. Only real,0
tokens are attended to.,0
Zero-pad up to the sequence length.,0
"first, find longest sentence in batch",0
prepare id maps for BERT model,0
put encoded batch through BERT model to get all hidden states of all encoder layers,0
get aggregated embeddings for each BERT-subtoken in sentence,0
get the current sentence object,0
add concatenated embedding to sentence,0
use first subword embedding if pooling operation is 'first',0
"otherwise, do a mean over all subwords in token",0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
"if only one sentence is passed, convert to list of sentence",0
optional fine-tuning on top of embedding layer,0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
TODO: remove in future versions,1
embed words in the sentence,0
initialize zero-padded word embeddings tensor,0
sentence_tensor = torch.zeros(,0
[,0
"len(sentences),",0
"longest_token_sequence_in_batch,",0
"self.embeddings.embedding_length,",0
"],",0
"dtype=torch.float,",0
"device=flair.device,",0
),0
,0
"for s_id, sentence in enumerate(sentences):",0
# fill values with word embeddings,0
all_embs = list(),0
,0
"for index_token, token in enumerate(sentence):",0
embs = token.get_each_embedding(),0
if not all_embs:,0
all_embs = [list() for _ in range(len(embs))],0
"for index_emb, emb in enumerate(embs):",0
all_embs[index_emb].append(emb),0
,0
concat_word_emb = [torch.stack(embs) for embs in all_embs],0
"concat_sentence_emb = torch.cat(concat_word_emb, dim=1)",0
sentence_tensor[s_id][: len(sentence)] = concat_sentence_emb,0
before-RNN dropout,0
reproject if set,0
push through RNN,0
after-RNN dropout,0
extract embeddings from RNN,0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
IMPORTANT: add embeddings as torch modules,0
iterate over sentences,0
"if its a forward LM, take last state",0
GLOVE embeddings,0
"<cls> token initially set to 1/D, so it attends to all image features equally",0
add positional encodings,0
reshape the pixels into the sequence,0
layer norm after convolution and positional encodings,0
add <cls> token,0
"transformer requires input in the shape [h*w+1, b, d]",0
the output is an embedding of <cls> token,0
"TODO: keep for backwards compatibility, but remove in future",1
save the sentence piece model as binary file (not as path which may change),0
write out the binary sentence piece model into the expected directory,0
"if the model was saved as binary and it is not found on disk, write to appropriate path",0
"otherwise, use normal process and potentially trigger another download",0
"once the modes if there, load it with sentence piece",0
from allennlp.common.tqdm import Tqdm,0
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
Extract all the contents of zip file in current directory,0
get cache path to put the file,0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
first determine the distribution of classes in the dataset,0
weight for each sample,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups,1
see https://github.com/zalandoresearch/flair/issues/351,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
Maintains max of all exp. moving avg. of sq. grad. values,0
Decay the first and second moment running average coefficient,0
Maintains the maximum of all 2nd moment running avg. till now,0
Use the max. for normalizing running avg. of gradient,0
conll 2000 column format,0
conll 03 NER column format,0
WNUT-17,0
-- WikiNER datasets,0
-- Universal Dependencies,0
Germanic,0
Romance,0
West-Slavic,0
South-Slavic,0
East-Slavic,0
Scandinavian,0
Asian,0
Language isolates,0
recent Universal Dependencies,0
other datasets,0
text classification format,0
text regression format,0
"first, try to fetch dataset online",0
default dataset folder is the cache root,0
get string value if enum is passed,0
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)",0
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag",0
the CoNLL 03 task for German has an additional lemma column,0
the CoNLL 03 task for Dutch has no NP column,0
the CoNLL 03 task for Spanish only has two columns,0
the GERMEVAL task only has two columns: text and ner,0
WSD tasks may be put into this column format,0
"the UD corpora follow the CoNLL-U format, for which we have a special reader",0
"for text classifiers, we use our own special format",0
NER corpus for Basque,0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
get train and test data,0
"read in test file if exists, otherwise sample 10% of train data as test dataset",0
"read in dev file if exists, otherwise sample 10% of train data as dev dataset",0
convert tag scheme to iobes,0
automatically identify train / test / dev files,0
automatically identify train / test / dev files,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
conll 2000 chunking task,0
Support both TREC-6 and TREC-50,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
Wikiner NER task,0
unpack and write out in CoNLL column-like format,0
CoNLL 02/03 NER,0
universal dependencies,0
--- UD Germanic,0
--- UD Romance,0
--- UD West-Slavic,0
--- UD Scandinavian,0
--- UD South-Slavic,0
--- UD Asian,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
increment for last token in sentence if not followed by whitespace,0
determine offsets for whitespace_after field,0
determine offsets for whitespace_after field,0
"if text is passed, instantiate sentence with tokens (words)",0
log a warning if the dataset is empty,0
set token idx if not set,0
non-set tags are OUT tags,0
anything that is not a BIOES tag is a SINGLE tag,0
anything that is not OUT is IN,0
single and begin tags start a new span,0
remember previous tag,0
move sentence embeddings to device,0
move token embeddings to device,0
clear sentence embeddings,0
clear token embeddings,0
infer whitespace after field,0
No character at the corresponding code point: remove it,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
Make the tag dictionary,0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
get train data,0
"read in test file if exists, otherwise sample 10% of train data as test dataset",0
"read in dev file if exists, otherwise sample 10% of train data as dev dataset",0
automatically identify train / test / dev files,0
get train data,0
get test data,0
get dev data,0
automatically identify train / test / dev files,0
use test_file to create test split if available,0
"otherwise, sample test data from train data",0
use dev_file to create test split if available,0
"otherwise, sample dev data from dev data",0
cache Feidegger config file,0
cache Feidegger images,0
replace image URL with local cached file,0
automatically identify train / test / dev files,0
check if dataset is supported,0
set file names,0
download and unzip in file structure if necessary,0
instantiate corpus,0
cast to list if necessary,0
cast to list if necessary,0
"store either Sentence objects in memory, or only file offsets",0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
determine encoding of text file,0
variables,0
different handling of in_memory data than streaming data,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
test if format is OK,0
test if at least one label given,0
append Sentence-Image data point,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download data if necessary,0
unpack and write out in CoNLL column-like format,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"in certain cases, multi-CPU data loading makes no sense and slows",0
"everything down. For this reason, we detect if a dataset is in-memory:",0
"if so, num_workers is set to 0 for faster processing",0
global variable: cache_root,0
global variable: device,0
global variable: embedding_storage_mode,0
# dummy return to fulfill trainer.train() needs,0
if memory mode option 'none' delete everything,0
else delete only dynamic embeddings (otherwise autograd will keep everything in memory),0
find out which ones are dynamic embeddings,0
find out which ones are dynamic embeddings,0
memory management - option 1: send everything to CPU (pin to memory if we train on GPU),0
record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class),0
to enable %matplotlib inline if running in ipynb,0
change from Agg to TkAgg for interative mode,0
change from Agg to TkAgg for interative mode,0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
print(rows),0
"figsize = (16, 16)",0
plot i,0
save plots,0
save plots,0
plt.show(),0
save plot,0
take the average over the last three scores of training,0
take average over the scores from the different training runs,0
remove previous embeddings,0
clearing token embeddings to save memory,0
#TODO: not saving lines yet,1
== similarity measures ==,0
helper class for ModelSimilarity,0
-- works with binary cross entropy loss --,0
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}",0
-- works with ranking/triplet loss --,0
normalize the embeddings,0
== similarity losses ==,0
"we want that logits for corresponding pairs are high, and for non-corresponding low",0
TODO: this assumes eye matrix,0
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa",0
== similarity learner ==,0
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both",0
assumes that for each data pair there's at least one embedding per modality,0
pre-compute embeddings for all targets in evaluation dataset,0
compute the similarity,0
sort the similarity matrix across modality 1,0
"get the ranks, so +1 to start counting ranks from 1",0
The conversion from old model's constructor interface,0
auto-spawn on GPU if available,0
pad strings with whitespaces to longest sentence,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
set the dictionaries,0
initialize the network architecture,0
dropouts,0
bidirectional LSTM on top of embedding layer,0
Create initial hidden state and initialize it,0
TODO: Decide how to initialize the hidden state variables,1
self.hs_initializer(self.lstm_init_h),0
self.hs_initializer(self.lstm_init_c),0
final linear map to tag space,0
reverse sort all sequences by their length,0
remove previous embeddings,0
progress bar for verbosity,0
stop if all sentences are empty,0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
clearing token embeddings to save memory,0
append both to file for evaluation,0
make list of gold tags,0
make list of predicted tags,0
"check for true positives, false positives and false negatives",0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
"if initial hidden state is trainable, use this state",0
word dropout only before LSTM - TODO: more experimentation needed,1
if self.use_word_dropout > 0.0:,0
sentence_tensor = self.word_dropout(sentence_tensor),0
get the tags in this sentence,0
add tags as tensor,0
pad tags if using batch-CRF decoder,0
reduce raw values to avoid NaN during exp,0
broadcasting will do the job of reshaping and is more efficient than calling repeat,0
default value,0
auto-spawn on GPU if available,0
reverse sort all sequences by their length,0
remove previous embeddings,0
progress bar for verbosity,0
stop if all sentences are empty,0
clearing token embeddings to save memory,0
cast string to Path,0
"determine what splits (train, dev, test) to evaluate and log",0
prepare loss logging file and set up header,0
"minimize training loss if training with dev data, else maximize dev score",0
"if training also uses dev data, include in training set",0
initialize sampler if provided,0
init with default values if only class is provided,0
set dataset to sample from,0
At any point you can hit Ctrl + C to break out of training early.,0
get new learning rate,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
process mini-batches,0
zero the gradients on the model and optimizer,0
"if necessary, make batch_steps",0
forward and backward for batch,0
forward pass,0
Backward,0
do the optimizer step,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"anneal against train loss if training with dev, otherwise anneal against dev score",0
evaluate on train / dev / test split depending on training settings,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
calculate scores using dev data if available,0
append dev score to score history,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
determine learning rate annealing through scheduler,0
determine bad epoch number,0
log bad epochs,0
output log file,0
make headers on first epoch,0
"if checkpoint is enabled, save model at each epoch",0
"if we use dev data, remember best model based on dev evaluation score",0
"if we do not use dev data for model selection, save final model",0
test best model if test data is present,0
"if we are training over multiple datasets, do evaluation for each",0
get and return the final test score of best model,0
cast string to Path,0
forward pass,0
update optimizer and scheduler,0
Add chars to the dictionary,0
charsplit file content,0
charsplit file content,0
Add words to the dictionary,0
Tokenize file content,0
"TextDataset returns a list. valid and test are only one file, so return the first element",0
cast string to Path,0
Shuffle training files randomly after serially iterating through corpus one,0
"iterate through training data, starting at self.split (for checkpointing)",0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
Backward,0
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.,0
We detach the hidden state from how it was previously produced.,0
"If we didn't, the model would try backpropagating all the way to start of the dataset.",0
explicitly remove loss to clear up memory,0
##############################################################################,0
Save the model if the validation loss is the best we've seen so far.,0
##############################################################################,0
print info,0
##############################################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
clean up file,0
bioes tags,0
bio tags,0
broken tags,0
all tags,0
all weird tags,0
tags with confidence,0
bioes tags,0
bioes tags,0
clean up directory,0
clean up directory,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
def test_multiclass_metrics():,0
,0
"metric = Metric(""Test"")",0
"available_labels = [""A"", ""B"", ""C""]",0
,0
"predictions = [""A"", ""B""]",0
"true_values = [""A""]",0
TextClassifier._evaluate_sentence_for_text_classification(,0
"metric, available_labels, predictions, true_values",0
),0
,0
"predictions = [""C"", ""B""]",0
"true_values = [""A"", ""B""]",0
TextClassifier._evaluate_sentence_for_text_classification(,0
"metric, available_labels, predictions, true_values",0
),0
,0
print(metric),0
from flair.trainers.trainer_regression import RegressorTrainer,0
def test_trainer_results(tasks_base_path):,0
"corpus, model, trainer = init(tasks_base_path)",0
"results = trainer.train(""regression_train/"", max_epochs=1)",0
"assert results[""test_score""] > 0",0
"assert len(results[""dev_loss_history""]) == 1",0
"assert len(results[""dev_score_history""]) == 1",0
"assert len(results[""train_loss_history""]) == 1",0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
initialize trainer,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
clean up results directory,0
get default dictionary,0
get the example corpus and process at character level in forward direction,0
define search space,0
sequence tagger parameter,0
model trainer parameter,0
training parameter,0
find best parameter settings,0
clean up results directory,0
document embeddings parameter,0
training parameter,0
clean up results directory,0
0           1      2       3        4         5       6      7      8       9      10     11     12     13    14      15,0
,0
"'<s>',      'Ber', 'lin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'pupp', 'ete', 'er', 'to', 'see', '.',  '</s>'",0
\     /       |        |         |       |      |      |         \      |      /     |      |      |,0
Berlin      and    Munich     have      a     lot     of           puppeteer        to    see     .,0
,0
0          1        2         3       4      5       6               7             8     9      10,0
First subword embedding,0
Last subword embedding,0
First token is splitted into two subwords.,0
"As we use ""last"" as pooling operation, we consider the last subword as ""first token"" here",0
First and last subword embedding,0
Mean of all subword embeddings,0
Check embedding dimension when using multiple layers,0
Check embedding dimension when using multiple layers and scalar mix,0
0             1           2            3          4         5         6        7       8       9        10        11         12,0
,0
"'berlin</w>', 'and</w>', 'munich</w>', 'have</w>', 'a</w>', 'lot</w>', 'of</w>', 'pupp', 'ete', 'er</w>', 'to</w>', 'see</w>', '.</w>'",0
|             |           |            |          |         |         |         \      |      /          |         |          |,0
Berlin         and        Munich        have        a        lot        of           puppeteer             to       see         .,0
,0
0             1           2            3          4         5         6                7                  8        9          10,0
First subword embedding,0
Last subword embedding,0
First and last subword embedding,0
Mean of all subword embeddings,0
Check embedding dimension when using multiple layers,0
Check embedding dimension when using multiple layers and scalar mix,0
0           1      2       3        4         5       6      7      8       9      10     11     12     13    14          15,0
,0
"'<|endoftext|>', 'Ber', 'lin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'pupp', 'ete', 'er', 'to', 'see', '.', '<|endoftext|>'",0
\     /       |        |         |       |      |      |         \      |      /     |      |      |,0
Berlin      and    Munich     have      a     lot     of           puppeteer        to    see     .,0
,0
0          1        2         3       4      5       6               7             8     9      10,0
First subword embedding,0
Last subword embedding,0
First token is splitted into two subwords.,0
"As we use ""last"" as pooling operation, we consider the last subword as ""first token"" here",0
First and last subword embedding,0
Mean of all subword embeddings,0
Check embedding dimension when using multiple layers,0
Check embedding dimension when using multiple layers and scalar mix,0
0        1         2         3         4      5      6      7        8        9      10      11   12   13     14,0
,0
"'<s>', 'Berlin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'puppet', 'eer', 'to', 'see', '', '.', '</s>'",0
|          |         |         |      |      |      |         \      /       |       |     \    /,0
Berlin      and     Munich     have     a     lot     of       puppeteer       to     see       .,0
,0
0          1         2         3      4      5       6           7           8       9        10,0
First subword embedding,0
Last subword embedding,0
First and last subword embedding,0
Mean of all subword embeddings,0
Check embedding dimension when using multiple layers,0
Check embedding dimension when using multiple layers and scalar mix,0
0       1        2        3     4     5      6        7        8      9     10     11,0
,0
"'Berlin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'puppeteer', 'to', 'see', '.', '<eos>'",0
|       |        |        |     |     |      |        |        |      |     |,0
Berlin    and    Munich    have    a    lot    of    puppeteer    to    see    .,0
,0
0       1        2        3     4     5      6        7        8      9     10,0
Check embedding dimension when using multiple layers,0
Check embedding dimension when using multiple layers and scalar mix,0
0      1             2           3            4          5         6         7         8       9      10        11       12         13        14,0
,0
"<s>  'berlin</w>', 'and</w>', 'munich</w>', 'have</w>', 'a</w>', 'lot</w>', 'of</w>', 'pupp', 'ete', 'er</w>', 'to</w>', 'see</w>', '.</w>', '</s>",0
|             |           |            |          |         |         |         \      |      /          |         |          |,0
Berlin         and        Munich        have        a        lot        of           puppeteer             to       see         .,0
,0
0             1           2            3          4         5          6               7                  8        9          10,0
First subword embedding,0
Last subword embedding,0
First and last subword embedding,0
Mean of all subword embeddings,0
Check embedding dimension when using multiple layers,0
Check embedding dimension when using multiple layers and scalar mix,0
"get training, test and dev data",0
1. get the corpus,0
2. what tag do we want to predict?,0
3. make the tag dictionary from the corpus,0
initialize embeddings,0
comment in this line to use character embeddings,0
"CharacterEmbeddings(),",0
comment in these lines to use contextual string embeddings,0
,0
"FlairEmbeddings('news-forward'),",0
,0
"FlairEmbeddings('news-backward'),",0
initialize sequence tagger,0
initialize trainer,0
"if only one sentence is passed, convert to list of sentence",0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNINOS embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
fix serialized models,0
max_tokens = 500,0
model architecture,0
save the sentence piece model as binary file (not as path which may change),0
write out the binary sentence piece model into the expected directory,0
"if the model was saved as binary and it is not found on disk, write to appropriate path",0
"otherwise, use normal process and potentially trigger another download",0
"once the modes if there, load it with sentence piece",0
download if necessary,0
load the model,0
empty words get no embedding,0
all other words get embedded,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
embed a dummy sentence to determine embedding_length,0
Avoid conflicts with flair's Token class,0
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol,0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
multilingual models,0
English models,0
Arabic,0
Bulgarian,0
Czech,0
Danish,0
German,0
Spanish,0
Basque,0
Persian,0
Finnish,0
French,0
Hebrew,0
Hindi,0
Croatian,0
Indonesian,0
Italian,0
Japanese,0
Dutch,0
Norwegian,0
Polish,0
Portuguese,0
Pubmed,0
Slovenian,0
Swedish,0
Tamil,0
load model if in pretrained model map,0
embeddings are static if we don't do finetuning,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
make compatible with serialized models (TODO: remove),1
gradients are enable if fine-tuning is enabled,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
pad strings with whitespaces to longest sentence,0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
add embeddings after updating,0
The mask has 1 for real tokens and 0 for padding tokens. Only real,0
tokens are attended to.,0
Zero-pad up to the sequence length.,0
"first, find longest sentence in batch",0
prepare id maps for BERT model,0
put encoded batch through BERT model to get all hidden states of all encoder layers,0
get aggregated embeddings for each BERT-subtoken in sentence,0
get the current sentence object,0
add concatenated embedding to sentence,0
use first subword embedding if pooling operation is 'first',0
"otherwise, do a mean over all subwords in token",0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
pad strings with whitespaces to longest sentence,0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
"if only one sentence is passed, convert to list of sentence",0
optional fine-tuning on top of embedding layer,0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
"the permutation that sorts the sentences by length, descending",0
the inverse permutation that restores the input order; it's an index tensor therefore LongTensor,0
sort sentences by number of tokens,0
all_sentence_tensors = [],0
initialize zero-padded word embeddings tensor,0
fill values with word embeddings,0
TODO: this can only be removed once the implementations of word_dropout and locked_dropout have a batch_first mode,1
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM RNN,0
--------------------------------------------------------------------,0
restore original order of sentences in the batch,0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
IMPORTANT: add embeddings as torch modules,0
iterate over sentences,0
"if its a forward LM, take last state",0
GLOVE embeddings,0
from allennlp.common.tqdm import Tqdm,0
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
unpack and write out in CoNLL column-like format,0
Extract all the contents of zip file in current directory,0
get cache path to put the file,0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
first determine the distribution of classes in the dataset,0
weight for each sample,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
Create blocks,0
shuffle the blocks,0
concatenate the shuffled blocks,0
additional fields for model checkpointing,0
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups,1
see https://github.com/zalandoresearch/flair/issues/351,0
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups,1
see https://github.com/zalandoresearch/flair/issues/351,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
Maintains max of all exp. moving avg. of sq. grad. values,0
Decay the first and second moment running average coefficient,0
Maintains the maximum of all 2nd moment running avg. till now,0
Use the max. for normalizing running avg. of gradient,0
conll 2000 column format,0
conll 03 NER column format,0
WNUT-17,0
-- WikiNER datasets,0
-- Universal Dependencies,0
Germanic,0
Romance,0
West-Slavic,0
South-Slavic,0
East-Slavic,0
Scandinavian,0
Asian,0
Language isolates,0
recent Universal Dependencies,0
other datasets,0
text classification format,0
text regression format,0
"first, try to fetch dataset online",0
default dataset folder is the cache root,0
get string value if enum is passed,0
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)",0
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag",0
the CoNLL 03 task for German has an additional lemma column,0
the CoNLL 03 task for Dutch has no NP column,0
the CoNLL 03 task for Spanish only has two columns,0
the GERMEVAL task only has two columns: text and ner,0
WSD tasks may be put into this column format,0
"the UD corpora follow the CoNLL-U format, for which we have a special reader",0
"for text classifiers, we use our own special format",0
NER corpus for Basque,0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
get train and test data,0
"read in test file if exists, otherwise sample 10% of train data as test dataset",0
"read in dev file if exists, otherwise sample 10% of train data as dev dataset",0
convert tag scheme to iobes,0
automatically identify train / test / dev files,0
automatically identify train / test / dev files,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
conll 2000 chunking task,0
Support both TREC-6 and TREC-50,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
Wikiner NER task,0
unpack and write out in CoNLL column-like format,0
CoNLL 02/03 NER,0
universal dependencies,0
--- UD Germanic,0
--- UD Romance,0
--- UD West-Slavic,0
--- UD Scandinavian,0
--- UD South-Slavic,0
--- UD Asian,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
"if text is passed, instantiate sentence with tokens (words)",0
tokenize the text first if option selected,0
use segtok for tokenization,0
determine offsets for whitespace_after field,0
otherwise assumes whitespace tokenized text,0
add each word in tokenized string as Token object to Sentence,0
increment for last token in sentence if not followed by whtespace,0
log a warning if the dataset is empty,0
set token idx if not set,0
non-set tags are OUT tags,0
anything that is not a BIOES tag is a SINGLE tag,0
anything that is not OUT is IN,0
single and begin tags start a new span,0
remember previous tag,0
move sentence embeddings to device,0
move token embeddings to device,0
clear sentence embeddings,0
clear token embeddings,0
infer whitespace after field,0
find out empty sentence indices,0
create subset of non-empty sentence indices,0
Make the tag dictionary,0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
get train data,0
"read in test file if exists, otherwise sample 10% of train data as test dataset",0
"read in dev file if exists, otherwise sample 10% of train data as dev dataset",0
automatically identify train / test / dev files,0
get train data,0
get test data,0
get dev data,0
automatically identify train / test / dev files,0
automatically identify train / test / dev files,0
cast to list if necessary,0
"store either Sentence objects in memory, or only file offsets",0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
determine encoding of text file,0
variables,0
different handling of in_memory data than streaming data,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
test if format is OK,0
test if at least one label given,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download data if necessary,0
unpack and write out in CoNLL column-like format,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
"in certain cases, multi-CPU data loading makes no sense and slows",0
"everything down. For this reason, we detect if a dataset is in-memory:",0
"if so, num_workers is set to 0 for faster processing",0
global variable: cache_root,0
global variable: device,0
# dummy return to fulfill trainer.train() needs,0
if memory mode option 'none' delete everything,0
else delete only dynamic embeddings (otherwise autograd will keep everything in memory),0
find out which ones are dynamic embeddings,0
find out which ones are dynamic embeddings,0
memory management - option 1: send everything to CPU,0
to enable %matplotlib inline if running in ipynb,0
change from Agg to TkAgg for interative mode,0
change from Agg to TkAgg for interative mode,0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
print(rows),0
"figsize = (16, 16)",0
plot i,0
save plots,0
save plots,0
plt.show(),0
save plot,0
take the average over the last three scores of training,0
take average over the scores from the different training runs,0
remove previous embeddings,0
clearing token embeddings to save memory,0
#TODO: not saving lines yet,1
auto-spawn on GPU if available,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
set the dictionaries,0
initialize the network architecture,0
dropouts,0
bidirectional LSTM on top of embedding layer,0
Create initial hidden state and initialize it,0
TODO: Decide how to initialize the hidden state variables,1
self.hs_initializer(self.lstm_init_h),0
self.hs_initializer(self.lstm_init_c),0
final linear map to tag space,0
append both to file for evaluation,0
make list of gold tags,0
make list of predicted tags,0
"check for true positives, false positives and false negatives",0
remove previous embeddings,0
reverse sort all sequences by their length,0
make mini-batches,0
progress bar for verbosity,0
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided",0
clearing token embeddings to save memory,0
initialize zero-padded word embeddings tensor,0
fill values with word embeddings,0
TODO: this can only be removed once the implementations of word_dropout and locked_dropout have a batch_first mode,1
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
"if initial hidden state is trainable, use this state",0
word dropout only before LSTM - TODO: more experimentation needed,1
if self.use_word_dropout > 0.0:,0
sentence_tensor = self.word_dropout(sentence_tensor),0
transpose to batch_first mode,0
get the tags in this sentence,0
add tags as tensor,0
pad tags if using batch-CRF decoder,0
return all scores if so selected,0
auto-spawn on GPU if available,0
remove previous embeddings,0
clearing token embeddings to save memory,0
cast string to Path,0
"determine what splits (train, dev, test) to evaluate and log",0
prepare loss logging file and set up header,0
"minimize training loss if training with dev data, else maximize dev score",0
"if training also uses dev data, include in training set",0
At any point you can hit Ctrl + C to break out of training early.,0
get new learning rate,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
process mini-batches,0
Backward,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"anneal against train loss if training with dev, otherwise anneal against dev score",0
evaluate on train / dev / test split depending on training settings,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
calculate scores using dev data if available,0
append dev score to score history,0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
"depending on memory mode, embeddings are moved to CPU, GPU or deleted",0
determine learning rate annealing through scheduler,0
determine bad epoch number,0
log bad epochs,0
output log file,0
make headers on first epoch,0
"if checkpoint is enable, save model at each epoch",0
"if we use dev data, remember best model based on dev evaluation score",0
"if we do not use dev data for model selection, save final model",0
test best model if test data is present,0
"if we are training over multiple datasets, do evaluation for each",0
get and return the final test score of best model,0
cast string to Path,0
Add chars to the dictionary,0
charsplit file content,0
charsplit file content,0
Add words to the dictionary,0
Tokenize file content,0
"TextDataset returns a list. valid and test are only one file, so return the first element",0
cast string to Path,0
Shuffle training files randomly after serially iterating through corpus one,0
"iterate through training data, starting at self.split (for checkpointing)",0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
Backward,0
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.,0
We detach the hidden state from how it was previously produced.,0
"If we didn't, the model would try backpropagating all the way to start of the dataset.",0
explicitly remove loss to clear up memory,0
##############################################################################,0
Save the model if the validation loss is the best we've seen so far.,0
##############################################################################,0
print info,0
##############################################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
clean up file,0
bioes tags,0
bio tags,0
broken tags,0
all tags,0
all weird tags,0
tags with confidence,0
bioes tags,0
bioes tags,0
clean up directory,0
clean up directory,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
def test_multiclass_metrics():,0
,0
"metric = Metric(""Test"")",0
"available_labels = [""A"", ""B"", ""C""]",0
,0
"predictions = [""A"", ""B""]",0
"true_values = [""A""]",0
TextClassifier._evaluate_sentence_for_text_classification(,0
"metric, available_labels, predictions, true_values",0
),0
,0
"predictions = [""C"", ""B""]",0
"true_values = [""A"", ""B""]",0
TextClassifier._evaluate_sentence_for_text_classification(,0
"metric, available_labels, predictions, true_values",0
),0
,0
print(metric),0
from flair.trainers.trainer_regression import RegressorTrainer,0
def test_trainer_results(tasks_base_path):,0
"corpus, model, trainer = init(tasks_base_path)",0
"results = trainer.train(""regression_train/"", max_epochs=1)",0
"assert results[""test_score""] > 0",0
"assert len(results[""dev_loss_history""]) == 1",0
"assert len(results[""dev_score_history""]) == 1",0
"assert len(results[""train_loss_history""]) == 1",0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
initialize trainer,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
clean up results directory,0
get default dictionary,0
get the example corpus and process at character level in forward direction,0
define search space,0
sequence tagger parameter,0
model trainer parameter,0
training parameter,0
find best parameter settings,0
clean up results directory,0
document embeddings parameter,0
training parameter,0
clean up results directory,0
0           1      2       3        4         5       6      7      8       9      10     11     12     13    14      15,0
,0
"'<s>',      'Ber', 'lin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'pupp', 'ete', 'er', 'to', 'see', '.',  '</s>'",0
\     /       |        |         |       |      |      |         \      |      /     |      |      |,0
Berlin      and    Munich     have      a     lot     of           puppeteer        to    see     .,0
,0
0          1        2         3       4      5       6               7             8     9      10,0
First subword embedding,0
Last subword embedding,0
First token is splitted into two subwords.,0
"As we use ""last"" as pooling operation, we consider the last subword as ""first token"" here",0
First and last subword embedding,0
Mean of all subword embeddings,0
Check embedding dimension when using multiple layers,0
Check embedding dimension when using multiple layers and scalar mix,0
0             1           2            3          4         5         6        7       8       9        10        11         12,0
,0
"'berlin</w>', 'and</w>', 'munich</w>', 'have</w>', 'a</w>', 'lot</w>', 'of</w>', 'pupp', 'ete', 'er</w>', 'to</w>', 'see</w>', '.</w>'",0
|             |           |            |          |         |         |         \      |      /          |         |          |,0
Berlin         and        Munich        have        a        lot        of           puppeteer             to       see         .,0
,0
0             1           2            3          4         5         6                7                  8        9          10,0
First subword embedding,0
Last subword embedding,0
First and last subword embedding,0
Mean of all subword embeddings,0
Check embedding dimension when using multiple layers,0
Check embedding dimension when using multiple layers and scalar mix,0
0           1      2       3        4         5       6      7      8       9      10     11     12     13    14          15,0
,0
"'<|endoftext|>', 'Ber', 'lin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'pupp', 'ete', 'er', 'to', 'see', '.', '<|endoftext|>'",0
\     /       |        |         |       |      |      |         \      |      /     |      |      |,0
Berlin      and    Munich     have      a     lot     of           puppeteer        to    see     .,0
,0
0          1        2         3       4      5       6               7             8     9      10,0
First subword embedding,0
Last subword embedding,0
First token is splitted into two subwords.,0
"As we use ""last"" as pooling operation, we consider the last subword as ""first token"" here",0
First and last subword embedding,0
Mean of all subword embeddings,0
Check embedding dimension when using multiple layers,0
Check embedding dimension when using multiple layers and scalar mix,0
0        1         2         3         4      5      6      7        8        9      10      11   12   13     14,0
,0
"'<s>', 'Berlin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'puppet', 'eer', 'to', 'see', '', '.', '</s>'",0
|          |         |         |      |      |      |         \      /       |       |     \    /,0
Berlin      and     Munich     have     a     lot     of       puppeteer       to     see       .,0
,0
0          1         2         3      4      5       6           7           8       9        10,0
First subword embedding,0
Last subword embedding,0
First and last subword embedding,0
Mean of all subword embeddings,0
Check embedding dimension when using multiple layers,0
Check embedding dimension when using multiple layers and scalar mix,0
0       1        2        3     4     5      6        7        8      9     10     11,0
,0
"'Berlin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'puppeteer', 'to', 'see', '.', '<eos>'",0
|       |        |        |     |     |      |        |        |      |     |,0
Berlin    and    Munich    have    a    lot    of    puppeteer    to    see    .,0
,0
0       1        2        3     4     5      6        7        8      9     10,0
Check embedding dimension when using multiple layers,0
Check embedding dimension when using multiple layers and scalar mix,0
0      1             2           3            4          5         6         7         8       9      10        11       12         13        14,0
,0
"<s>  'berlin</w>', 'and</w>', 'munich</w>', 'have</w>', 'a</w>', 'lot</w>', 'of</w>', 'pupp', 'ete', 'er</w>', 'to</w>', 'see</w>', '.</w>', '</s>",0
|             |           |            |          |         |         |         \      |      /          |         |          |,0
Berlin         and        Munich        have        a        lot        of           puppeteer             to       see         .,0
,0
0             1           2            3          4         5          6               7                  8        9          10,0
First subword embedding,0
Last subword embedding,0
First and last subword embedding,0
Mean of all subword embeddings,0
Check embedding dimension when using multiple layers,0
Check embedding dimension when using multiple layers and scalar mix,0
"get training, test and dev data",0
1. get the corpus,0
2. what tag do we want to predict?,0
3. make the tag dictionary from the corpus,0
initialize embeddings,0
comment in this line to use character embeddings,0
"CharacterEmbeddings(),",0
comment in these lines to use contextual string embeddings,0
,0
"FlairEmbeddings('news-forward'),",0
,0
"FlairEmbeddings('news-backward'),",0
initialize sequence tagger,0
initialize trainer,0
"if only one sentence is passed, convert to list of sentence",0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNINOS embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
max_tokens = 500,0
model architecture,0
save the sentence piece model as binary file (not as path which may change),0
write out the binary sentence piece model into the expected directory,0
"if the model was saved as binary and it is not found on disk, write to appropriate path",0
"otherwise, use normal process and potentially trigger another download",0
"once the modes if there, load it with sentence piece",0
empty words get no embedding,0
all other words get embedded,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
embed a dummy sentence to determine embedding_length,0
Avoid conflicts with flair's Token class,0
Use embedding of first subword,0
Use embedding of first and last subword,0
"Otherwise, use mean over all subwords in token",0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
multilingual models,0
English models,0
Arabic,0
Bulgarian,0
Czech,0
Danish,0
German,0
Spanish,0
Basque,0
Farsi,0
Finnish,0
French,0
Hebrew,0
Hindi,0
Croatian,0
Indonesian,0
Italian,0
Japanese,0
Dutch,0
Norwegian,0
Polish,0
Portuguese,0
Pubmed,0
Slovenian,0
Swedish,0
load model if in pretrained model map,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
make compatible with serialized models,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
pad strings with whitespaces to longest sentence,0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
add embeddings after updating,0
The mask has 1 for real tokens and 0 for padding tokens. Only real,0
tokens are attended to.,0
Zero-pad up to the sequence length.,0
"first, find longest sentence in batch",0
prepare id maps for BERT model,0
put encoded batch through BERT model to get all hidden states of all encoder layers,0
get aggregated embeddings for each BERT-subtoken in sentence,0
get the current sentence object,0
add concatenated embedding to sentence,0
use first subword embedding if pooling operation is 'first',0
"otherwise, do a mean over all subwords in token",0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
pad strings with whitespaces to longest sentence,0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
"if only one sentence is passed, convert to list of sentence",0
optional fine-tuning on top of embedding layer,0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM RNN,0
--------------------------------------------------------------------,0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
iterate over sentences,0
"if its a forward LM, take last state",0
GLOVE embeddings,0
from allennlp.common.tqdm import Tqdm,0
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
unpack and write out in CoNLL column-like format,0
Extract all the contents of zip file in current directory,0
get cache path to put the file,0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
additional fields for model checkpointing,0
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups,1
see https://github.com/zalandoresearch/flair/issues/351,0
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups,1
see https://github.com/zalandoresearch/flair/issues/351,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
Maintains max of all exp. moving avg. of sq. grad. values,0
Decay the first and second moment running average coefficient,0
Maintains the maximum of all 2nd moment running avg. till now,0
Use the max. for normalizing running avg. of gradient,0
conll 2000 column format,0
conll 03 NER column format,0
WNUT-17,0
-- WikiNER datasets,0
-- Universal Dependencies,0
Germanic,0
Romance,0
West-Slavic,0
South-Slavic,0
East-Slavic,0
Scandinavian,0
Asian,0
Language isolates,0
recent Universal Dependencies,0
other datasets,0
text classification format,0
text regression format,0
"first, try to fetch dataset online",0
default dataset folder is the cache root,0
get string value if enum is passed,0
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)",0
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag",0
the CoNLL 03 task for German has an additional lemma column,0
the CoNLL 03 task for Dutch has no NP column,0
the CoNLL 03 task for Spanish only has two columns,0
the GERMEVAL task only has two columns: text and ner,0
WSD tasks may be put into this column format,0
"the UD corpora follow the CoNLL-U format, for which we have a special reader",0
"for text classifiers, we use our own special format",0
NER corpus for Basque,0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
get train and test data,0
"read in test file if exists, otherwise sample 10% of train data as test dataset",0
"read in dev file if exists, otherwise sample 10% of train data as dev dataset",0
convert tag scheme to iobes,0
automatically identify train / test / dev files,0
automatically identify train / test / dev files,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
conll 2000 chunking task,0
Support both TREC-6 and TREC-50,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
Wikiner NER task,0
unpack and write out in CoNLL column-like format,0
CoNLL 02/03 NER,0
universal dependencies,0
--- UD Germanic,0
--- UD Romance,0
--- UD West-Slavic,0
--- UD Scandinavian,0
--- UD South-Slavic,0
--- UD Asian,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
"if text is passed, instantiate sentence with tokens (words)",0
tokenize the text first if option selected,0
use segtok for tokenization,0
determine offsets for whitespace_after field,0
otherwise assumes whitespace tokenized text,0
add each word in tokenized string as Token object to Sentence,0
increment for last token in sentence if not followed by whtespace,0
log a warning if the dataset is empty,0
set token idx if not set,0
non-set tags are OUT tags,0
anything that is not a BIOES tag is a SINGLE tag,0
anything that is not OUT is IN,0
single and begin tags start a new span,0
remember previous tag,0
infer whitespace after field,0
Make the tag dictionary,0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
get train data,0
"read in test file if exists, otherwise sample 10% of train data as test dataset",0
"read in dev file if exists, otherwise sample 10% of train data as dev dataset",0
automatically identify train / test / dev files,0
get train data,0
get test data,0
get dev data,0
automatically identify train / test / dev files,0
"store either Sentence objects in memory, or only file offsets",0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
determine encoding of text file,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
check if data there,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
download data if necessary,0
unpack and write out in CoNLL column-like format,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
column format,0
this dataset name,0
default dataset folder is the cache root,0
download data if necessary,0
global variable: cache_root,0
global variable: device,0
# dummy return to fulfill trainer.train() needs,0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
plot i,0
save plots,0
save plots,0
save plot,0
take the average over the last three scores of training,0
take average over the scores from the different training runs,0
#TODO: not saving lines yet,1
auto-spawn on GPU if available,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
set the dictionaries,0
initialize the network architecture,0
dropouts,0
bidirectional LSTM on top of embedding layer,0
final linear map to tag space,0
append both to file for evaluation,0
make list of gold tags,0
make list of predicted tags,0
"check for true positives, false positives and false negatives",0
remove previous embeddings,0
revere sort all sequences by their length,0
make mini-batches,0
progress bar for verbosity,0
clearing token embeddings to save memory,0
initialize zero-padded word embeddings tensor,0
fill values with word embeddings,0
get the tags in this sentence,0
add tags as tensor,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
word dropout only before LSTM - TODO: more experimentation needed,1
if self.use_word_dropout > 0.0:,0
sentence_tensor = self.word_dropout(sentence_tensor),0
get the tags in this sentence,0
add tags as tensor,0
pad tags if using batch-CRF decoder,0
auto-spawn on GPU if available,0
cast string to Path,0
"determine what splits (train, dev, test) to evaluate and log",0
"minimize training loss if training with dev data, else maximize dev score",0
"if training also uses dev data, include in training set",0
At any point you can hit Ctrl + C to break out of training early.,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
"anneal against train loss if training with dev, otherwise anneal against dev score",0
calculate scores using dev data if available,0
append dev score to score history,0
"if checkpoint is enable, save model at each epoch",0
"if we use dev data, remember best model based on dev evaluation score",0
"if we do not use dev data for model selection, save final model",0
test best model if test data is present,0
"if we are training over multiple datasets, do evaluation for each",0
get and return the final test score of best model,0
cast string to Path,0
Add chars to the dictionary,0
charsplit file content,0
charsplit file content,0
Add words to the dictionary,0
Tokenize file content,0
"TextDataset returns a list. valid and test are only one file, so return the first element",0
cast string to Path,0
Shuffle training files randomly after serially iterating through corpus one,0
"iterate through training data, starting at self.split (for checkpointing)",0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.,0
We detach the hidden state from how it was previously produced.,0
"If we didn't, the model would try backpropagating all the way to start of the dataset.",0
explicitly remove loss to clear up memory,0
##############################################################################,0
Save the model if the validation loss is the best we've seen so far.,0
##############################################################################,0
print info,0
##############################################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
clean up file,0
bioes tags,0
bio tags,0
broken tags,0
all tags,0
all weird tags,0
tags with confidence,0
bioes tags,0
bioes tags,0
clean up directory,0
clean up directory,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
def test_multiclass_metrics():,0
,0
"metric = Metric(""Test"")",0
"available_labels = [""A"", ""B"", ""C""]",0
,0
"predictions = [""A"", ""B""]",0
"true_values = [""A""]",0
TextClassifier._evaluate_sentence_for_text_classification(,0
"metric, available_labels, predictions, true_values",0
),0
,0
"predictions = [""C"", ""B""]",0
"true_values = [""A"", ""B""]",0
TextClassifier._evaluate_sentence_for_text_classification(,0
"metric, available_labels, predictions, true_values",0
),0
,0
print(metric),0
from flair.trainers.trainer_regression import RegressorTrainer,0
def test_trainer_results(tasks_base_path):,0
"corpus, model, trainer = init(tasks_base_path)",0
"results = trainer.train(""regression_train/"", max_epochs=1)",0
"assert results[""test_score""] > 0",0
"assert len(results[""dev_loss_history""]) == 1",0
"assert len(results[""dev_score_history""]) == 1",0
"assert len(results[""train_loss_history""]) == 1",0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
make a temporary cache directory that we remove afterwards,0
initialize trainer,0
remove the cache directory,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
initialize trainer,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
clean up results directory,0
get default dictionary,0
get the example corpus and process at character level in forward direction,0
define search space,0
sequence tagger parameter,0
model trainer parameter,0
training parameter,0
find best parameter settings,0
clean up results directory,0
document embeddings parameter,0
training parameter,0
clean up results directory,0
"get training, test and dev data",0
1. get the corpus,0
2. what tag do we want to predict?,0
3. make the tag dictionary from the corpus,0
initialize embeddings,0
comment in this line to use character embeddings,0
"CharacterEmbeddings(),",0
comment in these lines to use contextual string embeddings,0
,0
"CharLMEmbeddings('news-forward'),",0
,0
"CharLMEmbeddings('news-backward'),",0
initialize sequence tagger,0
initialize trainer,0
"if only one sentence is passed, convert to list of sentence",0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
GLOVE embeddings,0
TURIAN embeddings,0
KOMNIOS embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
save the sentence piece model as binary file (not as path which may change),0
write out the binary sentence piece model into the expected directory,0
"if the model was saved as binary and it is not found on disk, write to appropriate path",0
"otherwise, use normal process and potentially trigger another download",0
"once the modes if there, load it with sentence piece",0
empty words get no embedding,0
all other words get embedded,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
embed a dummy sentence to determine embedding_length,0
Avoid conflicts with flair's Token class,0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
"multilingual forward fast (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward fast (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
Basque forward,0
Basque backward,0
Spanish forward fast,0
Spanish backward fast,0
Spanish forward,0
Spanish backward,0
Pubmed forward,0
Pubmed backward,0
Japanese forward,0
Japanese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
make compatible with serialized models,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
pad strings with whitespaces to longest sentence,0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
add embeddings after updating,0
The mask has 1 for real tokens and 0 for padding tokens. Only real,0
tokens are attended to.,0
Zero-pad up to the sequence length.,0
"first, find longest sentence in batch",0
prepare id maps for BERT model,0
put encoded batch through BERT model to get all hidden states of all encoder layers,0
get aggregated embeddings for each BERT-subtoken in sentence,0
get the current sentence object,0
add concatenated embedding to sentence,0
use first subword embedding if pooling operation is 'first',0
"otherwise, do a mean over all subwords in token",0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
pad strings with whitespaces to longest sentence,0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
"if only one sentence is passed, convert to list of sentence",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional RNN on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM RNN,0
--------------------------------------------------------------------,0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
iterate over sentences,0
"if its a forward LM, take last state",0
from allennlp.common.tqdm import Tqdm,0
mmap seems to be much more memory efficient,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
Maintains max of all exp. moving avg. of sq. grad. values,0
Decay the first and second moment running average coefficient,0
Maintains the maximum of all 2nd moment running avg. till now,0
Use the max. for normalizing running avg. of gradient,0
conll 2000 column format,0
conll 03 NER column format,0
WNUT-17,0
-- WikiNER datasets,0
-- Universal Dependencies,0
Germanic,0
Romance,0
West-Slavic,0
South-Slavic,0
East-Slavic,0
Scandinavian,0
Asian,0
Language isolates,0
other datasets,0
text classification format,0
"first, try to fetch dataset online",0
default dataset folder is the cache root,0
get string value if enum is passed,0
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)",0
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag",0
the CoNLL 03 task for German has an additional lemma column,0
the CoNLL 03 task for Dutch has no NP column,0
the CoNLL 03 task for Spanish only has two columns,0
the GERMEVAL task only has two columns: text and ner,0
WSD tasks may be put into this column format,0
"the UD corpora follow the CoNLL-U format, for which we have a special reader",0
"for text classifiers, we use our own special format",0
NER corpus for Basque,0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
get train and test data,0
"read in test file if exists, otherwise sample 10% of train data as test dataset",0
"read in dev file if exists, otherwise sample 10% of train data as dev dataset",0
convert tag scheme to iobes,0
automatically identify train / test / dev files,0
automatically identify train / test / dev files,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
conll 2000 chunking task,0
Support both TREC-6 and TREC-50,0
Create flair compatible labels,0
TREC-6 : NUM:dist -> __label__NUM,0
TREC-50: NUM:dist -> __label__NUM:dist,0
Wikiner NER task,0
unpack and write out in CoNLL column-like format,0
CoNLL 02/03 NER,0
universal dependencies,0
--- UD Germanic,0
--- UD Romance,0
--- UD West-Slavic,0
--- UD Scandinavian,0
--- UD South-Slavic,0
--- UD Asian,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
"if text is passed, instantiate sentence with tokens (words)",0
tokenize the text first if option selected,0
use segtok for tokenization,0
determine offsets for whitespace_after field,0
otherwise assumes whitespace tokenized text,0
add each word in tokenized string as Token object to Sentence,0
increment for last token in sentence if not followed by whtespace,0
set token idx if not set,0
non-set tags are OUT tags,0
anything that is not a BIOES tag is a SINGLE tag,0
anything that is not OUT is IN,0
single and begin tags start a new span,0
remember previous tag,0
infer whitespace after field,0
Make the tag dictionary,0
Make the tag dictionary,0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
plot i,0
save plots,0
plot 1,0
plot 2,0
plot 3,0
save plots,0
save plot,0
take the average over the last three scores of training,0
take average over the scores from the different training runs,0
auto-spawn on GPU if available,0
cut up the input into chunks of max charlength = chunk_size,0
push each chunk through the RNN language model,0
concatenate all chunks to make final output,0
initial hidden state,0
get predicted weights,0
divide by temperature,0
"to prevent overflow problem with small temperature values, substract largest value from all",0
this makes a vector in which the largest value is 0,0
compute word weights with exponential function,0
try sampling multinomial distribution for next character,0
print(word_idx),0
input ids,0
push list of character IDs through model,0
the target is always the next character,0
use cross entropy loss to compare output of forward pass with targets,0
exponentiate cross-entropy loss to calculate perplexity,0
set the dictionaries,0
initialize the network architecture,0
dropouts,0
bidirectional LSTM on top of embedding layer,0
final linear map to tag space,0
ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive,0
serialization of torch objects,0
https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings,0
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups,1
see https://github.com/zalandoresearch/flair/issues/351,0
remove previous embeddings,0
revere sort all sequences by their length,0
make mini-batches,0
progress bar for verbosity,0
clearing token embeddings to save memory,0
"if sorting is enabled, sort sentences by number of tokens",0
initialize zero-padded word embeddings tensor,0
fill values with word embeddings,0
get the tags in this sentence,0
add tags as tensor,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
word dropout only before LSTM - TODO: more experimentation needed,1
if self.use_word_dropout > 0.0:,0
sentence_tensor = self.word_dropout(sentence_tensor),0
pad tags if using batch-CRF decoder,0
auto-spawn on GPU if available,0
ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive,0
serialization of torch objects,0
https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings,0
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups,1
see https://github.com/zalandoresearch/flair/issues/351,0
cast string to Path,0
annealing scheduler,0
"if training also uses dev data, include in training set",0
At any point you can hit Ctrl + C to break out of training early.,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
calculate scores using dev data if available,0
append dev score to score history,0
"anneal against train loss if training with dev, otherwise anneal against dev score",0
"if checkpoint is enable, save model at each epoch",0
"if we use dev data, remember best model based on dev evaluation score",0
"if we do not use dev data for model selection, save final model",0
test best model if test data is present,0
"if we are training over multiple datasets, do evaluation for each",0
get and return the final test score of best model,0
append both to file for evaluation,0
make list of gold tags,0
make list of predicted tags,0
"check for true positives, false positives and false negatives",0
cast string to Path,0
Add chars to the dictionary,0
charsplit file content,0
charsplit file content,0
Add words to the dictionary,0
Tokenize file content,0
"TextDataset returns a list. valid and test are only one file, so return the first element",0
cast string to Path,0
Shuffle training files randomly after serially iterating through corpus one,0
"iterate through training data, starting at self.split (for checkpointing)",0
off by one for printing,0
go into train mode,0
reset variables,0
not really sure what this does,1
do the forward pass in the model,0
try to predict the targets,0
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.,0
We detach the hidden state from how it was previously produced.,0
"If we didn't, the model would try backpropagating all the way to start of the dataset.",0
explicitly remove loss to clear up memory,0
##############################################################################,0
Save the model if the validation loss is the best we've seen so far.,0
##############################################################################,0
print info,0
##############################################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
clean up file,0
bioes tags,0
bio tags,0
broken tags,0
all tags,0
all weird tags,0
tags with confidence,0
bioes tags,0
bioes tags,0
clean up directory,0
clean up directory,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
make a temporary cache directory that we remove afterwards,0
initialize trainer,0
remove the cache directory,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
clean up results directory,0
"corpus = NLPTaskDataFetcher.load_corpus('multi_class', base_path=tasks_base_path)",0
clean up results directory,0
clean up results directory,0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
initialize trainer,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
clean up results directory,0
get default dictionary,0
get the example corpus and process at character level in forward direction,0
define search space,0
sequence tagger parameter,0
model trainer parameter,0
training parameter,0
find best parameter settings,0
clean up results directory,0
document embeddings parameter,0
training parameter,0
clean up results directory,0
1. get the corpus,0
2. what tag do we want to predict?,0
3. make the tag dictionary from the corpus,0
initialize embeddings,0
comment in this line to use character embeddings,0
"CharacterEmbeddings(),",0
comment in these lines to use contextual string embeddings,0
,0
"CharLMEmbeddings('news-forward'),",0
,0
"CharLMEmbeddings('news-backward'),",0
initialize sequence tagger,0
initialize trainer,0
"if only one sentence is passed, convert to list of sentence",0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
GLOVE embeddings,0
KOMNIOS embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
twitter embeddings,0
two-letter language code wiki embeddings,0
two-letter language code wiki embeddings,0
two-letter language code crawl embeddings,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
"multilingual forward fast (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward fast (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
pad strings with whitespaces to longest sentence,0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
use the character language model embeddings as basis,0
length is twice the original character LM embedding length,0
these fields are for the embedding memory,0
whether to add only capitalized words to memory (faster runtime and lower memory consumption),0
we re-compute embeddings dynamically at each epoch,0
set the memory method,0
memory is wiped each time we do a training run,0
"if we keep a pooling, it needs to be updated continuously",0
update embedding,0
add embeddings after updating,0
"the default model for ELMo is the 'original' model, which is very large",0
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name",0
put on Cuda if available,0
embed a dummy sentence to determine embedding_length,0
The mask has 1 for real tokens and 0 for padding tokens. Only real,0
tokens are attended to.,0
Zero-pad up to the sequence length.,0
"first, find longest sentence in batch",0
prepare id maps for BERT model,0
put encoded batch through BERT model to get all hidden states of all encoder layers,0
get aggregated embeddings for each BERT-subtoken in sentence,0
get the current sentence object,0
add concatenated embedding to sentence,0
use first subword embedding if pooling operation is 'first',0
"otherwise, do a mean over all subwords in token",0
"multilingual forward (English, German, French, Italian, Dutch, Polish)",0
"multilingual backward  (English, German, French, Italian, Dutch, Polish)",0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
Dutch forward,0
Dutch backward,0
Swedish forward,0
Swedish backward,0
French forward,0
French backward,0
Czech forward,0
Czech backward,0
Portuguese forward,0
Portuguese backward,0
initialize cache if use_cache set,0
embed a dummy sentence to determine embedding_length,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"if cache is used, try setting embeddings from cache first",0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
pad strings with whitespaces to longest sentence,0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
"if only one sentence is passed, convert to list of sentence",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
iterate over sentences,0
"if its a forward LM, take last state",0
from allennlp.common.tqdm import Tqdm,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
State initialization,0
Exponential moving average of gradient values,0
Exponential moving average of squared gradient values,0
Maintains max of all exp. moving avg. of sq. grad. values,0
Decay the first and second moment running average coefficient,0
Maintains the maximum of all 2nd moment running avg. till now,0
Use the max. for normalizing running avg. of gradient,0
conll 2000 column format,0
conll 03 NER column format,0
WNUT-17,0
-- WikiNER datasets,0
-- Universal Dependencies,0
Germanic,0
Romance,0
West-Slavic,0
South-Slavic,0
East-Slavic,0
Scandinavian,0
Asian,0
other datasets,0
text classification format,0
"first, try to fetch dataset online",0
default dataset folder is the cache root,0
get string value if enum is passed,0
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)",0
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag",0
the CoNLL 03 task for German has an additional lemma column,0
the CoNLL 03 task for Dutch has no NP column,0
the CoNLL 03 task for Spanish only has two columns,0
the GERMEVAL task only has two columns: text and ner,0
WSD tasks may be put into this column format,0
"the UD corpora follow the CoNLL-U format, for which we have a special reader",0
"for text classifiers, we use our own special format",0
automatically identify train / test / dev files,0
"if no test file is found, take any file with 'test' in name",0
get train and test data,0
"read in test file if exists, otherwise sample 10% of train data as test dataset",0
"read in dev file if exists, otherwise sample 10% of train data as dev dataset",0
convert tag scheme to iobes,0
automatically identify train / test / dev files,0
automatically identify train / test / dev files,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
conll 2000 chunking task,0
Wikiner NER task,0
unpack and write out in CoNLL column-like format,0
CoNLL 02/03 NER,0
universal dependencies,0
--- UD Germanic,0
--- UD Romance,0
--- UD West-Slavic,0
--- UD Scandinavian,0
--- UD South-Slavic,0
--- UD Asian,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
"if text is passed, instantiate sentence with tokens (words)",0
tokenize the text first if option selected,0
use segtok for tokenization,0
determine offsets for whitespace_after field,0
otherwise assumes whitespace tokenized text,0
add each word in tokenized string as Token object to Sentence,0
increment for last token in sentence if not followed by whtespace,0
set token idx if not set,0
non-set tags are OUT tags,0
anything that is not a BIOES tag is a SINGLE tag,0
anything that is not OUT is IN,0
single and begin tags start a new span,0
remember previous tag,0
infer whitespace after field,0
Make the tag dictionary,0
Make the tag dictionary,0
header for 'weights.txt',0
"determine the column index of loss, f-score and accuracy for train, dev and test split",0
then get all relevant values from the tsv,0
then get all relevant values from the tsv,0
plot i,0
save plots,0
plot 1,0
plot 2,0
plot 3,0
save plots,0
save plot,0
take the average over the last three scores of training,0
take average over the scores from the different training runs,0
auto-spawn on GPU if available,0
set the dictionaries,0
initialize the network architecture,0
dropouts,0
bidirectional LSTM on top of embedding layer,0
final linear map to tag space,0
ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive,0
serialization of torch objects,0
https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings,0
remove previous embeddings,0
make mini-batches,0
"first, sort sentences by number of tokens",0
initialize zero-padded word embeddings tensor,0
fill values with word embeddings,0
get the tags in this sentence,0
add tags as tensor,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
word dropout only before LSTM - TODO: more experimentation needed,1
if self.use_word_dropout > 0.0:,0
sentence_tensor = self.word_dropout(sentence_tensor),0
pad tags if using batch-CRF decoder,0
auto-spawn on GPU if available,0
ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive,0
serialization of torch objects,0
https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings,0
cast string to Path,0
annealing scheduler,0
"if training also uses dev data, include in training set",0
At any point you can hit Ctrl + C to break out of training early.,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
calculate scores using dev data if available,0
append dev score to score history,0
"anneal against train loss if training with dev, otherwise anneal against dev score",0
"if checkpoint is enable, save model at each epoch",0
"if we use dev data, remember best model based on dev evaluation score",0
"if we do not use dev data for model selection, save final model",0
test best model on test data,0
"if we are training over multiple datasets, do evaluation for each",0
get and return the final test score of best model,0
append both to file for evaluation,0
make list of gold tags,0
make list of predicted tags,0
"check for true positives, false positives and false negatives",0
cast string to Path,0
,0
Add chars to the dictionary,0
charsplit file content,0
charsplit file content,0
Add words to the dictionary,0
Tokenize file content,0
cast string to Path,0
"an epoch has a number, so calculate total max splits bby multiplying max_epochs with number_of_splits",0
"after pass over all splits, increment epoch count",0
go into train mode,0
reset variables,0
not really sure what this does,1
do batches,0
"Starting each batch, we detach the hidden state from how it was previously produced.",0
"If we didn't, the model would try backpropagating all the way to start of the dataset.",0
do the forward pass in the model,0
try to predict the targets,0
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.,0
##############################################################################,0
TEST,0
##############################################################################,0
Save the model if the validation loss is the best we've seen so far.,0
##############################################################################,0
print info,0
##############################################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
clean up file,0
bioes tags,0
bio tags,0
broken tags,0
all tags,0
all weird tags,0
tags with confidence,0
bioes tags,0
bioes tags,0
clean up directory,0
clean up directory,0
clean up directory,0
clean up directory,0
clean up directory,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
get two corpora as one,0
"get training, test and dev data for full English UD corpus from web",0
clean up data directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
make a temporary cache directory that we remove afterwards,0
initialize trainer,0
remove the cache directory,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
initialize trainer,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
clean up results directory,0
get default dictionary,0
get the example corpus and process at character level in forward direction,0
define search space,0
sequence tagger parameter,0
model trainer parameter,0
training parameter,0
find best parameter settings,0
clean up results directory,0
document embeddings parameter,0
training parameter,0
clean up results directory,0
1. get the corpus,0
2. what tag do we want to predict?,0
3. make the tag dictionary from the corpus,0
initialize embeddings,0
comment in this line to use character embeddings,0
"CharacterEmbeddings(),",0
comment in these lines to use contextual string embeddings,0
,0
"CharLMEmbeddings('news-forward'),",0
,0
"CharLMEmbeddings('news-backward'),",0
initialize sequence tagger,0
initialize trainer,0
"if only one sentence is passed, convert to list of sentence",0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
GLOVE embeddings,0
KOMNIOS embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
other language fasttext embeddings,0
add label if in training mode,0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
Slovenian forward,0
Slovenian backward,0
Bulgarian forward,0
Bulgarian backward,0
caching variables,0
set to eval mode,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
this whole block is for compatibility with older serialized models  TODO: remove in version 0.4,1
"if cache is used, try setting embeddings from cache first",0
lazy initialization of cache,0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
pad strings with whitespaces to longest sentence,0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
"if only one sentence is passed, convert to list of sentence",0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
iterate over sentences,0
"if its a forward LM, take last state",0
from allennlp.common.tqdm import Tqdm,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
conll column format,0
conll-u format,0
text classification format,0
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)",0
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag",0
the CoNLL 03 task for German has an additional lemma column,0
the GERMEVAL task only has two columns: text and ner,0
WSD tasks may be put into this column format,0
"the UD corpora follow the CoNLL-U format, for which we have a special reader",0
"get train, test and dev data",0
"get train, test and dev data",0
"get train, test and dev data",0
"get train, test and dev data",0
"for text classifiers, we use our own special format",0
"for text classifiers, we use our own special format",0
"TODO: move all paths to use pathlib.Path, for now convert to str for compatibility",1
get train and test data,0
sample dev data from train,0
convert tag scheme to iobes,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
"if text is passed, instantiate sentence with tokens (words)",0
tokenize the text first if option selected,0
use segtok for tokenization,0
determine offsets for whitespace_after field,0
otherwise assumes whitespace tokenized text,0
add each word in tokenized string as Token object to Sentence,0
set token idx if not set,0
non-set tags are OUT tags,0
anything that is not a BIOES tag is a SINGLE tag,0
anything that is not OUT is IN,0
single and begin tags start a new span,0
remember previous tag,0
infer whitespace after field,0
Make the tag dictionary,0
print(tag),0
header for 'loss.tsv',0
header for 'weights.txt',0
plot i,0
save plots,0
plot 1,0
plot 2,0
plot 3,0
save plots,0
auto-spawn on GPU if available,0
initial hidden state,0
set the dictionaries,0
initialize the network architecture,0
dropouts,0
bidirectional LSTM on top of embedding layer,0
final linear map to tag space,0
suppress torch warnings:,0
https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings,0
"first, sort sentences by number of tokens",0
initialize zero-padded word embeddings tensor,0
fill values with word embeddings,0
get the tags in this sentence,0
add tags as tensor,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
word dropout only before LSTM - TODO: more experimentation needed,1
if self.use_word_dropout > 0.0:,0
sentence_tensor = self.word_dropout(sentence_tensor),0
pad tags if using batch-CRF decoder,0
remove previous embeddings,0
make mini-batches,0
get the predicted tag,0
auto-spawn on GPU if available,0
ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive,0
serialization of torch objects,0
https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings,0
annealing scheduler,0
"if training also uses dev data, include in training set",0
At any point you can hit Ctrl + C to break out of training early.,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
"Step 4. Compute the loss, gradients, and update the parameters by calling optimizer.step()",0
switch to eval mode,0
"if checkpointing is enable, save model at each epoch",0
"anneal against train loss if training with dev, otherwise anneal against dev score",0
logging info,0
"if we use dev data, remember best model based on dev evaluation score",0
"if we do not use dev data for model selection, save final model",0
get the predicted tag,0
add predicted tags,0
append both to file for evaluation,0
make list of gold tags,0
make list of predicted tags,0
"check for true positives, false positives and false negatives",0
,0
Add chars to the dictionary,0
charsplit file content,0
charsplit file content,0
Add words to the dictionary,0
Tokenize file content,0
"an epoch has a number, so calculate total max splits bby multiplying max_epochs with number_of_splits",0
"after pass over all splits, increment epoch count",0
go into train mode,0
reset variables,0
not really sure what this does,1
do batches,0
"Starting each batch, we detach the hidden state from how it was previously produced.",0
"If we didn't, the model would try backpropagating all the way to start of the dataset.",0
do the forward pass in the model,0
try to predict the targets,0
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.,0
##############################################################################,0
TEST,0
##############################################################################,0
Save the model if the validation loss is the best we've seen so far.,0
##############################################################################,0
print info,0
##############################################################################,0
##############################################################################,0
final testing,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
"if training also uses dev data, include in training set",0
At any point you can hit Ctrl + C to break out of training early.,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
"if checkpoint is enable, save model at each epoch",0
"anneal against train loss if training with dev, otherwise anneal against dev score",0
"if we use dev data, remember best model based on dev evaluation score",0
clean up file,0
bioes tags,0
bio tags,0
broken tags,0
all tags,0
all weird tags,0
tags with confidence,0
bioes tags,0
bioes tags,0
clean up directory,0
clean up directory,0
clean up directory,0
clean up directory,0
clean up directory,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
make a temporary cache directory that we remove afterwards,0
initialize trainer,0
remove the cache directory,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
1. get the corpus,0
2. what tag do we want to predict?,0
3. make the tag dictionary from the corpus,0
initialize embeddings,0
comment in this line to use character embeddings,0
"CharacterEmbeddings(),",0
comment in these lines to use contextual string embeddings,0
,0
"CharLMEmbeddings('news-forward'),",0
,0
"CharLMEmbeddings('news-backward'),",0
initialize sequence tagger,0
initialize trainer,0
"if only one sentence is passed, convert to list of sentence",0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
GLOVE embeddings,0
twitter embeddings,0
KOMNIOS embeddings,0
NUMBERBATCH embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
GERMAN FASTTEXT embeddings,0
NUMBERBATCH embeddings,0
SWEDISCH FASTTEXT embeddings,0
add label if in training mode,0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
caching variables,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
this whole block is for compatibility with older serialized models  TODO: remove in version 0.4,1
"if cache is used, try setting embeddings from cache first",0
lazy initialization of cache,0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
pad strings with whitespaces to longest sentence,0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
iterate over sentences,0
"if its a forward LM, take last state",0
from allennlp.common.tqdm import Tqdm,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
conll column format,0
conll-u format,0
text classification format,0
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)",0
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag",0
the CoNLL 03 task for German has an additional lemma column,0
the GERMEVAL task only has two columns: text and ner,0
WSD tasks may be put into this column format,0
"the UD corpora follow the CoNLL-U format, for which we have a special reader",0
"get train, test and dev data",0
"get train, test and dev data",0
"get train, test and dev data",0
"get train, test and dev data",0
"for text classifiers, we use our own special format",0
"for text classifiers, we use our own special format",0
get train and test data,0
sample dev data from train,0
convert tag scheme to iobes,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
"if text is passed, instantiate sentence with tokens (words)",0
tokenize the text first if option selected,0
use segtok for tokenization,0
determine offsets for whitespace_after field,0
otherwise assumes whitespace tokenized text,0
add each word in tokenized string as Token object to Sentence,0
set token idx if not set,0
non-set tags are OUT tags,0
anything that is not a BIOES tag is a SINGLE tag,0
anything that is not OUT is IN,0
single and begin tags start a new span,0
remember previous tag,0
infer whitespace after field,0
Make the tag dictionary,0
print(tag),0
header for 'loss.tsv',0
header for 'weights.txt',0
plot i,0
save plots,0
plot 1,0
plot 2,0
plot 3,0
save plots,0
auto-spawn on GPU if available,0
set the dictionaries,0
initialize the network architecture,0
dropouts,0
bidirectional LSTM on top of embedding layer,0
final linear map to tag space,0
"first, sort sentences by number of tokens",0
initialize zero-padded word embeddings tensor,0
fill values with word embeddings,0
get the tags in this sentence,0
add tags as tensor,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
pad tags if using batch-CRF decoder,0
remove previous embeddings,0
make mini-batches,0
get the predicted tag,0
auto-spawn on GPU if available,0
ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive,0
serialization of torch objects,0
annealing scheduler,0
"if training also uses dev data, include in training set",0
At any point you can hit Ctrl + C to break out of training early.,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
"Step 4. Compute the loss, gradients, and update the parameters by calling optimizer.step()",0
switch to eval mode,0
"if checkpointing is enable, save model at each epoch",0
"anneal against train loss if training with dev, otherwise anneal against dev score",0
logging info,0
"if we use dev data, remember best model based on dev evaluation score",0
"if we do not use dev data for model selection, save final model",0
get the predicted tag,0
add predicted tags,0
append both to file for evaluation,0
make list of gold tags,0
make list of predicted tags,0
"check for true positives, false positives and false negatives",0
,0
Add chars to the dictionary,0
charsplit file content,0
charsplit file content,0
Add words to the dictionary,0
Tokenize file content,0
"an epoch has a number, so calculate total max splits bby multiplying max_epochs with number_of_splits",0
"after pass over all splits, increment epoch count",0
go into train mode,0
reset variables,0
not really sure what this does,1
do batches,0
"Starting each batch, we detach the hidden state from how it was previously produced.",0
"If we didn't, the model would try backpropagating all the way to start of the dataset.",0
do the forward pass in the model,0
try to predict the targets,0
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.,0
##############################################################################,0
TEST,0
##############################################################################,0
Save the model if the validation loss is the best we've seen so far.,0
##############################################################################,0
print info,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
"if training also uses dev data, include in training set",0
At any point you can hit Ctrl + C to break out of training early.,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
"if checkpoint is enable, save model at each epoch",0
"anneal against train loss if training with dev, otherwise anneal against dev score",0
"if we use dev data, remember best model based on dev evaluation score",0
clean up file,0
with pytest.raises(ValueError):,0
label.name = '',0
bioes tags,0
bio tags,0
broken tags,0
all tags,0
all weird tags,0
tags with confidence,0
bioes tags,0
bioes tags,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
clean up directory,0
clean up directory,0
clean up directory,0
clean up directory,0
clean up directory,0
clean up results directory,0
clean up results directory,0
test tagging,0
test re-tagging,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
initialize trainer,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
make a temporary cache directory that we remove afterwards,0
initialize trainer,0
remove the cache directory,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
clean up results directory,0
initialize trainer,0
clean up results directory,0
1. get the corpus,0
2. what tag do we want to predict?,0
3. make the tag dictionary from the corpus,0
initialize embeddings,0
comment in this line to use character embeddings,0
"CharacterEmbeddings(),",0
comment in these lines to use contextual string embeddings,0
,0
"CharLMEmbeddings('news-forward'),",0
,0
"CharLMEmbeddings('news-backward'),",0
initialize sequence tagger,0
initialize trainer,0
"if only one sentence is passed, convert to list of sentence",0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
GLOVE embeddings,0
twitter embeddings,0
KOMNIOS embeddings,0
NUMBERBATCH embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
GERMAN FASTTEXT embeddings,0
NUMBERBATCH embeddings,0
SWEDISCH FASTTEXT embeddings,0
add label if in training mode,0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-german-forward,0
mix-german-backward,0
common crawl Polish forward,0
common crawl Polish backward,0
caching variables,0
Copy the object's state from self.__dict__ which contains,0
all our instance attributes. Always use the dict.copy(),0
method to avoid modifying the original state.,0
Remove the unpicklable entries.,0
"by default, use_cache is false (for older pre-trained models TODO: remove in version 0.4)",1
"if cache is used, try setting embeddings from cache first",0
lazy initialization of cache,0
try populating embeddings from cache,0
"if this is not possible, use LM to generate embedding. First, get text sentences",0
pad strings with whitespaces to longest sentence,0
get hidden states from language model,0
take first or last hidden states from language model as word representation,0
if self.tokenized_lm or token.whitespace_after:,0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
dropouts,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
iterate over sentences,0
"if its a forward LM, take last state",0
from allennlp.common.tqdm import Tqdm,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
conll column format,0
conll-u format,0
text classification format,0
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)",0
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag",0
the CoNLL 03 task for German has an additional lemma column,0
the GERMEVAL task only has two columns: text and ner,0
WSD tasks may be put into this column format,0
"the UD corpora follow the CoNLL-U format, for which we have a special reader",0
"get train, test and dev data",0
"get train, test and dev data",0
"get train, test and dev data",0
"get train, test and dev data",0
"for text classifiers, we use our own special format",0
"for text classifiers, we use our own special format",0
get train and test data,0
sample dev data from train,0
convert tag scheme to iobes,0
"most data sets have the token text in the first column, if not, pass 'text' as column",0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
"if text is passed, instantiate sentence with tokens (words)",0
tokenize the text first if option selected,0
use segtok for tokenization,0
determine offsets for whitespace_after field,0
otherwise assumes whitespace tokenized text,0
add each word in tokenized string as Token object to Sentence,0
set token idx if not set,0
non-set tags are OUT tags,0
anything that is not a BIOES tag is a SINGLE tag,0
anything that is not OUT is IN,0
single and begin tags start a new span,0
remember previous tag,0
infer whitespace after field,0
Make the tag dictionary,0
print(tag),0
header for 'loss.tsv',0
header for 'weights.txt',0
plot i,0
save plots,0
plot 1,0
plot 2,0
plot 3,0
save plots,0
auto-spawn on GPU if available,0
set the dictionaries,0
initialize the network architecture,0
dropouts,0
bidirectional LSTM on top of embedding layer,0
final linear map to tag space,0
"first, sort sentences by number of tokens",0
initialize zero-padded word embeddings tensor,0
fill values with word embeddings,0
get the tags in this sentence,0
add tags as tensor,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
use word dropout if set,0
pad tags if using batch-CRF decoder,0
remove previous embeddings,0
make mini-batches,0
get the predicted tag,0
auto-spawn on GPU if available,0
ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive,0
serialization of torch objects,0
annealing scheduler,0
"if training also uses dev data, include in training set",0
At any point you can hit Ctrl + C to break out of training early.,0
reload last best model if annealing with restarts is enabled,0
stop training if learning rate becomes too small,0
"Step 4. Compute the loss, gradients, and update the parameters by calling optimizer.step()",0
switch to eval mode,0
"if checkpointing is enable, save model at each epoch",0
"anneal against train loss if training with dev, otherwise anneal against dev score",0
logging info,0
"if we use dev data, remember best model based on dev evaluation score",0
"if we do not use dev data for model selection, save final model",0
get the predicted tag,0
add predicted tags,0
append both to file for evaluation,0
make list of gold tags,0
make list of predicted tags,0
"check for true positives, false positives and false negatives",0
,0
Add chars to the dictionary,0
charsplit file content,0
charsplit file content,0
Add words to the dictionary,0
Tokenize file content,0
"an epoch has a number, so calculate total max splits bby multiplying max_epochs with number_of_splits",0
"after pass over all splits, increment epoch count",0
go into train mode,0
reset variables,0
not really sure what this does,1
do batches,0
"Starting each batch, we detach the hidden state from how it was previously produced.",0
"If we didn't, the model would try backpropagating all the way to start of the dataset.",0
do the forward pass in the model,0
try to predict the targets,0
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.,0
##############################################################################,0
TEST,0
##############################################################################,0
Save the model if the validation loss is the best we've seen so far.,0
##############################################################################,0
print info,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
"if training also uses dev data, include in training set",0
At any point you can hit Ctrl + C to break out of training early.,0
record overall best dev scores and best loss,0
"anneal against train loss if training with dev, otherwise anneal against dev score",0
clean up file,0
with pytest.raises(ValueError):,0
label.name = '',0
bioes tags,0
bio tags,0
broken tags,0
all tags,0
all weird tags,0
tags with confidence,0
bioes tags,0
bioes tags,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
clean up directory,0
clean up directory,0
clean up directory,0
clean up directory,0
clean up directory,0
clean up results directory,0
clean up results directory,0
test tagging,0
test re-tagging,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
initialize trainer,0
clean up results directory,0
1. get the corpus,0
2. what tag do we want to predict?,0
3. make the tag dictionary from the corpus,0
initialize embeddings,0
comment in this line to use character embeddings,0
"CharacterEmbeddings(),",0
comment in these lines to use contextual string embeddings,0
,0
"CharLMEmbeddings('news-forward'),",0
,0
"CharLMEmbeddings('news-backward'),",0
initialize sequence tagger,0
initialize trainer,0
"if only one sentence is passed, convert to list of sentence",0
IMPORTANT: add embeddings as torch modules,0
"if only one sentence is passed, convert to list of sentence",0
GLOVE embeddings,0
twitter embeddings,0
KOMNIOS embeddings,0
NUMBERBATCH embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
GERMAN FASTTEXT embeddings,0
NUMBERBATCH embeddings,0
SWEDISCH FASTTEXT embeddings,0
use list of common characters if none provided,0
translate words in sentence into ints using dictionary,0
"sort words by length, for batching and masking",0
chars for rnn processing,0
news-english-forward,0
news-english-backward,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-english-forward,0
mix-english-backward,0
find longest sentence by characters,0
get states from LM,0
"if only one sentence is passed, convert to list of sentence",0
bidirectional LSTM on top of embedding layer,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
EXTRACT EMBEDDINGS FROM LSTM,0
--------------------------------------------------------------------,0
iterate over sentences,0
"if its a forward LM, take last state",0
from allennlp.common.tqdm import Tqdm,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
print(line),0
print(line),0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
"optionally, directly instantiate with sentence tokens",0
"tokenize the text first if option selected, otherwise assumes whitespace tokenized text",0
add each word in tokenized string as Token object to Sentence,0
set token idx if not set,0
"def to_tag_string(self, tag_type: str = 'tag') -> str:",0
,0
list = [],0
for token in self.tokens:,0
list.append(token.text),0
if token.get_tag(tag_type) == '' or token.get_tag(tag_type) == 'O': continue,0
list.append('<' + token.get_tag(tag_type) + '>'),0
return ' '.join(list),0
,0
def to_ner_string(self) -> str:,0
list = [],0
for token in self.tokens:,0
if token.get_tag('ner') == 'O' or token.get_tag('ner') == '':,0
list.append(token.text),0
else:,0
list.append(token.text),0
list.append('<' + token.get_tag('ner') + '>'),0
return ' '.join(list),0
Make the tag dictionary,0
auto-spawn on GPU if available,0
vec 2D: 1 * tagset_size,0
set the dictionaries,0
initialize the network architecture,0
self.dropout = nn.Dropout(0.5),0
bidirectional LSTM on top of embedding layer,0
final linear map to tag space,0
"trans is also a score tensor, not a probability: THIS THING NEEDS TO GO!!!!",0
ACHTUNG: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive,0
serialization of torch objects,0
"first, sort sentences by number of tokens",0
get the tags in this sentence,0
get the tag,0
get the word embeddings,0
pad shorter sentences out,0
padded tensor for entire batch,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
sentence_tensor = self.nonlinearity(sentence_tensor),0
"tags is ground_truth, a list of ints, length is len(sentence)",0
"feats is a 2D tensor, len(sentence) * tagset_size",0
analogous to forward,0
"calculate the score of the ground_truth, in CRF",0
calculate in log domain,0
feats is len(sentence) * tagset_size,0
initialize alpha with a Tensor with values all equal to -10000.,0
Z(x),0
viterbi to get tag_seq,0
get the predicted tag,0
remove previous embeddings,0
make mini-batches,0
get the predicted tag,0
viterbi to get tag_seq,0
overall_score += score,0
auto-spawn on GPU if available,0
ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive,0
serialization of torch objects,0
"if training also uses dev data, include in training set",0
At any point you can hit Ctrl + C to break out of training early.,0
"Step 4. Compute the loss, gradients, and update the parameters by calling optimizer.step()",0
switch to eval mode,0
switch back to train mode,0
"anneal against train loss if training with dev, otherwise anneal against dev score",0
save if model is current best and we use dev data for model selection,0
"if we do not use dev data for model selection, save final model",0
Step 3. Run our forward pass.,0
Step 5. Compute predictions,0
get the predicted tag,0
get the gold tag,0
append both to file for evaluation,0
positives,0
true positives,0
false positive,0
negatives,0
true negative,0
false negative,0
get the eval script,0
parse the result file,0
,0
print(chars),0
Add chars to the dictionary,0
charsplit file content,0
charsplit file content,0
Add words to the dictionary,0
Tokenize file content,0
go into train mode,0
reset variables,0
not really sure what this does,1
do batches,0
"Starting each batch, we detach the hidden state from how it was previously produced.",0
"If we didn't, the model would try backpropagating all the way to start of the dataset.",0
do the forward pass in the model,0
try to predict the targets,0
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.,0
##############################################################################,0
TEST,0
##############################################################################,0
Save the model if the validation loss is the best we've seen so far.,0
##############################################################################,0
print info,0
##############################################################################,0
Turn on evaluation mode which disables dropout.,0
Work out how cleanly we can divide the dataset into bsz parts.,0
Trim off any extra elements that wouldn't cleanly fit (remainders).,0
Evenly divide the data across the bsz batches.,0
"if training also uses dev data, include in training set",0
At any point you can hit Ctrl + C to break out of training early.,0
record overall best dev scores and best loss,0
IMPORTANT: Switch to eval mode,0
IMPORTANT: Switch back to train mode,0
"anneal against train loss if training with dev, otherwise anneal against dev score",0
clean up file,0
get default dictionary,0
init forward LM with 128 hidden states and 1 layer,0
get the example corpus and process at character level in forward direction,0
train the language model,0
use the character LM as embeddings to embed the example sentence 'I love Berlin',0
clean up results directory,0
clean up results directory,0
clean up results directory,0
test tagging,0
test re-tagging,0
"get training, test and dev data",0
"get training, test and dev data",0
"get training, test and dev data",0
initialize trainer,0
clean up results directory,0
1. get the corpus,0
2. what tag do we want to predict?,0
3. make the tag dictionary from the corpus,0
initialize embeddings,0
comment in this line to use character embeddings,0
comment in these lines to use contextual string embeddings,0
initialize sequence tagger,0
initialize trainer,0
"if only one sentence is passed, convert to list of sentence",0
IMPORTANT: add embeddings as torch modules,0
GLOVE embeddings,0
KOMNIOS embeddings,0
NUMBERBATCH embeddings,0
FT-CRAWL embeddings,0
FT-CRAWL embeddings,0
GERMAN FASTTEXT embeddings,0
NUMBERBATCH embeddings,0
SWEDISCH FASTTEXT embeddings,0
get list of common characters if none provided,0
load dictionary,0
print(self.char_dictionary.item2idx),0
translate words in sentence into ints using dictionary,0
print(token),0
"sort words by length, for batching and masking",0
chars for rnn processing,0
news-english-forward,0
news-english-backward,0
mix-english-forward,0
mix-english-backward,0
mix-english-forward,0
mix-english-backward,0
find longest sentence by characters,0
print(sentences_padded),0
get states from LM,0
if not torch.cuda.is_available():,0
embedding = embedding.cpu(),0
lines: List[str] = [],0
lines.append(vec),0
"if only one sentence is passed, convert to list of sentence",0
mean_embedding /= len(paragraph.tokens),0
self.embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=word_embeddings),0
self.__embedding_length: int = hidden_states,0
bidirectional LSTM on top of embedding layer,0
"first, sort sentences by number of tokens",0
go through each sentence in batch,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
iterate over sentences,0
"if its a forward LM, take last state",0
from allennlp.common.tqdm import Tqdm,0
Remove quotes from etag,0
"If there is an etag, it's everything after the first period",0
"Otherwise, use None",0
"URL, so get it from the cache (downloading if necessary)",0
"File, and it exists.",0
"File, but it doesn't exist.",0
Something unknown,0
TODO(joelgrus): do we want to do checksums or anything like that?,1
get cache path to put the file,0
make HEAD request to check ETag,0
add ETag to filename if it exists,0
"etag = response.headers.get(""ETag"")",0
"Download to temporary file, then copy to cache dir once finished.",0
Otherwise you get corrupt cache entries if the download gets interrupted.,0
GET file object,0
These defaults are the same as the argument defaults in tqdm.,0
init dictionaries,0
"in order to deal with unknown tokens, add <unk>",0
"optionally, directly instantiate with sentence tokens",0
"tokenize the text first if option selected, otherwise assumes whitespace tokenized text",0
add each word in tokenized string as Token object to Sentence,0
set token idx if not set,0
"def to_tag_string(self, tag_type: str = 'tag') -> str:",0
,0
list = [],0
for token in self.tokens:,0
list.append(token.text),0
if token.get_tag(tag_type) == '' or token.get_tag(tag_type) == 'O': continue,0
list.append('<' + token.get_tag(tag_type) + '>'),0
return ' '.join(list),0
,0
def to_ner_string(self) -> str:,0
list = [],0
for token in self.tokens:,0
if token.get_tag('ner') == 'O' or token.get_tag('ner') == '':,0
list.append(token.text),0
else:,0
list.append(token.text),0
list.append('<' + token.get_tag('ner') + '>'),0
return ' '.join(list),0
Make the tag dictionary,0
print(line),0
print(line),0
,0
print(chars),0
Add chars to the dictionary,0
charsplit file content,0
charsplit file content,0
Add words to the dictionary,0
Tokenize file content,0
"if training also uses dev data, include in training set",0
At any point you can hit Ctrl + C to break out of training early.,0
record overall best dev scores and best loss,0
best_dev_score = 0,0
best_loss: float = 10000,0
this variable is used for annealing schemes,0
"Step 4. Compute the loss, gradients, and update the parameters by calling optimizer.step()",0
IMPORTANT: Switch to eval mode,0
IMPORTANT: Switch back to train mode,0
checkpoint model,0
is this the best model so far?,0
"if dev data is used for model selection, use dev F1 score to determine best model",0
"if dev data is used for training, use training loss to determine best model",0
save model,0
anneal after 3 epochs of no improvement if anneal mode,0
print info,0
Step 3. Run our forward pass.,0
Step 5. Compute predictions,0
print(token),0
get the predicted tag,0
get the gold tag,0
append both to file for evaluation,0
parse the result file,0
vec 2D: 1 * tagset_size,0
set the dictionaries,0
initialize the network architecture,0
self.dropout = nn.Dropout(0.5),0
bidirectional LSTM on top of embedding layer,0
final linear map to tag space,0
"trans is also a score tensor, not a probability: THIS THING NEEDS TO GO!!!!",0
ACHTUNG: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive,0
serialization of torch objects,0
"first, sort sentences by number of tokens",0
print(sent),0
print(sent.tokens[0].get_embedding()[0:7]),0
go through each sentence in batch,0
get the tags in this sentence,0
get the tag,0
PADDING: pad shorter sentences out,0
ADD TO SENTENCE LIST: add the representation,0
--------------------------------------------------------------------,0
GET REPRESENTATION FOR ENTIRE BATCH,0
--------------------------------------------------------------------,0
--------------------------------------------------------------------,0
FF PART,0
--------------------------------------------------------------------,0
print(tags),0
"tags is ground_truth, a list of ints, length is len(sentence)",0
"feats is a 2D tensor, len(sentence) * tagset_size",0
analogous to forward,0
"sentence, tags is a list of ints",0
"features is a 2D tensor, len(sentence) * self.tagset_size",0
for sentence in sentences:,0
print(sentence),0
"calculate the score of the ground_truth, in CRF",0
calculate in log domain,0
feats is len(sentence) * tagset_size,0
initialize alpha with a Tensor with values all equal to -10000.,0
Z(x),0
viterbi to get tag_seq,0
get the predicted tag,0
