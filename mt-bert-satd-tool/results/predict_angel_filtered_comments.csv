Commit Message,predict
set basic configuration keys,0
use local deploy mode and dummy data spliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
todo:implements,1
todo: implements,1
import jdk.nashorn.internal.runtime.regexp.joni.Config;,0
"paras[1] = ""abc"";",0
"paras[2] = ""123"";",0
Add standard Hadoop classes,0
Feature number of train data,0
Total iteration number,0
Learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set input data path,0
Set save model path,0
Set actionType train,0
QSLRRunner runner = new QSLRRunner();,0
runner.train(conf);,0
"conf.setInt(AngelConf.ANGEL_STALENESS, -1);",0
Dataset,0
Data format,0
Feature number of train data,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Ratio of validation,0
Learning rate,0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set DeepFM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
Model type,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set FM algorithm parameters,0
Set model class,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set WideAndDeep algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
"Set LDA parameters #V, #K",0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set LDA algorithm parameters,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set SVM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Load model meta,0
Convert model,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Convert model,0
load hadoop configuration,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model,0
load hadoop configuration,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add sparse double matrix,0
Start PS,0
Start to run application,0
mMatrix.setNnz(100000000);,0
mMatrix.setNnz(100000000);,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
worker register,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
attempt 0,0
attempt1,0
attempt1,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
set basic configuration keys,0
"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());",0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
Thread.sleep(5000);,0
"response = master.getJobReport(null, request);",0
"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);",0
"assertEquals(response.getJobReport().getCurIteration(), jobIteration);",0
"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
add matrix,0
"DenseIntVector deltaVec = new DenseIntVector(100000, delta);",0
deltaVec.setMatrixId(matrixW1Id);,0
deltaVec.setRowId(0);,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
testDenseDoubleCompUDF();,0
testSparseDoubleCompUDF();,0
testDenseFloatCompUDF();,0
testSparseFloatCompUDF();,0
testDenseIntCompUDF();,0
testSparseIntCompUDF();,0
testDenseLongCompUDF();,0
testSparseLongCompUDF();,0
testSparseDoubleLongKeyCompUDF();,0
testSparseFloatLongKeyCompUDF();,0
testSparseIntLongKeyCompUDF();,0
testSparseLongLongKeyCompUDF();,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntFloatVector row = (IntFloatVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntLongVector row = (IntLongVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add comp dense float matrix,0
add comp dense float matrix,0
Start PS,0
Start to run application,0
import com.tencent.angel.psagent.consistency.SSPConsistencyController;,0
@RunWith(MockitoJUnitRunner.class),0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);",0
get a angel client,0
add matrix,0
psAgent.initAndStart();,0
test conf,0
test master location,0
test app id,0
test user,0
test ps agent attempt id,0
test connection,0
test master client,0
test ip,0
test loc,0
test master location,0
test ps location,0
"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));",0
test all ps ids,0
test all matrix ids,0
test all matrix names,0
test matrix attribute,0
test matrix meta,0
test ps location,0
test partitions,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
https://blog.csdn.net/cocoonyang/article/details/63068108,0
v1[i] = v1[i] + da * v2[i];,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
v1[i] = v1[i] + da * v2[i];,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
test worker getActiveTaskNum,0
test worker getTaskNum,0
test worker getTaskManager,0
test workerId,0
test workerAttemptId,0
tet worker initFinished,0
test worker getInitMinclock,0
test worker loacation,0
test AppId,0
test Conf,0
test UserName,0
master location,0
masterClient,0
test psAgent,0
test worker get dataBlockManager,0
workerGroup.getSplits();,0
application,0
lcation,0
workerGroup info,0
worker info,0
task,0
Matrix parameters,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
Create an Angel client,0
Add different types of matrix,0
"DenseIntMatrix expect = new DenseIntMatrix(diRow, diCol);",0
using mock object,0
verification,0
Stubbing,0
Default does nothing.,0
The app injection is optional,0
"renderText(""hello world"");",0
"user choose a workerGroupID from the workergroups page,",0
now we should change the AngelApp params and render the workergroup page;,0
"static final String WORKER_ID = ""worker.id"";",0
"div(""#logo"").",0
"img(""/static/hadoop-st.png"")._().",0
import org.apache.hadoop.yarn.webapp.view.FooterBlock;,0
import org.apache.hadoop.yarn.webapp.view.HeaderBlock;,0
JQueryUI.jsnotice(html);,0
import org.apache.hadoop.conf.Configuration;,0
import java.lang.reflect.Field;,0
get block locations from file system,0
create a list of all block and their locations,0
"if the file is not splitable, just create the one block with",0
full file length,0
each split can be a maximum of maxSize,0
if remainder is between max and 2*max - then,0
"instead of creating splits of size max, left-max we",0
create splits of size left/2 and left/2. This is,0
a heuristic to avoid creating really really small,0
splits.,0
add this block to the block --> node locations map,0
"For blocks that do not have host/rack information,",0
assign to default  rack.,0
add this block to the rack --> block map,0
Add this host to rackToNodes map,0
add this block to the node --> block map,0
"if the file system does not have any rack information, then",0
use dummy rack location.,0
The topology paths have the host name included as the last,0
component. Strip it.,0
get tokens for all the required FileSystems..,0
"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,",0
job.getConfiguration());,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
all the files in input set,0
it is allowed for maxSize to be 0. Disable smoothing load for such cases,0
process all nodes and create splits that are local to a node. Generate,0
"one split per node iteration, and walk over nodes multiple times to",0
distribute the splits across nodes.,0
Skip the node if it has previously been marked as completed.,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
Remove all blocks which may already have been assigned to other,0
splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
Remove entries from blocksInNode so that we don't walk these,0
again.,0
Done creating a single split for this node. Move on to the next,0
node so that splits are distributed across nodes.,0
This implies that the last few blocks (or all in case maxSize=0),0
were not part of a split. The node is complete.,0
if there were any blocks left over and their combined size is,0
"larger than minSplitNode, then combine them into one split.",0
Otherwise add them back to the unprocessed pool. It is likely,0
that they will be combined with other blocks from the,0
same rack later on.,0
This condition also kicks in when max split size is not set. All,0
blocks on a node will be grouped together into a single split.,0
haven't created any split on this machine. so its ok to add a,0
smaller one for parallelism. Otherwise group it in the rack for,0
balanced size create an input split and add it to the splits,0
array,0
Remove entries from blocksInNode so that we don't walk this again.,0
The node is done. This was the last set of blocks for this node.,0
Put the unplaced blocks back into the pool for later rack-allocation.,0
Node is done. All blocks were fit into node-local splits.,0
Check if node-local assignments are complete.,0
All nodes have been walked over and marked as completed or all blocks,0
have been assigned. The rest should be handled via rackLock assignment.,0
"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """,0
"+ completedNodes.size() + "", size left: "" + totalLength);",0
"if blocks in a rack are below the specified minimum size, then keep them",0
"in 'overflow'. After the processing of all racks is complete, these",0
overflow blocks will be combined into splits.,0
Process all racks over and over again until there is no more work to do.,0
Create one split for this rack before moving over to the next rack.,0
Come back to this rack after creating a single split for each of the,0
remaining racks.,0
"Process one rack location at a time, Combine all possible blocks that",0
reside on this rack as one split. (constrained by minimum and maximum,0
split size).,0
iterate over all racks,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"if we created a split, then just go to the next rack",0
"if there is a minimum size specified, then create a single split",0
"otherwise, store these blocks into overflow data structure",0
There were a few blocks in this rack that,0
remained to be processed. Keep them in 'overflow' block list.,0
These will be combined later.,0
Process all overflow blocks,0
"This might cause an exiting rack location to be re-added,",0
but it should be ok.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"Process any remaining blocks, if any.",0
create an input split,0
add this split to the list that is returned,0
long num = totLength / maxSize;,0
all blocks for all the files in input set,0
mapping from a rack name to the list of blocks it has,0
mapping from a block to the nodes on which it has replicas,0
mapping from a node to the list of blocks that it contains,0
populate all the blocks for all files,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
"Security framework already loaded the tokens into current UGI, just use",0
them,0
Now remove the AM->RM token so tasks don't have it,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init container allocator,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
Init Client manager,0
Init PS Client manager,0
init parameter server manager,0
recover task information if needed,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
Init model saver & loader,0
start a web service if use yarn deploy mode,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
Check Workers,0
Check PSS,0
Check Clients,0
Check PS Clients,0
parse parameter server counters,0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
refresh last heartbeat timestamp,0
send a state update event to the specific PSAttempt,0
Check is there save request,0
Check is there load request,0
check matrix metadata inconsistencies between master and parameter server.,0
"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix",0
"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix",0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
choose a unused port,0
start RPC server,0
remove this parameter server attempt from monitor set,0
remove this parameter server attempt from monitor set,0
"if worker attempt id is not in monitor set, we should shutdown it",0
find workergroup in worker manager,0
"if this worker group does not initialized, just return WORKERGROUP_NOTREADY",0
"if this worker group run over, just return WORKERGROUP_EXITED",0
"if this worker group is running now, return tasks, workers, data splits for it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update the clock for this matrix,0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update task iteration,0
"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);",0
"Calculate how many splits we need. As each task handles a separate split of data, so we want",0
the number of splits equal to the number of tasks,0
split data,0
dispatch the splits to workergroups,0
split data,0
dispatch the splits to workergroups,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
write meta data to a temporary file,0
rename the temporary file to final file,0
"if the file exists, read from file and deserialize it",0
write task meta,0
write ps meta,0
generate a temporary file,0
write task meta to the temporary file first,0
rename the temporary file to the final file,0
"if last final task file exist, remove it",0
find task meta file which has max timestamp,0
"if the file does not exist, just return null",0
read task meta from file and deserialize it,0
generate a temporary file,0
write ps meta to the temporary file first.,0
rename the temporary file to the final file,0
"if the old final file exist, just remove it",0
find ps meta file,0
"if ps meta file does not exist, just return null",0
read ps meta from file and deserialize it,0
"saveContext.setTmpSavePath(HdfsUtil.generateTmpDirectory(context.getConf(),",0
"context.getApplicationId().toString(), new Path(saveContext.getSavePath())).toString());",0
Filter old epoch trigger first,0
Split the user request to sub-requests to pss,0
Init matrix files meta,0
Move output files,0
Write the meta file,0
Split the user request to sub-requests to pss,0
check whether psagent heartbeat timeout,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
LocalStorageToken is needed irrespective of whether security is enabled,0
or not.,0
"TokenCache.setJobToken(jobToken, taskCredentials);",0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Construct the actual Container,0
The null fields are per-container and will be constructed for each,0
container separately.,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
"a * in the classpath will only find a .jar, so we need to filter out",0
all .jars and add everything else,0
Propagate the system classpath when using the mini cluster,0
Add standard Hadoop classes,0
Add mr,0
Cache archives,0
Cache files,0
Sanity check,0
Add URI fragment or just the filename,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Final commmand,0
"if amTask is not null, we should clone task state from it",0
"if all parameter server complete commit, master can commit now",0
check whether parameter server heartbeat timeout,0
Transitions from the NEW state.,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event",0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event dispatched before",0
PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.COMMITTING state,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
reqeuest resource:send a resource request to the resource allocator,0
"Once the resource is applied, build and send the launch request to the container launcher",0
deallocator the resource of the ps attempt:send a resource deallocator request to the,0
resource allocator,0
set the launch time,0
add the ps attempt to the heartbeat timeout monitoring list,0
parse ps attempt location and put it to location manager,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt",0
or failed,0
remove ps attempt id from heartbeat timeout monitor list,0
release container:send a release request to container launcher,0
set the finish time only if launch time is set,0
private long scheduledTime;,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
Refresh ps location & matrix meta,0
start a new attempt for this ps,0
notify ps manager,0
"getContext().getLocationManager().setPsLocation(id, null);",0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
notify the event handler of state change,0
"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor",0
"if forcedState is set, just return",0
else get state from state machine,0
add this worker group to the success set,0
check if all worker group run over,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
calculate the actual number of worker groups and the total number of tasks based on the number of data split,0
"init all tasks , workers and worker groups and put them to the corresponding maps",0
just return the total task number now,0
TODO,1
check whether worker heartbeat timeout,0
"if workerAttempt is not null, we should clone task state from it",0
from NEW state,0
from SCHEDULED state,0
get data splits location for data locality,0
reqeuest resource:send a resource request to the resource allocator,0
"once the resource is applied, build and send the launch request to the container launcher",0
notify failed message to the worker,0
notify killed message to the worker,0
release the allocated container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
release the allocated container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
clean the container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
record the finish time,0
clean the container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listening list,0
record the finish time,0
"if the worker attempt launch successfully, add it to heartbeat timeout listening list",0
set worker attempt location,0
notify the register message to the worker,0
record the launch time,0
update worker attempt metrics,0
update tasks metrics,0
clean the container,0
notify the worker attempt run successfully message to the worker,0
record the finish time,0
init a worker attempt for the worker,0
schedule the worker attempt,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
"If we need Yarn to restart a new application master, we should not unregister from Yarn RM",0
register to Yarn RM,0
send heartbeat to Yarn RM every rmPollInterval milliseconds,0
"catch YarnRuntimeException, we should exit and need not retry",0
build heartbeat request,0
send heartbeat request to rm,0
"This can happen if the RM has been restarted. If it is in that state,",0
this application must clean itself up.,0
Setting NMTokens,0
assgin containers,0
"if some container is not assigned, release them",0
handle finish containers,0
dispatch container exit message to corresponding components,0
killed by framework,0
killed by framework,0
get application finish state,0
build application diagnostics,0
TODO:add a job history for angel,1
build unregister request,0
send unregister request to rm,0
Note this down for next interaction with ResourceManager,0
based on blacklisting comments above we can end up decrementing more,0
than requested. so guard for that.,0
send the updated resource request to RM,0
send 0 container count requests also to cancel previous requests,0
Update resource requests,0
try to assign to all nodes first to match node local,0
try to match all rack local,0
assign remaining,0
Update resource requests,0
send the container-assigned event to task attempt,0
build the start container request use launch context,0
send the start request to Yarn nm,0
send the message that the container starts successfully to the corresponding component,0
"after launching, send launched event to task attempt to move",0
it from ASSIGNED to RUNNING state,0
send the message that the container starts failed to the corresponding component,0
kill the remote container if already launched,0
start a thread pool to startup the container,0
See if we need up the pool size only if haven't reached the,0
maximum limit yet.,0
nodes where containers will run at *this* point of time. This is,0
*not* the cluster size and doesn't need to be.,0
"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the",0
later is just a buffer so we are not always increasing the,0
pool-size,0
the events from the queue are handled in parallel,0
using a thread pool,0
return if already stopped,0
shutdown any containers that might be left running,0
Add one sync matrix,0
addSyncMatrix();,0
"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {",0
"LOG.info(""ps id = "" + psEntry.getKey());",0
"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();",0
"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {",0
"LOG.info(""matrix id = "" + metaEntry.getKey());",0
"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());",0
},0
},0
get matrix ids in the parameter server report,0
get the matrices parameter server need to create and delete,0
"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix",0
"if a matrix exists on master but not exist on parameter server, this parameter server need build it.",0
Init control connection manager,0
Get ps locations from master and put them to the location cache.,0
Build and initialize rpc client to master,0
Get psagent id,0
Build PS control rpc client manager,0
Build local location,0
Initialize matrix meta information,0
Start all services,0
Stop all modules,0
Stop all modules,0
clock first,0
wait,0
Update generic resource counters,0
Updating resources specified in ResourceCalculatorProcessTree,0
Remove the CPU time consumed previously by JVM reuse,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Plus a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Update a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get values from pss use row/column indices,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"/ PSF get/update, use can implement their own psf",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get a row or a batch of rows,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
TODO:,1
Generate a flush request and put it to request queue,0
Generate a clock request and put it to request queue,0
Generate a merge request and put it to request queue,0
Generate a merge request and put it to request queue,0
"If the matrix op log cache does not exist for the matrix, create a new one for the",0
matrix,0
and add it to cache maps,0
Add the message to the tree map,0
"If there are flush / clock requests blocked, we need to put this merge request into",0
the waiting queue,0
Launch a merge worker to merge the update to matrix op log cache,0
Remove the message from the tree map,0
Wake up blocked flush/clock request,0
Add flush/clock request to listener list to waiting for all the existing,0
updates are merged,0
Wake up blocked flush/clock request,0
"If all updates are merged for this matrix, we need wake up flush/clock requests which are",0
blocked.,0
Get next merge message sequence id,0
Wake up listeners(flush/clock requests) that have little sequence id than current merge,0
position,0
Wake up blocked merge requests,0
Get minimal sequence id from listeners,0
"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we",0
should flush updates to local matrix storage,0
unused now,1
TODO:,1
Doing average or not,0
Filter un-important update,0
Split this row according the matrix partitions,0
Set split context,0
Remove the row from matrix,0
TODO,1
TODO: write map default value,1
TODO:,1
TODO:,1
TODO:,1
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"LOG.error(""channel "" + ctx.channel() + "" inactive"");",0
"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());",0
"LOG.error(""put response message queue failed "", e);",0
Use Epoll for linux,0
Update location table,0
Remove the server from failed list,0
Notify refresh success message to request dispatcher,0
Check PS exist or not,0
Check heartbeat timeout,0
Check PS restart or not,0
private final HashSet<ParameterServerId> refreshingServerSet;,0
Add it to failed rpc list,0
Add the server to gray server list,0
Add it to failed rpc list,0
Add the server to gray server list,0
Move from gray server list to failed server list,0
Handle the RPCS to this server,0
Submit the schedulable failed get RPCS,0
Submit new get RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"If the queue is empty, just return 0",0
"If request is not over limit, just submit it",0
Submit the schedulable failed get RPCS,0
Submit new put RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"LOG.info(""choose put server "" + psIds[index]);",0
Check all pending RPCS,0
Check get channel context,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
channelManager.printPools();,0
"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {",0
if(ts - entry.getValue() > requestTimeOut * 2)  {,0
"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())",0
"+ "" milliseconds, close all channels to it"");",0
closeChannels(entry.getKey());,0
"psLocToLastChannelTsMap.put(entry.getKey(), ts);",0
},0
},0
"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);",0
Remove all pending RPCS,0
Close all channel to this PS,0
private Channel getChannel(Location loc) throws Exception {,0
"return channelManager.getOrCreateChannel(new Location(loc.getIp(), loc.getPort() + 1));",0
},0
private GenericObjectPool<Channel> getChannelPool(Location loc) throws InterruptedException {,0
"return channelManager.getOrCreateChannelPool(new Location(loc.getIp(), loc.getPort() + 1), PSAgentContext",0
.get(),0
.getConf(),0
".getInt(AngelConf.ANGEL_WORKER_TASK_NUMBER,",0
AngelConf.DEFAULT_ANGEL_WORKER_TASK_NUMBER));,0
},0
"LOG.error(""send request "" + request + "" is interrupted"");",0
"LOG.error(""send request "" + request + "" failed, "", e);",0
Get server id and location for this request,0
"If location is null, means that the server is not ready",0
Get the channel for the location,0
Check if need get token first,0
Serialize the request,0
Send the request,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
request.getContext().setChannelPool(pool);,0
Allocate the bytebuf and serialize the request,0
find the partition request context from cache,0
"updateMatrixCache(request.getPartKey(), response.getPartition());",0
"updateMatrixCache(request.getPartKey(), response.getRowsSplit());",0
TODO,1
ServerRow rowSplit = PSAgentContext.get().getMatricesCache(),0
".getRowSplit(request.getPartKey().getMatrixId(), request.getPartKey(),",0
request.getRowIndex());,0
response.setRowSplit(rowSplit);,0
"updateMatrixCache(request.getPartKey(), response.getRowSplit());",0
"LOG.info(""user request id "" + request.getUserRequestId());",0
"LOG.info(""user request id "" + request.getUserRequestId());",0
TODO,1
"private void updateMatrixCache(PartitionKey partKey, ServerPartition partition) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, partition);",0
},0
"private void updateMatrixCache(PartitionKey partKey, ServerRow rowSplit) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowSplit);",0
},0
"private void updateMatrixCache(PartitionKey partKey, List<ServerRow> rowsSplit) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowsSplit);",0
},0
Get partitions for this row,0
Distinct get row requests,0
Need get from ps or storage/cache,0
"Switch to new request id, send a new request",0
First get this row from matrix storage,0
MatrixStorage matrixStorage =,0
PSAgentContext.get().getMatrixStorageManager().getMatrixStoage(matrixId);,0
TVector row = matrixStorage.getRow(rowIndex);,0
if (row != null && row.getClock() >= clock) {,0
result.set(row);,0
return row;,0
},0
Get row splits of this row from the matrix cache first,0
responseCache.addSubResponse(rowSplit);,0
"If the row split does not exist in cache, get it from parameter server",0
Wait the final result,0
Put it to the matrix cache,0
"matrixStorage.addRow(rowIndex, row);",0
Just wait result,0
Split the param use matrix partitions,0
Send request to PSS,0
Split the matrix oplog according to the matrix partitions,0
"If need update clock, we should send requests to all partitions",0
Send request to PSS,0
Filter the rowIds which are fetching now,0
Send the rowIndex to rpc dispatcher and return immediately,0
"LOG.info(""get row split use time="" + (System.currentTimeMillis() - startTs));",0
"LOG.info(""start to request "" + requestId);",0
"LOG.info(""start to request "" + requestId);",0
Split param use matrix partitons,0
"If all sub-results are received, just remove request and result cache",0
"LOG.info(""request = "" + request + "", cache = "" + cache);",0
"LOG.info(""start to merge "" + cache + "" for request "" + request);",0
"LOG.info(""psf get merge use time = "" + (System.currentTimeMillis() - startTs));",0
Split this row according the matrix partitions,0
Set split context,0
Split this row according the matrix partitions,0
Set split context,0
long startTs = System.currentTimeMillis();,0
"LOG.error(""combine use time = "" + (System.currentTimeMillis() - startTs));",0
Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition,0
Generate dispatch items and add them to the corresponding queues,0
Filter the rowIds which are fetching now,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Put the row split to the cache(row index to row splits map),0
"If all splits of the row are received, means this row can be merged",0
TODO,1
TODO,1
/////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO,1
TODO,1
///////////////////////////////////////////////////////////////////////////////////////////////,0
Now we just support pipelined row splits merging for dense type row,0
Pre-fetching is disable default,0
matrix id to clock map,0
"task index, it must be unique for whole application",0
Deserialize data splits meta,0
Get workers,0
Send request to every ps,0
Wait the responses,0
Update clock cache,0
if(syncNum % 1024 == 0) {,0
},0
"Use simple flow, do not use any cache",0
Get row from cache.,0
"if row clock is satisfy ssp staleness limit, just return.",0
Get row from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC mode, just get from pss.",0
"For BSP/SSP, get rows from storage/cache first",0
Get from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC, just get rows from pss.",0
no more retries.,0
calculate sleep time and return.,0
parse the i-th sleep-time,0
parse the i-th number-of-retries,0
calculateSleepTime may overflow.,0
"A few common retry policies, with no delays.",0
close is a local operation and should finish within milliseconds; timeout just to be safe,0
response will be null for one way messages.,0
maxFrameLength = 2G,0
lengthFieldOffset = 0,0
lengthFieldLength = 8,0
"lengthAdjustment = -8, i.e. exclude the 8 byte length itself",0
"initialBytesToStrip = 8, i.e. strip out the length field itself",0
indicates whether this connection's life cycle is managed,0
See if we already have a connection (common case),0
create a unique lock for this RS + protocol (if necessary),0
get the RS lock,0
do one more lookup in case we were stalled above,0
Only create isa when we need to.,0
definitely a cache miss. establish an RPC for,0
this RS,0
Throw what the RemoteException was carrying.,0
check,0
every,0
minutes,0
TODO,1
failoverHandler,0
"The number of times this invocation handler has ever been failed over,",0
before this method invocation attempt. Used to prevent concurrent,0
failed method invocations from triggering multiple failover attempts.,0
Make sure that concurrent failed method invocations,0
only cause a,0
single actual fail over.,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
"LOG.info(""method "" + method.getName() + ""construct request time = """,0
+ (System.currentTimeMillis() - beforeConstructTs));,0
get an instance of the method arg type,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
Message (hand written code usually has only a single,0
argument),0
log any RPC responses that are slower than the configured,0
warn,0
response time or larger than configured warning size,0
"when tagging, we let TooLarge trump TooSmall to keep",0
output simple,0
note that large responses will often also be slow.,0
provides a count of log-reported slow responses,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
unexpected,0
"in the protobuf methods, args[1] is the only significant argument",0
for JSON encoding,0
base information that is reported regardless of type of call,0
Disable Nagle's Algorithm since we don't want packets to wait,0
Configure the event pipeline factory.,0
Make a new connection.,0
Remove all pending requests (will be canceled after relinquishing,0
write lock).,0
Cancel any pending requests by sending errors to the callbacks:,0
Close the channel:,0
Close the connection:,0
Shut down all thread pools to exit.,0
"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());",0
See NettyServer.prepareResponse for where we write out the response.,0
"It writes the call.id (int), a boolean signifying any error (and if",0
"so the exception name/trace), and the response bytes",0
Read the call id.,0
"When the stream is closed, protobuf doesn't raise an EOFException,",0
"instead, it returns a null message object.",0
"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +",0
System.currentTimeMillis());,0
"It would be good widen this to just Throwable, but IOException is what we",0
allow now,0
not implemented,1
not implemented,1
"track what RpcEngine is used by a proxy class, for stopProxy()",0
cache of RpcEngines by protocol,0
return the RpcEngine configured to handle a protocol,0
We only handle the ConnectException.,0
This is the exception we can't handle.,0
check if timed out,0
wait for retry,0
IGNORE,0
return the RpcEngine that handles a proxy object,0
The default implementation works synchronously,0
punt: allocate a new buffer & copy into it,0
Parse cmd parameters,0
load hadoop configuration,0
load angel system configuration,0
load user configuration:,0
load user config file,0
load command line parameters,0
load user job resource files,0
load ml conf file for graph based algorithm,0
load user job jar if it exist,0
Expand the environment variable,0
Add default fs(local fs) for lib jars.,0
"LOG.info(System.getProperty(""user.dir""));",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
a simple hdfs copy function assume src path and dest path are in same hdfs,0
and FileSystem object has same schema,0
"LOG.warn(""interrupted while sleeping"", ie);",0
public static String getHostname() {,0
try {,0
"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();",0
} catch (UnknownHostException uhe) {,0
},0
"return new StringBuilder().append("""").append(uhe).toString();",0
},0
"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {",0
String hostname = getHostname();,0
String classname = clazz.getSimpleName();,0
"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new",0
"StringBuilder().append(""Starting "").append(classname).toString(), new",0
"StringBuilder().append(""  host = "").append(hostname).toString(), new",0
"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new",0
"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new",0
"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));",0
,0
"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {",0
public void run() {,0
"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +",0
"this.val$classname + "" at "" + this.val$hostname}));",0
},0
});,0
},0
"We we interrupted because we're meant to stop? If not, just",0
continue ignoring the interruption,0
Recalculate waitTime.,0
// Begin delegation to Thread,0
// End delegation to Thread,0
instance submitter class,0
Obtain filename from path,0
Split filename to prexif and suffix (extension),0
Check if the filename is okay,0
Prepare temporary file,0
Prepare buffer for data copying,0
Open and check input stream,0
Open output stream and copy data between source file in JAR and the temporary file,0
"If read/write fails, close streams safely before throwing an exception",0
"Finally, load the library",0
little endian load order,0
tail,0
fallthrough,0
fallthrough,0
finalization,0
fmix(h1);,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
throw new AngelException(e);,0
JobStateProto jobState = report.getJobState();,0
Used for java code to get a AngelClient instance,0
Used for python code to get a AngelClient instance,0
the leaf level file should be readable by others,0
the subdirs in the path should have execute permissions for,0
others,0
2.get job id,0
Credentials credentials = new Credentials();,0
4.copy resource files to hdfs,0
5.write configuration to a xml file,0
6.create am container context,0
7.Submit to ResourceManager,0
8.get app master client,0
Create a number of filenames in the JobTracker's fs namespace,0
add all the command line files/ jars and archive,0
first copy them to jobtrackers filesystem,0
should not throw a uri exception,0
should not throw an uri excpetion,0
set the timestamps of the archives and files,0
set the public/private visibility of the archives and files,0
get DelegationToken for each cached file,0
check if we do not need to copy the files,0
is jt using the same file system.,0
just checking for uri strings... doing no dns lookups,0
to see if the filesystems are the same. This is not optimal.,0
but avoids name resolution.,0
this might have name collisions. copy will throw an exception,0
parse the original path to create new path,0
check for ports,0
Write job file to JobTracker's fs,0
Setup resource requirements,0
Setup LocalResources,0
Setup security tokens,0
Setup the command to run the AM,0
Add AM user command opts,0
Final command,0
Setup the CLASSPATH in environment,0
"i.e. add { Hadoop jars, job jar, CWD } to classpath.",0
Setup the environment variables for Admin first,0
"Setup the environment variables (LD_LIBRARY_PATH, etc)",0
Parse distributed cache,0
Setup ContainerLaunchContext for AM container,0
Set up the ApplicationSubmissionContext,0
private volatile PS2PSPusherImpl ps2PSPusher;,0
TODO,1
Add tokens to new user so that it may execute its task correctly.,0
TODO,1
to exit,0
TODO,1
TODO,1
ps2PSPusher.recover(ProtobufUtil.convert(needRecoverParts.get(i)));,0
context.getSnapshotManager().processRecovery();,0
First check snapshot,0
Check load path setting,0
TODO,1
if(ps2PSPusher != null) {,0
ps2PSPusher.start();,0
},0
public PS2PSPusherImpl getPs2PSPusher() {,0
return ps2PSPusher;,0
},0
"LOG.error(""send response of request "" + requestToString(clientId, seqId) + "" failed "");",0
"LOG.error(""send response of request failed, request seqId="" + seqId + "", channel="" + ch, ex);",0
Release the input buffer,0
Release the input buffer,0
"1. handle the rpc, get the response",0
Release the input buffer,0
2. Serialize the response,0
Send the serialized response,0
Just serialize the head,0
Exception happened,0
Just serialize the head,0
Exception happened,0
Reset the response and allocate buffer again,0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this location, only master ps can accept the update",0
Check the partition state again,0
Start to put the update to the slave pss,0
TODO,1
"context.getPS2PSPusher().put(request, in, partLoc);",0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this partition, if not, just return failed",0
Start to put the update to the slave pss,0
TODO,1
return ServerState.GENERAL;,0
Use Epoll for linux,0
public String uuid;,0
public void setChannelPool(GenericObjectPool<Channel> channelPool) {,0
this.channelPool = channelPool;,0
},0
private final ParameterServer psServer;,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO:default value,1
TODO,1
///////////////////////////////////////////////////////////////////////////////////////////////,0
"////// disk io method, for model read/load",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"////// network io method, for model transform",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO: dynamic add/delete row,1
private final List<PartitionKey> partitionKeys;,0
Read matrix meta from meta file,0
Load partitions from file use fork-join,0
Save partitions to files use fork-join,0
Write the ps matrix meta to the meta file,0
Use Epoll for linux,0
find the partition request context from cache,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
channelManager.removeChannelPool(loc);,0
Generate seq id,0
Create a RecoverPartRequest,0
Serialize the request,0
Change the seqId for the request,0
Serialize the request,0
"First check the state of the channels in the pool, if a channel is unused, just return",0
"If all channels are in use, create a new channel or wait",0
Create a new channel,0
"add the PSAgentContext,need fix",0
TODO:add more vector type,1
TODO : subDim set,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
write the max abs,0
---------------------------------------------------,0
---------------------------------------------------,0
---------------------------------------------------------------,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"dger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)",0
"sger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)",0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
add dense double matrix,0
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
get configuration from config file,0
set localDir with enviroment set by nm.,0
get master location,0
init task manager and start tasks,0
start heartbeat thread,0
taskManager.assignTaskIds(response.getTaskidsList());,0
todo,1
"if worker timeout, it may be knocked off.",0
"SUCCESS, do nothing",0
heartbeatFailedTime = 0;,0
private KEY currentKey;,0
will be created,0
TODO Auto-generated method stub,1
Bitmap bitmap = new Bitmap();,0
int max = indexArray[size - 1];,0
byte [] bitIndexArray = new byte[max / 8 + 1];,0
for(int i = 0; i < size; i++){,0
int bitIndex = indexArray[i] >> 3;,0
int bitOffset = indexArray[i] - (bitIndex << 3);,0
switch(bitOffset){,0
case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;,0
case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;,0
case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;,0
case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;,0
case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;,0
case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;,0
case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;,0
case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;,0
},0
},0
"true, false",0
//////////////////////////////,0
Application Configs,0
//////////////////////////////,0
//////////////////////////////,0
Master Configs,0
//////////////////////////////,0
//////////////////////////////,0
Worker Configs,0
//////////////////////////////,0
//////////////////////////////,0
Task Configs,0
//////////////////////////////,0
//////////////////////////////,0
ParameterServer Configs,0
//////////////////////////////,0
////////////////// IPC //////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
Configs used to ANGEL_PS_PSAGENT running mode future.,0
model parse,0
Mark whether use pyangel or not.,0
private Configuration conf;,0
"Configuration that should be used in python environment, there should only be one",0
configuration instance in each Angel context.,0
Use private access means jconf should not be changed or modified in this way.,0
Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:,0
Do nothing,0
To-DO: add other ways to justify different value types,0
"This is so ugly, must re-implement by more elegance way",1
"Create python path which include angel's jars, the python directory in ANGEL_HOME,",0
and other files submitted by user.,0
Launch python process,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);",0
predictTest();,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
class number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set log path,0
Set trainning data path,0
Set save model path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
predictTest();,0
Set actionType train,0
Set training data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log save path,0
Set actionType train,0
Set testing data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
predictTest();,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Data is classification,0
Model is classification,0
Train batch number per epoch.,0
loss delta,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
predictTest();,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Data is classification,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
"gradient descent first, then truncated",0
StringBuffer sb = new StringBuffer();,0
"sb.append(""GetColsParam values "");",0
"sb.append(values[i] +"" "");",0
LOG.error(sb.toString());,0
StringBuffer sb = new StringBuffer();,0
"sb.append(""GetColsParams "");",0
for (int i = 0; i < this.cols.length; i ++) {,0
"sb.append(this.cols[i] + "" "");",0
},0
LOG.error(sb.toString());,0
Arrays.sort(rows);,0
int sum = 0;,0
"System.out.println(""pkeys.size="" + pkeys.size());",0
"params.add(new PartitionGetColsParam(matrixId, pkeys.get(0), rows, cols));",0
sum += part.length;,0
"LOG.info(""split length = "" + sum + "", cols = "" + cols.length);",0
"System.out.println(""deserialize cols.length="" + nCols);",0
"System.out.print(""deserialize "");",0
"System.out.print(cols[c] + "" "");",0
System.out.println();,0
"System.out.print(""PartitionGet "");",0
for (int i = 0; i < cols.length; i ++) {,0
"System.out.print(cols[i] + "" "");",0
},0
System.out.println();,0
"System.out.println(""doGet Double cols.length="" + cols.length);",0
"LOG.info(""Here merge"");",0
int sum = 0;,0
sum += rrr.cols.length;,0
"LOG.error(""double rrr.cols = "" + sum + "", map size = "" + maps.size());",0
IntDoubleVector[] allVectors = new IntDoubleVector[allColumns.length];,0
for (int i = 0; i < allColumns.length; i ++),0
allVectors[i] = maps.get(allColumns[i]);,0
"return new GetColsResult(VFactory.denseLongVector(allColumns),",0
"VFactory.compIntDoubleVector(dim, allVectors, allVectors.length));",0
int sum = 0;,0
sum += cols.length;,0
"System.out.print("" "" + cols[i]);",0
System.out.println();,0
"LOG.error(""float rrr.cols = "" + sum + "", map size = "" + maps.size());",0
IntFloatVector[] allVectors = new IntFloatVector[allColumns.length];,0
for (int i = 0; i < allColumns.length; i ++),0
allVectors[i] = maps.get(allColumns[i]);,0
"return new GetColsResult(VFactory.denseLongVector(allColumns),",0
"VFactory.compIntFloatVector(dim, allVectors, allVectors.length));",0
TODO Auto-generated method stub,1
"ground truth: positive, precision: positive",0
start row index for words,0
start row index for docs,0
doc ids,0
topic assignments,0
word to docs reverse index,0
count word,0
build word start index,0
build word to doc reverse idx,0
build dks,0
dks = new TraverseHashMap[n_docs];,0
for (int d = 0; d < n_docs; d++) {,0
if (K < Short.MAX_VALUE) {,0
if (docs.get(d).len < Byte.MAX_VALUE),0
dks[d] = new S2BTraverseMap(docs.get(d).len);,0
if (docs.get(d).len < Short.MAX_VALUE),0
"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));",0
else,0
"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));",0
} else {,0
"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));",0
},0
},0
build dks,0
allocate update maps,0
Skip if no token for this word,0
Check whether error when fetching word-topic,0
Build FTree for current word,0
current doc,0
old topic assignment,0
"Check if error happens. if this happen, it's probably that failures happen to servers.",0
We need to adjust the memory settings or network fetching parameters.,0
Update statistics if needed,0
Calculate psum and sample new topic,0
Update statistics if needed,0
Assign new topic,0
Skip if no token for this word,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
Write #rows,0
Write each row,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
dense,0
sparse,0
calculate columns,0
loss function,0
gradient and hessian,0
"categorical feature set, null: none, empty: all, else: partial",0
"node's end index in instancePos, instances in [start, end] belong to a tree node",0
initialize the phase,0
current tree and depth,0
create loss function,0
calculate grad info of each instance,0
"create data sketch, push candidate split value to PS",0
1. calculate candidate split value,0
categorical features,0
2. push local sketch to PS,0
the leader worker,0
merge categorical features,0
create updates,0
"pull the global sketch from PS, only called once by each worker",0
number of categorical feature,0
sample feature,0
push sampled feature set to the current tree,0
create new tree,0
"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,",0
calculate gradient,0
"1. create new tree, initialize tree nodes and node stats",0
"2. initialize feature set, if sampled, get from PS, otherwise use all the features",0
2.1. pull the sampled features of the current tree,0
this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();,0
"2.2. if use all the features, only called one",0
"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle",0
4. set root node to active,0
"5. reset instance position, set the root node's span",0
6. calculate gradient,0
1. decide nodes that should be calculated,0
2. decide calculated and subtracted tree nodes,0
3. calculate threads,0
wait until all threads finish,0
4. subtract threads,0
wait until all threads finish,0
5. send histograms to PS,0
6. update histogram cache,0
clock,0
find split,0
"1. find responsible tree node, using RR scheme",0
2. pull gradient histogram,0
2.1. get the name of this node's gradient histogram on PS,0
2.2. pull the histogram,0
2.3. find best split result of this tree node,0
2.3.1 using server split,0
"update the grad stats of the root node on PS, only called once by leader worker",0
update the grad stats of children node,0
update the left child,0
update the right child,0
"2.3.2 the updated split result (tree node/feature/value/gain) on PS,",0
"2.3.3 otherwise, the returned histogram contains the gradient info",0
"2.3.4 the updated split result (tree node/feature/value/gain) on PS,",0
2.3.5 reset this tree node's gradient histogram to 0,0
3. push split feature to PS,0
4. push split value to PS,0
5. push split gain to PS,0
6. set phase to AFTER_SPLIT,0
this.phase = GBDTPhase.AFTER_SPLIT;,0
clock,0
1. get split feature,0
2. get split value,0
3. get split gain,0
4. get node weight,0
5. split node,0
update local replica,0
create AfterSplit task,0
"2. check thread stats, if all threads finish, return",0
6. clock,0
"split the span of one node, reset the instance position",0
in case this worker has no instance on this node,0
set the span of left child,0
set the span of right child,0
"1. left to right, find the first instance that should be in the right child",0
"2. right to left, find the first instance that should be in the left child",0
3. swap two instances,0
4. find the cut pos,0
5. set the span of left child,0
6. set the span of right child,0
set tree node to active,0
set node to leaf,0
set node to inactive,0
finish current depth,0
finish current tree,0
set the tree phase,0
check if there is active node,0
check if finish all the tree,0
update node's grad stats on PS,0
"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split",0
the root node's stats is updated by leader worker,0
1. create the update,0
2. push the update to PS,0
1. update predictions of training data,0
2. update predictions of validation data,0
the leader task adds node prediction to flush list,0
1. name of this node's grad histogram on PS,0
2. build the grad histogram of this node,0
3. push the histograms to PS,0
4. reset thread stats to finished,0
5.1. set the children nodes of this node,0
5.2. set split info and grad stats to this node,0
5.2. create children nodes,0
"5.3. create node stats for children nodes, and add them to the tree",0
5.4. reset instance position,0
"5.5. add new active nodes if possible, inc depth, otherwise finish this tree",0
5.6. set children nodes to leaf nodes,0
5.7. set nid to leaf node,0
5.8. deactivate active node,0
"get feature type, 0:empty 1:all equal 2:real",0
"if not -1, sufficient space will be allocated at once",0
copy the highest levels,0
copy baseBuffer,0
merge two non-empty quantile sketches,0
left child <= split value; right child > split value,0
"the first: minimal, the last: maximal",0
categorical features,0
continuous features,0
left child <= split value; right child > split value,0
feature index used to split,0
feature value used to split,0
loss change after split this node,0
grad stats of the left child,0
grad stats of the right child,0
"LOG.info(""Constructor with fid = -1"");",0
fid = -1: no split currently,0
the minimal split value is the minimal value of feature,0
the splits do not include the maximal value of feature,0
"1. the average distance, (maxValue - minValue) / splitNum",0
2. calculate the candidate split value,0
1. new feature's histogram (grad + hess),0
size: sampled_featureNum * (2 * splitNum),0
"in other words, concatenate each feature's histogram",0
2. get the span of this node,0
------ 3. using sparse-aware method to build histogram ---,0
"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances",0
3.1. get the instance index,0
3.2. get the grad and hess of the instance,0
3.3. add to the sum,0
3.4. loop the non-zero entries,0
3.4.1. get feature value,0
3.4.2. current feature's position in the sampled feature set,0
"int fPos = findFidPlace(this.controller.fSet, fid);",0
3.4.3. find the position of feature value in a histogram,0
"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]",0
3.4.4. add the grad and hess to the corresponding bin,0
3.4.5. add the reverse to the bin that contains 0.0f,0
4. add the grad and hess sum to the zero bin of all features,0
find the best split result of the histogram of a tree node,0
1. calculate the gradStats of the root node,0
"1.1. update the grad stats of the root node on PS, only called once by leader worker",0
2. loop over features,0
2.1. get the ture feature id in the sampled feature set,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"update the grad stats of the root node on PS, only called once by leader worker",0
3. update the grad stats of children node,0
3.1. update the left child,0
3.2. update the right child,0
find the best split result of one feature,0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
split value = sketches[splitIdx],0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
partition number,0
cols of each partition,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
"loop all the possible split value, start from split[0], the first item is the minimal feature value",0
if (left > end) return end - start;,0
find the best split result of the histogram of a tree node,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
find the best split result of one feature,0
1. set the feature id,0
splitEntry.setFid(fid);,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]",0
the task use index to find fvalue,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting",0
"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking.",0
max and min of each feature,0
clear all the information,0
calculate the sum of gradient and hess,0
"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,",0
ridx),0
check if necessary information is ready,0
"same as add, reduce is used in All Reduce",0
"features used in this tree, if equals null, means use all the features without sampling",0
node in the tree,0
the gradient info of each instances,0
initialize nodes,0
gradient,0
second order gradient,0
TODO: only support dense double now,1
int sendStartCol = (int) row.getStartCol();,0
find the max abs,0
compress data,0
logistic loss for binary classification task.,0
"logistic loss, but predict un-transformed margin",0
check if label in range,0
return the default evaluation metric for the objective,0
"task type: classification, regression, or ranking",0
"quantile sketch, size = featureNum * splitNum",0
"gradient histograms, size = treeNodeNum * featureNum * splitNum",0
"active tree nodes, size = pow(2, treeDepth) -1",0
sampled features. size = treeNum * sampleRatio * featureNum,0
categorical feature. size = workerNum * cateFeatNum * splitNum,0
"split features, size = treeNum * treeNodeNum",0
"split values, size = treeNum * treeNodeNum",0
"split gains, size = treeNum * treeNodeNum",0
"node weights, size = treeNum * treeNodeNum",0
"node preds, size = treeNum * treeNodeNum",0
if using PS to perform split,0
step size for a tree,0
number of class,0
minimum loss change required for a split,0
maximum depth of a tree,0
number of features,0
number of nonzero,0
number of candidates split value,0
----- the rest parameters are less important ----,0
base instance weight,0
minimum amount of hessian(weight) allowed in a child,0
L2 regularization factor,0
L1 regularization factor,0
default direction choice,0
maximum delta update we can add in weight estimation,0
this parameter can be used to stabilize update,0
default=0 means no constraint on weight delta,0
whether we want to do subsample for row,0
whether to subsample columns for each tree,0
accuracy of sketch,0
accuracy of sketch,0
leaf vector size,0
option for parallelization,0
option to open cacheline optimization,0
whether to not print info during training.,0
maximum depth of the tree,0
number of features used for tree construction,0
"minimum loss change required for a split, otherwise stop split",0
----- the rest parameters are less important ----,0
default direction choice,0
whether we want to do sample data,0
whether to sample columns during tree construction,0
whether to use histogram for split,0
number of histogram units,0
whether to print info during training.,0
----- the rest parameters are obtained after training ----,0
total number of nodes,0
number of deleted nodes */,0
@maxIndex: this variable contains the max index of node/word,0
some params,0
max index for node/word,0
compute number of nodes for one row,0
check the length of dot values,0
merge dot values from all partitions,0
Skip-Gram model,0
Negative sampling,0
used to accumulate the updates for input vectors,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
Skip-Gram model,0
Negative sampling,0
used to accumulate the updates for input vectors,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
update output,0
Some params,0
compute number of nodes for one row,0
window size,0
Skip-Gram model,0
Accumulate the input vectors from context,0
Negative sampling,0
used to accumulate the updates for input vectors,0
window size,0
skip-gram model,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
update output,0
some params,0
batch sentences,0
max index for node/word,0
compute number of nodes for one row,0
check the length of dot values,0
merge dot values from all partitions,0
locates the input vectors to local array to prevent randomly access,0
on the large server row.,0
fill 0 for context vector,0
window size,0
Continuous bag-of-words Models,0
Accumulate the input vectors from context,0
Calculate the partial dot values,0
We should guarantee here that the sample would not equal the ``word``,0
used to accumulate the context input vectors,0
locates the input vector into local arrays to prevent randomly access for,0
the large server row.,0
window size,0
while true to prevent sampling out a positive target,0
how to prevent the randomly access to the output vectors??,0
accumulate gradients for the input vectors,0
update output vectors,0
update input,0
update output,0
Some params,0
compute number of nodes for one row,0
set basic configuration keys,0
use local deploy mode and dummy data spliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
import jdk.nashorn.internal.runtime.regexp.joni.Config;,0
"paras[1] = ""abc"";",0
"paras[2] = ""123"";",0
Add standard Hadoop classes,0
Feature number of train data,0
Total iteration number,0
Learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set input data path,0
Set save model path,0
Set actionType train,0
QSLRRunner runner = new QSLRRunner();,0
runner.train(conf);,0
"conf.setInt(AngelConf.ANGEL_STALENESS, -1);",0
Dataset,0
"Data format, libsvm or dummy",0
class number,0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set Softmax algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
Feature number of train data,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Ratio of validation,0
Learning rate,0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set DeepFM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
Model type,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set FM algorithm parameters,0
Set model class,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set WideAndDeep algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
"Set LDA parameters #V, #K",0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set LDA algorithm parameters,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set SVM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Load model meta,0
Convert model,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Convert model,0
load hadoop configuration,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model,0
load hadoop configuration,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
worker register,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
attempt 0,0
attempt1,0
attempt1,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
set basic configuration keys,0
"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());",0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
Thread.sleep(5000);,0
"response = master.getJobReport(null, request);",0
"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);",0
"assertEquals(response.getJobReport().getCurIteration(), jobIteration);",0
"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
testDenseDoubleCompUDF();,0
testSparseDoubleCompUDF();,0
testDenseFloatCompUDF();,0
testSparseFloatCompUDF();,0
testDenseIntCompUDF();,0
testSparseIntCompUDF();,0
testDenseLongCompUDF();,0
testSparseLongCompUDF();,0
testSparseDoubleLongKeyCompUDF();,0
testSparseFloatLongKeyCompUDF();,0
testSparseIntLongKeyCompUDF();,0
testSparseLongLongKeyCompUDF();,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add sparse double matrix,0
add comp dense double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"LOG.info(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntFloatVector row = (IntFloatVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntLongVector row = (IntLongVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
import com.tencent.angel.psagent.consistency.SSPConsistencyController;,0
@RunWith(MockitoJUnitRunner.class),0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);",0
get a angel client,0
add matrix,0
psAgent.initAndStart();,0
test conf,0
test master location,0
test app id,0
test user,0
test ps agent attempt id,0
test connection,0
test master client,0
test ip,0
test loc,0
test master location,0
test ps location,0
"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));",0
test all ps ids,0
test all matrix ids,0
test all matrix names,0
test matrix attribute,0
test matrix meta,0
test ps location,0
test partitions,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
https://blog.csdn.net/cocoonyang/article/details/63068108,0
v1[i] = v1[i] + da * v2[i];,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
v1[i] = v1[i] + da * v2[i];,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
test worker getActiveTaskNum,0
test worker getTaskNum,0
test worker getTaskManager,0
test workerId,0
test workerAttemptId,0
tet worker initFinished,0
test worker getInitMinclock,0
test worker loacation,0
test AppId,0
test Conf,0
test UserName,0
master location,0
masterClient,0
test psAgent,0
test worker get dataBlockManager,0
workerGroup.getSplits();,0
application,0
lcation,0
workerGroup info,0
worker info,0
task,0
using mock object,0
verification,0
Stubbing,0
Default does nothing.,0
The app injection is optional,0
"renderText(""hello world"");",0
"user choose a workerGroupID from the workergroups page,",0
now we should change the AngelApp params and render the workergroup page;,0
"static final String WORKER_ID = ""worker.id"";",0
"div(""#logo"").",0
"img(""/static/hadoop-st.png"")._().",0
import org.apache.hadoop.yarn.webapp.view.FooterBlock;,0
import org.apache.hadoop.yarn.webapp.view.HeaderBlock;,0
JQueryUI.jsnotice(html);,0
import org.apache.hadoop.conf.Configuration;,0
import java.lang.reflect.Field;,0
get block locations from file system,0
create a list of all block and their locations,0
"if the file is not splitable, just create the one block with",0
full file length,0
each split can be a maximum of maxSize,0
if remainder is between max and 2*max - then,0
"instead of creating splits of size max, left-max we",0
create splits of size left/2 and left/2. This is,0
a heuristic to avoid creating really really small,0
splits.,0
add this block to the block --> node locations map,0
"For blocks that do not have host/rack information,",0
assign to default  rack.,0
add this block to the rack --> block map,0
Add this host to rackToNodes map,0
add this block to the node --> block map,0
"if the file system does not have any rack information, then",0
use dummy rack location.,0
The topology paths have the host name included as the last,0
component. Strip it.,0
get tokens for all the required FileSystems..,0
"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,",0
job.getConfiguration());,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
all the files in input set,0
it is allowed for maxSize to be 0. Disable smoothing load for such cases,0
process all nodes and create splits that are local to a node. Generate,0
"one split per node iteration, and walk over nodes multiple times to",0
distribute the splits across nodes.,0
Skip the node if it has previously been marked as completed.,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
Remove all blocks which may already have been assigned to other,0
splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
Remove entries from blocksInNode so that we don't walk these,0
again.,0
Done creating a single split for this node. Move on to the next,0
node so that splits are distributed across nodes.,0
This implies that the last few blocks (or all in case maxSize=0),0
were not part of a split. The node is complete.,0
if there were any blocks left over and their combined size is,0
"larger than minSplitNode, then combine them into one split.",0
Otherwise add them back to the unprocessed pool. It is likely,0
that they will be combined with other blocks from the,0
same rack later on.,0
This condition also kicks in when max split size is not set. All,0
blocks on a node will be grouped together into a single split.,0
haven't created any split on this machine. so its ok to add a,0
smaller one for parallelism. Otherwise group it in the rack for,0
balanced size create an input split and add it to the splits,0
array,0
Remove entries from blocksInNode so that we don't walk this again.,0
The node is done. This was the last set of blocks for this node.,0
Put the unplaced blocks back into the pool for later rack-allocation.,0
Node is done. All blocks were fit into node-local splits.,0
Check if node-local assignments are complete.,0
All nodes have been walked over and marked as completed or all blocks,0
have been assigned. The rest should be handled via rackLock assignment.,0
"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """,0
"+ completedNodes.size() + "", size left: "" + totalLength);",0
"if blocks in a rack are below the specified minimum size, then keep them",0
"in 'overflow'. After the processing of all racks is complete, these",0
overflow blocks will be combined into splits.,0
Process all racks over and over again until there is no more work to do.,0
Create one split for this rack before moving over to the next rack.,0
Come back to this rack after creating a single split for each of the,0
remaining racks.,0
"Process one rack location at a time, Combine all possible blocks that",0
reside on this rack as one split. (constrained by minimum and maximum,0
split size).,0
iterate over all racks,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"if we created a split, then just go to the next rack",0
"if there is a minimum size specified, then create a single split",0
"otherwise, store these blocks into overflow data structure",0
There were a few blocks in this rack that,0
remained to be processed. Keep them in 'overflow' block list.,0
These will be combined later.,0
Process all overflow blocks,0
"This might cause an exiting rack location to be re-added,",0
but it should be ok.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"Process any remaining blocks, if any.",0
create an input split,0
add this split to the list that is returned,0
long num = totLength / maxSize;,0
all blocks for all the files in input set,0
mapping from a rack name to the list of blocks it has,0
mapping from a block to the nodes on which it has replicas,0
mapping from a node to the list of blocks that it contains,0
populate all the blocks for all files,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
"Security framework already loaded the tokens into current UGI, just use",0
them,0
Now remove the AM->RM token so tasks don't have it,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init container allocator,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
Init Client manager,0
Init PS Client manager,0
init parameter server manager,0
recover task information if needed,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
Init model saver & loader,0
start a web service if use yarn deploy mode,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
Check Workers,0
Check PSS,0
Check Clients,0
Check PS Clients,0
parse parameter server counters,0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
refresh last heartbeat timestamp,0
send a state update event to the specific PSAttempt,0
Check is there save request,0
Check is there load request,0
check matrix metadata inconsistencies between master and parameter server.,0
"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix",0
"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix",0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
choose a unused port,0
start RPC server,0
remove this parameter server attempt from monitor set,0
remove this parameter server attempt from monitor set,0
"if worker attempt id is not in monitor set, we should shutdown it",0
find workergroup in worker manager,0
"if this worker group does not initialized, just return WORKERGROUP_NOTREADY",0
"if this worker group run over, just return WORKERGROUP_EXITED",0
"if this worker group is running now, return tasks, workers, data splits for it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update the clock for this matrix,0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update task iteration,0
"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);",0
"Calculate how many splits we need. As each task handles a separate split of data, so we want",0
the number of splits equal to the number of tasks,0
split data,0
dispatch the splits to workergroups,0
split data,0
dispatch the splits to workergroups,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
write meta data to a temporary file,0
rename the temporary file to final file,0
"if the file exists, read from file and deserialize it",0
write task meta,0
write ps meta,0
generate a temporary file,0
write task meta to the temporary file first,0
rename the temporary file to the final file,0
"if last final task file exist, remove it",0
find task meta file which has max timestamp,0
"if the file does not exist, just return null",0
read task meta from file and deserialize it,0
generate a temporary file,0
write ps meta to the temporary file first.,0
rename the temporary file to the final file,0
"if the old final file exist, just remove it",0
find ps meta file,0
"if ps meta file does not exist, just return null",0
read ps meta from file and deserialize it,0
"Path tmpPath = new Path(new Path(context.getConf().get(AngelConf.ANGEL_JOB_TMP_OUTPUT_PATH)),",0
String.valueOf(requestId));,0
Path tmpPath = HdfsUtil.toTmpPath(new Path(saveContext.getSavePath()));,0
saveContext.setTmpSavePath(tmpPath.toString());,0
Filter old epoch trigger first,0
Split the user request to sub-requests to pss,0
Init matrix files meta,0
Move output files,0
Write the meta file,0
Split the user request to sub-requests to pss,0
check whether psagent heartbeat timeout,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
LocalStorageToken is needed irrespective of whether security is enabled,0
or not.,0
"TokenCache.setJobToken(jobToken, taskCredentials);",0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Construct the actual Container,0
The null fields are per-container and will be constructed for each,0
container separately.,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
"a * in the classpath will only find a .jar, so we need to filter out",0
all .jars and add everything else,0
Propagate the system classpath when using the mini cluster,0
Add standard Hadoop classes,0
Add mr,0
Cache archives,0
Cache files,0
Sanity check,0
Add URI fragment or just the filename,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Final commmand,0
"if amTask is not null, we should clone task state from it",0
"if all parameter server complete commit, master can commit now",0
check whether parameter server heartbeat timeout,0
Transitions from the NEW state.,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event",0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event dispatched before",0
PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.COMMITTING state,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
reqeuest resource:send a resource request to the resource allocator,0
"Once the resource is applied, build and send the launch request to the container launcher",0
deallocator the resource of the ps attempt:send a resource deallocator request to the,0
resource allocator,0
set the launch time,0
add the ps attempt to the heartbeat timeout monitoring list,0
parse ps attempt location and put it to location manager,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt",0
or failed,0
remove ps attempt id from heartbeat timeout monitor list,0
release container:send a release request to container launcher,0
set the finish time only if launch time is set,0
private long scheduledTime;,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
Refresh ps location & matrix meta,0
start a new attempt for this ps,0
notify ps manager,0
"getContext().getLocationManager().setPsLocation(id, null);",0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
notify the event handler of state change,0
"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor",0
"if forcedState is set, just return",0
else get state from state machine,0
add this worker group to the success set,0
check if all worker group run over,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
calculate the actual number of worker groups and the total number of tasks based on the number of data split,0
"init all tasks , workers and worker groups and put them to the corresponding maps",0
just return the total task number now,0
TODO,1
check whether worker heartbeat timeout,0
"if workerAttempt is not null, we should clone task state from it",0
from NEW state,0
from SCHEDULED state,0
get data splits location for data locality,0
reqeuest resource:send a resource request to the resource allocator,0
"once the resource is applied, build and send the launch request to the container launcher",0
notify failed message to the worker,0
notify killed message to the worker,0
release the allocated container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
release the allocated container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
clean the container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
record the finish time,0
clean the container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listening list,0
record the finish time,0
"if the worker attempt launch successfully, add it to heartbeat timeout listening list",0
set worker attempt location,0
notify the register message to the worker,0
record the launch time,0
update worker attempt metrics,0
update tasks metrics,0
clean the container,0
notify the worker attempt run successfully message to the worker,0
record the finish time,0
init a worker attempt for the worker,0
schedule the worker attempt,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
"If we need Yarn to restart a new application master, we should not unregister from Yarn RM",0
register to Yarn RM,0
send heartbeat to Yarn RM every rmPollInterval milliseconds,0
"catch YarnRuntimeException, we should exit and need not retry",0
build heartbeat request,0
send heartbeat request to rm,0
"This can happen if the RM has been restarted. If it is in that state,",0
this application must clean itself up.,0
Setting NMTokens,0
assgin containers,0
"if some container is not assigned, release them",0
handle finish containers,0
dispatch container exit message to corresponding components,0
killed by framework,0
killed by framework,0
get application finish state,0
build application diagnostics,0
TODO:add a job history for angel,1
build unregister request,0
send unregister request to rm,0
Note this down for next interaction with ResourceManager,0
based on blacklisting comments above we can end up decrementing more,0
than requested. so guard for that.,0
send the updated resource request to RM,0
send 0 container count requests also to cancel previous requests,0
Update resource requests,0
try to assign to all nodes first to match node local,0
try to match all rack local,0
assign remaining,0
Update resource requests,0
send the container-assigned event to task attempt,0
build the start container request use launch context,0
send the start request to Yarn nm,0
send the message that the container starts successfully to the corresponding component,0
"after launching, send launched event to task attempt to move",0
it from ASSIGNED to RUNNING state,0
send the message that the container starts failed to the corresponding component,0
kill the remote container if already launched,0
start a thread pool to startup the container,0
See if we need up the pool size only if haven't reached the,0
maximum limit yet.,0
nodes where containers will run at *this* point of time. This is,0
*not* the cluster size and doesn't need to be.,0
"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the",0
later is just a buffer so we are not always increasing the,0
pool-size,0
the events from the queue are handled in parallel,0
using a thread pool,0
return if already stopped,0
shutdown any containers that might be left running,0
Add one sync matrix,0
addSyncMatrix();,0
"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {",0
"LOG.info(""ps id = "" + psEntry.getKey());",0
"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();",0
"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {",0
"LOG.info(""matrix id = "" + metaEntry.getKey());",0
"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());",0
},0
},0
get matrix ids in the parameter server report,0
get the matrices parameter server need to create and delete,0
"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix",0
"if a matrix exists on master but not exist on parameter server, this parameter server need build it.",0
Init control connection manager,0
Get ps locations from master and put them to the location cache.,0
Build and initialize rpc client to master,0
Get psagent id,0
Build PS control rpc client manager,0
Build local location,0
Initialize matrix meta information,0
Start all services,0
Stop all modules,0
Stop all modules,0
clock first,0
wait,0
Update generic resource counters,0
Updating resources specified in ResourceCalculatorProcessTree,0
Remove the CPU time consumed previously by JVM reuse,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Plus a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Update a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get values from pss use row/column indices,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"/ PSF get/update, use can implement their own psf",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get a row or a batch of rows,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
TODO:,1
Generate a flush request and put it to request queue,0
Generate a clock request and put it to request queue,0
Generate a merge request and put it to request queue,0
Generate a merge request and put it to request queue,0
"If the matrix op log cache does not exist for the matrix, create a new one for the",0
matrix,0
and add it to cache maps,0
Add the message to the tree map,0
"If there are flush / clock requests blocked, we need to put this merge request into",0
the waiting queue,0
Launch a merge worker to merge the update to matrix op log cache,0
Remove the message from the tree map,0
Wake up blocked flush/clock request,0
Add flush/clock request to listener list to waiting for all the existing,0
updates are merged,0
Wake up blocked flush/clock request,0
"If all updates are merged for this matrix, we need wake up flush/clock requests which are",0
blocked.,0
Get next merge message sequence id,0
Wake up listeners(flush/clock requests) that have little sequence id than current merge,0
position,0
Wake up blocked merge requests,0
Get minimal sequence id from listeners,0
"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we",0
should flush updates to local matrix storage,0
unused now,1
TODO:,1
Doing average or not,0
Filter un-important update,0
Split this row according the matrix partitions,0
Set split context,0
Remove the row from matrix,0
buf.writeDouble(0.0);,0
TODO,1
TODO: write map default value,1
buf.writeDouble(0);,0
TODO:,1
TODO:,1
TODO:,1
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"LOG.error(""channel "" + ctx.channel() + "" inactive"");",0
"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());",0
"LOG.error(""put response message queue failed "", e);",0
Use Epoll for linux,0
Update location table,0
Remove the server from failed list,0
Notify refresh success message to request dispatcher,0
Check PS exist or not,0
Check heartbeat timeout,0
Check PS restart or not,0
private final HashSet<ParameterServerId> refreshingServerSet;,0
Add it to failed rpc list,0
Add the server to gray server list,0
Add it to failed rpc list,0
Add the server to gray server list,0
Move from gray server list to failed server list,0
Handle the RPCS to this server,0
Submit the schedulable failed get RPCS,0
Submit new get RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"If the queue is empty, just return 0",0
"If request is not over limit, just submit it",0
Submit the schedulable failed get RPCS,0
Submit new put RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"LOG.info(""choose put server "" + psIds[index]);",0
Check all pending RPCS,0
Check get channel context,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
channelManager.printPools();,0
"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {",0
if(ts - entry.getValue() > requestTimeOut * 2)  {,0
"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())",0
"+ "" milliseconds, close all channels to it"");",0
closeChannels(entry.getKey());,0
"psLocToLastChannelTsMap.put(entry.getKey(), ts);",0
},0
},0
"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);",0
Remove all pending RPCS,0
Close all channel to this PS,0
private Channel getChannel(Location loc) throws Exception {,0
"return channelManager.getOrCreateChannel(new Location(loc.getIp(), loc.getPort() + 1));",0
},0
private GenericObjectPool<Channel> getChannelPool(Location loc) throws InterruptedException {,0
"return channelManager.getOrCreateChannelPool(new Location(loc.getIp(), loc.getPort() + 1), PSAgentContext",0
.get(),0
.getConf(),0
".getInt(AngelConf.ANGEL_WORKER_TASK_NUMBER,",0
AngelConf.DEFAULT_ANGEL_WORKER_TASK_NUMBER));,0
},0
"LOG.error(""send request "" + request + "" is interrupted"");",0
"LOG.error(""send request "" + request + "" failed, "", e);",0
Get server id and location for this request,0
"If location is null, means that the server is not ready",0
Get the channel for the location,0
Check if need get token first,0
Serialize the request,0
Send the request,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
request.getContext().setChannelPool(pool);,0
Allocate the bytebuf and serialize the request,0
find the partition request context from cache,0
"updateMatrixCache(request.getPartKey(), response.getPartition());",0
"updateMatrixCache(request.getPartKey(), response.getRowsSplit());",0
TODO,1
ServerRow rowSplit = PSAgentContext.get().getMatricesCache(),0
".getRowSplit(request.getPartKey().getMatrixId(), request.getPartKey(),",0
request.getRowIndex());,0
response.setRowSplit(rowSplit);,0
"updateMatrixCache(request.getPartKey(), response.getRowSplit());",0
"LOG.info(""user request id "" + request.getUserRequestId());",0
"LOG.info(""user request id "" + request.getUserRequestId());",0
TODO,1
"private void updateMatrixCache(PartitionKey partKey, ServerPartition partition) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, partition);",0
},0
"private void updateMatrixCache(PartitionKey partKey, ServerRow rowSplit) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowSplit);",0
},0
"private void updateMatrixCache(PartitionKey partKey, List<ServerRow> rowsSplit) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowsSplit);",0
},0
Get partitions for this row,0
Distinct get row requests,0
Need get from ps or storage/cache,0
"Switch to new request id, send a new request",0
First get this row from matrix storage,0
MatrixStorage matrixStorage =,0
PSAgentContext.get().getMatrixStorageManager().getMatrixStoage(matrixId);,0
TVector row = matrixStorage.getRow(rowIndex);,0
if (row != null && row.getClock() >= clock) {,0
result.set(row);,0
return row;,0
},0
Get row splits of this row from the matrix cache first,0
responseCache.addSubResponse(rowSplit);,0
"If the row split does not exist in cache, get it from parameter server",0
Wait the final result,0
Put it to the matrix cache,0
"matrixStorage.addRow(rowIndex, row);",0
Just wait result,0
Split the param use matrix partitions,0
Send request to PSS,0
Split the matrix oplog according to the matrix partitions,0
"If need update clock, we should send requests to all partitions",0
Send request to PSS,0
Filter the rowIds which are fetching now,0
Send the rowIndex to rpc dispatcher and return immediately,0
"LOG.info(""get row split use time="" + (System.currentTimeMillis() - startTs));",0
"LOG.info(""start to request "" + requestId);",0
"LOG.info(""start to request "" + requestId);",0
Split param use matrix partitons,0
"If all sub-results are received, just remove request and result cache",0
"LOG.info(""request = "" + request + "", cache = "" + cache);",0
"LOG.info(""start to merge "" + cache + "" for request "" + request);",0
"LOG.info(""psf get merge use time = "" + (System.currentTimeMillis() - startTs));",0
Split this row according the matrix partitions,0
Set split context,0
Split this row according the matrix partitions,0
Set split context,0
long startTs = System.currentTimeMillis();,0
"LOG.error(""combine use time = "" + (System.currentTimeMillis() - startTs));",0
Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition,0
Generate dispatch items and add them to the corresponding queues,0
Filter the rowIds which are fetching now,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {",0
return key1.getStartCol() < key2.getStartCol() ? -1 : 1;,0
});,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {",0
return key1.getStartCol() < key2.getStartCol() ? -1 : 1;,0
});,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Put the row split to the cache(row index to row splits map),0
"If all splits of the row are received, means this row can be merged",0
TODO,1
TODO,1
/////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO,1
buf.writeDouble(0);,0
TODO,1
///////////////////////////////////////////////////////////////////////////////////////////////,0
Now we just support pipelined row splits merging for dense type row,0
Pre-fetching is disable default,0
matrix id to clock map,0
"task index, it must be unique for whole application",0
Deserialize data splits meta,0
Get workers,0
Send request to every ps,0
Wait the responses,0
Update clock cache,0
if(syncNum % 1024 == 0) {,0
},0
"Use simple flow, do not use any cache",0
Get row from cache.,0
"if row clock is satisfy ssp staleness limit, just return.",0
Get row from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC mode, just get from pss.",0
"For BSP/SSP, get rows from storage/cache first",0
Get from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC, just get rows from pss.",0
no more retries.,0
calculate sleep time and return.,0
parse the i-th sleep-time,0
parse the i-th number-of-retries,0
calculateSleepTime may overflow.,0
"A few common retry policies, with no delays.",0
Read matrix meta from meta file,0
Save partitions to files use fork-join,0
Write the ps matrix meta to the meta file,0
matrix.startServering();,0
return;,0
Read matrix meta from meta file,0
Load partitions from file use fork-join,0
Read matrix meta from meta file,0
Sort partitions,0
int size = rows.length;,0
int size = rows.length;,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
close is a local operation and should finish within milliseconds; timeout just to be safe,0
response will be null for one way messages.,0
maxFrameLength = 2G,0
lengthFieldOffset = 0,0
lengthFieldLength = 8,0
"lengthAdjustment = -8, i.e. exclude the 8 byte length itself",0
"initialBytesToStrip = 8, i.e. strip out the length field itself",0
"return Objects.toStringHelper(this).add(""serial"", serial).add(""listSize"", datas.size())",0
.toString();,0
indicates whether this connection's life cycle is managed,0
See if we already have a connection (common case),0
create a unique lock for this RS + protocol (if necessary),0
get the RS lock,0
do one more lookup in case we were stalled above,0
Only create isa when we need to.,0
definitely a cache miss. establish an RPC for,0
this RS,0
Throw what the RemoteException was carrying.,0
check,0
every,0
minutes,0
TODO,1
failoverHandler,0
"The number of times this invocation handler has ever been failed over,",0
before this method invocation attempt. Used to prevent concurrent,0
failed method invocations from triggering multiple failover attempts.,0
Make sure that concurrent failed method invocations,0
only cause a,0
single actual fail over.,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
"LOG.info(""method "" + method.getName() + ""construct request time = """,0
+ (System.currentTimeMillis() - beforeConstructTs));,0
get an instance of the method arg type,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
Message (hand written code usually has only a single,0
argument),0
log any RPC responses that are slower than the configured,0
warn,0
response time or larger than configured warning size,0
"when tagging, we let TooLarge trump TooSmall to keep",0
output simple,0
note that large responses will often also be slow.,0
provides a count of log-reported slow responses,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
unexpected,0
"in the protobuf methods, args[1] is the only significant argument",0
for JSON encoding,0
base information that is reported regardless of type of call,0
Disable Nagle's Algorithm since we don't want packets to wait,0
Configure the event pipeline factory.,0
Make a new connection.,0
Remove all pending requests (will be canceled after relinquishing,0
write lock).,0
Cancel any pending requests by sending errors to the callbacks:,0
Close the channel:,0
Close the connection:,0
Shut down all thread pools to exit.,0
"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());",0
See NettyServer.prepareResponse for where we write out the response.,0
"It writes the call.id (int), a boolean signifying any error (and if",0
"so the exception name/trace), and the response bytes",0
Read the call id.,0
"When the stream is closed, protobuf doesn't raise an EOFException,",0
"instead, it returns a null message object.",0
"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +",0
System.currentTimeMillis());,0
"It would be good widen this to just Throwable, but IOException is what we",0
allow now,0
not implemented,1
not implemented,1
"track what RpcEngine is used by a proxy class, for stopProxy()",0
cache of RpcEngines by protocol,0
return the RpcEngine configured to handle a protocol,0
We only handle the ConnectException.,0
This is the exception we can't handle.,0
check if timed out,0
wait for retry,0
IGNORE,0
return the RpcEngine that handles a proxy object,0
The default implementation works synchronously,0
punt: allocate a new buffer & copy into it,0
Parse cmd parameters,0
load hadoop configuration,0
load angel system configuration,0
load user configuration:,0
load user config file,0
load command line parameters,0
load user job resource files,0
load ml conf file for graph based algorithm,0
load user job jar if it exist,0
Expand the environment variable,0
Add default fs(local fs) for lib jars.,0
"LOG.info(System.getProperty(""user.dir""));",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
a simple hdfs copy function assume src path and dest path are in same hdfs,0
and FileSystem object has same schema,0
"LOG.warn(""interrupted while sleeping"", ie);",0
public static String getHostname() {,0
try {,0
"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();",0
} catch (UnknownHostException uhe) {,0
},0
"return new StringBuilder().append("""").append(uhe).toString();",0
},0
"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {",0
String hostname = getHostname();,0
String classname = clazz.getSimpleName();,0
"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new",0
"StringBuilder().append(""Starting "").append(classname).toString(), new",0
"StringBuilder().append(""  host = "").append(hostname).toString(), new",0
"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new",0
"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new",0
"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));",0
,0
"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {",0
public void run() {,0
"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +",0
"this.val$classname + "" at "" + this.val$hostname}));",0
},0
});,0
},0
"We we interrupted because we're meant to stop? If not, just",0
continue ignoring the interruption,0
Recalculate waitTime.,0
// Begin delegation to Thread,0
// End delegation to Thread,0
instance submitter class,0
Obtain filename from path,0
Split filename to prexif and suffix (extension),0
Check if the filename is okay,0
Prepare temporary file,0
Prepare buffer for data copying,0
Open and check input stream,0
Open output stream and copy data between source file in JAR and the temporary file,0
"If read/write fails, close streams safely before throwing an exception",0
"Finally, load the library",0
little endian load order,0
tail,0
fallthrough,0
fallthrough,0
finalization,0
fmix(h1);,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
throw new AngelException(e);,0
JobStateProto jobState = report.getJobState();,0
Check need load matrices,0
Used for java code to get a AngelClient instance,0
Used for python code to get a AngelClient instance,0
load user job resource files,0
the leaf level file should be readable by others,0
the subdirs in the path should have execute permissions for,0
others,0
2.get job id,0
Credentials credentials = new Credentials();,0
4.copy resource files to hdfs,0
5.write configuration to a xml file,0
6.create am container context,0
7.Submit to ResourceManager,0
8.get app master client,0
Create a number of filenames in the JobTracker's fs namespace,0
add all the command line files/ jars and archive,0
first copy them to jobtrackers filesystem,0
should not throw a uri exception,0
should not throw an uri excpetion,0
set the timestamps of the archives and files,0
set the public/private visibility of the archives and files,0
get DelegationToken for each cached file,0
check if we do not need to copy the files,0
is jt using the same file system.,0
just checking for uri strings... doing no dns lookups,0
to see if the filesystems are the same. This is not optimal.,0
but avoids name resolution.,0
this might have name collisions. copy will throw an exception,0
parse the original path to create new path,0
check for ports,0
Write job file to JobTracker's fs,0
Setup resource requirements,0
Setup LocalResources,0
Setup security tokens,0
Setup the command to run the AM,0
Add AM user command opts,0
Final command,0
Setup the CLASSPATH in environment,0
"i.e. add { Hadoop jars, job jar, CWD } to classpath.",0
Setup the environment variables for Admin first,0
"Setup the environment variables (LD_LIBRARY_PATH, etc)",0
Parse distributed cache,0
Setup ContainerLaunchContext for AM container,0
Set up the ApplicationSubmissionContext,0
private volatile PS2PSPusherImpl ps2PSPusher;,0
TODO,1
Add tokens to new user so that it may execute its task correctly.,0
TODO,1
to exit,0
TODO,1
TODO,1
ps2PSPusher.recover(ProtobufUtil.convert(needRecoverParts.get(i)));,0
context.getSnapshotManager().processRecovery();,0
Recover PS from snapshot or load path,0
First check snapshot,0
Check load path setting,0
TODO,1
if(ps2PSPusher != null) {,0
ps2PSPusher.start();,0
},0
public PS2PSPusherImpl getPs2PSPusher() {,0
return ps2PSPusher;,0
},0
"LOG.error(""send response of request "" + requestToString(clientId, seqId) + "" failed "");",0
"LOG.error(""send response of request failed, request seqId="" + seqId + "", channel="" + ch, ex);",0
Release the input buffer,0
Release the input buffer,0
"1. handle the rpc, get the response",0
Release the input buffer,0
2. Serialize the response,0
Send the serialized response,0
Exception happened,0
write seq id,0
Just serialize the head,0
Exception happened,0
Allocate result buffer,0
Exception happened,0
Just serialize the head,0
Exception happened,0
Reset the response and allocate buffer again,0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this location, only master ps can accept the update",0
Check the partition state again,0
Start to put the update to the slave pss,0
TODO,1
"context.getPS2PSPusher().put(request, in, partLoc);",0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this partition, if not, just return failed",0
Start to put the update to the slave pss,0
TODO,1
return ServerState.GENERAL;,0
Use Epoll for linux,0
public String uuid;,0
public void setChannelPool(GenericObjectPool<Channel> channelPool) {,0
this.channelPool = channelPool;,0
},0
private final ParameterServer psServer;,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO,1
///////////////////////////////////////////////////////////////////////////////////////////////,0
"////// network io method, for model transform",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
public ObjectIterator<Long2FloatMap.Entry> getIter() {,0
return ((LongFloatVector) row).getStorage().entryIterator();,0
},0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
public ObjectIterator<Long2LongMap.Entry> getIter() {,0
return ((LongLongVector) row).getStorage().entryIterator();,0
},0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO: dynamic add/delete row,1
private final List<PartitionKey> partitionKeys;,0
Use Epoll for linux,0
find the partition request context from cache,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
channelManager.removeChannelPool(loc);,0
Generate seq id,0
Create a RecoverPartRequest,0
Serialize the request,0
Change the seqId for the request,0
Serialize the request,0
"First check the state of the channels in the pool, if a channel is unused, just return",0
"If all channels are in use, create a new channel or wait",0
Create a new channel,0
"add the PSAgentContext,need fix",0
TODO:add more vector type,1
TODO : subDim set,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
write the max abs,0
---------------------------------------------------,0
---------------------------------------------------,0
---------------------------------------------------------------,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"dger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)",0
"sger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)",0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
add dense double matrix,0
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
get configuration from config file,0
set localDir with enviroment set by nm.,0
get master location,0
init task manager and start tasks,0
start heartbeat thread,0
taskManager.assignTaskIds(response.getTaskidsList());,0
todo,1
"if worker timeout, it may be knocked off.",0
"SUCCESS, do nothing",0
heartbeatFailedTime = 0;,0
private KEY currentKey;,0
will be created,0
TODO Auto-generated method stub,1
Bitmap bitmap = new Bitmap();,0
int max = indexArray[size - 1];,0
byte [] bitIndexArray = new byte[max / 8 + 1];,0
for(int i = 0; i < size; i++){,0
int bitIndex = indexArray[i] >> 3;,0
int bitOffset = indexArray[i] - (bitIndex << 3);,0
switch(bitOffset){,0
case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;,0
case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;,0
case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;,0
case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;,0
case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;,0
case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;,0
case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;,0
case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;,0
},0
},0
"true, false",0
//////////////////////////////,0
Application Configs,0
//////////////////////////////,0
//////////////////////////////,0
Master Configs,0
//////////////////////////////,0
//////////////////////////////,0
Worker Configs,0
//////////////////////////////,0
//////////////////////////////,0
Task Configs,0
//////////////////////////////,0
//////////////////////////////,0
ParameterServer Configs,0
//////////////////////////////,0
////////////////// IPC //////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
Configs used to ANGEL_PS_PSAGENT running mode future.,0
model parse,0
Mark whether use pyangel or not.,0
private Configuration conf;,0
"Configuration that should be used in python environment, there should only be one",0
configuration instance in each Angel context.,0
Use private access means jconf should not be changed or modified in this way.,0
Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:,0
Do nothing,0
To-DO: add other ways to justify different value types,0
"This is so ugly, must re-implement by more elegance way",1
"Create python path which include angel's jars, the python directory in ANGEL_HOME,",0
and other files submitted by user.,0
Launch python process,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Data format,0
Feature number of train data,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Ratio of validation,0
Learning rate,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"Set angel resource, #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Set training data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set predict data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);",0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
class number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set log path,0
Set trainning data path,0
Set save model path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set training data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log save path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set testing data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Data is classification,0
Model is classification,0
Train batch number per epoch.,0
loss delta,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Data is classification,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
"gradient descent first, then truncated",0
TODO: optimize int key indices,1
"System.out.println(""deserialize cols.length="" + nCols);",0
"System.out.print(""deserialize "");",0
"System.out.print(cols[c] + "" "");",0
System.out.println();,0
TODO Auto-generated method stub,1
"ground truth: positive, precision: positive",0
start row index for words,0
start row index for docs,0
doc ids,0
topic assignments,0
word to docs reverse index,0
count word,0
build word start index,0
build word to doc reverse idx,0
build dks,0
dks = new TraverseHashMap[n_docs];,0
for (int d = 0; d < n_docs; d++) {,0
if (K < Short.MAX_VALUE) {,0
if (docs.get(d).len < Byte.MAX_VALUE),0
dks[d] = new S2BTraverseMap(docs.get(d).len);,0
if (docs.get(d).len < Short.MAX_VALUE),0
"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));",0
else,0
"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));",0
} else {,0
"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));",0
},0
},0
build dks,0
allocate update maps,0
Skip if no token for this word,0
Check whether error when fetching word-topic,0
Build FTree for current word,0
current doc,0
old topic assignment,0
"Check if error happens. if this happen, it's probably that failures happen to servers.",0
We need to adjust the memory settings or network fetching parameters.,0
Update statistics if needed,0
Calculate psum and sample new topic,0
Update statistics if needed,0
Assign new topic,0
Skip if no token for this word,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
Write #rows,0
Write each row,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
dense,0
sparse,0
calculate columns,0
loss function,0
gradient and hessian,0
"categorical feature set, null: none, empty: all, else: partial",0
"node's end index in instancePos, instances in [start, end] belong to a tree node",0
initialize the phase,0
current tree and depth,0
create loss function,0
calculate grad info of each instance,0
"create data sketch, push candidate split value to PS",0
1. calculate candidate split value,0
categorical features,0
2. push local sketch to PS,0
the leader worker,0
merge categorical features,0
create updates,0
"pull the global sketch from PS, only called once by each worker",0
number of categorical feature,0
sample feature,0
push sampled feature set to the current tree,0
create new tree,0
"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,",0
calculate gradient,0
"1. create new tree, initialize tree nodes and node stats",0
"2. initialize feature set, if sampled, get from PS, otherwise use all the features",0
2.1. pull the sampled features of the current tree,0
this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();,0
"2.2. if use all the features, only called one",0
"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle",0
4. set root node to active,0
"5. reset instance position, set the root node's span",0
6. calculate gradient,0
1. decide nodes that should be calculated,0
2. decide calculated and subtracted tree nodes,0
3. calculate threads,0
wait until all threads finish,0
4. subtract threads,0
wait until all threads finish,0
5. send histograms to PS,0
6. update histogram cache,0
clock,0
find split,0
"1. find responsible tree node, using RR scheme",0
2. pull gradient histogram,0
2.1. get the name of this node's gradient histogram on PS,0
2.2. pull the histogram,0
2.3. find best split result of this tree node,0
2.3.1 using server split,0
"update the grad stats of the root node on PS, only called once by leader worker",0
update the grad stats of children node,0
update the left child,0
update the right child,0
"2.3.2 the updated split result (tree node/feature/value/gain) on PS,",0
"2.3.3 otherwise, the returned histogram contains the gradient info",0
"2.3.4 the updated split result (tree node/feature/value/gain) on PS,",0
2.3.5 reset this tree node's gradient histogram to 0,0
3. push split feature to PS,0
4. push split value to PS,0
5. push split gain to PS,0
6. set phase to AFTER_SPLIT,0
this.phase = GBDTPhase.AFTER_SPLIT;,0
clock,0
1. get split feature,0
2. get split value,0
3. get split gain,0
4. get node weight,0
5. split node,0
update local replica,0
create AfterSplit task,0
"2. check thread stats, if all threads finish, return",0
6. clock,0
"split the span of one node, reset the instance position",0
in case this worker has no instance on this node,0
set the span of left child,0
set the span of right child,0
"1. left to right, find the first instance that should be in the right child",0
"2. right to left, find the first instance that should be in the left child",0
3. swap two instances,0
4. find the cut pos,0
5. set the span of left child,0
6. set the span of right child,0
set tree node to active,0
set node to leaf,0
set node to inactive,0
finish current depth,0
finish current tree,0
set the tree phase,0
check if there is active node,0
check if finish all the tree,0
update node's grad stats on PS,0
"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split",0
the root node's stats is updated by leader worker,0
1. create the update,0
2. push the update to PS,0
1. update predictions of training data,0
2. update predictions of validation data,0
the leader task adds node prediction to flush list,0
1. name of this node's grad histogram on PS,0
2. build the grad histogram of this node,0
3. push the histograms to PS,0
4. reset thread stats to finished,0
5.1. set the children nodes of this node,0
5.2. set split info and grad stats to this node,0
5.2. create children nodes,0
"5.3. create node stats for children nodes, and add them to the tree",0
5.4. reset instance position,0
"5.5. add new active nodes if possible, inc depth, otherwise finish this tree",0
5.6. set children nodes to leaf nodes,0
5.7. set nid to leaf node,0
5.8. deactivate active node,0
"get feature type, 0:empty 1:all equal 2:real",0
"if not -1, sufficient space will be allocated at once",0
copy the highest levels,0
copy baseBuffer,0
merge two non-empty quantile sketches,0
left child <= split value; right child > split value,0
"the first: minimal, the last: maximal",0
categorical features,0
continuous features,0
left child <= split value; right child > split value,0
feature index used to split,0
feature value used to split,0
loss change after split this node,0
grad stats of the left child,0
grad stats of the right child,0
"LOG.info(""Constructor with fid = -1"");",0
fid = -1: no split currently,0
the minimal split value is the minimal value of feature,0
the splits do not include the maximal value of feature,0
"1. the average distance, (maxValue - minValue) / splitNum",0
2. calculate the candidate split value,0
1. new feature's histogram (grad + hess),0
size: sampled_featureNum * (2 * splitNum),0
"in other words, concatenate each feature's histogram",0
2. get the span of this node,0
------ 3. using sparse-aware method to build histogram ---,0
"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances",0
3.1. get the instance index,0
3.2. get the grad and hess of the instance,0
3.3. add to the sum,0
3.4. loop the non-zero entries,0
3.4.1. get feature value,0
3.4.2. current feature's position in the sampled feature set,0
"int fPos = findFidPlace(this.controller.fSet, fid);",0
3.4.3. find the position of feature value in a histogram,0
"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]",0
3.4.4. add the grad and hess to the corresponding bin,0
3.4.5. add the reverse to the bin that contains 0.0f,0
4. add the grad and hess sum to the zero bin of all features,0
find the best split result of the histogram of a tree node,0
1. calculate the gradStats of the root node,0
"1.1. update the grad stats of the root node on PS, only called once by leader worker",0
2. loop over features,0
2.1. get the ture feature id in the sampled feature set,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"update the grad stats of the root node on PS, only called once by leader worker",0
3. update the grad stats of children node,0
3.1. update the left child,0
3.2. update the right child,0
find the best split result of one feature,0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
split value = sketches[splitIdx],0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
partition number,0
cols of each partition,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
"loop all the possible split value, start from split[0], the first item is the minimal feature value",0
assert fvalue >= sketch[start] && fvalue <= sketch[end];,0
if (left > end) return end - start;,0
find the best split result of the histogram of a tree node,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
find the best split result of one feature,0
1. set the feature id,0
splitEntry.setFid(fid);,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]",0
the task use index to find fvalue,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting",0
"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking.",0
max and min of each feature,0
clear all the information,0
calculate the sum of gradient and hess,0
"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,",0
ridx),0
check if necessary information is ready,0
"same as add, reduce is used in All Reduce",0
"features used in this tree, if equals null, means use all the features without sampling",0
node in the tree,0
the gradient info of each instances,0
initialize nodes,0
gradient,0
second order gradient,0
TODO: only support dense double now,1
int sendStartCol = (int) row.getStartCol();,0
find the max abs,0
compress data,0
logistic loss for binary classification task.,0
"logistic loss, but predict un-transformed margin",0
check if label in range,0
return the default evaluation metric for the objective,0
"task type: classification, regression, or ranking",0
"quantile sketch, size = featureNum * splitNum",0
"gradient histograms, size = treeNodeNum * featureNum * splitNum",0
"active tree nodes, size = pow(2, treeDepth) -1",0
sampled features. size = treeNum * sampleRatio * featureNum,0
categorical feature. size = workerNum * cateFeatNum * splitNum,0
"split features, size = treeNum * treeNodeNum",0
"split values, size = treeNum * treeNodeNum",0
"split gains, size = treeNum * treeNodeNum",0
"node weights, size = treeNum * treeNodeNum",0
"node preds, size = treeNum * treeNodeNum",0
if using PS to perform split,0
step size for a tree,0
number of class,0
minimum loss change required for a split,0
maximum depth of a tree,0
number of features,0
number of nonzero,0
number of candidates split value,0
----- the rest parameters are less important ----,0
base instance weight,0
minimum amount of hessian(weight) allowed in a child,0
L2 regularization factor,0
L1 regularization factor,0
default direction choice,0
maximum delta update we can add in weight estimation,0
this parameter can be used to stabilize update,0
default=0 means no constraint on weight delta,0
whether we want to do subsample for row,0
whether to subsample columns for each tree,0
accuracy of sketch,0
accuracy of sketch,0
leaf vector size,0
option for parallelization,0
option to open cacheline optimization,0
whether to not print info during training.,0
maximum depth of the tree,0
number of features used for tree construction,0
"minimum loss change required for a split, otherwise stop split",0
----- the rest parameters are less important ----,0
default direction choice,0
whether we want to do sample data,0
whether to sample columns during tree construction,0
whether to use histogram for split,0
number of histogram units,0
whether to print info during training.,0
----- the rest parameters are obtained after training ----,0
total number of nodes,0
number of deleted nodes */,0
@maxIndex: this variable contains the max index of node/word,0
values[b + offset] = (random.nextFloat() - 0.5f) / dimension;,0
some params,0
max index for node/word,0
compute number of nodes for one row,0
check the length of dot values,0
merge dot values from all partitions,0
Skip-Gram model,0
Negative sampling,0
used to accumulate the updates for input vectors,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
Skip-Gram model,0
Negative sampling,0
used to accumulate the updates for input vectors,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
update output,0
Some params,0
compute number of nodes for one row,0
window size,0
Skip-Gram model,0
Accumulate the input vectors from context,0
Negative sampling,0
used to accumulate the updates for input vectors,0
window size,0
skip-gram model,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
update output,0
some params,0
batch sentences,0
max index for node/word,0
compute number of nodes for one row,0
check the length of dot values,0
merge dot values from all partitions,0
locates the input vectors to local array to prevent randomly access,0
on the large server row.,0
fill 0 for context vector,0
window size,0
Continuous bag-of-words Models,0
Accumulate the input vectors from context,0
Calculate the partial dot values,0
We should guarantee here that the sample would not equal the ``word``,0
used to accumulate the context input vectors,0
locates the input vector into local arrays to prevent randomly access for,0
the large server row.,0
window size,0
while true to prevent sampling out a positive target,0
how to prevent the randomly access to the output vectors??,0
accumulate gradients for the input vectors,0
update output vectors,0
update input,0
update output,0
Some params,0
compute number of nodes for one row,0
calculate bias,0
Do nothing.,0
current word,0
neu1 stores the average value of input vectors in the context (CBOW),0
Continuous Bag-of-Words Model,0
Accumulate the input vectors from context,0
negative sampling,0
Using the sigmoid value from the pre-computed table,0
accumulate for the hidden layer,0
update output layer,0
add the counter for target,0
update hidden layer,0
Update the input vector for each word in the context,0
add the counter to input,0
update input layers,0
update output layers,0
for (int a = 0; a < layers.length; a++) deltas[a] = layers[a] - deltas[a];,0
set basic configuration keys,0
use local deploy mode and dummy data spliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
import jdk.nashorn.internal.runtime.regexp.joni.Config;,0
"paras[1] = ""abc"";",0
"paras[2] = ""123"";",0
Add standard Hadoop classes,0
Feature number of train data,0
Total iteration number,0
Learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set input data path,0
Set save model path,0
Set actionType train,0
QSLRRunner runner = new QSLRRunner();,0
runner.train(conf);,0
"conf.setInt(AngelConf.ANGEL_STALENESS, -1);",0
Dataset,0
"Data format, libsvm or dummy",0
class number,0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set Softmax algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
Feature number of train data,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Ratio of validation,0
Learning rate,0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set DeepFM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
Model type,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set FM algorithm parameters,0
Set model class,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set WideAndDeep algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
"Set LDA parameters #V, #K",0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set LDA algorithm parameters,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set SVM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Load model meta,0
Convert model,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Convert model,0
load hadoop configuration,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model,0
load hadoop configuration,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
https://blog.csdn.net/cocoonyang/article/details/63068108,0
v1[i] = v1[i] + da * v2[i];,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
v1[i] = v1[i] + da * v2[i];,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
Default does nothing.,0
The app injection is optional,0
"renderText(""hello world"");",0
"user choose a workerGroupID from the workergroups page,",0
now we should change the AngelApp params and render the workergroup page;,0
"static final String WORKER_ID = ""worker.id"";",0
"div(""#logo"").",0
"img(""/static/hadoop-st.png"")._().",0
import org.apache.hadoop.yarn.webapp.view.FooterBlock;,0
import org.apache.hadoop.yarn.webapp.view.HeaderBlock;,0
JQueryUI.jsnotice(html);,0
import org.apache.hadoop.conf.Configuration;,0
import java.lang.reflect.Field;,0
get block locations from file system,0
create a list of all block and their locations,0
"if the file is not splitable, just create the one block with",0
full file length,0
each split can be a maximum of maxSize,0
if remainder is between max and 2*max - then,0
"instead of creating splits of size max, left-max we",0
create splits of size left/2 and left/2. This is,0
a heuristic to avoid creating really really small,0
splits.,0
add this block to the block --> node locations map,0
"For blocks that do not have host/rack information,",0
assign to default  rack.,0
add this block to the rack --> block map,0
Add this host to rackToNodes map,0
add this block to the node --> block map,0
"if the file system does not have any rack information, then",0
use dummy rack location.,0
The topology paths have the host name included as the last,0
component. Strip it.,0
get tokens for all the required FileSystems..,0
"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,",0
job.getConfiguration());,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
all the files in input set,0
it is allowed for maxSize to be 0. Disable smoothing load for such cases,0
process all nodes and create splits that are local to a node. Generate,0
"one split per node iteration, and walk over nodes multiple times to",0
distribute the splits across nodes.,0
Skip the node if it has previously been marked as completed.,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
Remove all blocks which may already have been assigned to other,0
splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
Remove entries from blocksInNode so that we don't walk these,0
again.,0
Done creating a single split for this node. Move on to the next,0
node so that splits are distributed across nodes.,0
This implies that the last few blocks (or all in case maxSize=0),0
were not part of a split. The node is complete.,0
if there were any blocks left over and their combined size is,0
"larger than minSplitNode, then combine them into one split.",0
Otherwise add them back to the unprocessed pool. It is likely,0
that they will be combined with other blocks from the,0
same rack later on.,0
This condition also kicks in when max split size is not set. All,0
blocks on a node will be grouped together into a single split.,0
haven't created any split on this machine. so its ok to add a,0
smaller one for parallelism. Otherwise group it in the rack for,0
balanced size create an input split and add it to the splits,0
array,0
Remove entries from blocksInNode so that we don't walk this again.,0
The node is done. This was the last set of blocks for this node.,0
Put the unplaced blocks back into the pool for later rack-allocation.,0
Node is done. All blocks were fit into node-local splits.,0
Check if node-local assignments are complete.,0
All nodes have been walked over and marked as completed or all blocks,0
have been assigned. The rest should be handled via rackLock assignment.,0
"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """,0
"+ completedNodes.size() + "", size left: "" + totalLength);",0
"if blocks in a rack are below the specified minimum size, then keep them",0
"in 'overflow'. After the processing of all racks is complete, these",0
overflow blocks will be combined into splits.,0
Process all racks over and over again until there is no more work to do.,0
Create one split for this rack before moving over to the next rack.,0
Come back to this rack after creating a single split for each of the,0
remaining racks.,0
"Process one rack location at a time, Combine all possible blocks that",0
reside on this rack as one split. (constrained by minimum and maximum,0
split size).,0
iterate over all racks,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"if we created a split, then just go to the next rack",0
"if there is a minimum size specified, then create a single split",0
"otherwise, store these blocks into overflow data structure",0
There were a few blocks in this rack that,0
remained to be processed. Keep them in 'overflow' block list.,0
These will be combined later.,0
Process all overflow blocks,0
"This might cause an exiting rack location to be re-added,",0
but it should be ok.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"Process any remaining blocks, if any.",0
create an input split,0
add this split to the list that is returned,0
long num = totLength / maxSize;,0
all blocks for all the files in input set,0
mapping from a rack name to the list of blocks it has,0
mapping from a block to the nodes on which it has replicas,0
mapping from a node to the list of blocks that it contains,0
populate all the blocks for all files,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
"Security framework already loaded the tokens into current UGI, just use",0
them,0
Now remove the AM->RM token so tasks don't have it,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init container allocator,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
Init Client manager,0
Init PS Client manager,0
init parameter server manager,0
recover task information if needed,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
Init model saver & loader,0
start a web service if use yarn deploy mode,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
Check Workers,0
Check PSS,0
Check Clients,0
Check PS Clients,0
parse parameter server counters,0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
refresh last heartbeat timestamp,0
send a state update event to the specific PSAttempt,0
Check is there save request,0
Check is there load request,0
check matrix metadata inconsistencies between master and parameter server.,0
"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix",0
"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix",0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
choose a unused port,0
start RPC server,0
remove this parameter server attempt from monitor set,0
remove this parameter server attempt from monitor set,0
"if worker attempt id is not in monitor set, we should shutdown it",0
find workergroup in worker manager,0
"if this worker group does not initialized, just return WORKERGROUP_NOTREADY",0
"if this worker group run over, just return WORKERGROUP_EXITED",0
"if this worker group is running now, return tasks, workers, data splits for it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update the clock for this matrix,0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update task iteration,0
"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);",0
"Calculate how many splits we need. As each task handles a separate split of data, so we want",0
the number of splits equal to the number of tasks,0
split data,0
dispatch the splits to workergroups,0
split data,0
dispatch the splits to workergroups,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
write meta data to a temporary file,0
rename the temporary file to final file,0
"if the file exists, read from file and deserialize it",0
write task meta,0
write ps meta,0
generate a temporary file,0
write task meta to the temporary file first,0
rename the temporary file to the final file,0
"if last final task file exist, remove it",0
find task meta file which has max timestamp,0
"if the file does not exist, just return null",0
read task meta from file and deserialize it,0
generate a temporary file,0
write ps meta to the temporary file first.,0
rename the temporary file to the final file,0
"if the old final file exist, just remove it",0
find ps meta file,0
"if ps meta file does not exist, just return null",0
read ps meta from file and deserialize it,0
"Path tmpPath = new Path(new Path(context.getConf().get(AngelConf.ANGEL_JOB_TMP_OUTPUT_PATH)),",0
String.valueOf(requestId));,0
Path tmpPath = HdfsUtil.toTmpPath(new Path(saveContext.getSavePath()));,0
saveContext.setTmpSavePath(tmpPath.toString());,0
Filter old epoch trigger first,0
Split the user request to sub-requests to pss,0
Init matrix files meta,0
Move output files,0
Write the meta file,0
Split the user request to sub-requests to pss,0
check whether psagent heartbeat timeout,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
LocalStorageToken is needed irrespective of whether security is enabled,0
or not.,0
"TokenCache.setJobToken(jobToken, taskCredentials);",0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Construct the actual Container,0
The null fields are per-container and will be constructed for each,0
container separately.,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
"a * in the classpath will only find a .jar, so we need to filter out",0
all .jars and add everything else,0
Propagate the system classpath when using the mini cluster,0
Add standard Hadoop classes,0
Add mr,0
Cache archives,0
Cache files,0
Sanity check,0
Add URI fragment or just the filename,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Final commmand,0
"if amTask is not null, we should clone task state from it",0
"if all parameter server complete commit, master can commit now",0
check whether parameter server heartbeat timeout,0
Transitions from the NEW state.,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event",0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event dispatched before",0
PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.COMMITTING state,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
reqeuest resource:send a resource request to the resource allocator,0
"Once the resource is applied, build and send the launch request to the container launcher",0
deallocator the resource of the ps attempt:send a resource deallocator request to the,0
resource allocator,0
set the launch time,0
add the ps attempt to the heartbeat timeout monitoring list,0
parse ps attempt location and put it to location manager,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt",0
or failed,0
remove ps attempt id from heartbeat timeout monitor list,0
release container:send a release request to container launcher,0
set the finish time only if launch time is set,0
private long scheduledTime;,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
Refresh ps location & matrix meta,0
start a new attempt for this ps,0
notify ps manager,0
"getContext().getLocationManager().setPsLocation(id, null);",0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
notify the event handler of state change,0
"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor",0
"if forcedState is set, just return",0
else get state from state machine,0
add this worker group to the success set,0
check if all worker group run over,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
calculate the actual number of worker groups and the total number of tasks based on the number of data split,0
"init all tasks , workers and worker groups and put them to the corresponding maps",0
just return the total task number now,0
TODO,1
check whether worker heartbeat timeout,0
"if workerAttempt is not null, we should clone task state from it",0
from NEW state,0
from SCHEDULED state,0
get data splits location for data locality,0
reqeuest resource:send a resource request to the resource allocator,0
"once the resource is applied, build and send the launch request to the container launcher",0
notify failed message to the worker,0
notify killed message to the worker,0
release the allocated container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
release the allocated container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
clean the container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
record the finish time,0
clean the container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listening list,0
record the finish time,0
"if the worker attempt launch successfully, add it to heartbeat timeout listening list",0
set worker attempt location,0
notify the register message to the worker,0
record the launch time,0
update worker attempt metrics,0
update tasks metrics,0
clean the container,0
notify the worker attempt run successfully message to the worker,0
record the finish time,0
init a worker attempt for the worker,0
schedule the worker attempt,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
"If we need Yarn to restart a new application master, we should not unregister from Yarn RM",0
register to Yarn RM,0
send heartbeat to Yarn RM every rmPollInterval milliseconds,0
"catch YarnRuntimeException, we should exit and need not retry",0
build heartbeat request,0
send heartbeat request to rm,0
"This can happen if the RM has been restarted. If it is in that state,",0
this application must clean itself up.,0
Setting NMTokens,0
assgin containers,0
"if some container is not assigned, release them",0
handle finish containers,0
dispatch container exit message to corresponding components,0
killed by framework,0
killed by framework,0
get application finish state,0
build application diagnostics,0
TODO:add a job history for angel,1
build unregister request,0
send unregister request to rm,0
Note this down for next interaction with ResourceManager,0
based on blacklisting comments above we can end up decrementing more,0
than requested. so guard for that.,0
send the updated resource request to RM,0
send 0 container count requests also to cancel previous requests,0
Update resource requests,0
try to assign to all nodes first to match node local,0
try to match all rack local,0
assign remaining,0
Update resource requests,0
send the container-assigned event to task attempt,0
build the start container request use launch context,0
send the start request to Yarn nm,0
send the message that the container starts successfully to the corresponding component,0
"after launching, send launched event to task attempt to move",0
it from ASSIGNED to RUNNING state,0
send the message that the container starts failed to the corresponding component,0
kill the remote container if already launched,0
start a thread pool to startup the container,0
See if we need up the pool size only if haven't reached the,0
maximum limit yet.,0
nodes where containers will run at *this* point of time. This is,0
*not* the cluster size and doesn't need to be.,0
"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the",0
later is just a buffer so we are not always increasing the,0
pool-size,0
the events from the queue are handled in parallel,0
using a thread pool,0
return if already stopped,0
shutdown any containers that might be left running,0
Add one sync matrix,0
addSyncMatrix();,0
"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {",0
"LOG.info(""ps id = "" + psEntry.getKey());",0
"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();",0
"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {",0
"LOG.info(""matrix id = "" + metaEntry.getKey());",0
"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());",0
},0
},0
get matrix ids in the parameter server report,0
get the matrices parameter server need to create and delete,0
"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix",0
"if a matrix exists on master but not exist on parameter server, this parameter server need build it.",0
Init control connection manager,0
Get ps locations from master and put them to the location cache.,0
Build and initialize rpc client to master,0
Get psagent id,0
Build PS control rpc client manager,0
Build local location,0
Initialize matrix meta information,0
Start all services,0
Stop all modules,0
Stop all modules,0
clock first,0
wait,0
Update generic resource counters,0
Updating resources specified in ResourceCalculatorProcessTree,0
Remove the CPU time consumed previously by JVM reuse,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Plus a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Update a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get values from pss use row/column indices,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"/ PSF get/update, use can implement their own psf",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get a row or a batch of rows,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
TODO:,1
Generate a flush request and put it to request queue,0
Generate a clock request and put it to request queue,0
Generate a merge request and put it to request queue,0
Generate a merge request and put it to request queue,0
"If the matrix op log cache does not exist for the matrix, create a new one for the",0
matrix,0
and add it to cache maps,0
Add the message to the tree map,0
"If there are flush / clock requests blocked, we need to put this merge request into",0
the waiting queue,0
Launch a merge worker to merge the update to matrix op log cache,0
Remove the message from the tree map,0
Wake up blocked flush/clock request,0
Add flush/clock request to listener list to waiting for all the existing,0
updates are merged,0
Wake up blocked flush/clock request,0
"If all updates are merged for this matrix, we need wake up flush/clock requests which are",0
blocked.,0
Get next merge message sequence id,0
Wake up listeners(flush/clock requests) that have little sequence id than current merge,0
position,0
Wake up blocked merge requests,0
Get minimal sequence id from listeners,0
"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we",0
should flush updates to local matrix storage,0
unused now,1
TODO:,1
Doing average or not,0
Filter un-important update,0
Split this row according the matrix partitions,0
Set split context,0
Remove the row from matrix,0
buf.writeDouble(0.0);,0
TODO,1
TODO: write map default value,1
buf.writeDouble(0);,0
TODO:,1
TODO:,1
TODO:,1
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"LOG.error(""channel "" + ctx.channel() + "" inactive"");",0
"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());",0
"LOG.error(""put response message queue failed "", e);",0
Use Epoll for linux,0
Update location table,0
Remove the server from failed list,0
Notify refresh success message to request dispatcher,0
Check PS exist or not,0
Check heartbeat timeout,0
Check PS restart or not,0
private final HashSet<ParameterServerId> refreshingServerSet;,0
Add it to failed rpc list,0
Add the server to gray server list,0
Add it to failed rpc list,0
Add the server to gray server list,0
Move from gray server list to failed server list,0
Handle the RPCS to this server,0
Submit the schedulable failed get RPCS,0
Submit new get RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"If the queue is empty, just return 0",0
"If request is not over limit, just submit it",0
Submit the schedulable failed get RPCS,0
Submit new put RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"LOG.info(""choose put server "" + psIds[index]);",0
Check all pending RPCS,0
Check get channel context,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
channelManager.printPools();,0
"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {",0
if(ts - entry.getValue() > requestTimeOut * 2)  {,0
"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())",0
"+ "" milliseconds, close all channels to it"");",0
closeChannels(entry.getKey());,0
"psLocToLastChannelTsMap.put(entry.getKey(), ts);",0
},0
},0
"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);",0
Remove all pending RPCS,0
Close all channel to this PS,0
private Channel getChannel(Location loc) throws Exception {,0
"return channelManager.getOrCreateChannel(new Location(loc.getIp(), loc.getPort() + 1));",0
},0
private GenericObjectPool<Channel> getChannelPool(Location loc) throws InterruptedException {,0
"return channelManager.getOrCreateChannelPool(new Location(loc.getIp(), loc.getPort() + 1), PSAgentContext",0
.get(),0
.getConf(),0
".getInt(AngelConf.ANGEL_WORKER_TASK_NUMBER,",0
AngelConf.DEFAULT_ANGEL_WORKER_TASK_NUMBER));,0
},0
"LOG.error(""send request "" + request + "" is interrupted"");",0
"LOG.error(""send request "" + request + "" failed, "", e);",0
Get server id and location for this request,0
"If location is null, means that the server is not ready",0
Get the channel for the location,0
Check if need get token first,0
Serialize the request,0
Send the request,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
request.getContext().setChannelPool(pool);,0
Allocate the bytebuf and serialize the request,0
find the partition request context from cache,0
"updateMatrixCache(request.getPartKey(), response.getPartition());",0
"updateMatrixCache(request.getPartKey(), response.getRowsSplit());",0
TODO,1
ServerRow rowSplit = PSAgentContext.get().getMatricesCache(),0
".getRowSplit(request.getPartKey().getMatrixId(), request.getPartKey(),",0
request.getRowIndex());,0
response.setRowSplit(rowSplit);,0
"updateMatrixCache(request.getPartKey(), response.getRowSplit());",0
"LOG.info(""user request id "" + request.getUserRequestId());",0
"LOG.info(""user request id "" + request.getUserRequestId());",0
TODO,1
"private void updateMatrixCache(PartitionKey partKey, ServerPartition partition) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, partition);",0
},0
"private void updateMatrixCache(PartitionKey partKey, ServerRow rowSplit) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowSplit);",0
},0
"private void updateMatrixCache(PartitionKey partKey, List<ServerRow> rowsSplit) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowsSplit);",0
},0
Get partitions for this row,0
Distinct get row requests,0
Need get from ps or storage/cache,0
"Switch to new request id, send a new request",0
First get this row from matrix storage,0
MatrixStorage matrixStorage =,0
PSAgentContext.get().getMatrixStorageManager().getMatrixStoage(matrixId);,0
TVector row = matrixStorage.getRow(rowIndex);,0
if (row != null && row.getClock() >= clock) {,0
result.set(row);,0
return row;,0
},0
Get row splits of this row from the matrix cache first,0
responseCache.addSubResponse(rowSplit);,0
"If the row split does not exist in cache, get it from parameter server",0
Wait the final result,0
Put it to the matrix cache,0
"matrixStorage.addRow(rowIndex, row);",0
Just wait result,0
Split the param use matrix partitions,0
Send request to PSS,0
Split the matrix oplog according to the matrix partitions,0
"If need update clock, we should send requests to all partitions",0
Send request to PSS,0
Filter the rowIds which are fetching now,0
Send the rowIndex to rpc dispatcher and return immediately,0
"LOG.info(""get row split use time="" + (System.currentTimeMillis() - startTs));",0
"LOG.info(""start to request "" + requestId);",0
"LOG.info(""start to request "" + requestId);",0
Split param use matrix partitons,0
"If all sub-results are received, just remove request and result cache",0
"LOG.info(""request = "" + request + "", cache = "" + cache);",0
"LOG.info(""start to merge "" + cache + "" for request "" + request);",0
"LOG.info(""psf get merge use time = "" + (System.currentTimeMillis() - startTs));",0
Split this row according the matrix partitions,0
Set split context,0
Split this row according the matrix partitions,0
Set split context,0
long startTs = System.currentTimeMillis();,0
"LOG.error(""combine use time = "" + (System.currentTimeMillis() - startTs));",0
Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition,0
Generate dispatch items and add them to the corresponding queues,0
Filter the rowIds which are fetching now,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {",0
return key1.getStartCol() < key2.getStartCol() ? -1 : 1;,0
});,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {",0
return key1.getStartCol() < key2.getStartCol() ? -1 : 1;,0
});,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Put the row split to the cache(row index to row splits map),0
"If all splits of the row are received, means this row can be merged",0
TODO,1
TODO,1
/////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO,1
buf.writeDouble(0);,0
TODO,1
///////////////////////////////////////////////////////////////////////////////////////////////,0
Now we just support pipelined row splits merging for dense type row,0
Pre-fetching is disable default,0
matrix id to clock map,0
"task index, it must be unique for whole application",0
Deserialize data splits meta,0
Get workers,0
Send request to every ps,0
Wait the responses,0
Update clock cache,0
if(syncNum % 1024 == 0) {,0
},0
"Use simple flow, do not use any cache",0
Get row from cache.,0
"if row clock is satisfy ssp staleness limit, just return.",0
Get row from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC mode, just get from pss.",0
"For BSP/SSP, get rows from storage/cache first",0
Get from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC, just get rows from pss.",0
no more retries.,0
calculate sleep time and return.,0
parse the i-th sleep-time,0
parse the i-th number-of-retries,0
calculateSleepTime may overflow.,0
"A few common retry policies, with no delays.",0
Read matrix meta from meta file,0
Save partitions to files use fork-join,0
Write the ps matrix meta to the meta file,0
matrix.startServering();,0
return;,0
Read matrix meta from meta file,0
Load partitions from file use fork-join,0
Read matrix meta from meta file,0
Sort partitions,0
int size = rows.length;,0
int size = rows.length;,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
close is a local operation and should finish within milliseconds; timeout just to be safe,0
response will be null for one way messages.,0
maxFrameLength = 2G,0
lengthFieldOffset = 0,0
lengthFieldLength = 8,0
"lengthAdjustment = -8, i.e. exclude the 8 byte length itself",0
"initialBytesToStrip = 8, i.e. strip out the length field itself",0
"return Objects.toStringHelper(this).add(""serial"", serial).add(""listSize"", datas.size())",0
.toString();,0
indicates whether this connection's life cycle is managed,0
See if we already have a connection (common case),0
create a unique lock for this RS + protocol (if necessary),0
get the RS lock,0
do one more lookup in case we were stalled above,0
Only create isa when we need to.,0
definitely a cache miss. establish an RPC for,0
this RS,0
Throw what the RemoteException was carrying.,0
check,0
every,0
minutes,0
TODO,1
failoverHandler,0
"The number of times this invocation handler has ever been failed over,",0
before this method invocation attempt. Used to prevent concurrent,0
failed method invocations from triggering multiple failover attempts.,0
Make sure that concurrent failed method invocations,0
only cause a,0
single actual fail over.,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
"LOG.info(""method "" + method.getName() + ""construct request time = """,0
+ (System.currentTimeMillis() - beforeConstructTs));,0
get an instance of the method arg type,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
Message (hand written code usually has only a single,0
argument),0
log any RPC responses that are slower than the configured,0
warn,0
response time or larger than configured warning size,0
"when tagging, we let TooLarge trump TooSmall to keep",0
output simple,0
note that large responses will often also be slow.,0
provides a count of log-reported slow responses,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
unexpected,0
"in the protobuf methods, args[1] is the only significant argument",0
for JSON encoding,0
base information that is reported regardless of type of call,0
Disable Nagle's Algorithm since we don't want packets to wait,0
Configure the event pipeline factory.,0
Make a new connection.,0
Remove all pending requests (will be canceled after relinquishing,0
write lock).,0
Cancel any pending requests by sending errors to the callbacks:,0
Close the channel:,0
Close the connection:,0
Shut down all thread pools to exit.,0
"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());",0
See NettyServer.prepareResponse for where we write out the response.,0
"It writes the call.id (int), a boolean signifying any error (and if",0
"so the exception name/trace), and the response bytes",0
Read the call id.,0
"When the stream is closed, protobuf doesn't raise an EOFException,",0
"instead, it returns a null message object.",0
"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +",0
System.currentTimeMillis());,0
"It would be good widen this to just Throwable, but IOException is what we",0
allow now,0
not implemented,1
not implemented,1
"track what RpcEngine is used by a proxy class, for stopProxy()",0
cache of RpcEngines by protocol,0
return the RpcEngine configured to handle a protocol,0
We only handle the ConnectException.,0
This is the exception we can't handle.,0
check if timed out,0
wait for retry,0
IGNORE,0
return the RpcEngine that handles a proxy object,0
The default implementation works synchronously,0
punt: allocate a new buffer & copy into it,0
Parse cmd parameters,0
load hadoop configuration,0
load angel system configuration,0
load user configuration:,0
load user config file,0
load command line parameters,0
load user job resource files,0
load ml conf file for graph based algorithm,0
load user job jar if it exist,0
Expand the environment variable,0
Add default fs(local fs) for lib jars.,0
"LOG.info(System.getProperty(""user.dir""));",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
a simple hdfs copy function assume src path and dest path are in same hdfs,0
and FileSystem object has same schema,0
"LOG.warn(""interrupted while sleeping"", ie);",0
public static String getHostname() {,0
try {,0
"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();",0
} catch (UnknownHostException uhe) {,0
},0
"return new StringBuilder().append("""").append(uhe).toString();",0
},0
"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {",0
String hostname = getHostname();,0
String classname = clazz.getSimpleName();,0
"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new",0
"StringBuilder().append(""Starting "").append(classname).toString(), new",0
"StringBuilder().append(""  host = "").append(hostname).toString(), new",0
"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new",0
"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new",0
"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));",0
,0
"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {",0
public void run() {,0
"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +",0
"this.val$classname + "" at "" + this.val$hostname}));",0
},0
});,0
},0
"We we interrupted because we're meant to stop? If not, just",0
continue ignoring the interruption,0
Recalculate waitTime.,0
// Begin delegation to Thread,0
// End delegation to Thread,0
instance submitter class,0
Obtain filename from path,0
Split filename to prexif and suffix (extension),0
Check if the filename is okay,0
Prepare temporary file,0
Prepare buffer for data copying,0
Open and check input stream,0
Open output stream and copy data between source file in JAR and the temporary file,0
"If read/write fails, close streams safely before throwing an exception",0
"Finally, load the library",0
little endian load order,0
tail,0
fallthrough,0
fallthrough,0
finalization,0
fmix(h1);,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
throw new AngelException(e);,0
JobStateProto jobState = report.getJobState();,0
Check need load matrices,0
Used for java code to get a AngelClient instance,0
Used for python code to get a AngelClient instance,0
load user job resource files,0
the leaf level file should be readable by others,0
the subdirs in the path should have execute permissions for,0
others,0
2.get job id,0
Credentials credentials = new Credentials();,0
4.copy resource files to hdfs,0
5.write configuration to a xml file,0
6.create am container context,0
7.Submit to ResourceManager,0
8.get app master client,0
Create a number of filenames in the JobTracker's fs namespace,0
add all the command line files/ jars and archive,0
first copy them to jobtrackers filesystem,0
should not throw a uri exception,0
should not throw an uri excpetion,0
set the timestamps of the archives and files,0
set the public/private visibility of the archives and files,0
get DelegationToken for each cached file,0
check if we do not need to copy the files,0
is jt using the same file system.,0
just checking for uri strings... doing no dns lookups,0
to see if the filesystems are the same. This is not optimal.,0
but avoids name resolution.,0
this might have name collisions. copy will throw an exception,0
parse the original path to create new path,0
check for ports,0
Write job file to JobTracker's fs,0
Setup resource requirements,0
Setup LocalResources,0
Setup security tokens,0
Setup the command to run the AM,0
Add AM user command opts,0
Final command,0
Setup the CLASSPATH in environment,0
"i.e. add { Hadoop jars, job jar, CWD } to classpath.",0
Setup the environment variables for Admin first,0
"Setup the environment variables (LD_LIBRARY_PATH, etc)",0
Parse distributed cache,0
Setup ContainerLaunchContext for AM container,0
Set up the ApplicationSubmissionContext,0
private volatile PS2PSPusherImpl ps2PSPusher;,0
TODO,1
Add tokens to new user so that it may execute its task correctly.,0
TODO,1
to exit,0
TODO,1
TODO,1
ps2PSPusher.recover(ProtobufUtil.convert(needRecoverParts.get(i)));,0
context.getSnapshotManager().processRecovery();,0
Recover PS from snapshot or load path,0
First check snapshot,0
Check load path setting,0
TODO,1
if(ps2PSPusher != null) {,0
ps2PSPusher.start();,0
},0
public PS2PSPusherImpl getPs2PSPusher() {,0
return ps2PSPusher;,0
},0
"LOG.error(""send response of request "" + requestToString(clientId, seqId) + "" failed "");",0
"LOG.error(""send response of request failed, request seqId="" + seqId + "", channel="" + ch, ex);",0
Release the input buffer,0
Release the input buffer,0
"1. handle the rpc, get the response",0
Release the input buffer,0
2. Serialize the response,0
Send the serialized response,0
Exception happened,0
write seq id,0
Just serialize the head,0
Exception happened,0
Allocate result buffer,0
Exception happened,0
Just serialize the head,0
Exception happened,0
Reset the response and allocate buffer again,0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this location, only master ps can accept the update",0
Check the partition state again,0
Start to put the update to the slave pss,0
TODO,1
"context.getPS2PSPusher().put(request, in, partLoc);",0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this partition, if not, just return failed",0
Start to put the update to the slave pss,0
TODO,1
return ServerState.GENERAL;,0
Use Epoll for linux,0
public String uuid;,0
public void setChannelPool(GenericObjectPool<Channel> channelPool) {,0
this.channelPool = channelPool;,0
},0
private final ParameterServer psServer;,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO,1
///////////////////////////////////////////////////////////////////////////////////////////////,0
"////// network io method, for model transform",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
public ObjectIterator<Long2FloatMap.Entry> getIter() {,0
return ((LongFloatVector) row).getStorage().entryIterator();,0
},0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
public ObjectIterator<Long2LongMap.Entry> getIter() {,0
return ((LongLongVector) row).getStorage().entryIterator();,0
},0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO: dynamic add/delete row,1
private final List<PartitionKey> partitionKeys;,0
Use Epoll for linux,0
find the partition request context from cache,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
channelManager.removeChannelPool(loc);,0
Generate seq id,0
Create a RecoverPartRequest,0
Serialize the request,0
Change the seqId for the request,0
Serialize the request,0
"First check the state of the channels in the pool, if a channel is unused, just return",0
"If all channels are in use, create a new channel or wait",0
Create a new channel,0
"add the PSAgentContext,need fix",0
TODO:add more vector type,1
TODO : subDim set,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
write the max abs,0
---------------------------------------------------,0
---------------------------------------------------,0
---------------------------------------------------------------,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
-----------------,0
-----------------,0
-----------------,0
-----------------,0
-----------------,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
"slower but memory efficient, for small vector only",0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"Transform mat1, generate a new matrix",0
Split the row indices of mat1Trans,0
Parallel execute use fork-join,0
"Transform mat1, generate a new matrix",0
Split the row indices of mat1Trans,0
Parallel execute use fork-join,0
"Get the sub-matrix of left matrix, split by row",0
"Get the sub-matrix of left matrix, split by row",0
"dger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)",0
"sger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)",0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
add dense double matrix,0
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
get configuration from config file,0
set localDir with enviroment set by nm.,0
get master location,0
init task manager and start tasks,0
start heartbeat thread,0
taskManager.assignTaskIds(response.getTaskidsList());,0
todo,1
"if worker timeout, it may be knocked off.",0
"SUCCESS, do nothing",0
heartbeatFailedTime = 0;,0
private KEY currentKey;,0
will be created,0
TODO Auto-generated method stub,1
Bitmap bitmap = new Bitmap();,0
int max = indexArray[size - 1];,0
byte [] bitIndexArray = new byte[max / 8 + 1];,0
for(int i = 0; i < size; i++){,0
int bitIndex = indexArray[i] >> 3;,0
int bitOffset = indexArray[i] - (bitIndex << 3);,0
switch(bitOffset){,0
case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;,0
case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;,0
case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;,0
case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;,0
case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;,0
case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;,0
case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;,0
case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;,0
},0
},0
"true, false",0
//////////////////////////////,0
Application Configs,0
//////////////////////////////,0
//////////////////////////////,0
Master Configs,0
//////////////////////////////,0
//////////////////////////////,0
Worker Configs,0
//////////////////////////////,0
//////////////////////////////,0
Task Configs,0
//////////////////////////////,0
//////////////////////////////,0
ParameterServer Configs,0
//////////////////////////////,0
////////////////// IPC //////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
Configs used to ANGEL_PS_PSAGENT running mode future.,0
model parse,0
Mark whether use pyangel or not.,0
private Configuration conf;,0
"Configuration that should be used in python environment, there should only be one",0
configuration instance in each Angel context.,0
Use private access means jconf should not be changed or modified in this way.,0
Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:,0
Do nothing,0
To-DO: add other ways to justify different value types,0
"This is so ugly, must re-implement by more elegance way",1
"Create python path which include angel's jars, the python directory in ANGEL_HOME,",0
and other files submitted by user.,0
Launch python process,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Data format,0
Feature number of train data,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Ratio of validation,0
Learning rate,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"Set angel resource, #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Set training data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set predict data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);",0
predictTest();,0
"Data format, libsvm or dummy",0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set data format,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
class number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set log path,0
Set trainning data path,0
Set save model path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set training data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log save path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set testing data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Data is classification,0
Model is classification,0
Train batch number per epoch.,0
loss delta,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Data is classification,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
TODO: optimize int key indices,1
"System.out.println(""deserialize cols.length="" + nCols);",0
"System.out.print(""deserialize "");",0
"System.out.print(cols[c] + "" "");",0
System.out.println();,0
TODO Auto-generated method stub,1
"LOG.info(""original float length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));",0
"LOG.info(""original double length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));",0
"LOG.info(""parsed float length: "" + length + Arrays.toString(arr));",0
"LOG.info(""parsed double length: "" + length + Arrays.toString(arr));",0
"ground truth: positive, precision: positive",0
start row index for words,0
start row index for docs,0
doc ids,0
topic assignments,0
word to docs reverse index,0
count word,0
build word start index,0
build word to doc reverse idx,0
build dks,0
dks = new TraverseHashMap[n_docs];,0
for (int d = 0; d < n_docs; d++) {,0
if (K < Short.MAX_VALUE) {,0
if (docs.get(d).len < Byte.MAX_VALUE),0
dks[d] = new S2BTraverseMap(docs.get(d).len);,0
if (docs.get(d).len < Short.MAX_VALUE),0
"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));",0
else,0
"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));",0
} else {,0
"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));",0
},0
},0
build dks,0
allocate update maps,0
Skip if no token for this word,0
Check whether error when fetching word-topic,0
Build FTree for current word,0
current doc,0
old topic assignment,0
"Check if error happens. if this happen, it's probably that failures happen to servers.",0
We need to adjust the memory settings or network fetching parameters.,0
Update statistics if needed,0
Calculate psum and sample new topic,0
Update statistics if needed,0
Assign new topic,0
Skip if no token for this word,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
Write #rows,0
Write each row,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
dense,0
sparse,0
calculate columns,0
loss function,0
gradient and hessian,0
"categorical feature set, null: none, empty: all, else: partial",0
"node's end index in instancePos, instances in [start, end] belong to a tree node",0
initialize the phase,0
current tree and depth,0
create loss function,0
calculate grad info of each instance,0
"create data sketch, push candidate split value to PS",0
1. calculate candidate split value,0
categorical features,0
2. push local sketch to PS,0
the leader worker,0
merge categorical features,0
create updates,0
"pull the global sketch from PS, only called once by each worker",0
number of categorical feature,0
sample feature,0
push sampled feature set to the current tree,0
create new tree,0
"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,",0
calculate gradient,0
"1. create new tree, initialize tree nodes and node stats",0
"2. initialize feature set, if sampled, get from PS, otherwise use all the features",0
2.1. pull the sampled features of the current tree,0
this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();,0
"2.2. if use all the features, only called one",0
"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle",0
4. set root node to active,0
"5. reset instance position, set the root node's span",0
6. calculate gradient,0
1. decide nodes that should be calculated,0
2. decide calculated and subtracted tree nodes,0
3. calculate threads,0
wait until all threads finish,0
4. subtract threads,0
wait until all threads finish,0
5. send histograms to PS,0
6. update histogram cache,0
clock,0
find split,0
"1. find responsible tree node, using RR scheme",0
2. pull gradient histogram,0
2.1. get the name of this node's gradient histogram on PS,0
2.2. pull the histogram,0
2.3. find best split result of this tree node,0
2.3.1 using server split,0
"update the grad stats of the root node on PS, only called once by leader worker",0
update the grad stats of children node,0
update the left child,0
update the right child,0
"2.3.2 the updated split result (tree node/feature/value/gain) on PS,",0
"2.3.3 otherwise, the returned histogram contains the gradient info",0
"2.3.4 the updated split result (tree node/feature/value/gain) on PS,",0
2.3.5 reset this tree node's gradient histogram to 0,0
3. push split feature to PS,0
4. push split value to PS,0
5. push split gain to PS,0
6. set phase to AFTER_SPLIT,0
this.phase = GBDTPhase.AFTER_SPLIT;,0
clock,0
1. get split feature,0
2. get split value,0
3. get split gain,0
4. get node weight,0
5. split node,0
update local replica,0
create AfterSplit task,0
"2. check thread stats, if all threads finish, return",0
6. clock,0
"split the span of one node, reset the instance position",0
in case this worker has no instance on this node,0
set the span of left child,0
set the span of right child,0
"1. left to right, find the first instance that should be in the right child",0
"2. right to left, find the first instance that should be in the left child",0
3. swap two instances,0
4. find the cut pos,0
5. set the span of left child,0
6. set the span of right child,0
set tree node to active,0
set node to leaf,0
set node to inactive,0
finish current depth,0
finish current tree,0
set the tree phase,0
check if there is active node,0
check if finish all the tree,0
update node's grad stats on PS,0
"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split",0
the root node's stats is updated by leader worker,0
1. create the update,0
2. push the update to PS,0
1. update predictions of training data,0
2. update predictions of validation data,0
the leader task adds node prediction to flush list,0
1. name of this node's grad histogram on PS,0
2. build the grad histogram of this node,0
3. push the histograms to PS,0
4. reset thread stats to finished,0
5.1. set the children nodes of this node,0
5.2. set split info and grad stats to this node,0
5.2. create children nodes,0
"5.3. create node stats for children nodes, and add them to the tree",0
5.4. reset instance position,0
"5.5. add new active nodes if possible, inc depth, otherwise finish this tree",0
5.6. set children nodes to leaf nodes,0
5.7. set nid to leaf node,0
5.8. deactivate active node,0
"get feature type, 0:empty 1:all equal 2:real",0
"if not -1, sufficient space will be allocated at once",0
copy the highest levels,0
copy baseBuffer,0
merge two non-empty quantile sketches,0
left child <= split value; right child > split value,0
"the first: minimal, the last: maximal",0
categorical features,0
continuous features,0
left child <= split value; right child > split value,0
feature index used to split,0
feature value used to split,0
loss change after split this node,0
grad stats of the left child,0
grad stats of the right child,0
"LOG.info(""Constructor with fid = -1"");",0
fid = -1: no split currently,0
the minimal split value is the minimal value of feature,0
the splits do not include the maximal value of feature,0
"1. the average distance, (maxValue - minValue) / splitNum",0
2. calculate the candidate split value,0
1. new feature's histogram (grad + hess),0
size: sampled_featureNum * (2 * splitNum),0
"in other words, concatenate each feature's histogram",0
2. get the span of this node,0
------ 3. using sparse-aware method to build histogram ---,0
"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances",0
3.1. get the instance index,0
3.2. get the grad and hess of the instance,0
3.3. add to the sum,0
3.4. loop the non-zero entries,0
3.4.1. get feature value,0
3.4.2. current feature's position in the sampled feature set,0
"int fPos = findFidPlace(this.controller.fSet, fid);",0
3.4.3. find the position of feature value in a histogram,0
"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]",0
3.4.4. add the grad and hess to the corresponding bin,0
3.4.5. add the reverse to the bin that contains 0.0f,0
4. add the grad and hess sum to the zero bin of all features,0
find the best split result of the histogram of a tree node,0
1. calculate the gradStats of the root node,0
"1.1. update the grad stats of the root node on PS, only called once by leader worker",0
2. loop over features,0
2.1. get the ture feature id in the sampled feature set,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"update the grad stats of the root node on PS, only called once by leader worker",0
3. update the grad stats of children node,0
3.1. update the left child,0
3.2. update the right child,0
find the best split result of one feature,0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
split value = sketches[splitIdx],0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
partition number,0
cols of each partition,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
"loop all the possible split value, start from split[0], the first item is the minimal feature value",0
assert fvalue >= sketch[start] && fvalue <= sketch[end];,0
if (left > end) return end - start;,0
find the best split result of the histogram of a tree node,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
find the best split result of one feature,0
1. set the feature id,0
splitEntry.setFid(fid);,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]",0
the task use index to find fvalue,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting",0
"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking.",0
max and min of each feature,0
clear all the information,0
calculate the sum of gradient and hess,0
"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,",0
ridx),0
check if necessary information is ready,0
"same as add, reduce is used in All Reduce",0
"features used in this tree, if equals null, means use all the features without sampling",0
node in the tree,0
the gradient info of each instances,0
initialize nodes,0
gradient,0
second order gradient,0
int sendStartCol = (int) row.getStartCol();,0
logistic loss for binary classification task.,0
"logistic loss, but predict un-transformed margin",0
check if label in range,0
return the default evaluation metric for the objective,0
"task type: classification, regression, or ranking",0
"quantile sketch, size = featureNum * splitNum",0
"gradient histograms, size = treeNodeNum * featureNum * splitNum",0
"active tree nodes, size = pow(2, treeDepth) -1",0
sampled features. size = treeNum * sampleRatio * featureNum,0
categorical feature. size = workerNum * cateFeatNum * splitNum,0
"split features, size = treeNum * treeNodeNum",0
"split values, size = treeNum * treeNodeNum",0
"split gains, size = treeNum * treeNodeNum",0
"node weights, size = treeNum * treeNodeNum",0
"node preds, size = treeNum * treeNodeNum",0
if using PS to perform split,0
step size for a tree,0
number of class,0
minimum loss change required for a split,0
maximum depth of a tree,0
number of features,0
number of nonzero,0
number of candidates split value,0
----- the rest parameters are less important ----,0
base instance weight,0
minimum amount of hessian(weight) allowed in a child,0
L2 regularization factor,0
L1 regularization factor,0
default direction choice,0
maximum delta update we can add in weight estimation,0
this parameter can be used to stabilize update,0
default=0 means no constraint on weight delta,0
whether we want to do subsample for row,0
whether to subsample columns for each tree,0
accuracy of sketch,0
accuracy of sketch,0
leaf vector size,0
option for parallelization,0
option to open cacheline optimization,0
whether to not print info during training.,0
maximum depth of the tree,0
number of features used for tree construction,0
"minimum loss change required for a split, otherwise stop split",0
----- the rest parameters are less important ----,0
default direction choice,0
whether we want to do sample data,0
whether to sample columns during tree construction,0
whether to use histogram for split,0
number of histogram units,0
whether to print info during training.,0
----- the rest parameters are obtained after training ----,0
total number of nodes,0
number of deleted nodes */,0
@maxIndex: this variable contains the max index of node/word,0
values[b + offset] = (random.nextFloat() - 0.5f) / dimension;,0
some params,0
max index for node/word,0
compute number of nodes for one row,0
check the length of dot values,0
merge dot values from all partitions,0
Skip-Gram model,0
Negative sampling,0
used to accumulate the updates for input vectors,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
Skip-Gram model,0
Negative sampling,0
used to accumulate the updates for input vectors,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
update output,0
Some params,0
compute number of nodes for one row,0
window size,0
Skip-Gram model,0
Accumulate the input vectors from context,0
Negative sampling,0
used to accumulate the updates for input vectors,0
window size,0
skip-gram model,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
update output,0
some params,0
batch sentences,0
max index for node/word,0
compute number of nodes for one row,0
check the length of dot values,0
merge dot values from all partitions,0
locates the input vectors to local array to prevent randomly access,0
on the large server row.,0
fill 0 for context vector,0
window size,0
Continuous bag-of-words Models,0
Accumulate the input vectors from context,0
Calculate the partial dot values,0
We should guarantee here that the sample would not equal the ``word``,0
used to accumulate the context input vectors,0
locates the input vector into local arrays to prevent randomly access for,0
the large server row.,0
window size,0
while true to prevent sampling out a positive target,0
how to prevent the randomly access to the output vectors??,0
accumulate gradients for the input vectors,0
update output vectors,0
update input,0
update output,0
Some params,0
compute number of nodes for one row,0
calculate bias,0
Do nothing.,0
current word,0
neu1 stores the average value of input vectors in the context (CBOW),0
Continuous Bag-of-Words Model,0
Accumulate the input vectors from context,0
negative sampling,0
Using the sigmoid value from the pre-computed table,0
accumulate for the hidden layer,0
update output layer,0
add the counter for target,0
update hidden layer,0
Update the input vector for each word in the context,0
add the counter to input,0
update input layers,0
update output layers,0
for (int a = 0; a < layers.length; a++) deltas[a] = layers[a] - deltas[a];,0
set basic configuration keys,0
use local deploy mode and dummy data spliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
import jdk.nashorn.internal.runtime.regexp.joni.Config;,0
"paras[1] = ""abc"";",0
"paras[2] = ""123"";",0
Add standard Hadoop classes,0
Feature number of train data,0
Total iteration number,0
Learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set input data path,0
Set save model path,0
Set actionType train,0
QSLRRunner runner = new QSLRRunner();,0
runner.train(conf);,0
"conf.setInt(AngelConf.ANGEL_STALENESS, -1);",0
Dataset,0
"Data format, libsvm or dummy",0
class number,0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set Softmax algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
Feature number of train data,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Ratio of validation,0
Learning rate,0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set DeepFM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
Model type,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set FM algorithm parameters,0
Set model class,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set WideAndDeep algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
"Set LDA parameters #V, #K",0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set LDA algorithm parameters,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set SVM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Load model meta,0
Convert model,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Convert model,0
load hadoop configuration,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model,0
load hadoop configuration,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
https://blog.csdn.net/cocoonyang/article/details/63068108,0
v1[i] = v1[i] + da * v2[i];,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
v1[i] = v1[i] + da * v2[i];,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
Default does nothing.,0
The app injection is optional,0
"renderText(""hello world"");",0
"user choose a workerGroupID from the workergroups page,",0
now we should change the AngelApp params and render the workergroup page;,0
"static final String WORKER_ID = ""worker.id"";",0
"div(""#logo"").",0
"img(""/static/hadoop-st.png"")._().",0
import org.apache.hadoop.yarn.webapp.view.FooterBlock;,0
import org.apache.hadoop.yarn.webapp.view.HeaderBlock;,0
JQueryUI.jsnotice(html);,0
import org.apache.hadoop.conf.Configuration;,0
import java.lang.reflect.Field;,0
get block locations from file system,0
create a list of all block and their locations,0
"if the file is not splitable, just create the one block with",0
full file length,0
each split can be a maximum of maxSize,0
if remainder is between max and 2*max - then,0
"instead of creating splits of size max, left-max we",0
create splits of size left/2 and left/2. This is,0
a heuristic to avoid creating really really small,0
splits.,0
add this block to the block --> node locations map,0
"For blocks that do not have host/rack information,",0
assign to default  rack.,0
add this block to the rack --> block map,0
Add this host to rackToNodes map,0
add this block to the node --> block map,0
"if the file system does not have any rack information, then",0
use dummy rack location.,0
The topology paths have the host name included as the last,0
component. Strip it.,0
get tokens for all the required FileSystems..,0
"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,",0
job.getConfiguration());,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
all the files in input set,0
it is allowed for maxSize to be 0. Disable smoothing load for such cases,0
process all nodes and create splits that are local to a node. Generate,0
"one split per node iteration, and walk over nodes multiple times to",0
distribute the splits across nodes.,0
Skip the node if it has previously been marked as completed.,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
Remove all blocks which may already have been assigned to other,0
splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
Remove entries from blocksInNode so that we don't walk these,0
again.,0
Done creating a single split for this node. Move on to the next,0
node so that splits are distributed across nodes.,0
This implies that the last few blocks (or all in case maxSize=0),0
were not part of a split. The node is complete.,0
if there were any blocks left over and their combined size is,0
"larger than minSplitNode, then combine them into one split.",0
Otherwise add them back to the unprocessed pool. It is likely,0
that they will be combined with other blocks from the,0
same rack later on.,0
This condition also kicks in when max split size is not set. All,0
blocks on a node will be grouped together into a single split.,0
haven't created any split on this machine. so its ok to add a,0
smaller one for parallelism. Otherwise group it in the rack for,0
balanced size create an input split and add it to the splits,0
array,0
Remove entries from blocksInNode so that we don't walk this again.,0
The node is done. This was the last set of blocks for this node.,0
Put the unplaced blocks back into the pool for later rack-allocation.,0
Node is done. All blocks were fit into node-local splits.,0
Check if node-local assignments are complete.,0
All nodes have been walked over and marked as completed or all blocks,0
have been assigned. The rest should be handled via rackLock assignment.,0
"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """,0
"+ completedNodes.size() + "", size left: "" + totalLength);",0
"if blocks in a rack are below the specified minimum size, then keep them",0
"in 'overflow'. After the processing of all racks is complete, these",0
overflow blocks will be combined into splits.,0
Process all racks over and over again until there is no more work to do.,0
Create one split for this rack before moving over to the next rack.,0
Come back to this rack after creating a single split for each of the,0
remaining racks.,0
"Process one rack location at a time, Combine all possible blocks that",0
reside on this rack as one split. (constrained by minimum and maximum,0
split size).,0
iterate over all racks,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"if we created a split, then just go to the next rack",0
"if there is a minimum size specified, then create a single split",0
"otherwise, store these blocks into overflow data structure",0
There were a few blocks in this rack that,0
remained to be processed. Keep them in 'overflow' block list.,0
These will be combined later.,0
Process all overflow blocks,0
"This might cause an exiting rack location to be re-added,",0
but it should be ok.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"Process any remaining blocks, if any.",0
create an input split,0
add this split to the list that is returned,0
long num = totLength / maxSize;,0
all blocks for all the files in input set,0
mapping from a rack name to the list of blocks it has,0
mapping from a block to the nodes on which it has replicas,0
mapping from a node to the list of blocks that it contains,0
populate all the blocks for all files,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
"Security framework already loaded the tokens into current UGI, just use",0
them,0
Now remove the AM->RM token so tasks don't have it,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init container allocator,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
Init Client manager,0
Init PS Client manager,0
init parameter server manager,0
recover task information if needed,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
Init model saver & loader,0
start a web service if use yarn deploy mode,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
Check Workers,0
Check PSS,0
Check Clients,0
Check PS Clients,0
parse parameter server counters,0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
refresh last heartbeat timestamp,0
send a state update event to the specific PSAttempt,0
Check is there save request,0
Check is there load request,0
check matrix metadata inconsistencies between master and parameter server.,0
"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix",0
"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix",0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
choose a unused port,0
start RPC server,0
remove this parameter server attempt from monitor set,0
remove this parameter server attempt from monitor set,0
"if worker attempt id is not in monitor set, we should shutdown it",0
find workergroup in worker manager,0
"if this worker group does not initialized, just return WORKERGROUP_NOTREADY",0
"if this worker group run over, just return WORKERGROUP_EXITED",0
"if this worker group is running now, return tasks, workers, data splits for it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update the clock for this matrix,0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update task iteration,0
"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);",0
"Calculate how many splits we need. As each task handles a separate split of data, so we want",0
the number of splits equal to the number of tasks,0
split data,0
dispatch the splits to workergroups,0
split data,0
dispatch the splits to workergroups,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
write meta data to a temporary file,0
rename the temporary file to final file,0
"if the file exists, read from file and deserialize it",0
write task meta,0
write ps meta,0
generate a temporary file,0
write task meta to the temporary file first,0
rename the temporary file to the final file,0
"if last final task file exist, remove it",0
find task meta file which has max timestamp,0
"if the file does not exist, just return null",0
read task meta from file and deserialize it,0
generate a temporary file,0
write ps meta to the temporary file first.,0
rename the temporary file to the final file,0
"if the old final file exist, just remove it",0
find ps meta file,0
"if ps meta file does not exist, just return null",0
read ps meta from file and deserialize it,0
"Path tmpPath = new Path(new Path(context.getConf().get(AngelConf.ANGEL_JOB_TMP_OUTPUT_PATH)),",0
String.valueOf(requestId));,0
Path tmpPath = HdfsUtil.toTmpPath(new Path(saveContext.getSavePath()));,0
saveContext.setTmpSavePath(tmpPath.toString());,0
Filter old epoch trigger first,0
Split the user request to sub-requests to pss,0
Init matrix files meta,0
Move output files,0
Write the meta file,0
Split the user request to sub-requests to pss,0
check whether psagent heartbeat timeout,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
LocalStorageToken is needed irrespective of whether security is enabled,0
or not.,0
"TokenCache.setJobToken(jobToken, taskCredentials);",0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Construct the actual Container,0
The null fields are per-container and will be constructed for each,0
container separately.,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
"a * in the classpath will only find a .jar, so we need to filter out",0
all .jars and add everything else,0
Propagate the system classpath when using the mini cluster,0
Add standard Hadoop classes,0
Add mr,0
Cache archives,0
Cache files,0
Sanity check,0
Add URI fragment or just the filename,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Final commmand,0
"if amTask is not null, we should clone task state from it",0
"if all parameter server complete commit, master can commit now",0
check whether parameter server heartbeat timeout,0
Transitions from the NEW state.,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event",0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event dispatched before",0
PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.COMMITTING state,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
reqeuest resource:send a resource request to the resource allocator,0
"Once the resource is applied, build and send the launch request to the container launcher",0
deallocator the resource of the ps attempt:send a resource deallocator request to the,0
resource allocator,0
set the launch time,0
add the ps attempt to the heartbeat timeout monitoring list,0
parse ps attempt location and put it to location manager,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt",0
or failed,0
remove ps attempt id from heartbeat timeout monitor list,0
release container:send a release request to container launcher,0
set the finish time only if launch time is set,0
private long scheduledTime;,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
Refresh ps location & matrix meta,0
start a new attempt for this ps,0
notify ps manager,0
"getContext().getLocationManager().setPsLocation(id, null);",0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
notify the event handler of state change,0
"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor",0
"if forcedState is set, just return",0
else get state from state machine,0
add this worker group to the success set,0
check if all worker group run over,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
calculate the actual number of worker groups and the total number of tasks based on the number of data split,0
"init all tasks , workers and worker groups and put them to the corresponding maps",0
just return the total task number now,0
TODO,1
check whether worker heartbeat timeout,0
"if workerAttempt is not null, we should clone task state from it",0
from NEW state,0
from SCHEDULED state,0
get data splits location for data locality,0
reqeuest resource:send a resource request to the resource allocator,0
"once the resource is applied, build and send the launch request to the container launcher",0
notify failed message to the worker,0
notify killed message to the worker,0
release the allocated container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
release the allocated container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
clean the container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
record the finish time,0
clean the container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listening list,0
record the finish time,0
"if the worker attempt launch successfully, add it to heartbeat timeout listening list",0
set worker attempt location,0
notify the register message to the worker,0
record the launch time,0
update worker attempt metrics,0
update tasks metrics,0
clean the container,0
notify the worker attempt run successfully message to the worker,0
record the finish time,0
init a worker attempt for the worker,0
schedule the worker attempt,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
"If we need Yarn to restart a new application master, we should not unregister from Yarn RM",0
register to Yarn RM,0
send heartbeat to Yarn RM every rmPollInterval milliseconds,0
"catch YarnRuntimeException, we should exit and need not retry",0
build heartbeat request,0
send heartbeat request to rm,0
"This can happen if the RM has been restarted. If it is in that state,",0
this application must clean itself up.,0
Setting NMTokens,0
assgin containers,0
"if some container is not assigned, release them",0
handle finish containers,0
dispatch container exit message to corresponding components,0
killed by framework,0
killed by framework,0
get application finish state,0
build application diagnostics,0
TODO:add a job history for angel,1
build unregister request,0
send unregister request to rm,0
Note this down for next interaction with ResourceManager,0
based on blacklisting comments above we can end up decrementing more,0
than requested. so guard for that.,0
send the updated resource request to RM,0
send 0 container count requests also to cancel previous requests,0
Update resource requests,0
try to assign to all nodes first to match node local,0
try to match all rack local,0
assign remaining,0
Update resource requests,0
send the container-assigned event to task attempt,0
build the start container request use launch context,0
send the start request to Yarn nm,0
send the message that the container starts successfully to the corresponding component,0
"after launching, send launched event to task attempt to move",0
it from ASSIGNED to RUNNING state,0
send the message that the container starts failed to the corresponding component,0
kill the remote container if already launched,0
start a thread pool to startup the container,0
See if we need up the pool size only if haven't reached the,0
maximum limit yet.,0
nodes where containers will run at *this* point of time. This is,0
*not* the cluster size and doesn't need to be.,0
"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the",0
later is just a buffer so we are not always increasing the,0
pool-size,0
the events from the queue are handled in parallel,0
using a thread pool,0
return if already stopped,0
shutdown any containers that might be left running,0
Add one sync matrix,0
addSyncMatrix();,0
"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {",0
"LOG.info(""ps id = "" + psEntry.getKey());",0
"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();",0
"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {",0
"LOG.info(""matrix id = "" + metaEntry.getKey());",0
"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());",0
},0
},0
get matrix ids in the parameter server report,0
get the matrices parameter server need to create and delete,0
"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix",0
"if a matrix exists on master but not exist on parameter server, this parameter server need build it.",0
Init control connection manager,0
Get ps locations from master and put them to the location cache.,0
Build and initialize rpc client to master,0
Get psagent id,0
Build PS control rpc client manager,0
Build local location,0
Initialize matrix meta information,0
Start all services,0
Stop all modules,0
Stop all modules,0
clock first,0
wait,0
Update generic resource counters,0
Updating resources specified in ResourceCalculatorProcessTree,0
Remove the CPU time consumed previously by JVM reuse,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Plus a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Update a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get values from pss use row/column indices,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"/ PSF get/update, use can implement their own psf",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get a row or a batch of rows,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
TODO:,1
Generate a flush request and put it to request queue,0
Generate a clock request and put it to request queue,0
Generate a merge request and put it to request queue,0
Generate a merge request and put it to request queue,0
"If the matrix op log cache does not exist for the matrix, create a new one for the",0
matrix,0
and add it to cache maps,0
Add the message to the tree map,0
"If there are flush / clock requests blocked, we need to put this merge request into",0
the waiting queue,0
Launch a merge worker to merge the update to matrix op log cache,0
Remove the message from the tree map,0
Wake up blocked flush/clock request,0
Add flush/clock request to listener list to waiting for all the existing,0
updates are merged,0
Wake up blocked flush/clock request,0
"If all updates are merged for this matrix, we need wake up flush/clock requests which are",0
blocked.,0
Get next merge message sequence id,0
Wake up listeners(flush/clock requests) that have little sequence id than current merge,0
position,0
Wake up blocked merge requests,0
Get minimal sequence id from listeners,0
"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we",0
should flush updates to local matrix storage,0
unused now,1
TODO:,1
Doing average or not,0
Filter un-important update,0
Split this row according the matrix partitions,0
Set split context,0
Remove the row from matrix,0
buf.writeDouble(0.0);,0
TODO,1
TODO: write map default value,1
buf.writeDouble(0);,0
TODO:,1
TODO:,1
TODO:,1
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"LOG.error(""channel "" + ctx.channel() + "" inactive"");",0
"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());",0
"LOG.error(""put response message queue failed "", e);",0
Use Epoll for linux,0
Update location table,0
Remove the server from failed list,0
Notify refresh success message to request dispatcher,0
Check PS exist or not,0
Check heartbeat timeout,0
Check PS restart or not,0
private final HashSet<ParameterServerId> refreshingServerSet;,0
Add it to failed rpc list,0
Add the server to gray server list,0
Add it to failed rpc list,0
Add the server to gray server list,0
Move from gray server list to failed server list,0
Handle the RPCS to this server,0
Submit the schedulable failed get RPCS,0
Submit new get RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"If the queue is empty, just return 0",0
"If request is not over limit, just submit it",0
Submit the schedulable failed get RPCS,0
Submit new put RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"LOG.info(""choose put server "" + psIds[index]);",0
Check all pending RPCS,0
Check get channel context,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
channelManager.printPools();,0
"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {",0
if(ts - entry.getValue() > requestTimeOut * 2)  {,0
"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())",0
"+ "" milliseconds, close all channels to it"");",0
closeChannels(entry.getKey());,0
"psLocToLastChannelTsMap.put(entry.getKey(), ts);",0
},0
},0
"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);",0
Remove all pending RPCS,0
Close all channel to this PS,0
private Channel getChannel(Location loc) throws Exception {,0
"return channelManager.getOrCreateChannel(new Location(loc.getIp(), loc.getPort() + 1));",0
},0
private GenericObjectPool<Channel> getChannelPool(Location loc) throws InterruptedException {,0
"return channelManager.getOrCreateChannelPool(new Location(loc.getIp(), loc.getPort() + 1), PSAgentContext",0
.get(),0
.getConf(),0
".getInt(AngelConf.ANGEL_WORKER_TASK_NUMBER,",0
AngelConf.DEFAULT_ANGEL_WORKER_TASK_NUMBER));,0
},0
"LOG.error(""send request "" + request + "" is interrupted"");",0
"LOG.error(""send request "" + request + "" failed, "", e);",0
Get server id and location for this request,0
"If location is null, means that the server is not ready",0
Get the channel for the location,0
Check if need get token first,0
Serialize the request,0
Send the request,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
request.getContext().setChannelPool(pool);,0
Allocate the bytebuf and serialize the request,0
find the partition request context from cache,0
"updateMatrixCache(request.getPartKey(), response.getPartition());",0
"updateMatrixCache(request.getPartKey(), response.getRowsSplit());",0
TODO,1
ServerRow rowSplit = PSAgentContext.get().getMatricesCache(),0
".getRowSplit(request.getPartKey().getMatrixId(), request.getPartKey(),",0
request.getRowIndex());,0
response.setRowSplit(rowSplit);,0
"updateMatrixCache(request.getPartKey(), response.getRowSplit());",0
"LOG.info(""user request id "" + request.getUserRequestId());",0
"LOG.info(""user request id "" + request.getUserRequestId());",0
TODO,1
"private void updateMatrixCache(PartitionKey partKey, ServerPartition partition) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, partition);",0
},0
"private void updateMatrixCache(PartitionKey partKey, ServerRow rowSplit) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowSplit);",0
},0
"private void updateMatrixCache(PartitionKey partKey, List<ServerRow> rowsSplit) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowsSplit);",0
},0
Get partitions for this row,0
Distinct get row requests,0
Need get from ps or storage/cache,0
"Switch to new request id, send a new request",0
First get this row from matrix storage,0
MatrixStorage matrixStorage =,0
PSAgentContext.get().getMatrixStorageManager().getMatrixStoage(matrixId);,0
TVector row = matrixStorage.getRow(rowIndex);,0
if (row != null && row.getClock() >= clock) {,0
result.set(row);,0
return row;,0
},0
Get row splits of this row from the matrix cache first,0
responseCache.addSubResponse(rowSplit);,0
"If the row split does not exist in cache, get it from parameter server",0
Wait the final result,0
Put it to the matrix cache,0
"matrixStorage.addRow(rowIndex, row);",0
Just wait result,0
Split the param use matrix partitions,0
Send request to PSS,0
Split the matrix oplog according to the matrix partitions,0
"If need update clock, we should send requests to all partitions",0
Send request to PSS,0
Filter the rowIds which are fetching now,0
Send the rowIndex to rpc dispatcher and return immediately,0
"LOG.info(""get row split use time="" + (System.currentTimeMillis() - startTs));",0
"LOG.info(""start to request "" + requestId);",0
"LOG.info(""start to request "" + requestId);",0
Split param use matrix partitons,0
"If all sub-results are received, just remove request and result cache",0
"LOG.info(""request = "" + request + "", cache = "" + cache);",0
"LOG.info(""start to merge "" + cache + "" for request "" + request);",0
"LOG.info(""psf get merge use time = "" + (System.currentTimeMillis() - startTs));",0
Split this row according the matrix partitions,0
Set split context,0
Split this row according the matrix partitions,0
Set split context,0
long startTs = System.currentTimeMillis();,0
"LOG.error(""combine use time = "" + (System.currentTimeMillis() - startTs));",0
Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition,0
Generate dispatch items and add them to the corresponding queues,0
Filter the rowIds which are fetching now,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {",0
return key1.getStartCol() < key2.getStartCol() ? -1 : 1;,0
});,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {",0
return key1.getStartCol() < key2.getStartCol() ? -1 : 1;,0
});,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Put the row split to the cache(row index to row splits map),0
"If all splits of the row are received, means this row can be merged",0
TODO,1
TODO,1
/////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO,1
buf.writeDouble(0);,0
TODO,1
///////////////////////////////////////////////////////////////////////////////////////////////,0
Now we just support pipelined row splits merging for dense type row,0
Pre-fetching is disable default,0
matrix id to clock map,0
"task index, it must be unique for whole application",0
Deserialize data splits meta,0
Get workers,0
Send request to every ps,0
Wait the responses,0
Update clock cache,0
if(syncNum % 1024 == 0) {,0
},0
"Use simple flow, do not use any cache",0
Get row from cache.,0
"if row clock is satisfy ssp staleness limit, just return.",0
Get row from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC mode, just get from pss.",0
"For BSP/SSP, get rows from storage/cache first",0
Get from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC, just get rows from pss.",0
no more retries.,0
calculate sleep time and return.,0
parse the i-th sleep-time,0
parse the i-th number-of-retries,0
calculateSleepTime may overflow.,0
"A few common retry policies, with no delays.",0
Read matrix meta from meta file,0
Save partitions to files use fork-join,0
Write the ps matrix meta to the meta file,0
matrix.startServering();,0
return;,0
Read matrix meta from meta file,0
Load partitions from file use fork-join,0
Read matrix meta from meta file,0
Sort partitions,0
int size = rows.length;,0
int size = rows.length;,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
close is a local operation and should finish within milliseconds; timeout just to be safe,0
response will be null for one way messages.,0
maxFrameLength = 2G,0
lengthFieldOffset = 0,0
lengthFieldLength = 8,0
"lengthAdjustment = -8, i.e. exclude the 8 byte length itself",0
"initialBytesToStrip = 8, i.e. strip out the length field itself",0
"return Objects.toStringHelper(this).add(""serial"", serial).add(""listSize"", datas.size())",0
.toString();,0
indicates whether this connection's life cycle is managed,0
See if we already have a connection (common case),0
create a unique lock for this RS + protocol (if necessary),0
get the RS lock,0
do one more lookup in case we were stalled above,0
Only create isa when we need to.,0
definitely a cache miss. establish an RPC for,0
this RS,0
Throw what the RemoteException was carrying.,0
check,0
every,0
minutes,0
TODO,1
failoverHandler,0
"The number of times this invocation handler has ever been failed over,",0
before this method invocation attempt. Used to prevent concurrent,0
failed method invocations from triggering multiple failover attempts.,0
Make sure that concurrent failed method invocations,0
only cause a,0
single actual fail over.,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
"LOG.info(""method "" + method.getName() + ""construct request time = """,0
+ (System.currentTimeMillis() - beforeConstructTs));,0
get an instance of the method arg type,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
Message (hand written code usually has only a single,0
argument),0
log any RPC responses that are slower than the configured,0
warn,0
response time or larger than configured warning size,0
"when tagging, we let TooLarge trump TooSmall to keep",0
output simple,0
note that large responses will often also be slow.,0
provides a count of log-reported slow responses,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
unexpected,0
"in the protobuf methods, args[1] is the only significant argument",0
for JSON encoding,0
base information that is reported regardless of type of call,0
Disable Nagle's Algorithm since we don't want packets to wait,0
Configure the event pipeline factory.,0
Make a new connection.,0
Remove all pending requests (will be canceled after relinquishing,0
write lock).,0
Cancel any pending requests by sending errors to the callbacks:,0
Close the channel:,0
Close the connection:,0
Shut down all thread pools to exit.,0
"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());",0
See NettyServer.prepareResponse for where we write out the response.,0
"It writes the call.id (int), a boolean signifying any error (and if",0
"so the exception name/trace), and the response bytes",0
Read the call id.,0
"When the stream is closed, protobuf doesn't raise an EOFException,",0
"instead, it returns a null message object.",0
"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +",0
System.currentTimeMillis());,0
"It would be good widen this to just Throwable, but IOException is what we",0
allow now,0
not implemented,1
not implemented,1
"track what RpcEngine is used by a proxy class, for stopProxy()",0
cache of RpcEngines by protocol,0
return the RpcEngine configured to handle a protocol,0
We only handle the ConnectException.,0
This is the exception we can't handle.,0
check if timed out,0
wait for retry,0
IGNORE,0
return the RpcEngine that handles a proxy object,0
The default implementation works synchronously,0
punt: allocate a new buffer & copy into it,0
Parse cmd parameters,0
load hadoop configuration,0
load angel system configuration,0
load user configuration:,0
load user config file,0
load command line parameters,0
load user job resource files,0
load ml conf file for graph based algorithm,0
load user job jar if it exist,0
Expand the environment variable,0
Add default fs(local fs) for lib jars.,0
"LOG.info(System.getProperty(""user.dir""));",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
a simple hdfs copy function assume src path and dest path are in same hdfs,0
and FileSystem object has same schema,0
"LOG.warn(""interrupted while sleeping"", ie);",0
public static String getHostname() {,0
try {,0
"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();",0
} catch (UnknownHostException uhe) {,0
},0
"return new StringBuilder().append("""").append(uhe).toString();",0
},0
"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {",0
String hostname = getHostname();,0
String classname = clazz.getSimpleName();,0
"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new",0
"StringBuilder().append(""Starting "").append(classname).toString(), new",0
"StringBuilder().append(""  host = "").append(hostname).toString(), new",0
"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new",0
"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new",0
"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));",0
,0
"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {",0
public void run() {,0
"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +",0
"this.val$classname + "" at "" + this.val$hostname}));",0
},0
});,0
},0
"We we interrupted because we're meant to stop? If not, just",0
continue ignoring the interruption,0
Recalculate waitTime.,0
// Begin delegation to Thread,0
// End delegation to Thread,0
instance submitter class,0
Obtain filename from path,0
Split filename to prexif and suffix (extension),0
Check if the filename is okay,0
Prepare temporary file,0
Prepare buffer for data copying,0
Open and check input stream,0
Open output stream and copy data between source file in JAR and the temporary file,0
"If read/write fails, close streams safely before throwing an exception",0
"Finally, load the library",0
little endian load order,0
tail,0
fallthrough,0
fallthrough,0
finalization,0
fmix(h1);,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
throw new AngelException(e);,0
JobStateProto jobState = report.getJobState();,0
Check need load matrices,0
Used for java code to get a AngelClient instance,0
Used for python code to get a AngelClient instance,0
load user job resource files,0
the leaf level file should be readable by others,0
the subdirs in the path should have execute permissions for,0
others,0
2.get job id,0
Credentials credentials = new Credentials();,0
4.copy resource files to hdfs,0
5.write configuration to a xml file,0
6.create am container context,0
7.Submit to ResourceManager,0
8.get app master client,0
Create a number of filenames in the JobTracker's fs namespace,0
add all the command line files/ jars and archive,0
first copy them to jobtrackers filesystem,0
should not throw a uri exception,0
should not throw an uri excpetion,0
set the timestamps of the archives and files,0
set the public/private visibility of the archives and files,0
get DelegationToken for each cached file,0
check if we do not need to copy the files,0
is jt using the same file system.,0
just checking for uri strings... doing no dns lookups,0
to see if the filesystems are the same. This is not optimal.,0
but avoids name resolution.,0
this might have name collisions. copy will throw an exception,0
parse the original path to create new path,0
check for ports,0
Write job file to JobTracker's fs,0
Setup resource requirements,0
Setup LocalResources,0
Setup security tokens,0
Setup the command to run the AM,0
Add AM user command opts,0
Final command,0
Setup the CLASSPATH in environment,0
"i.e. add { Hadoop jars, job jar, CWD } to classpath.",0
Setup the environment variables for Admin first,0
"Setup the environment variables (LD_LIBRARY_PATH, etc)",0
Parse distributed cache,0
Setup ContainerLaunchContext for AM container,0
Set up the ApplicationSubmissionContext,0
private volatile PS2PSPusherImpl ps2PSPusher;,0
TODO,1
Add tokens to new user so that it may execute its task correctly.,0
TODO,1
to exit,0
TODO,1
TODO,1
ps2PSPusher.recover(ProtobufUtil.convert(needRecoverParts.get(i)));,0
context.getSnapshotManager().processRecovery();,0
Recover PS from snapshot or load path,0
First check snapshot,0
Check load path setting,0
TODO,1
if(ps2PSPusher != null) {,0
ps2PSPusher.start();,0
},0
public PS2PSPusherImpl getPs2PSPusher() {,0
return ps2PSPusher;,0
},0
"LOG.error(""send response of request "" + requestToString(clientId, seqId) + "" failed "");",0
"LOG.error(""send response of request failed, request seqId="" + seqId + "", channel="" + ch, ex);",0
Release the input buffer,0
Release the input buffer,0
"1. handle the rpc, get the response",0
Release the input buffer,0
2. Serialize the response,0
Send the serialized response,0
Exception happened,0
write seq id,0
Just serialize the head,0
Exception happened,0
Allocate result buffer,0
Exception happened,0
Just serialize the head,0
Exception happened,0
Reset the response and allocate buffer again,0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this location, only master ps can accept the update",0
Check the partition state again,0
Start to put the update to the slave pss,0
TODO,1
"context.getPS2PSPusher().put(request, in, partLoc);",0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this partition, if not, just return failed",0
Start to put the update to the slave pss,0
TODO,1
return ServerState.GENERAL;,0
Use Epoll for linux,0
public String uuid;,0
public void setChannelPool(GenericObjectPool<Channel> channelPool) {,0
this.channelPool = channelPool;,0
},0
private final ParameterServer psServer;,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO,1
///////////////////////////////////////////////////////////////////////////////////////////////,0
"////// network io method, for model transform",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
public ObjectIterator<Long2FloatMap.Entry> getIter() {,0
return ((LongFloatVector) row).getStorage().entryIterator();,0
},0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
public ObjectIterator<Long2LongMap.Entry> getIter() {,0
return ((LongLongVector) row).getStorage().entryIterator();,0
},0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO: dynamic add/delete row,1
private final List<PartitionKey> partitionKeys;,0
Use Epoll for linux,0
find the partition request context from cache,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
channelManager.removeChannelPool(loc);,0
Generate seq id,0
Create a RecoverPartRequest,0
Serialize the request,0
Change the seqId for the request,0
Serialize the request,0
"First check the state of the channels in the pool, if a channel is unused, just return",0
"If all channels are in use, create a new channel or wait",0
Create a new channel,0
"add the PSAgentContext,need fix",0
TODO:add more vector type,1
TODO : subDim set,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
write the max abs,0
---------------------------------------------------,0
---------------------------------------------------,0
---------------------------------------------------------------,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
-----------------,0
-----------------,0
-----------------,0
-----------------,0
-----------------,0
KeepStorage is guaranteed,0
"ignore the isInplace option, since v2 is dense",0
"the value in old storage can be changed safe, so switch a storage",0
"but user required keep storage, we can prevent rehash",0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
KeepStorage is guaranteed,0
we gauss dense storage is more efficient,0
v1Size < v2Size * Constant.sparseThreshold,0
KeepStorage is guaranteed,0
"ignore the isInplace option, since v2 is dense",0
"the value in old storage can be changed safe, so switch a storage",0
"but user required keep storage, we can prevent rehash",0
we gauss dense storage is more efficient,0
prevent rehash,0
KeepStorage is guaranteed,0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
dense preferred,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
sorted preferred,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"Transform mat1, generate a new matrix",0
Split the row indices of mat1Trans,0
Parallel execute use fork-join,0
"Get the sub-matrix of left matrix, split by row",0
"Transform mat1, generate a new matrix",0
Split the row indices of mat1Trans,0
Parallel execute use fork-join,0
"Get the sub-matrix of left matrix, split by row",0
"mat1 trans true, mat trans true",0
"mat1 trans true, mat trans false",0
"mat1 trans false, mat trans true, important",0
"mat1 trans false, mat trans false",0
"mat1 trans true, mat trans true",0
"mat1 trans true, mat trans false",0
"mat1 trans false, mat trans true, important",0
"mat1 trans false, mat trans false",0
"dger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)",0
"sger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)",0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
add dense double matrix,0
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
get configuration from config file,0
set localDir with enviroment set by nm.,0
get master location,0
init task manager and start tasks,0
start heartbeat thread,0
taskManager.assignTaskIds(response.getTaskidsList());,0
todo,1
"if worker timeout, it may be knocked off.",0
"SUCCESS, do nothing",0
heartbeatFailedTime = 0;,0
private KEY currentKey;,0
will be created,0
TODO Auto-generated method stub,1
Bitmap bitmap = new Bitmap();,0
int max = indexArray[size - 1];,0
byte [] bitIndexArray = new byte[max / 8 + 1];,0
for(int i = 0; i < size; i++){,0
int bitIndex = indexArray[i] >> 3;,0
int bitOffset = indexArray[i] - (bitIndex << 3);,0
switch(bitOffset){,0
case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;,0
case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;,0
case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;,0
case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;,0
case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;,0
case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;,0
case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;,0
case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;,0
},0
},0
"true, false",0
//////////////////////////////,0
Application Configs,0
//////////////////////////////,0
//////////////////////////////,0
Master Configs,0
//////////////////////////////,0
//////////////////////////////,0
Worker Configs,0
//////////////////////////////,0
//////////////////////////////,0
Task Configs,0
//////////////////////////////,0
//////////////////////////////,0
ParameterServer Configs,0
//////////////////////////////,0
////////////////// IPC //////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
Configs used to ANGEL_PS_PSAGENT running mode future.,0
model parse,0
Mark whether use pyangel or not.,0
private Configuration conf;,0
"Configuration that should be used in python environment, there should only be one",0
configuration instance in each Angel context.,0
Use private access means jconf should not be changed or modified in this way.,0
Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:,0
Do nothing,0
To-DO: add other ways to justify different value types,0
"This is so ugly, must re-implement by more elegance way",1
"Create python path which include angel's jars, the python directory in ANGEL_HOME,",0
and other files submitted by user.,0
Launch python process,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Data format,0
Feature number of train data,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Ratio of validation,0
Learning rate,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"Set angel resource, #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Set training data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set predict data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model type,0
String modelType = String.valueOf(RowType.T_FLOAT_DENSE);,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
"conf.set(MLConf.ML_MODEL_TYPE(), modelType);",0
"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);",0
predictTest();,0
"Data format, libsvm or dummy",0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set data format,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
class number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set log path,0
Set trainning data path,0
Set save model path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set training data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log save path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set testing data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Data is classification,0
Model is classification,0
Train batch number per epoch.,0
loss delta,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Data is classification,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
TODO: optimize int key indices,1
"System.out.println(""deserialize cols.length="" + nCols);",0
"System.out.print(""deserialize "");",0
"System.out.print(cols[c] + "" "");",0
System.out.println();,0
TODO Auto-generated method stub,1
"LOG.info(""original float length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));",0
"LOG.info(""original double length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));",0
"LOG.info(""parsed float length: "" + length + Arrays.toString(arr));",0
"LOG.info(""parsed double length: "" + length + Arrays.toString(arr));",0
"ground truth: positive, precision: positive",0
start row index for words,0
start row index for docs,0
doc ids,0
topic assignments,0
word to docs reverse index,0
count word,0
build word start index,0
build word to doc reverse idx,0
build dks,0
dks = new TraverseHashMap[n_docs];,0
for (int d = 0; d < n_docs; d++) {,0
if (K < Short.MAX_VALUE) {,0
if (docs.get(d).len < Byte.MAX_VALUE),0
dks[d] = new S2BTraverseMap(docs.get(d).len);,0
if (docs.get(d).len < Short.MAX_VALUE),0
"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));",0
else,0
"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));",0
} else {,0
"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));",0
},0
},0
build dks,0
allocate update maps,0
Skip if no token for this word,0
Check whether error when fetching word-topic,0
Build FTree for current word,0
current doc,0
old topic assignment,0
"Check if error happens. if this happen, it's probably that failures happen to servers.",0
We need to adjust the memory settings or network fetching parameters.,0
Update statistics if needed,0
Calculate psum and sample new topic,0
Update statistics if needed,0
Assign new topic,0
Skip if no token for this word,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
Write #rows,0
Write each row,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
dense,0
sparse,0
calculate columns,0
loss function,0
gradient and hessian,0
"categorical feature set, null: none, empty: all, else: partial",0
"node's end index in instancePos, instances in [start, end] belong to a tree node",0
initialize the phase,0
current tree and depth,0
create loss function,0
calculate grad info of each instance,0
"create data sketch, push candidate split value to PS",0
1. calculate candidate split value,0
categorical features,0
2. push local sketch to PS,0
the leader worker,0
merge categorical features,0
create updates,0
"pull the global sketch from PS, only called once by each worker",0
number of categorical feature,0
sample feature,0
push sampled feature set to the current tree,0
create new tree,0
"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,",0
calculate gradient,0
"1. create new tree, initialize tree nodes and node stats",0
"2. initialize feature set, if sampled, get from PS, otherwise use all the features",0
2.1. pull the sampled features of the current tree,0
this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();,0
"2.2. if use all the features, only called one",0
"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle",0
4. set root node to active,0
"5. reset instance position, set the root node's span",0
6. calculate gradient,0
1. decide nodes that should be calculated,0
2. decide calculated and subtracted tree nodes,0
3. calculate threads,0
wait until all threads finish,0
4. subtract threads,0
wait until all threads finish,0
5. send histograms to PS,0
6. update histogram cache,0
clock,0
find split,0
"1. find responsible tree node, using RR scheme",0
2. pull gradient histogram,0
2.1. get the name of this node's gradient histogram on PS,0
2.2. pull the histogram,0
2.3. find best split result of this tree node,0
2.3.1 using server split,0
"update the grad stats of the root node on PS, only called once by leader worker",0
update the grad stats of children node,0
update the left child,0
update the right child,0
"2.3.2 the updated split result (tree node/feature/value/gain) on PS,",0
"2.3.3 otherwise, the returned histogram contains the gradient info",0
"2.3.4 the updated split result (tree node/feature/value/gain) on PS,",0
2.3.5 reset this tree node's gradient histogram to 0,0
3. push split feature to PS,0
4. push split value to PS,0
5. push split gain to PS,0
6. set phase to AFTER_SPLIT,0
this.phase = GBDTPhase.AFTER_SPLIT;,0
clock,0
1. get split feature,0
2. get split value,0
3. get split gain,0
4. get node weight,0
5. split node,0
update local replica,0
create AfterSplit task,0
"2. check thread stats, if all threads finish, return",0
6. clock,0
"split the span of one node, reset the instance position",0
in case this worker has no instance on this node,0
set the span of left child,0
set the span of right child,0
"1. left to right, find the first instance that should be in the right child",0
"2. right to left, find the first instance that should be in the left child",0
3. swap two instances,0
4. find the cut pos,0
5. set the span of left child,0
6. set the span of right child,0
set tree node to active,0
set node to leaf,0
set node to inactive,0
finish current depth,0
finish current tree,0
set the tree phase,0
check if there is active node,0
check if finish all the tree,0
update node's grad stats on PS,0
"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split",0
the root node's stats is updated by leader worker,0
1. create the update,0
2. push the update to PS,0
1. update predictions of training data,0
2. update predictions of validation data,0
the leader task adds node prediction to flush list,0
1. name of this node's grad histogram on PS,0
2. build the grad histogram of this node,0
3. push the histograms to PS,0
4. reset thread stats to finished,0
5.1. set the children nodes of this node,0
5.2. set split info and grad stats to this node,0
5.2. create children nodes,0
"5.3. create node stats for children nodes, and add them to the tree",0
5.4. reset instance position,0
"5.5. add new active nodes if possible, inc depth, otherwise finish this tree",0
5.6. set children nodes to leaf nodes,0
5.7. set nid to leaf node,0
5.8. deactivate active node,0
"get feature type, 0:empty 1:all equal 2:real",0
"if not -1, sufficient space will be allocated at once",0
copy the highest levels,0
copy baseBuffer,0
merge two non-empty quantile sketches,0
left child <= split value; right child > split value,0
"the first: minimal, the last: maximal",0
categorical features,0
continuous features,0
left child <= split value; right child > split value,0
feature index used to split,0
feature value used to split,0
loss change after split this node,0
grad stats of the left child,0
grad stats of the right child,0
"LOG.info(""Constructor with fid = -1"");",0
fid = -1: no split currently,0
the minimal split value is the minimal value of feature,0
the splits do not include the maximal value of feature,0
"1. the average distance, (maxValue - minValue) / splitNum",0
2. calculate the candidate split value,0
1. new feature's histogram (grad + hess),0
size: sampled_featureNum * (2 * splitNum),0
"in other words, concatenate each feature's histogram",0
2. get the span of this node,0
------ 3. using sparse-aware method to build histogram ---,0
"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances",0
3.1. get the instance index,0
3.2. get the grad and hess of the instance,0
3.3. add to the sum,0
3.4. loop the non-zero entries,0
3.4.1. get feature value,0
3.4.2. current feature's position in the sampled feature set,0
"int fPos = findFidPlace(this.controller.fSet, fid);",0
3.4.3. find the position of feature value in a histogram,0
"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]",0
3.4.4. add the grad and hess to the corresponding bin,0
3.4.5. add the reverse to the bin that contains 0.0f,0
4. add the grad and hess sum to the zero bin of all features,0
find the best split result of the histogram of a tree node,0
1. calculate the gradStats of the root node,0
"1.1. update the grad stats of the root node on PS, only called once by leader worker",0
2. loop over features,0
2.1. get the ture feature id in the sampled feature set,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"update the grad stats of the root node on PS, only called once by leader worker",0
3. update the grad stats of children node,0
3.1. update the left child,0
3.2. update the right child,0
find the best split result of one feature,0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
split value = sketches[splitIdx],0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
partition number,0
cols of each partition,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
"loop all the possible split value, start from split[0], the first item is the minimal feature value",0
assert fvalue >= sketch[start] && fvalue <= sketch[end];,0
if (left > end) return end - start;,0
find the best split result of the histogram of a tree node,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
find the best split result of one feature,0
1. set the feature id,0
splitEntry.setFid(fid);,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]",0
the task use index to find fvalue,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting",0
"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking.",0
max and min of each feature,0
clear all the information,0
calculate the sum of gradient and hess,0
"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,",0
ridx),0
check if necessary information is ready,0
"same as add, reduce is used in All Reduce",0
"features used in this tree, if equals null, means use all the features without sampling",0
node in the tree,0
the gradient info of each instances,0
initialize nodes,0
gradient,0
second order gradient,0
int sendStartCol = (int) row.getStartCol();,0
logistic loss for binary classification task.,0
"logistic loss, but predict un-transformed margin",0
check if label in range,0
return the default evaluation metric for the objective,0
"task type: classification, regression, or ranking",0
"quantile sketch, size = featureNum * splitNum",0
"gradient histograms, size = treeNodeNum * featureNum * splitNum",0
"active tree nodes, size = pow(2, treeDepth) -1",0
sampled features. size = treeNum * sampleRatio * featureNum,0
categorical feature. size = workerNum * cateFeatNum * splitNum,0
"split features, size = treeNum * treeNodeNum",0
"split values, size = treeNum * treeNodeNum",0
"split gains, size = treeNum * treeNodeNum",0
"node weights, size = treeNum * treeNodeNum",0
"node preds, size = treeNum * treeNodeNum",0
if using PS to perform split,0
step size for a tree,0
number of class,0
minimum loss change required for a split,0
maximum depth of a tree,0
number of features,0
number of nonzero,0
number of candidates split value,0
----- the rest parameters are less important ----,0
base instance weight,0
minimum amount of hessian(weight) allowed in a child,0
L2 regularization factor,0
L1 regularization factor,0
default direction choice,0
maximum delta update we can add in weight estimation,0
this parameter can be used to stabilize update,0
default=0 means no constraint on weight delta,0
whether we want to do subsample for row,0
whether to subsample columns for each tree,0
accuracy of sketch,0
accuracy of sketch,0
leaf vector size,0
option for parallelization,0
option to open cacheline optimization,0
whether to not print info during training.,0
maximum depth of the tree,0
number of features used for tree construction,0
"minimum loss change required for a split, otherwise stop split",0
----- the rest parameters are less important ----,0
default direction choice,0
whether we want to do sample data,0
whether to sample columns during tree construction,0
whether to use histogram for split,0
number of histogram units,0
whether to print info during training.,0
----- the rest parameters are obtained after training ----,0
total number of nodes,0
number of deleted nodes */,0
@maxIndex: this variable contains the max index of node/word,0
values[b + offset] = (random.nextFloat() - 0.5f) / dimension;,0
some params,0
max index for node/word,0
compute number of nodes for one row,0
check the length of dot values,0
merge dot values from all partitions,0
Skip-Gram model,0
Negative sampling,0
used to accumulate the updates for input vectors,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
Skip-Gram model,0
Negative sampling,0
used to accumulate the updates for input vectors,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
update output,0
Some params,0
compute number of nodes for one row,0
window size,0
Skip-Gram model,0
Accumulate the input vectors from context,0
Negative sampling,0
used to accumulate the updates for input vectors,0
window size,0
skip-gram model,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
update output,0
some params,0
batch sentences,0
max index for node/word,0
compute number of nodes for one row,0
check the length of dot values,0
merge dot values from all partitions,0
locates the input vectors to local array to prevent randomly access,0
on the large server row.,0
fill 0 for context vector,0
window size,0
Continuous bag-of-words Models,0
Accumulate the input vectors from context,0
Calculate the partial dot values,0
We should guarantee here that the sample would not equal the ``word``,0
used to accumulate the context input vectors,0
locates the input vector into local arrays to prevent randomly access for,0
the large server row.,0
window size,0
while true to prevent sampling out a positive target,0
how to prevent the randomly access to the output vectors??,0
accumulate gradients for the input vectors,0
update output vectors,0
update input,0
update output,0
Some params,0
compute number of nodes for one row,0
// calculate bias,0
if (param.getPartKey().getStartCol() <= 0 && param.getPartKey().getEndCol() > 0) {,0
"double zVal = VectorUtils.getDouble(z, 0);",0
"double nVal = VectorUtils.getDouble(n, 0);",0
"VectorUtils.setFloat(w, 0, (float) (-1.0 * alpha * zVal / (beta + Math.sqrt(nVal))));",0
},0
Do nothing.,0
current word,0
neu1 stores the average value of input vectors in the context (CBOW),0
Continuous Bag-of-Words Model,0
Accumulate the input vectors from context,0
negative sampling,0
Using the sigmoid value from the pre-computed table,0
accumulate for the hidden layer,0
update output layer,0
add the counter for target,0
update hidden layer,0
Update the input vector for each word in the context,0
add the counter to input,0
update input layers,0
update output layers,0
for (int a = 0; a < layers.length; a++) deltas[a] = layers[a] - deltas[a];,0
copy the highest levels,0
copy baseBuffer,0
merge two non-empty quantile sketches,0
"if not -1, sufficient space will be allocated at once",0
InstanceRow ins = instanceRows[insId];,0
int[] indices = ins.indices();,0
int[] bins = ins.bins();,0
int nnz = indices.length;,0
for (int j = 0; j < nnz; j++) {,0
int fid = indices[j];,0
if (isFeatUsed[fid - featLo]) {,0
"histograms[fid - featLo].accumulate(bins[j], gradPairs[insId]);",0
},0
},0
1. allocate histogram,0
"2. loop non-zero instances, accumulate to histogram",0
if (nnz <= nodeEnd - nodeStart + 1) { // loop all nnz of current feature,0
3. add remaining grad and hess to default bin,0
"return param.calcWeights(grad, hess);",0
"numClass is usually small, so we do not use arraycopy here",0
"numClass is usually small, so we do not use arraycopy here",0
TODO: use more schema on default bin,1
1. set default bin to left child,0
"2. for other bins, find its location",0
3. create split set,0
this.nodeStats = new GBTNodeStat[numClass == 2 ? 1 : numClass];,0
predict sparse instance with indices and values,0
predict libsvm data,0
"different types of tree node splits, enumerated by their complexity",0
"in order to reduce model size, we give priority to split point",0
"comparison between two split points, we give priority to lower feature index",0
TODO: comparison between two split sets,0
"public boolean leafwise;  // true if leaf-wise training, false if level-wise training",0
TODO: regularization,1
TODO: regularization,1
public float insSampleRatio;  // subsample ratio for instances,0
"Preconditions.checkArgument(preds.length == labels.length,",0
"""LogLossMetric should be used for binary-label classification"");",0
double loss = 0.0;,0
for (int i = 0; i < preds.length; i++) {,0
"loss += evalOne(preds[i], labels[i]);",0
},0
return loss / labels.length;,0
double error = 0.0;,0
if (preds.length == labels.length) {,0
for (int i = 0; i < preds.length; i++) {,0
"error += evalOne(preds[i], labels[i]);",0
},0
} else {,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"error += evalOne(pred, labels[i]);",0
},0
},0
return error / labels.length;,0
Preconditions.checkArgument(preds.length != labels.length,0
"&& preds.length % labels.length == 0,",0
"""CrossEntropyMetric should be used for multi-label classification"");",0
double loss = 0.0;,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"loss += evalOne(pred, labels[i]);",0
},0
return loss / labels.length;,0
double correct = 0.0;,0
if (preds.length == labels.length) {,0
for (int i = 0; i < preds.length; i++) {,0
"correct += evalOne(preds[i], labels[i]);",0
},0
} else {,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"correct += evalOne(pred, labels[i]);",0
},0
},0
return (float) (correct / labels.length);,0
double errSum = 0.0f;,0
if (preds.length == labels.length) {,0
for (int i = 0; i < preds.length; i++) {,0
"errSum += evalOne(preds[i], labels[i]);",0
},0
} else {,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"errSum += evalOne(pred, labels[i]);",0
},0
},0
return Math.sqrt(errSum / labels.length);,0
"System.out.println(""----------"");",0
"System.out.println(""read buffer = "" + Integer.toBinaryString(readBufferT & 0b11111111)",0
"+ "", mask = "" + Integer.toBinaryString(readMaskT));",0
readMaskT <<= 1;,0
"boolean[] bits = new boolean[]{true, true, false, false, true, false, false, false, true, true, false, true};",0
int n = bits.length;,0
BufferedBitSet writeBitSet = new BufferedBitSet(n);,0
"BufferedBitSet readBitSet = new BufferedBitSet(writeBitSet.getBytes(), n);",0
if (bitSet.get(i) != bits[i]) {,0
"throw new RuntimeException("""" + i);",0
},0
private final ByteBuffer bytes;,0
"public BufferedBitSetReader(ByteBuffer bytes, int numBits) {",0
int capacity = bytes.capacity() * 8;,0
readIndexT = bytes.capacity() - 1;,0
return bytes.get(index);,0
TODO: use arraycopy to make it faster,1
assert from >= this.from && to <= this.to;,0
"LOG.debug(String.format(""Create subset: [%d-%d]"", newFrom, newTo));",0
"LOG.debug(String.format(""Get overlap: [%d-%d]"", newFrom, newTo));",0
return bits.clone();,0
private final SerializableBuffer bytes;,0
private final ByteBuffer bytes;,0
this.bytes = ByteBuffer.allocate(numBytes);,0
public BufferedBitSetWriter(ByteBuffer bytes) {,0
this.bytes = bytes;,0
},0
"bytes.put(writeIndex++, (byte) writeBuffer);",0
public ByteBuffer getBytes() {,0
return bytes;,0
},0
ML TreeConf,0
GBDT TreeConf,0
set basic configuration keys,0
use local deploy mode and dummy data spliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
import jdk.nashorn.internal.runtime.regexp.joni.Config;,0
"paras[1] = ""abc"";",0
"paras[2] = ""123"";",0
Add standard Hadoop classes,0
Feature number of train data,0
Total iteration number,0
Learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set input data path,0
Set save model path,0
Set actionType train,0
QSLRRunner runner = new QSLRRunner();,0
runner.train(conf);,0
"conf.setInt(AngelConf.ANGEL_STALENESS, -1);",0
Dataset,0
"Data format, libsvm or dummy",0
class number,0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set Softmax algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
Feature number of train data,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Ratio of validation,0
Learning rate,0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set DeepFM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
Model type,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set FM algorithm parameters,0
Set model class,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set WideAndDeep algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
"Set LDA parameters #V, #K",0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set LDA algorithm parameters,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set SVM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Load model meta,0
Convert model,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Convert model,0
load hadoop configuration,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model,0
load hadoop configuration,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
https://blog.csdn.net/cocoonyang/article/details/63068108,0
v1[i] = v1[i] + da * v2[i];,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
v1[i] = v1[i] + da * v2[i];,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
Default does nothing.,0
The app injection is optional,0
"renderText(""hello world"");",0
"user choose a workerGroupID from the workergroups page,",0
now we should change the AngelApp params and render the workergroup page;,0
"static final String WORKER_ID = ""worker.id"";",0
"div(""#logo"").",0
"img(""/static/hadoop-st.png"")._().",0
import org.apache.hadoop.yarn.webapp.view.FooterBlock;,0
import org.apache.hadoop.yarn.webapp.view.HeaderBlock;,0
JQueryUI.jsnotice(html);,0
import org.apache.hadoop.conf.Configuration;,0
import java.lang.reflect.Field;,0
get block locations from file system,0
create a list of all block and their locations,0
"if the file is not splitable, just create the one block with",0
full file length,0
each split can be a maximum of maxSize,0
if remainder is between max and 2*max - then,0
"instead of creating splits of size max, left-max we",0
create splits of size left/2 and left/2. This is,0
a heuristic to avoid creating really really small,0
splits.,0
add this block to the block --> node locations map,0
"For blocks that do not have host/rack information,",0
assign to default  rack.,0
add this block to the rack --> block map,0
Add this host to rackToNodes map,0
add this block to the node --> block map,0
"if the file system does not have any rack information, then",0
use dummy rack location.,0
The topology paths have the host name included as the last,0
component. Strip it.,0
get tokens for all the required FileSystems..,0
"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,",0
job.getConfiguration());,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
all the files in input set,0
it is allowed for maxSize to be 0. Disable smoothing load for such cases,0
process all nodes and create splits that are local to a node. Generate,0
"one split per node iteration, and walk over nodes multiple times to",0
distribute the splits across nodes.,0
Skip the node if it has previously been marked as completed.,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
Remove all blocks which may already have been assigned to other,0
splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
Remove entries from blocksInNode so that we don't walk these,0
again.,0
Done creating a single split for this node. Move on to the next,0
node so that splits are distributed across nodes.,0
This implies that the last few blocks (or all in case maxSize=0),0
were not part of a split. The node is complete.,0
if there were any blocks left over and their combined size is,0
"larger than minSplitNode, then combine them into one split.",0
Otherwise add them back to the unprocessed pool. It is likely,0
that they will be combined with other blocks from the,0
same rack later on.,0
This condition also kicks in when max split size is not set. All,0
blocks on a node will be grouped together into a single split.,0
haven't created any split on this machine. so its ok to add a,0
smaller one for parallelism. Otherwise group it in the rack for,0
balanced size create an input split and add it to the splits,0
array,0
Remove entries from blocksInNode so that we don't walk this again.,0
The node is done. This was the last set of blocks for this node.,0
Put the unplaced blocks back into the pool for later rack-allocation.,0
Node is done. All blocks were fit into node-local splits.,0
Check if node-local assignments are complete.,0
All nodes have been walked over and marked as completed or all blocks,0
have been assigned. The rest should be handled via rackLock assignment.,0
"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """,0
"+ completedNodes.size() + "", size left: "" + totalLength);",0
"if blocks in a rack are below the specified minimum size, then keep them",0
"in 'overflow'. After the processing of all racks is complete, these",0
overflow blocks will be combined into splits.,0
Process all racks over and over again until there is no more work to do.,0
Create one split for this rack before moving over to the next rack.,0
Come back to this rack after creating a single split for each of the,0
remaining racks.,0
"Process one rack location at a time, Combine all possible blocks that",0
reside on this rack as one split. (constrained by minimum and maximum,0
split size).,0
iterate over all racks,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"if we created a split, then just go to the next rack",0
"if there is a minimum size specified, then create a single split",0
"otherwise, store these blocks into overflow data structure",0
There were a few blocks in this rack that,0
remained to be processed. Keep them in 'overflow' block list.,0
These will be combined later.,0
Process all overflow blocks,0
"This might cause an exiting rack location to be re-added,",0
but it should be ok.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"Process any remaining blocks, if any.",0
create an input split,0
add this split to the list that is returned,0
long num = totLength / maxSize;,0
all blocks for all the files in input set,0
mapping from a rack name to the list of blocks it has,0
mapping from a block to the nodes on which it has replicas,0
mapping from a node to the list of blocks that it contains,0
populate all the blocks for all files,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
"Security framework already loaded the tokens into current UGI, just use",0
them,0
Now remove the AM->RM token so tasks don't have it,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init container allocator,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
Init Client manager,0
Init PS Client manager,0
init parameter server manager,0
recover task information if needed,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
Init model saver & loader,0
start a web service if use yarn deploy mode,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
Check Workers,0
Check PSS,0
Check Clients,0
Check PS Clients,0
parse parameter server counters,0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
refresh last heartbeat timestamp,0
send a state update event to the specific PSAttempt,0
Check is there save request,0
Check is there load request,0
check matrix metadata inconsistencies between master and parameter server.,0
"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix",0
"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix",0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
choose a unused port,0
start RPC server,0
remove this parameter server attempt from monitor set,0
remove this parameter server attempt from monitor set,0
"if worker attempt id is not in monitor set, we should shutdown it",0
find workergroup in worker manager,0
"if this worker group does not initialized, just return WORKERGROUP_NOTREADY",0
"if this worker group run over, just return WORKERGROUP_EXITED",0
"if this worker group is running now, return tasks, workers, data splits for it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update the clock for this matrix,0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update task iteration,0
"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);",0
"Calculate how many splits we need. As each task handles a separate split of data, so we want",0
the number of splits equal to the number of tasks,0
split data,0
dispatch the splits to workergroups,0
split data,0
dispatch the splits to workergroups,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
write meta data to a temporary file,0
rename the temporary file to final file,0
"if the file exists, read from file and deserialize it",0
write task meta,0
write ps meta,0
generate a temporary file,0
write task meta to the temporary file first,0
rename the temporary file to the final file,0
"if last final task file exist, remove it",0
find task meta file which has max timestamp,0
"if the file does not exist, just return null",0
read task meta from file and deserialize it,0
generate a temporary file,0
write ps meta to the temporary file first.,0
rename the temporary file to the final file,0
"if the old final file exist, just remove it",0
find ps meta file,0
"if ps meta file does not exist, just return null",0
read ps meta from file and deserialize it,0
"Path tmpPath = new Path(new Path(context.getConf().get(AngelConf.ANGEL_JOB_TMP_OUTPUT_PATH)),",0
String.valueOf(requestId));,0
Path tmpPath = HdfsUtil.toTmpPath(new Path(saveContext.getSavePath()));,0
saveContext.setTmpSavePath(tmpPath.toString());,0
Filter old epoch trigger first,0
Split the user request to sub-requests to pss,0
Init matrix files meta,0
Move output files,0
Write the meta file,0
Split the user request to sub-requests to pss,0
check whether psagent heartbeat timeout,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
LocalStorageToken is needed irrespective of whether security is enabled,0
or not.,0
"TokenCache.setJobToken(jobToken, taskCredentials);",0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Construct the actual Container,0
The null fields are per-container and will be constructed for each,0
container separately.,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
"a * in the classpath will only find a .jar, so we need to filter out",0
all .jars and add everything else,0
Propagate the system classpath when using the mini cluster,0
Add standard Hadoop classes,0
Add mr,0
Cache archives,0
Cache files,0
Sanity check,0
Add URI fragment or just the filename,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Final commmand,0
"if amTask is not null, we should clone task state from it",0
"if all parameter server complete commit, master can commit now",0
check whether parameter server heartbeat timeout,0
Transitions from the NEW state.,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event",0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event dispatched before",0
PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.COMMITTING state,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
reqeuest resource:send a resource request to the resource allocator,0
"Once the resource is applied, build and send the launch request to the container launcher",0
deallocator the resource of the ps attempt:send a resource deallocator request to the,0
resource allocator,0
set the launch time,0
add the ps attempt to the heartbeat timeout monitoring list,0
parse ps attempt location and put it to location manager,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt",0
or failed,0
remove ps attempt id from heartbeat timeout monitor list,0
release container:send a release request to container launcher,0
set the finish time only if launch time is set,0
private long scheduledTime;,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
Refresh ps location & matrix meta,0
start a new attempt for this ps,0
notify ps manager,0
"getContext().getLocationManager().setPsLocation(id, null);",0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
notify the event handler of state change,0
"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor",0
"if forcedState is set, just return",0
else get state from state machine,0
add this worker group to the success set,0
check if all worker group run over,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
calculate the actual number of worker groups and the total number of tasks based on the number of data split,0
"init all tasks , workers and worker groups and put them to the corresponding maps",0
just return the total task number now,0
TODO,1
check whether worker heartbeat timeout,0
"if workerAttempt is not null, we should clone task state from it",0
from NEW state,0
from SCHEDULED state,0
get data splits location for data locality,0
reqeuest resource:send a resource request to the resource allocator,0
"once the resource is applied, build and send the launch request to the container launcher",0
notify failed message to the worker,0
notify killed message to the worker,0
release the allocated container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
release the allocated container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
clean the container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
record the finish time,0
clean the container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listening list,0
record the finish time,0
"if the worker attempt launch successfully, add it to heartbeat timeout listening list",0
set worker attempt location,0
notify the register message to the worker,0
record the launch time,0
update worker attempt metrics,0
update tasks metrics,0
clean the container,0
notify the worker attempt run successfully message to the worker,0
record the finish time,0
init a worker attempt for the worker,0
schedule the worker attempt,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
"If we need Yarn to restart a new application master, we should not unregister from Yarn RM",0
register to Yarn RM,0
send heartbeat to Yarn RM every rmPollInterval milliseconds,0
"catch YarnRuntimeException, we should exit and need not retry",0
build heartbeat request,0
send heartbeat request to rm,0
"This can happen if the RM has been restarted. If it is in that state,",0
this application must clean itself up.,0
Setting NMTokens,0
assgin containers,0
"if some container is not assigned, release them",0
handle finish containers,0
dispatch container exit message to corresponding components,0
killed by framework,0
killed by framework,0
get application finish state,0
build application diagnostics,0
TODO:add a job history for angel,1
build unregister request,0
send unregister request to rm,0
Note this down for next interaction with ResourceManager,0
based on blacklisting comments above we can end up decrementing more,0
than requested. so guard for that.,0
send the updated resource request to RM,0
send 0 container count requests also to cancel previous requests,0
Update resource requests,0
try to assign to all nodes first to match node local,0
try to match all rack local,0
assign remaining,0
Update resource requests,0
send the container-assigned event to task attempt,0
build the start container request use launch context,0
send the start request to Yarn nm,0
send the message that the container starts successfully to the corresponding component,0
"after launching, send launched event to task attempt to move",0
it from ASSIGNED to RUNNING state,0
send the message that the container starts failed to the corresponding component,0
kill the remote container if already launched,0
start a thread pool to startup the container,0
See if we need up the pool size only if haven't reached the,0
maximum limit yet.,0
nodes where containers will run at *this* point of time. This is,0
*not* the cluster size and doesn't need to be.,0
"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the",0
later is just a buffer so we are not always increasing the,0
pool-size,0
the events from the queue are handled in parallel,0
using a thread pool,0
return if already stopped,0
shutdown any containers that might be left running,0
Add one sync matrix,0
addSyncMatrix();,0
"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {",0
"LOG.info(""ps id = "" + psEntry.getKey());",0
"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();",0
"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {",0
"LOG.info(""matrix id = "" + metaEntry.getKey());",0
"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());",0
},0
},0
get matrix ids in the parameter server report,0
get the matrices parameter server need to create and delete,0
"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix",0
"if a matrix exists on master but not exist on parameter server, this parameter server need build it.",0
Init control connection manager,0
Get ps locations from master and put them to the location cache.,0
Build and initialize rpc client to master,0
Get psagent id,0
Build PS control rpc client manager,0
Build local location,0
Initialize matrix meta information,0
Start all services,0
Stop all modules,0
Stop all modules,0
clock first,0
wait,0
Update generic resource counters,0
Updating resources specified in ResourceCalculatorProcessTree,0
Remove the CPU time consumed previously by JVM reuse,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Plus a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Update a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get values from pss use row/column indices,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"/ PSF get/update, use can implement their own psf",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get a row or a batch of rows,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
TODO:,1
Generate a flush request and put it to request queue,0
Generate a clock request and put it to request queue,0
Generate a merge request and put it to request queue,0
Generate a merge request and put it to request queue,0
"If the matrix op log cache does not exist for the matrix, create a new one for the",0
matrix,0
and add it to cache maps,0
Add the message to the tree map,0
"If there are flush / clock requests blocked, we need to put this merge request into",0
the waiting queue,0
Launch a merge worker to merge the update to matrix op log cache,0
Remove the message from the tree map,0
Wake up blocked flush/clock request,0
Add flush/clock request to listener list to waiting for all the existing,0
updates are merged,0
Wake up blocked flush/clock request,0
"If all updates are merged for this matrix, we need wake up flush/clock requests which are",0
blocked.,0
Get next merge message sequence id,0
Wake up listeners(flush/clock requests) that have little sequence id than current merge,0
position,0
Wake up blocked merge requests,0
Get minimal sequence id from listeners,0
"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we",0
should flush updates to local matrix storage,0
unused now,1
TODO:,1
Doing average or not,0
Filter un-important update,0
Split this row according the matrix partitions,0
Set split context,0
Remove the row from matrix,0
buf.writeDouble(0.0);,0
TODO,1
TODO: write map default value,1
buf.writeDouble(0);,0
TODO:,1
TODO:,1
TODO:,1
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"LOG.error(""channel "" + ctx.channel() + "" inactive"");",0
"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());",0
"LOG.error(""put response message queue failed "", e);",0
Use Epoll for linux,0
Update location table,0
Remove the server from failed list,0
Notify refresh success message to request dispatcher,0
Check PS exist or not,0
Check heartbeat timeout,0
Check PS restart or not,0
private final HashSet<ParameterServerId> refreshingServerSet;,0
Add it to failed rpc list,0
Add the server to gray server list,0
Add it to failed rpc list,0
Add the server to gray server list,0
Move from gray server list to failed server list,0
Handle the RPCS to this server,0
Submit the schedulable failed get RPCS,0
Submit new get RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"If the queue is empty, just return 0",0
"If request is not over limit, just submit it",0
Submit the schedulable failed get RPCS,0
Submit new put RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"LOG.info(""choose put server "" + psIds[index]);",0
Check all pending RPCS,0
Check get channel context,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
channelManager.printPools();,0
"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {",0
if(ts - entry.getValue() > requestTimeOut * 2)  {,0
"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())",0
"+ "" milliseconds, close all channels to it"");",0
closeChannels(entry.getKey());,0
"psLocToLastChannelTsMap.put(entry.getKey(), ts);",0
},0
},0
"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);",0
Remove all pending RPCS,0
Close all channel to this PS,0
private Channel getChannel(Location loc) throws Exception {,0
"return channelManager.getOrCreateChannel(new Location(loc.getIp(), loc.getPort() + 1));",0
},0
private GenericObjectPool<Channel> getChannelPool(Location loc) throws InterruptedException {,0
"return channelManager.getOrCreateChannelPool(new Location(loc.getIp(), loc.getPort() + 1), PSAgentContext",0
.get(),0
.getConf(),0
".getInt(AngelConf.ANGEL_WORKER_TASK_NUMBER,",0
AngelConf.DEFAULT_ANGEL_WORKER_TASK_NUMBER));,0
},0
"LOG.error(""send request "" + request + "" is interrupted"");",0
"LOG.error(""send request "" + request + "" failed, "", e);",0
Get server id and location for this request,0
"If location is null, means that the server is not ready",0
Get the channel for the location,0
Check if need get token first,0
Serialize the request,0
Send the request,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
request.getContext().setChannelPool(pool);,0
Allocate the bytebuf and serialize the request,0
find the partition request context from cache,0
"updateMatrixCache(request.getPartKey(), response.getPartition());",0
"updateMatrixCache(request.getPartKey(), response.getRowsSplit());",0
TODO,1
ServerRow rowSplit = PSAgentContext.get().getMatricesCache(),0
".getRowSplit(request.getPartKey().getMatrixId(), request.getPartKey(),",0
request.getRowIndex());,0
response.setRowSplit(rowSplit);,0
"updateMatrixCache(request.getPartKey(), response.getRowSplit());",0
"LOG.info(""user request id "" + request.getUserRequestId());",0
"LOG.info(""user request id "" + request.getUserRequestId());",0
TODO,1
"private void updateMatrixCache(PartitionKey partKey, ServerPartition partition) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, partition);",0
},0
"private void updateMatrixCache(PartitionKey partKey, ServerRow rowSplit) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowSplit);",0
},0
"private void updateMatrixCache(PartitionKey partKey, List<ServerRow> rowsSplit) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowsSplit);",0
},0
Get partitions for this row,0
Distinct get row requests,0
Need get from ps or storage/cache,0
"Switch to new request id, send a new request",0
First get this row from matrix storage,0
MatrixStorage matrixStorage =,0
PSAgentContext.get().getMatrixStorageManager().getMatrixStoage(matrixId);,0
TVector row = matrixStorage.getRow(rowIndex);,0
if (row != null && row.getClock() >= clock) {,0
result.set(row);,0
return row;,0
},0
Get row splits of this row from the matrix cache first,0
responseCache.addSubResponse(rowSplit);,0
"If the row split does not exist in cache, get it from parameter server",0
Wait the final result,0
Put it to the matrix cache,0
"matrixStorage.addRow(rowIndex, row);",0
Just wait result,0
Split the param use matrix partitions,0
Send request to PSS,0
Split the matrix oplog according to the matrix partitions,0
"If need update clock, we should send requests to all partitions",0
Send request to PSS,0
Filter the rowIds which are fetching now,0
Send the rowIndex to rpc dispatcher and return immediately,0
"LOG.info(""get row split use time="" + (System.currentTimeMillis() - startTs));",0
"LOG.info(""start to request "" + requestId);",0
"LOG.info(""start to request "" + requestId);",0
Split param use matrix partitons,0
"If all sub-results are received, just remove request and result cache",0
"LOG.info(""request = "" + request + "", cache = "" + cache);",0
"LOG.info(""start to merge "" + cache + "" for request "" + request);",0
"LOG.info(""psf get merge use time = "" + (System.currentTimeMillis() - startTs));",0
Split this row according the matrix partitions,0
Set split context,0
Split this row according the matrix partitions,0
Set split context,0
long startTs = System.currentTimeMillis();,0
"LOG.error(""combine use time = "" + (System.currentTimeMillis() - startTs));",0
Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition,0
Generate dispatch items and add them to the corresponding queues,0
Filter the rowIds which are fetching now,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {",0
return key1.getStartCol() < key2.getStartCol() ? -1 : 1;,0
});,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {",0
return key1.getStartCol() < key2.getStartCol() ? -1 : 1;,0
});,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Put the row split to the cache(row index to row splits map),0
"If all splits of the row are received, means this row can be merged",0
TODO,1
TODO,1
/////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO,1
buf.writeDouble(0);,0
TODO,1
///////////////////////////////////////////////////////////////////////////////////////////////,0
Now we just support pipelined row splits merging for dense type row,0
Pre-fetching is disable default,0
matrix id to clock map,0
"task index, it must be unique for whole application",0
Deserialize data splits meta,0
Get workers,0
Send request to every ps,0
Wait the responses,0
Update clock cache,0
if(syncNum % 1024 == 0) {,0
},0
"Use simple flow, do not use any cache",0
Get row from cache.,0
"if row clock is satisfy ssp staleness limit, just return.",0
Get row from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC mode, just get from pss.",0
"For BSP/SSP, get rows from storage/cache first",0
Get from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC, just get rows from pss.",0
no more retries.,0
calculate sleep time and return.,0
parse the i-th sleep-time,0
parse the i-th number-of-retries,0
calculateSleepTime may overflow.,0
"A few common retry policies, with no delays.",0
Read matrix meta from meta file,0
Save partitions to files use fork-join,0
Write the ps matrix meta to the meta file,0
matrix.startServering();,0
return;,0
Read matrix meta from meta file,0
Load partitions from file use fork-join,0
Read matrix meta from meta file,0
Sort partitions,0
int size = rows.length;,0
int size = rows.length;,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
close is a local operation and should finish within milliseconds; timeout just to be safe,0
response will be null for one way messages.,0
maxFrameLength = 2G,0
lengthFieldOffset = 0,0
lengthFieldLength = 8,0
"lengthAdjustment = -8, i.e. exclude the 8 byte length itself",0
"initialBytesToStrip = 8, i.e. strip out the length field itself",0
"return Objects.toStringHelper(this).add(""serial"", serial).add(""listSize"", datas.size())",0
.toString();,0
indicates whether this connection's life cycle is managed,0
See if we already have a connection (common case),0
create a unique lock for this RS + protocol (if necessary),0
get the RS lock,0
do one more lookup in case we were stalled above,0
Only create isa when we need to.,0
definitely a cache miss. establish an RPC for,0
this RS,0
Throw what the RemoteException was carrying.,0
check,0
every,0
minutes,0
TODO,1
failoverHandler,0
"The number of times this invocation handler has ever been failed over,",0
before this method invocation attempt. Used to prevent concurrent,0
failed method invocations from triggering multiple failover attempts.,0
Make sure that concurrent failed method invocations,0
only cause a,0
single actual fail over.,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
"LOG.info(""method "" + method.getName() + ""construct request time = """,0
+ (System.currentTimeMillis() - beforeConstructTs));,0
get an instance of the method arg type,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
Message (hand written code usually has only a single,0
argument),0
log any RPC responses that are slower than the configured,0
warn,0
response time or larger than configured warning size,0
"when tagging, we let TooLarge trump TooSmall to keep",0
output simple,0
note that large responses will often also be slow.,0
provides a count of log-reported slow responses,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
unexpected,0
"in the protobuf methods, args[1] is the only significant argument",0
for JSON encoding,0
base information that is reported regardless of type of call,0
Disable Nagle's Algorithm since we don't want packets to wait,0
Configure the event pipeline factory.,0
Make a new connection.,0
Remove all pending requests (will be canceled after relinquishing,0
write lock).,0
Cancel any pending requests by sending errors to the callbacks:,0
Close the channel:,0
Close the connection:,0
Shut down all thread pools to exit.,0
"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());",0
See NettyServer.prepareResponse for where we write out the response.,0
"It writes the call.id (int), a boolean signifying any error (and if",0
"so the exception name/trace), and the response bytes",0
Read the call id.,0
"When the stream is closed, protobuf doesn't raise an EOFException,",0
"instead, it returns a null message object.",0
"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +",0
System.currentTimeMillis());,0
"It would be good widen this to just Throwable, but IOException is what we",0
allow now,0
not implemented,1
not implemented,1
"track what RpcEngine is used by a proxy class, for stopProxy()",0
cache of RpcEngines by protocol,0
return the RpcEngine configured to handle a protocol,0
We only handle the ConnectException.,0
This is the exception we can't handle.,0
check if timed out,0
wait for retry,0
IGNORE,0
return the RpcEngine that handles a proxy object,0
The default implementation works synchronously,0
punt: allocate a new buffer & copy into it,0
Parse cmd parameters,0
load hadoop configuration,0
load angel system configuration,0
load user configuration:,0
load user config file,0
load command line parameters,0
load user job resource files,0
load ml conf file for graph based algorithm,0
load user job jar if it exist,0
Expand the environment variable,0
Add default fs(local fs) for lib jars.,0
"LOG.info(System.getProperty(""user.dir""));",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
a simple hdfs copy function assume src path and dest path are in same hdfs,0
and FileSystem object has same schema,0
"LOG.warn(""interrupted while sleeping"", ie);",0
public static String getHostname() {,0
try {,0
"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();",0
} catch (UnknownHostException uhe) {,0
},0
"return new StringBuilder().append("""").append(uhe).toString();",0
},0
"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {",0
String hostname = getHostname();,0
String classname = clazz.getSimpleName();,0
"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new",0
"StringBuilder().append(""Starting "").append(classname).toString(), new",0
"StringBuilder().append(""  host = "").append(hostname).toString(), new",0
"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new",0
"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new",0
"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));",0
,0
"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {",0
public void run() {,0
"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +",0
"this.val$classname + "" at "" + this.val$hostname}));",0
},0
});,0
},0
"We we interrupted because we're meant to stop? If not, just",0
continue ignoring the interruption,0
Recalculate waitTime.,0
// Begin delegation to Thread,0
// End delegation to Thread,0
instance submitter class,0
Obtain filename from path,0
Split filename to prexif and suffix (extension),0
Check if the filename is okay,0
Prepare temporary file,0
Prepare buffer for data copying,0
Open and check input stream,0
Open output stream and copy data between source file in JAR and the temporary file,0
"If read/write fails, close streams safely before throwing an exception",0
"Finally, load the library",0
little endian load order,0
tail,0
fallthrough,0
fallthrough,0
finalization,0
fmix(h1);,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
throw new AngelException(e);,0
JobStateProto jobState = report.getJobState();,0
Check need load matrices,0
Used for java code to get a AngelClient instance,0
Used for python code to get a AngelClient instance,0
load user job resource files,0
the leaf level file should be readable by others,0
the subdirs in the path should have execute permissions for,0
others,0
2.get job id,0
Credentials credentials = new Credentials();,0
4.copy resource files to hdfs,0
5.write configuration to a xml file,0
6.create am container context,0
7.Submit to ResourceManager,0
8.get app master client,0
Create a number of filenames in the JobTracker's fs namespace,0
add all the command line files/ jars and archive,0
first copy them to jobtrackers filesystem,0
should not throw a uri exception,0
should not throw an uri excpetion,0
set the timestamps of the archives and files,0
set the public/private visibility of the archives and files,0
get DelegationToken for each cached file,0
check if we do not need to copy the files,0
is jt using the same file system.,0
just checking for uri strings... doing no dns lookups,0
to see if the filesystems are the same. This is not optimal.,0
but avoids name resolution.,0
this might have name collisions. copy will throw an exception,0
parse the original path to create new path,0
check for ports,0
Write job file to JobTracker's fs,0
Setup resource requirements,0
Setup LocalResources,0
Setup security tokens,0
Setup the command to run the AM,0
Add AM user command opts,0
Final command,0
Setup the CLASSPATH in environment,0
"i.e. add { Hadoop jars, job jar, CWD } to classpath.",0
Setup the environment variables for Admin first,0
"Setup the environment variables (LD_LIBRARY_PATH, etc)",0
Parse distributed cache,0
Setup ContainerLaunchContext for AM container,0
Set up the ApplicationSubmissionContext,0
private volatile PS2PSPusherImpl ps2PSPusher;,0
TODO,1
Add tokens to new user so that it may execute its task correctly.,0
TODO,1
to exit,0
TODO,1
TODO,1
ps2PSPusher.recover(ProtobufUtil.convert(needRecoverParts.get(i)));,0
context.getSnapshotManager().processRecovery();,0
Recover PS from snapshot or load path,0
First check snapshot,0
Check load path setting,0
TODO,1
if(ps2PSPusher != null) {,0
ps2PSPusher.start();,0
},0
public PS2PSPusherImpl getPs2PSPusher() {,0
return ps2PSPusher;,0
},0
"LOG.error(""send response of request "" + requestToString(clientId, seqId) + "" failed "");",0
"LOG.error(""send response of request failed, request seqId="" + seqId + "", channel="" + ch, ex);",0
Release the input buffer,0
Release the input buffer,0
"1. handle the rpc, get the response",0
Release the input buffer,0
2. Serialize the response,0
Send the serialized response,0
Exception happened,0
write seq id,0
Just serialize the head,0
Exception happened,0
Allocate result buffer,0
Exception happened,0
Just serialize the head,0
Exception happened,0
Reset the response and allocate buffer again,0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this location, only master ps can accept the update",0
Check the partition state again,0
Start to put the update to the slave pss,0
TODO,1
"context.getPS2PSPusher().put(request, in, partLoc);",0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this partition, if not, just return failed",0
Start to put the update to the slave pss,0
TODO,1
return ServerState.GENERAL;,0
Use Epoll for linux,0
public String uuid;,0
public void setChannelPool(GenericObjectPool<Channel> channelPool) {,0
this.channelPool = channelPool;,0
},0
private final ParameterServer psServer;,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO,1
///////////////////////////////////////////////////////////////////////////////////////////////,0
"////// network io method, for model transform",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
public ObjectIterator<Long2FloatMap.Entry> getIter() {,0
return ((LongFloatVector) row).getStorage().entryIterator();,0
},0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
public ObjectIterator<Long2LongMap.Entry> getIter() {,0
return ((LongLongVector) row).getStorage().entryIterator();,0
},0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO: dynamic add/delete row,1
private final List<PartitionKey> partitionKeys;,0
"if col == -1, we use the start/end index to calculate range,",0
we use double to store the range value since two long minus might exceed the,0
range of long.,0
Use Epoll for linux,0
find the partition request context from cache,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
channelManager.removeChannelPool(loc);,0
Generate seq id,0
Create a RecoverPartRequest,0
Serialize the request,0
Change the seqId for the request,0
Serialize the request,0
"First check the state of the channels in the pool, if a channel is unused, just return",0
"If all channels are in use, create a new channel or wait",0
Create a new channel,0
"add the PSAgentContext,need fix",0
If col == -1 and start/end not set,0
start/end set,0
"for dense type, we need to set the colNum to set dim for vectors",0
"colNum set, start/end not set",0
Row number must > 0,0
"both set, check its valid",0
TODO:add more vector type,1
TODO : subDim set,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
write the max abs,0
---------------------------------------------------,0
---------------------------------------------------,0
---------------------------------------------------------------,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
-----------------,0
-----------------,0
-----------------,0
-----------------,0
-----------------,0
KeepStorage is guaranteed,0
"ignore the isInplace option, since v2 is dense",0
"the value in old storage can be changed safe, so switch a storage",0
"but user required keep storage, we can prevent rehash",0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
KeepStorage is guaranteed,0
we gauss dense storage is more efficient,0
v1Size < v2Size * Constant.sparseThreshold,0
KeepStorage is guaranteed,0
"ignore the isInplace option, since v2 is dense",0
"the value in old storage can be changed safe, so switch a storage",0
"but user required keep storage, we can prevent rehash",0
we gauss dense storage is more efficient,0
prevent rehash,0
KeepStorage is guaranteed,0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
dense preferred,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
sorted preferred,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"Transform mat1, generate a new matrix",0
Split the row indices of mat1Trans,0
Parallel execute use fork-join,0
"Get the sub-matrix of left matrix, split by row",0
"Transform mat1, generate a new matrix",0
Split the row indices of mat1Trans,0
Parallel execute use fork-join,0
"Get the sub-matrix of left matrix, split by row",0
"mat1 trans true, mat trans true",0
"mat1 trans true, mat trans false",0
"mat1 trans false, mat trans true, important",0
"mat1 trans false, mat trans false",0
"mat1 trans true, mat trans true",0
"mat1 trans true, mat trans false",0
"mat1 trans false, mat trans true, important",0
"mat1 trans false, mat trans false",0
"dger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)",0
"sger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)",0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
add dense double matrix,0
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
get configuration from config file,0
set localDir with enviroment set by nm.,0
get master location,0
init task manager and start tasks,0
start heartbeat thread,0
taskManager.assignTaskIds(response.getTaskidsList());,0
todo,1
"if worker timeout, it may be knocked off.",0
"SUCCESS, do nothing",0
heartbeatFailedTime = 0;,0
private KEY currentKey;,0
will be created,0
TODO Auto-generated method stub,1
Bitmap bitmap = new Bitmap();,0
int max = indexArray[size - 1];,0
byte [] bitIndexArray = new byte[max / 8 + 1];,0
for(int i = 0; i < size; i++){,0
int bitIndex = indexArray[i] >> 3;,0
int bitOffset = indexArray[i] - (bitIndex << 3);,0
switch(bitOffset){,0
case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;,0
case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;,0
case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;,0
case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;,0
case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;,0
case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;,0
case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;,0
case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;,0
},0
},0
"true, false",0
//////////////////////////////,0
Application Configs,0
//////////////////////////////,0
//////////////////////////////,0
Master Configs,0
//////////////////////////////,0
//////////////////////////////,0
Worker Configs,0
//////////////////////////////,0
//////////////////////////////,0
Task Configs,0
//////////////////////////////,0
//////////////////////////////,0
ParameterServer Configs,0
//////////////////////////////,0
////////////////// IPC //////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
Configs used to ANGEL_PS_PSAGENT running mode future.,0
model parse,0
Mark whether use pyangel or not.,0
private Configuration conf;,0
"Configuration that should be used in python environment, there should only be one",0
configuration instance in each Angel context.,0
Use private access means jconf should not be changed or modified in this way.,0
Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:,0
Do nothing,0
To-DO: add other ways to justify different value types,0
"This is so ugly, must re-implement by more elegance way",1
"Create python path which include angel's jars, the python directory in ANGEL_HOME,",0
and other files submitted by user.,0
Launch python process,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Data format,0
Feature number of train data,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Ratio of validation,0
Learning rate,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"Set angel resource, #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Set training data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set predict data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model type,0
String modelType = String.valueOf(RowType.T_FLOAT_DENSE);,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
"conf.set(MLConf.ML_MODEL_TYPE(), modelType);",0
"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);",0
predictTest();,0
"Data format, libsvm or dummy",0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set data format,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
class number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set log path,0
Set trainning data path,0
Set save model path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set training data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log save path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set testing data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Data is classification,0
Model is classification,0
Train batch number per epoch.,0
loss delta,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Data is classification,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
TODO: optimize int key indices,1
"System.out.println(""deserialize cols.length="" + nCols);",0
"System.out.print(""deserialize "");",0
"System.out.print(cols[c] + "" "");",0
System.out.println();,0
TODO Auto-generated method stub,1
"LOG.info(""original float length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));",0
"LOG.info(""original double length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));",0
"LOG.info(""parsed float length: "" + length + Arrays.toString(arr));",0
"LOG.info(""parsed double length: "" + length + Arrays.toString(arr));",0
"ground truth: positive, precision: positive",0
start row index for words,0
start row index for docs,0
doc ids,0
topic assignments,0
word to docs reverse index,0
count word,0
build word start index,0
build word to doc reverse idx,0
build dks,0
dks = new TraverseHashMap[n_docs];,0
for (int d = 0; d < n_docs; d++) {,0
if (K < Short.MAX_VALUE) {,0
if (docs.get(d).len < Byte.MAX_VALUE),0
dks[d] = new S2BTraverseMap(docs.get(d).len);,0
if (docs.get(d).len < Short.MAX_VALUE),0
"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));",0
else,0
"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));",0
} else {,0
"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));",0
},0
},0
build dks,0
allocate update maps,0
Skip if no token for this word,0
Check whether error when fetching word-topic,0
Build FTree for current word,0
current doc,0
old topic assignment,0
"Check if error happens. if this happen, it's probably that failures happen to servers.",0
We need to adjust the memory settings or network fetching parameters.,0
Update statistics if needed,0
Calculate psum and sample new topic,0
Update statistics if needed,0
Assign new topic,0
Skip if no token for this word,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
Write #rows,0
Write each row,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
dense,0
sparse,0
calculate columns,0
loss function,0
gradient and hessian,0
"categorical feature set, null: none, empty: all, else: partial",0
"node's end index in instancePos, instances in [start, end] belong to a tree node",0
initialize the phase,0
current tree and depth,0
create loss function,0
calculate grad info of each instance,0
"create data sketch, push candidate split value to PS",0
1. calculate candidate split value,0
categorical features,0
2. push local sketch to PS,0
the leader worker,0
merge categorical features,0
create updates,0
"pull the global sketch from PS, only called once by each worker",0
number of categorical feature,0
sample feature,0
push sampled feature set to the current tree,0
create new tree,0
"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,",0
calculate gradient,0
"1. create new tree, initialize tree nodes and node stats",0
"2. initialize feature set, if sampled, get from PS, otherwise use all the features",0
2.1. pull the sampled features of the current tree,0
this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();,0
"2.2. if use all the features, only called one",0
"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle",0
4. set root node to active,0
"5. reset instance position, set the root node's span",0
6. calculate gradient,0
1. decide nodes that should be calculated,0
2. decide calculated and subtracted tree nodes,0
3. calculate threads,0
wait until all threads finish,0
4. subtract threads,0
wait until all threads finish,0
5. send histograms to PS,0
6. update histogram cache,0
clock,0
find split,0
"1. find responsible tree node, using RR scheme",0
2. pull gradient histogram,0
2.1. get the name of this node's gradient histogram on PS,0
2.2. pull the histogram,0
2.3. find best split result of this tree node,0
2.3.1 using server split,0
"update the grad stats of the root node on PS, only called once by leader worker",0
update the grad stats of children node,0
update the left child,0
update the right child,0
"2.3.2 the updated split result (tree node/feature/value/gain) on PS,",0
"2.3.3 otherwise, the returned histogram contains the gradient info",0
"2.3.4 the updated split result (tree node/feature/value/gain) on PS,",0
2.3.5 reset this tree node's gradient histogram to 0,0
3. push split feature to PS,0
4. push split value to PS,0
5. push split gain to PS,0
6. set phase to AFTER_SPLIT,0
this.phase = GBDTPhase.AFTER_SPLIT;,0
clock,0
1. get split feature,0
2. get split value,0
3. get split gain,0
4. get node weight,0
5. split node,0
update local replica,0
create AfterSplit task,0
"2. check thread stats, if all threads finish, return",0
6. clock,0
"split the span of one node, reset the instance position",0
in case this worker has no instance on this node,0
set the span of left child,0
set the span of right child,0
"1. left to right, find the first instance that should be in the right child",0
"2. right to left, find the first instance that should be in the left child",0
3. swap two instances,0
4. find the cut pos,0
5. set the span of left child,0
6. set the span of right child,0
set tree node to active,0
set node to leaf,0
set node to inactive,0
finish current depth,0
finish current tree,0
set the tree phase,0
check if there is active node,0
check if finish all the tree,0
update node's grad stats on PS,0
"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split",0
the root node's stats is updated by leader worker,0
1. create the update,0
2. push the update to PS,0
1. update predictions of training data,0
2. update predictions of validation data,0
the leader task adds node prediction to flush list,0
1. name of this node's grad histogram on PS,0
2. build the grad histogram of this node,0
3. push the histograms to PS,0
4. reset thread stats to finished,0
5.1. set the children nodes of this node,0
5.2. set split info and grad stats to this node,0
5.2. create children nodes,0
"5.3. create node stats for children nodes, and add them to the tree",0
5.4. reset instance position,0
"5.5. add new active nodes if possible, inc depth, otherwise finish this tree",0
5.6. set children nodes to leaf nodes,0
5.7. set nid to leaf node,0
5.8. deactivate active node,0
"get feature type, 0:empty 1:all equal 2:real",0
"if not -1, sufficient space will be allocated at once",0
copy the highest levels,0
copy baseBuffer,0
merge two non-empty quantile sketches,0
left child <= split value; right child > split value,0
"the first: minimal, the last: maximal",0
categorical features,0
continuous features,0
left child <= split value; right child > split value,0
feature index used to split,0
feature value used to split,0
loss change after split this node,0
grad stats of the left child,0
grad stats of the right child,0
"LOG.info(""Constructor with fid = -1"");",0
fid = -1: no split currently,0
the minimal split value is the minimal value of feature,0
the splits do not include the maximal value of feature,0
"1. the average distance, (maxValue - minValue) / splitNum",0
2. calculate the candidate split value,0
1. new feature's histogram (grad + hess),0
size: sampled_featureNum * (2 * splitNum),0
"in other words, concatenate each feature's histogram",0
2. get the span of this node,0
------ 3. using sparse-aware method to build histogram ---,0
"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances",0
3.1. get the instance index,0
3.2. get the grad and hess of the instance,0
3.3. add to the sum,0
3.4. loop the non-zero entries,0
3.4.1. get feature value,0
3.4.2. current feature's position in the sampled feature set,0
"int fPos = findFidPlace(this.controller.fSet, fid);",0
3.4.3. find the position of feature value in a histogram,0
"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]",0
3.4.4. add the grad and hess to the corresponding bin,0
3.4.5. add the reverse to the bin that contains 0.0f,0
4. add the grad and hess sum to the zero bin of all features,0
find the best split result of the histogram of a tree node,0
1. calculate the gradStats of the root node,0
"1.1. update the grad stats of the root node on PS, only called once by leader worker",0
2. loop over features,0
2.1. get the ture feature id in the sampled feature set,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"update the grad stats of the root node on PS, only called once by leader worker",0
3. update the grad stats of children node,0
3.1. update the left child,0
3.2. update the right child,0
find the best split result of one feature,0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
split value = sketches[splitIdx],0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
partition number,0
cols of each partition,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
"loop all the possible split value, start from split[0], the first item is the minimal feature value",0
assert fvalue >= sketch[start] && fvalue <= sketch[end];,0
if (left > end) return end - start;,0
find the best split result of the histogram of a tree node,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
find the best split result of one feature,0
1. set the feature id,0
splitEntry.setFid(fid);,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]",0
the task use index to find fvalue,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting",0
"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking.",0
max and min of each feature,0
clear all the information,0
calculate the sum of gradient and hess,0
"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,",0
ridx),0
check if necessary information is ready,0
"same as add, reduce is used in All Reduce",0
"features used in this tree, if equals null, means use all the features without sampling",0
node in the tree,0
the gradient info of each instances,0
initialize nodes,0
gradient,0
second order gradient,0
int sendStartCol = (int) row.getStartCol();,0
logistic loss for binary classification task.,0
"logistic loss, but predict un-transformed margin",0
check if label in range,0
return the default evaluation metric for the objective,0
"task type: classification, regression, or ranking",0
"quantile sketch, size = featureNum * splitNum",0
"gradient histograms, size = treeNodeNum * featureNum * splitNum",0
"active tree nodes, size = pow(2, treeDepth) -1",0
sampled features. size = treeNum * sampleRatio * featureNum,0
categorical feature. size = workerNum * cateFeatNum * splitNum,0
"split features, size = treeNum * treeNodeNum",0
"split values, size = treeNum * treeNodeNum",0
"split gains, size = treeNum * treeNodeNum",0
"node weights, size = treeNum * treeNodeNum",0
"node preds, size = treeNum * treeNodeNum",0
if using PS to perform split,0
step size for a tree,0
number of class,0
minimum loss change required for a split,0
maximum depth of a tree,0
number of features,0
number of nonzero,0
number of candidates split value,0
----- the rest parameters are less important ----,0
base instance weight,0
minimum amount of hessian(weight) allowed in a child,0
L2 regularization factor,0
L1 regularization factor,0
default direction choice,0
maximum delta update we can add in weight estimation,0
this parameter can be used to stabilize update,0
default=0 means no constraint on weight delta,0
whether we want to do subsample for row,0
whether to subsample columns for each tree,0
accuracy of sketch,0
accuracy of sketch,0
leaf vector size,0
option for parallelization,0
option to open cacheline optimization,0
whether to not print info during training.,0
maximum depth of the tree,0
number of features used for tree construction,0
"minimum loss change required for a split, otherwise stop split",0
----- the rest parameters are less important ----,0
default direction choice,0
whether we want to do sample data,0
whether to sample columns during tree construction,0
whether to use histogram for split,0
number of histogram units,0
whether to print info during training.,0
----- the rest parameters are obtained after training ----,0
total number of nodes,0
number of deleted nodes */,0
@maxIndex: this variable contains the max index of node/word,0
values[b + offset] = (random.nextFloat() - 0.5f) / dimension;,0
some params,0
max index for node/word,0
compute number of nodes for one row,0
check the length of dot values,0
merge dot values from all partitions,0
Skip-Gram model,0
Negative sampling,0
used to accumulate the updates for input vectors,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
Skip-Gram model,0
Negative sampling,0
used to accumulate the updates for input vectors,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
update output,0
Some params,0
compute number of nodes for one row,0
window size,0
Skip-Gram model,0
Accumulate the input vectors from context,0
Negative sampling,0
used to accumulate the updates for input vectors,0
window size,0
skip-gram model,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
update output,0
some params,0
batch sentences,0
max index for node/word,0
compute number of nodes for one row,0
check the length of dot values,0
merge dot values from all partitions,0
locates the input vectors to local array to prevent randomly access,0
on the large server row.,0
fill 0 for context vector,0
window size,0
Continuous bag-of-words Models,0
Accumulate the input vectors from context,0
Calculate the partial dot values,0
We should guarantee here that the sample would not equal the ``word``,0
used to accumulate the context input vectors,0
locates the input vector into local arrays to prevent randomly access for,0
the large server row.,0
window size,0
while true to prevent sampling out a positive target,0
how to prevent the randomly access to the output vectors??,0
accumulate gradients for the input vectors,0
update output vectors,0
update input,0
update output,0
Some params,0
compute number of nodes for one row,0
// calculate bias,0
if (param.getPartKey().getStartCol() <= 0 && param.getPartKey().getEndCol() > 0) {,0
"double zVal = VectorUtils.getDouble(z, 0);",0
"double nVal = VectorUtils.getDouble(n, 0);",0
"VectorUtils.setFloat(w, 0, (float) (-1.0 * alpha * zVal / (beta + Math.sqrt(nVal))));",0
},0
Do nothing.,0
current word,0
neu1 stores the average value of input vectors in the context (CBOW),0
Continuous Bag-of-Words Model,0
Accumulate the input vectors from context,0
negative sampling,0
Using the sigmoid value from the pre-computed table,0
accumulate for the hidden layer,0
update output layer,0
add the counter for target,0
update hidden layer,0
Update the input vector for each word in the context,0
add the counter to input,0
update input layers,0
update output layers,0
for (int a = 0; a < layers.length; a++) deltas[a] = layers[a] - deltas[a];,0
copy the highest levels,0
copy baseBuffer,0
merge two non-empty quantile sketches,0
"if not -1, sufficient space will be allocated at once",0
InstanceRow ins = instanceRows[insId];,0
int[] indices = ins.indices();,0
int[] bins = ins.bins();,0
int nnz = indices.length;,0
for (int j = 0; j < nnz; j++) {,0
int fid = indices[j];,0
if (isFeatUsed[fid - featLo]) {,0
"histograms[fid - featLo].accumulate(bins[j], gradPairs[insId]);",0
},0
},0
1. allocate histogram,0
"2. loop non-zero instances, accumulate to histogram",0
if (nnz <= nodeEnd - nodeStart + 1) { // loop all nnz of current feature,0
3. add remaining grad and hess to default bin,0
"return param.calcWeights(grad, hess);",0
"numClass is usually small, so we do not use arraycopy here",0
"numClass is usually small, so we do not use arraycopy here",0
TODO: use more schema on default bin,1
1. set default bin to left child,0
"2. for other bins, find its location",0
3. create split set,0
this.nodeStats = new GBTNodeStat[numClass == 2 ? 1 : numClass];,0
predict sparse instance with indices and values,0
predict libsvm data,0
"Preconditions.checkArgument(preds.length == labels.length,",0
"""LogLossMetric should be used for binary-label classification"");",0
double loss = 0.0;,0
for (int i = 0; i < preds.length; i++) {,0
"loss += evalOne(preds[i], labels[i]);",0
},0
return loss / labels.length;,0
double error = 0.0;,0
if (preds.length == labels.length) {,0
for (int i = 0; i < preds.length; i++) {,0
"error += evalOne(preds[i], labels[i]);",0
},0
} else {,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"error += evalOne(pred, labels[i]);",0
},0
},0
return error / labels.length;,0
Preconditions.checkArgument(preds.length != labels.length,0
"&& preds.length % labels.length == 0,",0
"""CrossEntropyMetric should be used for multi-label classification"");",0
double loss = 0.0;,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"loss += evalOne(pred, labels[i]);",0
},0
return loss / labels.length;,0
double correct = 0.0;,0
if (preds.length == labels.length) {,0
for (int i = 0; i < preds.length; i++) {,0
"correct += evalOne(preds[i], labels[i]);",0
},0
} else {,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"correct += evalOne(pred, labels[i]);",0
},0
},0
return (float) (correct / labels.length);,0
double errSum = 0.0f;,0
if (preds.length == labels.length) {,0
for (int i = 0; i < preds.length; i++) {,0
"errSum += evalOne(preds[i], labels[i]);",0
},0
} else {,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"errSum += evalOne(pred, labels[i]);",0
},0
},0
return Math.sqrt(errSum / labels.length);,0
"System.out.println(""----------"");",0
"System.out.println(""read buffer = "" + Integer.toBinaryString(readBufferT & 0b11111111)",0
"+ "", mask = "" + Integer.toBinaryString(readMaskT));",0
readMaskT <<= 1;,0
"boolean[] bits = new boolean[]{true, true, false, false, true, false, false, false, true, true, false, true};",0
int n = bits.length;,0
BufferedBitSet writeBitSet = new BufferedBitSet(n);,0
"BufferedBitSet readBitSet = new BufferedBitSet(writeBitSet.getBytes(), n);",0
if (bitSet.get(i) != bits[i]) {,0
"throw new RuntimeException("""" + i);",0
},0
private final ByteBuffer bytes;,0
"public BufferedBitSetReader(ByteBuffer bytes, int numBits) {",0
int capacity = bytes.capacity() * 8;,0
readIndexT = bytes.capacity() - 1;,0
return bytes.get(index);,0
TODO: use arraycopy to make it faster,1
assert from >= this.from && to <= this.to;,0
"LOG.debug(String.format(""Create subset: [%d-%d]"", newFrom, newTo));",0
"LOG.debug(String.format(""Get overlap: [%d-%d]"", newFrom, newTo));",0
return bits.clone();,0
private final SerializableBuffer bytes;,0
private final ByteBuffer bytes;,0
this.bytes = ByteBuffer.allocate(numBytes);,0
public BufferedBitSetWriter(ByteBuffer bytes) {,0
this.bytes = bytes;,0
},0
"bytes.put(writeIndex++, (byte) writeBuffer);",0
public ByteBuffer getBytes() {,0
return bytes;,0
},0
ML TreeConf,0
GBDT TreeConf,0
"different types of tree node splits, enumerated by their complexity",0
"in order to reduce model size, we give priority to split point",0
"comparison between two split points, we give priority to lower feature index",0
TODO: comparison between two split sets,0
"public boolean leafwise;  // true if leaf-wise training, false if level-wise training",0
TODO: regularization,1
TODO: regularization,1
public float insSampleRatio;  // subsample ratio for instances,0
set basic configuration keys,0
use local deploy mode and dummy data spliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
import jdk.nashorn.internal.runtime.regexp.joni.Config;,0
"paras[1] = ""abc"";",0
"paras[2] = ""123"";",0
Add standard Hadoop classes,0
Feature number of train data,0
Total iteration number,0
Learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set input data path,0
Set save model path,0
Set actionType train,0
QSLRRunner runner = new QSLRRunner();,0
runner.train(conf);,0
"conf.setInt(AngelConf.ANGEL_STALENESS, -1);",0
Dataset,0
"Data format, libsvm or dummy",0
class number,0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set Softmax algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
Feature number of train data,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Ratio of validation,0
Learning rate,0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set DeepFM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
Model type,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set FM algorithm parameters,0
Set model class,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set WideAndDeep algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
"Set LDA parameters #V, #K",0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set LDA algorithm parameters,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set SVM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Load model meta,0
Convert model,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Convert model,0
load hadoop configuration,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model,0
load hadoop configuration,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
https://blog.csdn.net/cocoonyang/article/details/63068108,0
v1[i] = v1[i] + da * v2[i];,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
v1[i] = v1[i] + da * v2[i];,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
Default does nothing.,0
The app injection is optional,0
"renderText(""hello world"");",0
"user choose a workerGroupID from the workergroups page,",0
now we should change the AngelApp params and render the workergroup page;,0
"static final String WORKER_ID = ""worker.id"";",0
"div(""#logo"").",0
"img(""/static/hadoop-st.png"")._().",0
import org.apache.hadoop.yarn.webapp.view.FooterBlock;,0
import org.apache.hadoop.yarn.webapp.view.HeaderBlock;,0
JQueryUI.jsnotice(html);,0
import org.apache.hadoop.conf.Configuration;,0
import java.lang.reflect.Field;,0
all the files in input set,0
Shuffle the file,0
Get the blocks for all files,0
Adjust the maxSize to make the split more balanced,0
Handle the splittable files,0
Handle the unsplittable files,0
Split the blocks,0
"If the remaining size of the current block is smaller than the required size,",0
the remaining blocks are divided into the current split,0
Update current split length and move to next block,0
Clear the current block offset,0
"Current split length is > maxSize, split the block and generate a new split",0
Clear blocks list for next split,0
Clear the current split length,0
"If splitBlocks is not empty, just genetate a split for it",0
get block locations from file system,0
create an input split,0
get block locations from file system,0
create a list of all block and their locations,0
"if the file is not splitable, just create the one block with",0
full file length,0
each split can be a maximum of maxSize,0
if remainder is between max and 2*max - then,0
"instead of creating splits of size max, left-max we",0
create splits of size left/2 and left/2. This is,0
a heuristic to avoid creating really really small,0
splits.,0
add this block to the block --> node locations map,0
"For blocks that do not have host/rack information,",0
assign to default  rack.,0
add this block to the rack --> block map,0
Add this host to rackToNodes map,0
add this block to the node --> block map,0
"if the file system does not have any rack information, then",0
use dummy rack location.,0
The topology paths have the host name included as the last,0
component. Strip it.,0
get tokens for all the required FileSystems..,0
"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,",0
job.getConfiguration());,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
all the files in input set,0
it is allowed for maxSize to be 0. Disable smoothing load for such cases,0
process all nodes and create splits that are local to a node. Generate,0
"one split per node iteration, and walk over nodes multiple times to",0
distribute the splits across nodes.,0
Skip the node if it has previously been marked as completed.,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
Remove all blocks which may already have been assigned to other,0
splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
Remove entries from blocksInNode so that we don't walk these,0
again.,0
Done creating a single split for this node. Move on to the next,0
node so that splits are distributed across nodes.,0
This implies that the last few blocks (or all in case maxSize=0),0
were not part of a split. The node is complete.,0
if there were any blocks left over and their combined size is,0
"larger than minSplitNode, then combine them into one split.",0
Otherwise add them back to the unprocessed pool. It is likely,0
that they will be combined with other blocks from the,0
same rack later on.,0
This condition also kicks in when max split size is not set. All,0
blocks on a node will be grouped together into a single split.,0
haven't created any split on this machine. so its ok to add a,0
smaller one for parallelism. Otherwise group it in the rack for,0
balanced size create an input split and add it to the splits,0
array,0
Remove entries from blocksInNode so that we don't walk this again.,0
The node is done. This was the last set of blocks for this node.,0
Put the unplaced blocks back into the pool for later rack-allocation.,0
Node is done. All blocks were fit into node-local splits.,0
Check if node-local assignments are complete.,0
All nodes have been walked over and marked as completed or all blocks,0
have been assigned. The rest should be handled via rackLock assignment.,0
"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """,0
"+ completedNodes.size() + "", size left: "" + totalLength);",0
"if blocks in a rack are below the specified minimum size, then keep them",0
"in 'overflow'. After the processing of all racks is complete, these",0
overflow blocks will be combined into splits.,0
Process all racks over and over again until there is no more work to do.,0
Create one split for this rack before moving over to the next rack.,0
Come back to this rack after creating a single split for each of the,0
remaining racks.,0
"Process one rack location at a time, Combine all possible blocks that",0
reside on this rack as one split. (constrained by minimum and maximum,0
split size).,0
iterate over all racks,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"if we created a split, then just go to the next rack",0
"if there is a minimum size specified, then create a single split",0
"otherwise, store these blocks into overflow data structure",0
There were a few blocks in this rack that,0
remained to be processed. Keep them in 'overflow' block list.,0
These will be combined later.,0
Process all overflow blocks,0
"This might cause an exiting rack location to be re-added,",0
but it should be ok.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"Process any remaining blocks, if any.",0
create an input split,0
add this split to the list that is returned,0
long num = totLength / maxSize;,0
all blocks for all the files in input set,0
mapping from a rack name to the list of blocks it has,0
mapping from a block to the nodes on which it has replicas,0
mapping from a node to the list of blocks that it contains,0
populate all the blocks for all files,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
"Security framework already loaded the tokens into current UGI, just use",0
them,0
Now remove the AM->RM token so tasks don't have it,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init container allocator,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
Init Client manager,0
Init PS Client manager,0
init parameter server manager,0
recover task information if needed,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
Init model saver & loader,0
start a web service if use yarn deploy mode,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
Check Workers,0
Check PSS,0
Check Clients,0
Check PS Clients,0
parse parameter server counters,0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
refresh last heartbeat timestamp,0
send a state update event to the specific PSAttempt,0
Check is there save request,0
Check is there load request,0
check matrix metadata inconsistencies between master and parameter server.,0
"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix",0
"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix",0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
choose a unused port,0
start RPC server,0
remove this parameter server attempt from monitor set,0
remove this parameter server attempt from monitor set,0
"if worker attempt id is not in monitor set, we should shutdown it",0
find workergroup in worker manager,0
"if this worker group does not initialized, just return WORKERGROUP_NOTREADY",0
"if this worker group run over, just return WORKERGROUP_EXITED",0
"if this worker group is running now, return tasks, workers, data splits for it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update the clock for this matrix,0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update task iteration,0
"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);",0
"Calculate how many splits we need. As each task handles a separate split of data, so we want",0
the number of splits equal to the number of tasks,0
split data,0
dispatch the splits to workergroups,0
split data,0
dispatch the splits to workergroups,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
write meta data to a temporary file,0
rename the temporary file to final file,0
"if the file exists, read from file and deserialize it",0
write task meta,0
write ps meta,0
generate a temporary file,0
write task meta to the temporary file first,0
rename the temporary file to the final file,0
"if last final task file exist, remove it",0
find task meta file which has max timestamp,0
"if the file does not exist, just return null",0
read task meta from file and deserialize it,0
generate a temporary file,0
write ps meta to the temporary file first.,0
rename the temporary file to the final file,0
"if the old final file exist, just remove it",0
find ps meta file,0
"if ps meta file does not exist, just return null",0
read ps meta from file and deserialize it,0
"Path tmpPath = new Path(new Path(context.getConf().get(AngelConf.ANGEL_JOB_TMP_OUTPUT_PATH)),",0
String.valueOf(requestId));,0
Path tmpPath = HdfsUtil.toTmpPath(new Path(saveContext.getSavePath()));,0
saveContext.setTmpSavePath(tmpPath.toString());,0
Filter old epoch trigger first,0
Split the user request to sub-requests to pss,0
Init matrix files meta,0
Move output files,0
Write the meta file,0
Split the user request to sub-requests to pss,0
check whether psagent heartbeat timeout,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
LocalStorageToken is needed irrespective of whether security is enabled,0
or not.,0
"TokenCache.setJobToken(jobToken, taskCredentials);",0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Construct the actual Container,0
The null fields are per-container and will be constructed for each,0
container separately.,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
"a * in the classpath will only find a .jar, so we need to filter out",0
all .jars and add everything else,0
Propagate the system classpath when using the mini cluster,0
Add standard Hadoop classes,0
Add mr,0
Cache archives,0
Cache files,0
Sanity check,0
Add URI fragment or just the filename,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Final commmand,0
"if amTask is not null, we should clone task state from it",0
"if all parameter server complete commit, master can commit now",0
check whether parameter server heartbeat timeout,0
Transitions from the NEW state.,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event",0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event dispatched before",0
PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.COMMITTING state,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
reqeuest resource:send a resource request to the resource allocator,0
"Once the resource is applied, build and send the launch request to the container launcher",0
deallocator the resource of the ps attempt:send a resource deallocator request to the,0
resource allocator,0
set the launch time,0
add the ps attempt to the heartbeat timeout monitoring list,0
parse ps attempt location and put it to location manager,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt",0
or failed,0
remove ps attempt id from heartbeat timeout monitor list,0
release container:send a release request to container launcher,0
set the finish time only if launch time is set,0
private long scheduledTime;,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
Refresh ps location & matrix meta,0
start a new attempt for this ps,0
notify ps manager,0
"getContext().getLocationManager().setPsLocation(id, null);",0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
notify the event handler of state change,0
"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor",0
"if forcedState is set, just return",0
else get state from state machine,0
add this worker group to the success set,0
check if all worker group run or run over,0
add this worker group to the success set,0
check if all worker group run over,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
calculate the actual number of worker groups and the total number of tasks based on the number of data split,0
"init all tasks , workers and worker groups and put them to the corresponding maps",0
just return the total task number now,0
TODO,1
check whether worker heartbeat timeout,0
"if workerAttempt is not null, we should clone task state from it",0
from NEW state,0
from SCHEDULED state,0
get data splits location for data locality,0
reqeuest resource:send a resource request to the resource allocator,0
"once the resource is applied, build and send the launch request to the container launcher",0
notify failed message to the worker,0
notify killed message to the worker,0
release the allocated container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
release the allocated container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
clean the container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
record the finish time,0
clean the container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listening list,0
record the finish time,0
"if the worker attempt launch successfully, add it to heartbeat timeout listening list",0
set worker attempt location,0
notify the register message to the worker,0
record the launch time,0
update worker attempt metrics,0
update tasks metrics,0
clean the container,0
notify the worker attempt run successfully message to the worker,0
record the finish time,0
init a worker attempt for the worker,0
schedule the worker attempt,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
"If we need Yarn to restart a new application master, we should not unregister from Yarn RM",0
register to Yarn RM,0
send heartbeat to Yarn RM every rmPollInterval milliseconds,0
"catch YarnRuntimeException, we should exit and need not retry",0
build heartbeat request,0
send heartbeat request to rm,0
"This can happen if the RM has been restarted. If it is in that state,",0
this application must clean itself up.,0
Setting NMTokens,0
assgin containers,0
"if some container is not assigned, release them",0
handle finish containers,0
dispatch container exit message to corresponding components,0
killed by framework,0
killed by framework,0
get application finish state,0
build application diagnostics,0
TODO:add a job history for angel,1
build unregister request,0
send unregister request to rm,0
Note this down for next interaction with ResourceManager,0
based on blacklisting comments above we can end up decrementing more,0
than requested. so guard for that.,0
send the updated resource request to RM,0
send 0 container count requests also to cancel previous requests,0
Update resource requests,0
try to assign to all nodes first to match node local,0
try to match all rack local,0
assign remaining,0
Update resource requests,0
send the container-assigned event to task attempt,0
build the start container request use launch context,0
send the start request to Yarn nm,0
send the message that the container starts successfully to the corresponding component,0
"after launching, send launched event to task attempt to move",0
it from ASSIGNED to RUNNING state,0
send the message that the container starts failed to the corresponding component,0
kill the remote container if already launched,0
start a thread pool to startup the container,0
See if we need up the pool size only if haven't reached the,0
maximum limit yet.,0
nodes where containers will run at *this* point of time. This is,0
*not* the cluster size and doesn't need to be.,0
"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the",0
later is just a buffer so we are not always increasing the,0
pool-size,0
the events from the queue are handled in parallel,0
using a thread pool,0
return if already stopped,0
shutdown any containers that might be left running,0
Add one sync matrix,0
addSyncMatrix();,0
"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {",0
"LOG.info(""ps id = "" + psEntry.getKey());",0
"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();",0
"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {",0
"LOG.info(""matrix id = "" + metaEntry.getKey());",0
"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());",0
},0
},0
get matrix ids in the parameter server report,0
get the matrices parameter server need to create and delete,0
"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix",0
"if a matrix exists on master but not exist on parameter server, this parameter server need build it.",0
Init control connection manager,0
Get ps locations from master and put them to the location cache.,0
Build and initialize rpc client to master,0
Get psagent id,0
Build PS control rpc client manager,0
Build local location,0
Initialize matrix meta information,0
Start all services,0
Stop all modules,0
Stop all modules,0
clock first,0
wait,0
Update generic resource counters,0
Updating resources specified in ResourceCalculatorProcessTree,0
Remove the CPU time consumed previously by JVM reuse,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Plus a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Update a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get values from pss use row/column indices,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"/ PSF get/update, use can implement their own psf",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get a row or a batch of rows,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
Just return,0
Just return,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
TODO:,1
Generate a flush request and put it to request queue,0
Generate a clock request and put it to request queue,0
Generate a merge request and put it to request queue,0
Generate a merge request and put it to request queue,0
"If the matrix op log cache does not exist for the matrix, create a new one for the",0
matrix,0
and add it to cache maps,0
Add the message to the tree map,0
"If there are flush / clock requests blocked, we need to put this merge request into",0
the waiting queue,0
Launch a merge worker to merge the update to matrix op log cache,0
Remove the message from the tree map,0
Wake up blocked flush/clock request,0
Add flush/clock request to listener list to waiting for all the existing,0
updates are merged,0
Wake up blocked flush/clock request,0
"If all updates are merged for this matrix, we need wake up flush/clock requests which are",0
blocked.,0
Get next merge message sequence id,0
Wake up listeners(flush/clock requests) that have little sequence id than current merge,0
position,0
Wake up blocked merge requests,0
Get minimal sequence id from listeners,0
"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we",0
should flush updates to local matrix storage,0
unused now,1
TODO:,1
Doing average or not,0
Filter un-important update,0
Split this row according the matrix partitions,0
Set split context,0
Remove the row from matrix,0
buf.writeDouble(0.0);,0
TODO,1
TODO: write map default value,1
buf.writeDouble(0);,0
TODO:,1
TODO:,1
TODO:,1
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"LOG.error(""channel "" + ctx.channel() + "" inactive"");",0
"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());",0
"LOG.error(""put response message queue failed "", e);",0
Use Epoll for linux,0
Update location table,0
Remove the server from failed list,0
Notify refresh success message to request dispatcher,0
Check PS exist or not,0
Check heartbeat timeout,0
Check PS restart or not,0
private final HashSet<ParameterServerId> refreshingServerSet;,0
Add it to failed rpc list,0
Add the server to gray server list,0
Add it to failed rpc list,0
Add the server to gray server list,0
Move from gray server list to failed server list,0
Handle the RPCS to this server,0
Submit the schedulable failed get RPCS,0
Submit new get RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"If the queue is empty, just return 0",0
"If request is not over limit, just submit it",0
Submit the schedulable failed get RPCS,0
Submit new put RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"LOG.info(""choose put server "" + psIds[index]);",0
Check all pending RPCS,0
Check get channel context,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
channelManager.printPools();,0
"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {",0
if(ts - entry.getValue() > requestTimeOut * 2)  {,0
"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())",0
"+ "" milliseconds, close all channels to it"");",0
closeChannels(entry.getKey());,0
"psLocToLastChannelTsMap.put(entry.getKey(), ts);",0
},0
},0
"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);",0
Remove all pending RPCS,0
Close all channel to this PS,0
private Channel getChannel(Location loc) throws Exception {,0
"return channelManager.getOrCreateChannel(new Location(loc.getIp(), loc.getPort() + 1));",0
},0
private GenericObjectPool<Channel> getChannelPool(Location loc) throws InterruptedException {,0
"return channelManager.getOrCreateChannelPool(new Location(loc.getIp(), loc.getPort() + 1), PSAgentContext",0
.get(),0
.getConf(),0
".getInt(AngelConf.ANGEL_WORKER_TASK_NUMBER,",0
AngelConf.DEFAULT_ANGEL_WORKER_TASK_NUMBER));,0
},0
"LOG.error(""send request "" + request + "" is interrupted"");",0
"LOG.error(""send request "" + request + "" failed, "", e);",0
Get server id and location for this request,0
"If location is null, means that the server is not ready",0
Get the channel for the location,0
Check if need get token first,0
Serialize the request,0
Send the request,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
request.getContext().setChannelPool(pool);,0
Allocate the bytebuf and serialize the request,0
find the partition request context from cache,0
"updateMatrixCache(request.getPartKey(), response.getPartition());",0
"updateMatrixCache(request.getPartKey(), response.getRowsSplit());",0
TODO,1
ServerRow rowSplit = PSAgentContext.get().getMatricesCache(),0
".getRowSplit(request.getPartKey().getMatrixId(), request.getPartKey(),",0
request.getRowIndex());,0
response.setRowSplit(rowSplit);,0
"updateMatrixCache(request.getPartKey(), response.getRowSplit());",0
"LOG.info(""user request id "" + request.getUserRequestId());",0
"LOG.info(""user request id "" + request.getUserRequestId());",0
TODO,1
"private void updateMatrixCache(PartitionKey partKey, ServerPartition partition) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, partition);",0
},0
"private void updateMatrixCache(PartitionKey partKey, ServerRow rowSplit) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowSplit);",0
},0
"private void updateMatrixCache(PartitionKey partKey, List<ServerRow> rowsSplit) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowsSplit);",0
},0
Get partitions for this row,0
Distinct get row requests,0
Need get from ps or storage/cache,0
"Switch to new request id, send a new request",0
First get this row from matrix storage,0
MatrixStorage matrixStorage =,0
PSAgentContext.get().getMatrixStorageManager().getMatrixStoage(matrixId);,0
TVector row = matrixStorage.getRow(rowIndex);,0
if (row != null && row.getClock() >= clock) {,0
result.set(row);,0
return row;,0
},0
Get row splits of this row from the matrix cache first,0
responseCache.addSubResponse(rowSplit);,0
"If the row split does not exist in cache, get it from parameter server",0
Wait the final result,0
Put it to the matrix cache,0
"matrixStorage.addRow(rowIndex, row);",0
Just wait result,0
Split the param use matrix partitions,0
Send request to PSS,0
Split the matrix oplog according to the matrix partitions,0
"If need update clock, we should send requests to all partitions",0
Send request to PSS,0
Filter the rowIds which are fetching now,0
Send the rowIndex to rpc dispatcher and return immediately,0
"LOG.info(""get row split use time="" + (System.currentTimeMillis() - startTs));",0
"LOG.info(""start to request "" + requestId);",0
"LOG.info(""start to request "" + requestId);",0
Split param use matrix partitons,0
"If all sub-results are received, just remove request and result cache",0
"LOG.info(""request = "" + request + "", cache = "" + cache);",0
"LOG.info(""start to merge "" + cache + "" for request "" + request);",0
"LOG.info(""psf get merge use time = "" + (System.currentTimeMillis() - startTs));",0
Split this row according the matrix partitions,0
Set split context,0
Split this row according the matrix partitions,0
Set split context,0
long startTs = System.currentTimeMillis();,0
"LOG.error(""combine use time = "" + (System.currentTimeMillis() - startTs));",0
Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition,0
Generate dispatch items and add them to the corresponding queues,0
Filter the rowIds which are fetching now,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {",0
return key1.getStartCol() < key2.getStartCol() ? -1 : 1;,0
});,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {",0
return key1.getStartCol() < key2.getStartCol() ? -1 : 1;,0
});,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Put the row split to the cache(row index to row splits map),0
"If all splits of the row are received, means this row can be merged",0
TODO,1
TODO,1
/////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO,1
buf.writeDouble(0);,0
TODO,1
///////////////////////////////////////////////////////////////////////////////////////////////,0
Now we just support pipelined row splits merging for dense type row,0
Pre-fetching is disable default,0
matrix id to clock map,0
"task index, it must be unique for whole application",0
Deserialize data splits meta,0
Get workers,0
Send request to every ps,0
Wait the responses,0
Update clock cache,0
if(syncNum % 1024 == 0) {,0
},0
"Use simple flow, do not use any cache",0
Get row from cache.,0
"if row clock is satisfy ssp staleness limit, just return.",0
Get row from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC mode, just get from pss.",0
"For BSP/SSP, get rows from storage/cache first",0
Get from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC, just get rows from pss.",0
no more retries.,0
calculate sleep time and return.,0
parse the i-th sleep-time,0
parse the i-th number-of-retries,0
calculateSleepTime may overflow.,0
"A few common retry policies, with no delays.",0
Read matrix meta from meta file,0
Save partitions to files use fork-join,0
Write the ps matrix meta to the meta file,0
matrix.startServering();,0
return;,0
Read matrix meta from meta file,0
Load partitions from file use fork-join,0
Read matrix meta from meta file,0
Sort partitions,0
int size = rows.length;,0
int size = rows.length;,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
close is a local operation and should finish within milliseconds; timeout just to be safe,0
response will be null for one way messages.,0
maxFrameLength = 2G,0
lengthFieldOffset = 0,0
lengthFieldLength = 8,0
"lengthAdjustment = -8, i.e. exclude the 8 byte length itself",0
"initialBytesToStrip = 8, i.e. strip out the length field itself",0
"return Objects.toStringHelper(this).add(""serial"", serial).add(""listSize"", datas.size())",0
.toString();,0
indicates whether this connection's life cycle is managed,0
See if we already have a connection (common case),0
create a unique lock for this RS + protocol (if necessary),0
get the RS lock,0
do one more lookup in case we were stalled above,0
Only create isa when we need to.,0
definitely a cache miss. establish an RPC for,0
this RS,0
Throw what the RemoteException was carrying.,0
check,0
every,0
minutes,0
TODO,1
failoverHandler,0
"The number of times this invocation handler has ever been failed over,",0
before this method invocation attempt. Used to prevent concurrent,0
failed method invocations from triggering multiple failover attempts.,0
Make sure that concurrent failed method invocations,0
only cause a,0
single actual fail over.,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
"LOG.info(""method "" + method.getName() + ""construct request time = """,0
+ (System.currentTimeMillis() - beforeConstructTs));,0
get an instance of the method arg type,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
Message (hand written code usually has only a single,0
argument),0
log any RPC responses that are slower than the configured,0
warn,0
response time or larger than configured warning size,0
"when tagging, we let TooLarge trump TooSmall to keep",0
output simple,0
note that large responses will often also be slow.,0
provides a count of log-reported slow responses,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
unexpected,0
"in the protobuf methods, args[1] is the only significant argument",0
for JSON encoding,0
base information that is reported regardless of type of call,0
Disable Nagle's Algorithm since we don't want packets to wait,0
Configure the event pipeline factory.,0
Make a new connection.,0
Remove all pending requests (will be canceled after relinquishing,0
write lock).,0
Cancel any pending requests by sending errors to the callbacks:,0
Close the channel:,0
Close the connection:,0
Shut down all thread pools to exit.,0
"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());",0
See NettyServer.prepareResponse for where we write out the response.,0
"It writes the call.id (int), a boolean signifying any error (and if",0
"so the exception name/trace), and the response bytes",0
Read the call id.,0
"When the stream is closed, protobuf doesn't raise an EOFException,",0
"instead, it returns a null message object.",0
"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +",0
System.currentTimeMillis());,0
"It would be good widen this to just Throwable, but IOException is what we",0
allow now,0
not implemented,1
not implemented,1
"track what RpcEngine is used by a proxy class, for stopProxy()",0
cache of RpcEngines by protocol,0
return the RpcEngine configured to handle a protocol,0
We only handle the ConnectException.,0
This is the exception we can't handle.,0
check if timed out,0
wait for retry,0
IGNORE,0
return the RpcEngine that handles a proxy object,0
The default implementation works synchronously,0
punt: allocate a new buffer & copy into it,0
Parse cmd parameters,0
load hadoop configuration,0
load angel system configuration,0
load user configuration:,0
load user config file,0
load command line parameters,0
load user job resource files,0
load ml conf file for graph based algorithm,0
load user job jar if it exist,0
Expand the environment variable,0
Add default fs(local fs) for lib jars.,0
"LOG.info(System.getProperty(""user.dir""));",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
a simple hdfs copy function assume src path and dest path are in same hdfs,0
and FileSystem object has same schema,0
"If out path exist , just remove it first",0
Create parent directory if not exist,0
Rename,0
"LOG.warn(""interrupted while sleeping"", ie);",0
public static String getHostname() {,0
try {,0
"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();",0
} catch (UnknownHostException uhe) {,0
},0
"return new StringBuilder().append("""").append(uhe).toString();",0
},0
"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {",0
String hostname = getHostname();,0
String classname = clazz.getSimpleName();,0
"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new",0
"StringBuilder().append(""Starting "").append(classname).toString(), new",0
"StringBuilder().append(""  host = "").append(hostname).toString(), new",0
"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new",0
"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new",0
"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));",0
,0
"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {",0
public void run() {,0
"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +",0
"this.val$classname + "" at "" + this.val$hostname}));",0
},0
});,0
},0
"We we interrupted because we're meant to stop? If not, just",0
continue ignoring the interruption,0
Recalculate waitTime.,0
// Begin delegation to Thread,0
// End delegation to Thread,0
instance submitter class,0
Obtain filename from path,0
Split filename to prexif and suffix (extension),0
Check if the filename is okay,0
Prepare temporary file,0
Prepare buffer for data copying,0
Open and check input stream,0
Open output stream and copy data between source file in JAR and the temporary file,0
"If read/write fails, close streams safely before throwing an exception",0
"Finally, load the library",0
little endian load order,0
tail,0
fallthrough,0
fallthrough,0
finalization,0
fmix(h1);,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
throw new AngelException(e);,0
JobStateProto jobState = report.getJobState();,0
Check need load matrices,0
Used for java code to get a AngelClient instance,0
Used for python code to get a AngelClient instance,0
load user job resource files,0
the leaf level file should be readable by others,0
the subdirs in the path should have execute permissions for,0
others,0
2.get job id,0
Credentials credentials = new Credentials();,0
4.copy resource files to hdfs,0
5.write configuration to a xml file,0
6.create am container context,0
7.Submit to ResourceManager,0
8.get app master client,0
Create a number of filenames in the JobTracker's fs namespace,0
add all the command line files/ jars and archive,0
first copy them to jobtrackers filesystem,0
should not throw a uri exception,0
should not throw an uri excpetion,0
set the timestamps of the archives and files,0
set the public/private visibility of the archives and files,0
get DelegationToken for each cached file,0
check if we do not need to copy the files,0
is jt using the same file system.,0
just checking for uri strings... doing no dns lookups,0
to see if the filesystems are the same. This is not optimal.,0
but avoids name resolution.,0
this might have name collisions. copy will throw an exception,0
parse the original path to create new path,0
check for ports,0
Write job file to JobTracker's fs,0
Setup resource requirements,0
Setup LocalResources,0
Setup security tokens,0
Setup the command to run the AM,0
Add AM user command opts,0
Final command,0
Setup the CLASSPATH in environment,0
"i.e. add { Hadoop jars, job jar, CWD } to classpath.",0
Setup the environment variables for Admin first,0
"Setup the environment variables (LD_LIBRARY_PATH, etc)",0
Parse distributed cache,0
Setup ContainerLaunchContext for AM container,0
Set up the ApplicationSubmissionContext,0
private volatile PS2PSPusherImpl ps2PSPusher;,0
TODO,1
Add tokens to new user so that it may execute its task correctly.,0
TODO,1
to exit,0
TODO,1
TODO,1
ps2PSPusher.recover(ProtobufUtil.convert(needRecoverParts.get(i)));,0
context.getSnapshotManager().processRecovery();,0
Recover PS from snapshot or load path,0
First check snapshot,0
Check load path setting,0
TODO,1
if(ps2PSPusher != null) {,0
ps2PSPusher.start();,0
},0
public PS2PSPusherImpl getPs2PSPusher() {,0
return ps2PSPusher;,0
},0
"LOG.error(""send response of request "" + requestToString(clientId, seqId) + "" failed "");",0
"LOG.error(""send response of request failed, request seqId="" + seqId + "", channel="" + ch, ex);",0
Release the input buffer,0
Release the input buffer,0
"1. handle the rpc, get the response",0
Release the input buffer,0
2. Serialize the response,0
Send the serialized response,0
Exception happened,0
write seq id,0
Just serialize the head,0
Exception happened,0
Allocate result buffer,0
Exception happened,0
Just serialize the head,0
Exception happened,0
Reset the response and allocate buffer again,0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this location, only master ps can accept the update",0
Check the partition state again,0
Start to put the update to the slave pss,0
TODO,1
"context.getPS2PSPusher().put(request, in, partLoc);",0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this partition, if not, just return failed",0
Start to put the update to the slave pss,0
TODO,1
return ServerState.GENERAL;,0
Use Epoll for linux,0
public String uuid;,0
public void setChannelPool(GenericObjectPool<Channel> channelPool) {,0
this.channelPool = channelPool;,0
},0
private final ParameterServer psServer;,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO:default value,1
buf.readDouble();,0
TODO,1
///////////////////////////////////////////////////////////////////////////////////////////////,0
"////// network io method, for model transform",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
public ObjectIterator<Long2FloatMap.Entry> getIter() {,0
return ((LongFloatVector) row).getStorage().entryIterator();,0
},0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
public ObjectIterator<Long2LongMap.Entry> getIter() {,0
return ((LongLongVector) row).getStorage().entryIterator();,0
},0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO: dynamic add/delete row,1
private final List<PartitionKey> partitionKeys;,0
"if col == -1, we use the start/end index to calculate range,",0
we use double to store the range value since two long minus might exceed the,0
range of long.,0
Use Epoll for linux,0
find the partition request context from cache,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
channelManager.removeChannelPool(loc);,0
Generate seq id,0
Create a RecoverPartRequest,0
Serialize the request,0
Change the seqId for the request,0
Serialize the request,0
"First check the state of the channels in the pool, if a channel is unused, just return",0
"If all channels are in use, create a new channel or wait",0
Create a new channel,0
"add the PSAgentContext,need fix",0
If col == -1 and start/end not set,0
start/end set,0
"for dense type, we need to set the colNum to set dim for vectors",0
"colNum set, start/end not set",0
Row number must > 0,0
"both set, check its valid",0
TODO:add more vector type,1
TODO : subDim set,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
write the max abs,0
---------------------------------------------------,0
---------------------------------------------------,0
---------------------------------------------------------------,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
-----------------,0
-----------------,0
-----------------,0
-----------------,0
-----------------,0
KeepStorage is guaranteed,0
"ignore the isInplace option, since v2 is dense",0
"the value in old storage can be changed safe, so switch a storage",0
"but user required keep storage, we can prevent rehash",0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
KeepStorage is guaranteed,0
we gauss dense storage is more efficient,0
v1Size < v2Size * Constant.sparseThreshold,0
KeepStorage is guaranteed,0
"ignore the isInplace option, since v2 is dense",0
"the value in old storage can be changed safe, so switch a storage",0
"but user required keep storage, we can prevent rehash",0
we gauss dense storage is more efficient,0
prevent rehash,0
KeepStorage is guaranteed,0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
dense preferred,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
sorted preferred,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"Transform mat1, generate a new matrix",0
Split the row indices of mat1Trans,0
Parallel execute use fork-join,0
"Get the sub-matrix of left matrix, split by row",0
"Transform mat1, generate a new matrix",0
Split the row indices of mat1Trans,0
Parallel execute use fork-join,0
"Get the sub-matrix of left matrix, split by row",0
"mat1 trans true, mat trans true",0
"mat1 trans true, mat trans false",0
"mat1 trans false, mat trans true, important",0
"mat1 trans false, mat trans false",0
"mat1 trans true, mat trans true",0
"mat1 trans true, mat trans false",0
"mat1 trans false, mat trans true, important",0
"mat1 trans false, mat trans false",0
"dger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)",0
"sger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)",0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
add dense double matrix,0
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
get configuration from config file,0
set localDir with enviroment set by nm.,0
get master location,0
init task manager and start tasks,0
start heartbeat thread,0
taskManager.assignTaskIds(response.getTaskidsList());,0
todo,1
"if worker timeout, it may be knocked off.",0
"SUCCESS, do nothing",0
heartbeatFailedTime = 0;,0
private KEY currentKey;,0
will be created,0
TODO Auto-generated method stub,1
Bitmap bitmap = new Bitmap();,0
int max = indexArray[size - 1];,0
byte [] bitIndexArray = new byte[max / 8 + 1];,0
for(int i = 0; i < size; i++){,0
int bitIndex = indexArray[i] >> 3;,0
int bitOffset = indexArray[i] - (bitIndex << 3);,0
switch(bitOffset){,0
case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;,0
case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;,0
case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;,0
case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;,0
case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;,0
case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;,0
case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;,0
case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;,0
},0
},0
"true, false",0
//////////////////////////////,0
Application Configs,0
//////////////////////////////,0
//////////////////////////////,0
Master Configs,0
//////////////////////////////,0
//////////////////////////////,0
Worker Configs,0
//////////////////////////////,0
//////////////////////////////,0
Task Configs,0
//////////////////////////////,0
//////////////////////////////,0
ParameterServer Configs,0
//////////////////////////////,0
////////////////// IPC //////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
Configs used to ANGEL_PS_PSAGENT running mode future.,0
model parse,0
Mark whether use pyangel or not.,0
private Configuration conf;,0
"Configuration that should be used in python environment, there should only be one",0
configuration instance in each Angel context.,0
Use private access means jconf should not be changed or modified in this way.,0
Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:,0
Do nothing,0
To-DO: add other ways to justify different value types,0
"This is so ugly, must re-implement by more elegance way",1
"Create python path which include angel's jars, the python directory in ANGEL_HOME,",0
and other files submitted by user.,0
Launch python process,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Data format,0
Feature number of train data,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Ratio of validation,0
Learning rate,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"Set angel resource, #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Set training data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set predict data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model type,0
String modelType = String.valueOf(RowType.T_FLOAT_DENSE);,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
"conf.set(MLConf.ML_MODEL_TYPE(), modelType);",0
"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);",0
predictTest();,0
"Data format, libsvm or dummy",0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set data format,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
class number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set log path,0
Set trainning data path,0
Set save model path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set training data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log save path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set testing data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Data is classification,0
Model is classification,0
Train batch number per epoch.,0
loss delta,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Data is classification,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
TODO: optimize int key indices,1
"System.out.println(""deserialize cols.length="" + nCols);",0
"System.out.print(""deserialize "");",0
"System.out.print(cols[c] + "" "");",0
System.out.println();,0
TODO Auto-generated method stub,1
"LOG.info(""original float length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));",0
"LOG.info(""original double length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));",0
"LOG.info(""parsed float length: "" + length + Arrays.toString(arr));",0
"LOG.info(""parsed double length: "" + length + Arrays.toString(arr));",0
"ground truth: positive, precision: positive",0
start row index for words,0
start row index for docs,0
doc ids,0
topic assignments,0
word to docs reverse index,0
count word,0
build word start index,0
build word to doc reverse idx,0
build dks,0
dks = new TraverseHashMap[n_docs];,0
for (int d = 0; d < n_docs; d++) {,0
if (K < Short.MAX_VALUE) {,0
if (docs.get(d).len < Byte.MAX_VALUE),0
dks[d] = new S2BTraverseMap(docs.get(d).len);,0
if (docs.get(d).len < Short.MAX_VALUE),0
"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));",0
else,0
"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));",0
} else {,0
"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));",0
},0
},0
build dks,0
allocate update maps,0
Skip if no token for this word,0
Check whether error when fetching word-topic,0
Build FTree for current word,0
current doc,0
old topic assignment,0
"Check if error happens. if this happen, it's probably that failures happen to servers.",0
We need to adjust the memory settings or network fetching parameters.,0
Update statistics if needed,0
Calculate psum and sample new topic,0
Update statistics if needed,0
Assign new topic,0
Skip if no token for this word,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
Write #rows,0
Write each row,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
dense,0
sparse,0
calculate columns,0
loss function,0
gradient and hessian,0
"categorical feature set, null: none, empty: all, else: partial",0
"node's end index in instancePos, instances in [start, end] belong to a tree node",0
initialize the phase,0
current tree and depth,0
create loss function,0
calculate grad info of each instance,0
"create data sketch, push candidate split value to PS",0
1. calculate candidate split value,0
categorical features,0
2. push local sketch to PS,0
the leader worker,0
merge categorical features,0
create updates,0
"pull the global sketch from PS, only called once by each worker",0
number of categorical feature,0
sample feature,0
push sampled feature set to the current tree,0
create new tree,0
"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,",0
calculate gradient,0
"1. create new tree, initialize tree nodes and node stats",0
"2. initialize feature set, if sampled, get from PS, otherwise use all the features",0
2.1. pull the sampled features of the current tree,0
this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();,0
"2.2. if use all the features, only called one",0
"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle",0
4. set root node to active,0
"5. reset instance position, set the root node's span",0
6. calculate gradient,0
1. decide nodes that should be calculated,0
2. decide calculated and subtracted tree nodes,0
3. calculate threads,0
wait until all threads finish,0
4. subtract threads,0
wait until all threads finish,0
5. send histograms to PS,0
6. update histogram cache,0
clock,0
find split,0
"1. find responsible tree node, using RR scheme",0
2. pull gradient histogram,0
2.1. get the name of this node's gradient histogram on PS,0
2.2. pull the histogram,0
2.3. find best split result of this tree node,0
2.3.1 using server split,0
"update the grad stats of the root node on PS, only called once by leader worker",0
update the grad stats of children node,0
update the left child,0
update the right child,0
"2.3.2 the updated split result (tree node/feature/value/gain) on PS,",0
"2.3.3 otherwise, the returned histogram contains the gradient info",0
"2.3.4 the updated split result (tree node/feature/value/gain) on PS,",0
2.3.5 reset this tree node's gradient histogram to 0,0
3. push split feature to PS,0
4. push split value to PS,0
5. push split gain to PS,0
6. set phase to AFTER_SPLIT,0
this.phase = GBDTPhase.AFTER_SPLIT;,0
clock,0
1. get split feature,0
2. get split value,0
3. get split gain,0
4. get node weight,0
5. split node,0
update local replica,0
create AfterSplit task,0
"2. check thread stats, if all threads finish, return",0
6. clock,0
"split the span of one node, reset the instance position",0
in case this worker has no instance on this node,0
set the span of left child,0
set the span of right child,0
"1. left to right, find the first instance that should be in the right child",0
"2. right to left, find the first instance that should be in the left child",0
3. swap two instances,0
4. find the cut pos,0
5. set the span of left child,0
6. set the span of right child,0
set tree node to active,0
set node to leaf,0
set node to inactive,0
finish current depth,0
finish current tree,0
set the tree phase,0
check if there is active node,0
check if finish all the tree,0
update node's grad stats on PS,0
"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split",0
the root node's stats is updated by leader worker,0
1. create the update,0
2. push the update to PS,0
1. update predictions of training data,0
2. update predictions of validation data,0
the leader task adds node prediction to flush list,0
1. name of this node's grad histogram on PS,0
2. build the grad histogram of this node,0
3. push the histograms to PS,0
4. reset thread stats to finished,0
5.1. set the children nodes of this node,0
5.2. set split info and grad stats to this node,0
5.2. create children nodes,0
"5.3. create node stats for children nodes, and add them to the tree",0
5.4. reset instance position,0
"5.5. add new active nodes if possible, inc depth, otherwise finish this tree",0
5.6. set children nodes to leaf nodes,0
5.7. set nid to leaf node,0
5.8. deactivate active node,0
"get feature type, 0:empty 1:all equal 2:real",0
"if not -1, sufficient space will be allocated at once",0
copy the highest levels,0
copy baseBuffer,0
merge two non-empty quantile sketches,0
left child <= split value; right child > split value,0
"the first: minimal, the last: maximal",0
categorical features,0
continuous features,0
left child <= split value; right child > split value,0
feature index used to split,0
feature value used to split,0
loss change after split this node,0
grad stats of the left child,0
grad stats of the right child,0
"LOG.info(""Constructor with fid = -1"");",0
fid = -1: no split currently,0
the minimal split value is the minimal value of feature,0
the splits do not include the maximal value of feature,0
"1. the average distance, (maxValue - minValue) / splitNum",0
2. calculate the candidate split value,0
1. new feature's histogram (grad + hess),0
size: sampled_featureNum * (2 * splitNum),0
"in other words, concatenate each feature's histogram",0
2. get the span of this node,0
------ 3. using sparse-aware method to build histogram ---,0
"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances",0
3.1. get the instance index,0
3.2. get the grad and hess of the instance,0
3.3. add to the sum,0
3.4. loop the non-zero entries,0
3.4.1. get feature value,0
3.4.2. current feature's position in the sampled feature set,0
"int fPos = findFidPlace(this.controller.fSet, fid);",0
3.4.3. find the position of feature value in a histogram,0
"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]",0
3.4.4. add the grad and hess to the corresponding bin,0
3.4.5. add the reverse to the bin that contains 0.0f,0
4. add the grad and hess sum to the zero bin of all features,0
find the best split result of the histogram of a tree node,0
1. calculate the gradStats of the root node,0
"1.1. update the grad stats of the root node on PS, only called once by leader worker",0
2. loop over features,0
2.1. get the ture feature id in the sampled feature set,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"update the grad stats of the root node on PS, only called once by leader worker",0
3. update the grad stats of children node,0
3.1. update the left child,0
3.2. update the right child,0
find the best split result of one feature,0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
split value = sketches[splitIdx],0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
partition number,0
cols of each partition,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
"loop all the possible split value, start from split[0], the first item is the minimal feature value",0
assert fvalue >= sketch[start] && fvalue <= sketch[end];,0
if (left > end) return end - start;,0
find the best split result of the histogram of a tree node,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
find the best split result of one feature,0
1. set the feature id,0
splitEntry.setFid(fid);,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]",0
the task use index to find fvalue,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting",0
"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking.",0
max and min of each feature,0
clear all the information,0
calculate the sum of gradient and hess,0
"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,",0
ridx),0
check if necessary information is ready,0
"same as add, reduce is used in All Reduce",0
"features used in this tree, if equals null, means use all the features without sampling",0
node in the tree,0
the gradient info of each instances,0
initialize nodes,0
gradient,0
second order gradient,0
int sendStartCol = (int) row.getStartCol();,0
logistic loss for binary classification task.,0
"logistic loss, but predict un-transformed margin",0
check if label in range,0
return the default evaluation metric for the objective,0
"task type: classification, regression, or ranking",0
"quantile sketch, size = featureNum * splitNum",0
"gradient histograms, size = treeNodeNum * featureNum * splitNum",0
"active tree nodes, size = pow(2, treeDepth) -1",0
sampled features. size = treeNum * sampleRatio * featureNum,0
categorical feature. size = workerNum * cateFeatNum * splitNum,0
"split features, size = treeNum * treeNodeNum",0
"split values, size = treeNum * treeNodeNum",0
"split gains, size = treeNum * treeNodeNum",0
"node weights, size = treeNum * treeNodeNum",0
"node preds, size = treeNum * treeNodeNum",0
if using PS to perform split,0
step size for a tree,0
number of class,0
minimum loss change required for a split,0
maximum depth of a tree,0
number of features,0
number of nonzero,0
number of candidates split value,0
----- the rest parameters are less important ----,0
base instance weight,0
minimum amount of hessian(weight) allowed in a child,0
L2 regularization factor,0
L1 regularization factor,0
default direction choice,0
maximum delta update we can add in weight estimation,0
this parameter can be used to stabilize update,0
default=0 means no constraint on weight delta,0
whether we want to do subsample for row,0
whether to subsample columns for each tree,0
accuracy of sketch,0
accuracy of sketch,0
leaf vector size,0
option for parallelization,0
option to open cacheline optimization,0
whether to not print info during training.,0
maximum depth of the tree,0
number of features used for tree construction,0
"minimum loss change required for a split, otherwise stop split",0
----- the rest parameters are less important ----,0
default direction choice,0
whether we want to do sample data,0
whether to sample columns during tree construction,0
whether to use histogram for split,0
number of histogram units,0
whether to print info during training.,0
----- the rest parameters are obtained after training ----,0
total number of nodes,0
number of deleted nodes */,0
@maxIndex: this variable contains the max index of node/word,0
values[b + offset] = (random.nextFloat() - 0.5f) / dimension;,0
some params,0
max index for node/word,0
compute number of nodes for one row,0
check the length of dot values,0
merge dot values from all partitions,0
Skip-Gram model,0
Negative sampling,0
used to accumulate the updates for input vectors,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
Skip-Gram model,0
Negative sampling,0
used to accumulate the updates for input vectors,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
update output,0
Some params,0
compute number of nodes for one row,0
window size,0
Skip-Gram model,0
Accumulate the input vectors from context,0
Negative sampling,0
used to accumulate the updates for input vectors,0
window size,0
skip-gram model,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
update output,0
some params,0
batch sentences,0
max index for node/word,0
compute number of nodes for one row,0
check the length of dot values,0
merge dot values from all partitions,0
locates the input vectors to local array to prevent randomly access,0
on the large server row.,0
fill 0 for context vector,0
window size,0
Continuous bag-of-words Models,0
Accumulate the input vectors from context,0
Calculate the partial dot values,0
We should guarantee here that the sample would not equal the ``word``,0
used to accumulate the context input vectors,0
locates the input vector into local arrays to prevent randomly access for,0
the large server row.,0
window size,0
while true to prevent sampling out a positive target,0
how to prevent the randomly access to the output vectors??,0
accumulate gradients for the input vectors,0
update output vectors,0
update input,0
update output,0
Some params,0
compute number of nodes for one row,0
// calculate bias,0
if (param.getPartKey().getStartCol() <= 0 && param.getPartKey().getEndCol() > 0) {,0
"double zVal = VectorUtils.getDouble(z, 0);",0
"double nVal = VectorUtils.getDouble(n, 0);",0
"VectorUtils.setFloat(w, 0, (float) (-1.0 * alpha * zVal / (beta + Math.sqrt(nVal))));",0
},0
Do nothing.,0
split updates,0
shuffle update splits,0
generate part update splits,0
"set split context: partition key, use int key for long key vector or net",0
how to do intersection for two dense vector with a given indices ??,0
current word,0
neu1 stores the average value of input vectors in the context (CBOW),0
Continuous Bag-of-Words Model,0
Accumulate the input vectors from context,0
negative sampling,0
Using the sigmoid value from the pre-computed table,0
accumulate for the hidden layer,0
update output layer,0
add the counter for target,0
update hidden layer,0
Update the input vector for each word in the context,0
add the counter to input,0
update input layers,0
update output layers,0
for (int a = 0; a < layers.length; a++) deltas[a] = layers[a] - deltas[a];,0
copy the highest levels,0
copy baseBuffer,0
merge two non-empty quantile sketches,0
"if not -1, sufficient space will be allocated at once",0
InstanceRow ins = instanceRows[insId];,0
int[] indices = ins.indices();,0
int[] bins = ins.bins();,0
int nnz = indices.length;,0
for (int j = 0; j < nnz; j++) {,0
int fid = indices[j];,0
if (isFeatUsed[fid - featLo]) {,0
"histograms[fid - featLo].accumulate(bins[j], gradPairs[insId]);",0
},0
},0
1. allocate histogram,0
"2. loop non-zero instances, accumulate to histogram",0
if (nnz <= nodeEnd - nodeStart + 1) { // loop all nnz of current feature,0
3. add remaining grad and hess to default bin,0
"return param.calcWeights(grad, hess);",0
"numClass is usually small, so we do not use arraycopy here",0
"numClass is usually small, so we do not use arraycopy here",0
TODO: use more schema on default bin,1
1. set default bin to left child,0
"2. for other bins, find its location",0
3. create split set,0
this.nodeStats = new GBTNodeStat[numClass == 2 ? 1 : numClass];,0
predict sparse instance with indices and values,0
predict libsvm data,0
"Preconditions.checkArgument(preds.length == labels.length,",0
"""LogLossMetric should be used for binary-label classification"");",0
double loss = 0.0;,0
for (int i = 0; i < preds.length; i++) {,0
"loss += evalOne(preds[i], labels[i]);",0
},0
return loss / labels.length;,0
double error = 0.0;,0
if (preds.length == labels.length) {,0
for (int i = 0; i < preds.length; i++) {,0
"error += evalOne(preds[i], labels[i]);",0
},0
} else {,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"error += evalOne(pred, labels[i]);",0
},0
},0
return error / labels.length;,0
Preconditions.checkArgument(preds.length != labels.length,0
"&& preds.length % labels.length == 0,",0
"""CrossEntropyMetric should be used for multi-label classification"");",0
double loss = 0.0;,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"loss += evalOne(pred, labels[i]);",0
},0
return loss / labels.length;,0
double correct = 0.0;,0
if (preds.length == labels.length) {,0
for (int i = 0; i < preds.length; i++) {,0
"correct += evalOne(preds[i], labels[i]);",0
},0
} else {,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"correct += evalOne(pred, labels[i]);",0
},0
},0
return (float) (correct / labels.length);,0
double errSum = 0.0f;,0
if (preds.length == labels.length) {,0
for (int i = 0; i < preds.length; i++) {,0
"errSum += evalOne(preds[i], labels[i]);",0
},0
} else {,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"errSum += evalOne(pred, labels[i]);",0
},0
},0
return Math.sqrt(errSum / labels.length);,0
"System.out.println(""----------"");",0
"System.out.println(""read buffer = "" + Integer.toBinaryString(readBufferT & 0b11111111)",0
"+ "", mask = "" + Integer.toBinaryString(readMaskT));",0
readMaskT <<= 1;,0
"boolean[] bits = new boolean[]{true, true, false, false, true, false, false, false, true, true, false, true};",0
int n = bits.length;,0
BufferedBitSet writeBitSet = new BufferedBitSet(n);,0
"BufferedBitSet readBitSet = new BufferedBitSet(writeBitSet.getBytes(), n);",0
if (bitSet.get(i) != bits[i]) {,0
"throw new RuntimeException("""" + i);",0
},0
private final ByteBuffer bytes;,0
"public BufferedBitSetReader(ByteBuffer bytes, int numBits) {",0
int capacity = bytes.capacity() * 8;,0
readIndexT = bytes.capacity() - 1;,0
return bytes.get(index);,0
TODO: use arraycopy to make it faster,1
assert from >= this.from && to <= this.to;,0
"LOG.debug(String.format(""Create subset: [%d-%d]"", newFrom, newTo));",0
"LOG.debug(String.format(""Get overlap: [%d-%d]"", newFrom, newTo));",0
return bits.clone();,0
private final SerializableBuffer bytes;,0
private final ByteBuffer bytes;,0
this.bytes = ByteBuffer.allocate(numBytes);,0
public BufferedBitSetWriter(ByteBuffer bytes) {,0
this.bytes = bytes;,0
},0
"bytes.put(writeIndex++, (byte) writeBuffer);",0
public ByteBuffer getBytes() {,0
return bytes;,0
},0
ML TreeConf,0
GBDT TreeConf,0
"edges=[x,...] firstFlow=1 => go to right if < x and go to left if > x",0
"edges=[x,...] firstFlow=0 => go to left if < x and go to right if > x",0
"different types of tree node splits, enumerated by their complexity",0
"in order to reduce model size, we give priority to split point",0
"comparison between two split points, we give priority to lower feature index",0
TODO: comparison between two split sets,0
"public boolean leafwise;  // true if leaf-wise training, false if level-wise training",0
TODO: regularization,1
TODO: regularization,1
public float insSampleRatio;  // subsample ratio for instances,0
set basic configuration keys,0
use local deploy mode and dummy data spliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighbor;,0
import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighborParam;,0
import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighborResult;,0
import com.tencent.angel.ml.matrix.psf.graph.adjacency.initneighbor.InitNeighbor;,0
import com.tencent.angel.ml.matrix.psf.graph.adjacency.initneighbor.InitNeighborParam;,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
@Test,0
"public void testInitAndGet() throws ExecutionException, InterruptedException {",0
Worker worker = LocalClusterContext.get().getWorker(worker0Attempt0Id).getWorker();,0
"MatrixClient client1 = worker.getPSAgent().getMatrixClient(""w2"", 0);",0
int matrixW1Id = client1.getMatrixId();,0
// Generate graph data,0
"Map<Integer, int []> adjMap = generateAdjTable(nodeNum, maxNeighborNum);",0
,0
// Init graph adj table,0
"InitNeighbor func = new InitNeighbor(new InitNeighborParam(matrixW1Id, adjMap));",0
client1.update(func);,0
,0
int [] nodeIds = new int[adjMap.size()];,0
int i = 0;,0
for(int nodeId : adjMap.keySet()) {,0
nodeIds[i++] = nodeId;,0
},0
,0
// Get graph adj table from PS,0
"GetNeighbor getFunc = new GetNeighbor(new GetNeighborParam(matrixW1Id, nodeIds, maxNeighborNum));",0
"Map<Integer, int[]> getResults = ((GetNeighborResult) (client1.get(getFunc)))",0
.getNodeIdToNeighborIndices();,0
,0
// Check the result,0
"for(Entry<Integer, int[]> entry : getResults.entrySet()) {",0
"Assert.assertArrayEquals(entry.getValue(), adjMap.get(entry.getKey()));",0
},0
},0
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
import jdk.nashorn.internal.runtime.regexp.joni.Config;,0
"paras[1] = ""abc"";",0
"paras[2] = ""123"";",0
Add standard Hadoop classes,0
Feature number of train data,0
Total iteration number,0
Learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set input data path,0
Set save model path,0
Set actionType train,0
QSLRRunner runner = new QSLRRunner();,0
runner.train(conf);,0
"conf.setInt(AngelConf.ANGEL_STALENESS, -1);",0
Dataset,0
"Data format, libsvm or dummy",0
class number,0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set Softmax algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
Feature number of train data,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Ratio of validation,0
Learning rate,0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set DeepFM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
Model type,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set FM algorithm parameters,0
Set model class,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set WideAndDeep algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
"Set LDA parameters #V, #K",0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set LDA algorithm parameters,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set SVM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Load model meta,0
Convert model,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Convert model,0
load hadoop configuration,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model,0
load hadoop configuration,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
worker register,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
attempt 0,0
attempt1,0
attempt1,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
set basic configuration keys,0
"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());",0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
Thread.sleep(5000);,0
"response = master.getJobReport(null, request);",0
"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);",0
"assertEquals(response.getJobReport().getCurIteration(), jobIteration);",0
"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
testDenseDoubleUDF();,0
testSparseDoubleUDF();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
testDenseDoubleUDF();,0
testSparseDoubleUDF();,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add sparse double matrix,0
add comp dense double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
testDenseDoubleUDF();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"LOG.info(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntFloatVector row = (IntFloatVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntLongVector row = (IntLongVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add sparse float matrix,0
siMat.setPartitionClass(CSRPartition.class);,0
siMat.setPartitionStorageClass(IntCSRStorage.class);,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);",0
get a angel client,0
add sparse float matrix,0
MatrixContext siMat = new MatrixContext();,0
siMat.setName(SPARSE_INT_MAT);,0
siMat.setRowType(RowType.T_ANY_INTKEY_SPARSE);,0
siMat.setRowNum(1);,0
siMat.setValidIndexNum(100);,0
siMat.setColNum(10000000000L);,0
siMat.setValueType(Node.class);,0
siMat.setPartitionStorageClass(LongElementMapStorage.class);,0
siMat.setPartitionClass(CSRPartition.class);,0
angelClient.addMatrix(siMat);,0
add sparse long-key double matrix,0
Start PS,0
Start to run application,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
import com.tencent.angel.psagent.consistency.SSPConsistencyController;,0
@RunWith(MockitoJUnitRunner.class),0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);",0
get a angel client,0
add matrix,0
psAgent.initAndStart();,0
test conf,0
test master location,0
test app id,0
test user,0
test ps agent attempt id,0
test connection,0
test master client,0
test ip,0
test loc,0
test master location,0
test ps location,0
"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));",0
test all ps ids,0
test all matrix ids,0
test all matrix names,0
test matrix attribute,0
test matrix meta,0
test ps location,0
test partitions,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
https://blog.csdn.net/cocoonyang/article/details/63068108,0
v1[i] = v1[i] + da * v2[i];,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
v1[i] = v1[i] + da * v2[i];,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
Default does nothing.,0
The app injection is optional,0
"renderText(""hello world"");",0
"user choose a workerGroupID from the workergroups page,",0
now we should change the AngelApp params and render the workergroup page;,0
"static final String WORKER_ID = ""worker.id"";",0
"div(""#logo"").",0
"img(""/static/hadoop-st.png"")._().",0
import org.apache.hadoop.yarn.webapp.view.FooterBlock;,0
import org.apache.hadoop.yarn.webapp.view.HeaderBlock;,0
JQueryUI.jsnotice(html);,0
import org.apache.hadoop.conf.Configuration;,0
import java.lang.reflect.Field;,0
all the files in input set,0
Shuffle the file,0
Get the blocks for all files,0
Adjust the maxSize to make the split more balanced,0
Handle the splittable files,0
Handle the unsplittable files,0
Split the blocks,0
"If the remaining size of the current block is smaller than the required size,",0
the remaining blocks are divided into the current split,0
Update current split length and move to next block,0
Clear the current block offset,0
"Current split length is > maxSize, split the block and generate a new split",0
Clear blocks list for next split,0
Clear the current split length,0
"If splitBlocks is not empty, just genetate a split for it",0
get block locations from file system,0
create an input split,0
get block locations from file system,0
create a list of all block and their locations,0
"if the file is not splitable, just create the one block with",0
full file length,0
each split can be a maximum of maxSize,0
if remainder is between max and 2*max - then,0
"instead of creating splits of size max, left-max we",0
create splits of size left/2 and left/2. This is,0
a heuristic to avoid creating really really small,0
splits.,0
add this block to the block --> node locations map,0
"For blocks that do not have host/rack information,",0
assign to default  rack.,0
add this block to the rack --> block map,0
Add this host to rackToNodes map,0
add this block to the node --> block map,0
"if the file system does not have any rack information, then",0
use dummy rack location.,0
The topology paths have the host name included as the last,0
component. Strip it.,0
get tokens for all the required FileSystems..,0
"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,",0
job.getConfiguration());,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
all the files in input set,0
it is allowed for maxSize to be 0. Disable smoothing load for such cases,0
process all nodes and create splits that are local to a node. Generate,0
"one split per node iteration, and walk over nodes multiple times to",0
distribute the splits across nodes.,0
Skip the node if it has previously been marked as completed.,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
Remove all blocks which may already have been assigned to other,0
splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
Remove entries from blocksInNode so that we don't walk these,0
again.,0
Done creating a single split for this node. Move on to the next,0
node so that splits are distributed across nodes.,0
This implies that the last few blocks (or all in case maxSize=0),0
were not part of a split. The node is complete.,0
if there were any blocks left over and their combined size is,0
"larger than minSplitNode, then combine them into one split.",0
Otherwise add them back to the unprocessed pool. It is likely,0
that they will be combined with other blocks from the,0
same rack later on.,0
This condition also kicks in when max split size is not set. All,0
blocks on a node will be grouped together into a single split.,0
haven't created any split on this machine. so its ok to add a,0
smaller one for parallelism. Otherwise group it in the rack for,0
balanced size create an input split and add it to the splits,0
array,0
Remove entries from blocksInNode so that we don't walk this again.,0
The node is done. This was the last set of blocks for this node.,0
Put the unplaced blocks back into the pool for later rack-allocation.,0
Node is done. All blocks were fit into node-local splits.,0
Check if node-local assignments are complete.,0
All nodes have been walked over and marked as completed or all blocks,0
have been assigned. The rest should be handled via rackLock assignment.,0
"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """,0
"+ completedNodes.size() + "", size left: "" + totalLength);",0
"if blocks in a rack are below the specified minimum size, then keep them",0
"in 'overflow'. After the processing of all racks is complete, these",0
overflow blocks will be combined into splits.,0
Process all racks over and over again until there is no more work to do.,0
Create one split for this rack before moving over to the next rack.,0
Come back to this rack after creating a single split for each of the,0
remaining racks.,0
"Process one rack location at a time, Combine all possible blocks that",0
reside on this rack as one split. (constrained by minimum and maximum,0
split size).,0
iterate over all racks,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"if we created a split, then just go to the next rack",0
"if there is a minimum size specified, then create a single split",0
"otherwise, store these blocks into overflow data structure",0
There were a few blocks in this rack that,0
remained to be processed. Keep them in 'overflow' block list.,0
These will be combined later.,0
Process all overflow blocks,0
"This might cause an exiting rack location to be re-added,",0
but it should be ok.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"Process any remaining blocks, if any.",0
create an input split,0
add this split to the list that is returned,0
long num = totLength / maxSize;,0
all blocks for all the files in input set,0
mapping from a rack name to the list of blocks it has,0
mapping from a block to the nodes on which it has replicas,0
mapping from a node to the list of blocks that it contains,0
populate all the blocks for all files,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
"Security framework already loaded the tokens into current UGI, just use",0
them,0
Now remove the AM->RM token so tasks don't have it,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init container allocator,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
Init Client manager,0
Init PS Client manager,0
init parameter server manager,0
recover task information if needed,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
Init model saver & loader,0
start a web service if use yarn deploy mode,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
Check Workers,0
Check PSS,0
Check Clients,0
Check PS Clients,0
parse parameter server counters,0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
refresh last heartbeat timestamp,0
send a state update event to the specific PSAttempt,0
Check is there save request,0
"LOG.info(""PS "" + psAttemptId + "" need save "" + subSaveContext);",0
Check is there load request,0
"LOG.info(""PS "" + psAttemptId + "" need load "" + subLoadContext);",0
check matrix metadata inconsistencies between master and parameter server.,0
"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix",0
"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix",0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
choose a unused port,0
start RPC server,0
remove this parameter server attempt from monitor set,0
remove this parameter server attempt from monitor set,0
"if worker attempt id is not in monitor set, we should shutdown it",0
find workergroup in worker manager,0
"if this worker group does not initialized, just return WORKERGROUP_NOTREADY",0
"if this worker group run over, just return WORKERGROUP_EXITED",0
"if this worker group is running now, return tasks, workers, data splits for it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update the clock for this matrix,0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update task iteration,0
"LOG.info(""error happened in psAttempt "" + psAttemptId + "" error msg="" + request.getMsg());",0
remove this parameter server attempt from monitor set,0
"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);",0
"Calculate how many splits we need. As each task handles a separate split of data, so we want",0
the number of splits equal to the number of tasks,0
split data,0
dispatch the splits to workergroups,0
split data,0
dispatch the splits to workergroups,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
write meta data to a temporary file,0
rename the temporary file to final file,0
"if the file exists, read from file and deserialize it",0
write task meta,0
write ps meta,0
generate a temporary file,0
write task meta to the temporary file first,0
rename the temporary file to the final file,0
"if last final task file exist, remove it",0
find task meta file which has max timestamp,0
"if the file does not exist, just return null",0
read task meta from file and deserialize it,0
generate a temporary file,0
write ps meta to the temporary file first.,0
rename the temporary file to the final file,0
"if the old final file exist, just remove it",0
find ps meta file,0
"if ps meta file does not exist, just return null",0
read ps meta from file and deserialize it,0
"Path tmpPath = new Path(new Path(context.getConf().get(AngelConf.ANGEL_JOB_TMP_OUTPUT_PATH)),",0
String.valueOf(requestId));,0
Path tmpPath = HdfsUtil.toTmpPath(new Path(saveContext.getSavePath()));,0
saveContext.setTmpSavePath(tmpPath.toString());,0
Filter old epoch trigger first,0
Split the user request to sub-requests to pss,0
Init matrix files meta,0
Move output files,0
Write the meta file,0
Split the user request to sub-requests to pss,0
check whether psagent heartbeat timeout,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
LocalStorageToken is needed irrespective of whether security is enabled,0
or not.,0
"TokenCache.setJobToken(jobToken, taskCredentials);",0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Construct the actual Container,0
The null fields are per-container and will be constructed for each,0
container separately.,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
"a * in the classpath will only find a .jar, so we need to filter out",0
all .jars and add everything else,0
Propagate the system classpath when using the mini cluster,0
Add standard Hadoop classes,0
Add mr,0
Cache archives,0
Cache files,0
Sanity check,0
Add URI fragment or just the filename,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
G1 params,0
".append("" -XX:G1NewSizePercent="").append(minNewRatio)",0
".append("" -XX:G1MaxNewSizePercent="").append(maxNewRatio)",0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
G1 params,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Final commmand,0
"if amTask is not null, we should clone task state from it",0
"if all parameter server complete commit, master can commit now",0
restartPS(psLoc);,0
check whether parameter server heartbeat timeout,0
Transitions from the NEW state.,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event",0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event dispatched before",0
PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.COMMITTING state,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
reqeuest resource:send a resource request to the resource allocator,0
"Once the resource is applied, build and send the launch request to the container launcher",0
deallocator the resource of the ps attempt:send a resource deallocator request to the,0
resource allocator,0
set the launch time,0
add the ps attempt to the heartbeat timeout monitoring list,0
parse ps attempt location and put it to location manager,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt",0
or failed,0
remove ps attempt id from heartbeat timeout monitor list,0
release container:send a release request to container launcher,0
set the finish time only if launch time is set,0
private long scheduledTime;,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
Refresh ps location & matrix meta,0
start a new attempt for this ps,0
notify ps manager,0
"getContext().getLocationManager().setPsLocation(id, null);",0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
notify the event handler of state change,0
"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor",0
"if forcedState is set, just return",0
else get state from state machine,0
add this worker group to the success set,0
check if all worker group run or run over,0
add this worker group to the success set,0
check if all worker group run over,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
calculate the actual number of worker groups and the total number of tasks based on the number of data split,0
"init all tasks , workers and worker groups and put them to the corresponding maps",0
just return the total task number now,0
TODO,1
check whether worker heartbeat timeout,0
"if workerAttempt is not null, we should clone task state from it",0
from NEW state,0
from SCHEDULED state,0
get data splits location for data locality,0
reqeuest resource:send a resource request to the resource allocator,0
"once the resource is applied, build and send the launch request to the container launcher",0
notify failed message to the worker,0
notify killed message to the worker,0
release the allocated container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
release the allocated container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
clean the container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
record the finish time,0
clean the container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listening list,0
record the finish time,0
"if the worker attempt launch successfully, add it to heartbeat timeout listening list",0
set worker attempt location,0
notify the register message to the worker,0
record the launch time,0
update worker attempt metrics,0
update tasks metrics,0
clean the container,0
notify the worker attempt run successfully message to the worker,0
record the finish time,0
init a worker attempt for the worker,0
schedule the worker attempt,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
"If we need Yarn to restart a new application master, we should not unregister from Yarn RM",0
register to Yarn RM,0
send heartbeat to Yarn RM every rmPollInterval milliseconds,0
"catch YarnException or YarnRuntimeException, we should exit and need not retry",0
build heartbeat request,0
send heartbeat request to rm,0
"This can happen if the RM has been restarted. If it is in that state,",0
this application must clean itself up.,0
Setting NMTokens,0
assgin containers,0
"if some container is not assigned, release them",0
handle finish containers,0
dispatch container exit message to corresponding components,0
killed by framework,0
killed by framework,0
get application finish state,0
build application diagnostics,0
TODO:add a job history for angel,1
build unregister request,0
send unregister request to rm,0
Note this down for next interaction with ResourceManager,0
based on blacklisting comments above we can end up decrementing more,0
than requested. so guard for that.,0
send the updated resource request to RM,0
send 0 container count requests also to cancel previous requests,0
Update resource requests,0
try to assign to all nodes first to match node local,0
try to match all rack local,0
assign remaining,0
Update resource requests,0
send the container-assigned event to task attempt,0
build the start container request use launch context,0
send the start request to Yarn nm,0
send the message that the container starts successfully to the corresponding component,0
"after launching, send launched event to task attempt to move",0
it from ASSIGNED to RUNNING state,0
send the message that the container starts failed to the corresponding component,0
kill the remote container if already launched,0
start a thread pool to startup the container,0
See if we need up the pool size only if haven't reached the,0
maximum limit yet.,0
nodes where containers will run at *this* point of time. This is,0
*not* the cluster size and doesn't need to be.,0
"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the",0
later is just a buffer so we are not always increasing the,0
pool-size,0
the events from the queue are handled in parallel,0
using a thread pool,0
return if already stopped,0
shutdown any containers that might be left running,0
Add one sync matrix,0
addSyncMatrix();,0
"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {",0
"LOG.info(""ps id = "" + psEntry.getKey());",0
"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();",0
"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {",0
"LOG.info(""matrix id = "" + metaEntry.getKey());",0
"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());",0
},0
},0
get matrix ids in the parameter server report,0
get the matrices parameter server need to create and delete,0
"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix",0
"if a matrix exists on master but not exist on parameter server, this parameter server need build it.",0
Init control connection manager,0
Get ps locations from master and put them to the location cache.,0
Build and initialize rpc client to master,0
Get psagent id,0
Build PS control rpc client manager,0
Build local location,0
Initialize matrix meta information,0
Start all services,0
Stop all modules,0
Stop all modules,0
clock first,0
wait,0
Update generic resource counters,0
Updating resources specified in ResourceCalculatorProcessTree,0
Remove the CPU time consumed previously by JVM reuse,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Plus a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Update a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get values from pss use row/column indices,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"/ PSF get/update, use can implement their own psf",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get a row or a batch of rows,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
Just return,0
Just return,0
Just return,0
Just return,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
Sort the partitions by start column index,0
Generate a flush request and put it to request queue,0
Generate a clock request and put it to request queue,0
Generate a merge request and put it to request queue,0
Generate a merge request and put it to request queue,0
"If the matrix op log cache does not exist for the matrix, create a new one for the",0
matrix,0
and add it to cache maps,0
Add the message to the tree map,0
"If there are flush / clock requests blocked, we need to put this merge request into",0
the waiting queue,0
Launch a merge worker to merge the update to matrix op log cache,0
Remove the message from the tree map,0
Wake up blocked flush/clock request,0
Add flush/clock request to listener list to waiting for all the existing,0
updates are merged,0
Wake up blocked flush/clock request,0
"If all updates are merged for this matrix, we need wake up flush/clock requests which are",0
blocked.,0
Get next merge message sequence id,0
Wake up listeners(flush/clock requests) that have little sequence id than current merge,0
position,0
Wake up blocked merge requests,0
Get minimal sequence id from listeners,0
"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we",0
should flush updates to local matrix storage,0
Doing average or not,0
Filter un-important update,0
Split this row according the matrix partitions,0
Set split context,0
Remove the row from matrix,0
buf.writeDouble(0.0);,0
TODO:,1
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"LOG.error(""channel "" + ctx.channel() + "" inactive"");",0
"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());",0
"LOG.error(""put response message queue failed "", e);",0
Use Epoll for linux,0
Update location table,0
Remove the server from failed list,0
Notify refresh success message to request dispatcher,0
Check PS exist or not,0
Check heartbeat timeout,0
getPSState(entry.getKey());,0
Check PS restart or not,0
private final HashSet<ParameterServerId> refreshingServerSet;,0
Add it to failed rpc list,0
Add the server to gray server list,0
Add it to failed rpc list,0
Add the server to gray server list,0
Move from gray server list to failed server list,0
Handle the RPCS to this server,0
Submit the schedulable failed get RPCS,0
Submit new get RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"If the queue is empty, just return 0",0
"If request is not over limit, just submit it",0
Submit the schedulable failed get RPCS,0
Submit new put RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"LOG.info(""choose put server "" + psIds[index]);",0
Check all pending RPCS,0
Check get channel context,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
channelManager.printPools();,0
"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {",0
if(ts - entry.getValue() > requestTimeOut * 2)  {,0
"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())",0
"+ "" milliseconds, close all channels to it"");",0
closeChannels(entry.getKey());,0
"psLocToLastChannelTsMap.put(entry.getKey(), ts);",0
},0
},0
"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);",0
Remove all pending RPCS,0
Close all channel to this PS,0
private Channel getChannel(Location loc) throws Exception {,0
"return channelManager.getOrCreateChannel(new Location(loc.getIp(), loc.getPort() + 1));",0
},0
private GenericObjectPool<Channel> getChannelPool(Location loc) throws InterruptedException {,0
"return channelManager.getOrCreateChannelPool(new Location(loc.getIp(), loc.getPort() + 1), PSAgentContext",0
.get(),0
.getConf(),0
".getInt(AngelConf.ANGEL_WORKER_TASK_NUMBER,",0
AngelConf.DEFAULT_ANGEL_WORKER_TASK_NUMBER));,0
},0
Get server id and location for this request,0
"If location is null, means that the server is not ready",0
Get the channel for the location,0
Check if need get token first,0
Serialize the request,0
Send the request,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
request.getContext().setChannelPool(pool);,0
Allocate the bytebuf and serialize the request,0
find the partition request context from cache,0
"updateMatrixCache(request.getPartKey(), response.getPartition());",0
"updateMatrixCache(request.getPartKey(), response.getRowsSplit());",0
TODO,1
ServerRow rowSplit = PSAgentContext.get().getMatricesCache(),0
".getRowSplit(request.getPartKey().getMatrixId(), request.getPartKey(),",0
request.getRowIndex());,0
response.setRowSplit(rowSplit);,0
"updateMatrixCache(request.getPartKey(), response.getRowSplit());",0
"LOG.info(""user request id "" + request.getUserRequestId());",0
"LOG.info(""user request id "" + request.getUserRequestId());",0
TODO,1
"private void updateMatrixCache(PartitionKey partKey, ServerPartition partition) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, partition);",0
},0
"private void updateMatrixCache(PartitionKey partKey, ServerRow rowSplit) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowSplit);",0
},0
"private void updateMatrixCache(PartitionKey partKey, List<ServerRow> rowsSplit) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowsSplit);",0
},0
Get partitions for this row,0
Distinct get row requests,0
Get row splits of this row from the matrix cache first,0
responseCache.addSubResponse(rowSplit);,0
"If the row split does not exist in cache, get it from parameter server",0
Split the param use matrix partitions,0
Send request to PSS,0
Split the matrix oplog according to the matrix partitions,0
"If need update clock, we should send requests to all partitions",0
Send request to PSS,0
Filter the rowIds which are fetching now,0
Send the rowIndex to rpc dispatcher and return immediately,0
"LOG.info(""get row split use time="" + (System.currentTimeMillis() - startTs));",0
"LOG.info(""start to request "" + requestId);",0
"LOG.info(""start to request "" + requestId);",0
Split param use matrix partitons,0
"If all sub-results are received, just remove request and result cache",0
Split this row according the matrix partitions,0
Set split context,0
Split this row according the matrix partitions,0
Set split context,0
long startTs = System.currentTimeMillis();,0
"LOG.error(""combine use time = "" + (System.currentTimeMillis() - startTs));",0
Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition,0
Generate dispatch items and add them to the corresponding queues,0
Filter the rowIds which are fetching now,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {",0
return key1.getStartCol() < key2.getStartCol() ? -1 : 1;,0
});,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {",0
return key1.getStartCol() < key2.getStartCol() ? -1 : 1;,0
});,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Put the row split to the cache(row index to row splits map),0
"If all splits of the row are received, means this row can be merged",0
TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage,1
TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage,1
TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage,1
TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage,1
TODO,1
TODO,1
/////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO,1
buf.writeDouble(0);,0
TODO,1
///////////////////////////////////////////////////////////////////////////////////////////////,0
Now we just support pipelined row splits merging for dense type row,0
Pre-fetching is disable default,0
matrix id to clock map,0
"task index, it must be unique for whole application",0
Deserialize data splits meta,0
Get workers,0
Send request to every ps,0
Wait the responses,0
Update clock cache,0
if(syncNum % 1024 == 0) {,0
},0
"Use simple flow, do not use any cache",0
Get row from cache.,0
"if row clock is satisfy ssp staleness limit, just return.",0
Get row from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC mode, just get from pss.",0
"For BSP/SSP, get rows from storage/cache first",0
Get from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC, just get rows from pss.",0
no more retries.,0
calculate sleep time and return.,0
parse the i-th sleep-time,0
parse the i-th number-of-retries,0
calculateSleepTime may overflow.,0
"A few common retry policies, with no delays.",0
Read matrix meta from meta file,0
Save partitions to files use fork-join,0
Write the ps matrix meta to the meta file,0
matrix.startServering();,0
return;,0
Read matrix meta from meta file,0
Load partitions from file use fork-join,0
Read matrix meta from meta file,0
Sort partitions,0
TODO:,1
int size = rows.length;,0
int size = rows.length;,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
close is a local operation and should finish within milliseconds; timeout just to be safe,0
response will be null for one way messages.,0
maxFrameLength = 2G,0
lengthFieldOffset = 0,0
lengthFieldLength = 8,0
"lengthAdjustment = -8, i.e. exclude the 8 byte length itself",0
"initialBytesToStrip = 8, i.e. strip out the length field itself",0
"return Objects.toStringHelper(this).add(""serial"", serial).add(""listSize"", datas.size())",0
.toString();,0
indicates whether this connection's life cycle is managed,0
See if we already have a connection (common case),0
create a unique lock for this RS + protocol (if necessary),0
get the RS lock,0
do one more lookup in case we were stalled above,0
Only create isa when we need to.,0
definitely a cache miss. establish an RPC for,0
this RS,0
Throw what the RemoteException was carrying.,0
check,0
every,0
minutes,0
TODO,1
failoverHandler,0
"The number of times this invocation handler has ever been failed over,",0
before this method invocation attempt. Used to prevent concurrent,0
failed method invocations from triggering multiple failover attempts.,0
Make sure that concurrent failed method invocations,0
only cause a,0
single actual fail over.,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
"LOG.info(""method "" + method.getName() + ""construct request time = """,0
+ (System.currentTimeMillis() - beforeConstructTs));,0
get an instance of the method arg type,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
Message (hand written code usually has only a single,0
argument),0
log any RPC responses that are slower than the configured,0
warn,0
response time or larger than configured warning size,0
"when tagging, we let TooLarge trump TooSmall to keep",0
output simple,0
note that large responses will often also be slow.,0
provides a count of log-reported slow responses,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
unexpected,0
"in the protobuf methods, args[1] is the only significant argument",0
for JSON encoding,0
base information that is reported regardless of type of call,0
Disable Nagle's Algorithm since we don't want packets to wait,0
Configure the event pipeline factory.,0
Make a new connection.,0
Remove all pending requests (will be canceled after relinquishing,0
write lock).,0
Cancel any pending requests by sending errors to the callbacks:,0
Close the channel:,0
Close the connection:,0
Shut down all thread pools to exit.,0
"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());",0
See NettyServer.prepareResponse for where we write out the response.,0
"It writes the call.id (int), a boolean signifying any error (and if",0
"so the exception name/trace), and the response bytes",0
Read the call id.,0
"When the stream is closed, protobuf doesn't raise an EOFException,",0
"instead, it returns a null message object.",0
"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +",0
System.currentTimeMillis());,0
"It would be good widen this to just Throwable, but IOException is what we",0
allow now,0
not implemented,1
not implemented,1
"track what RpcEngine is used by a proxy class, for stopProxy()",0
cache of RpcEngines by protocol,0
return the RpcEngine configured to handle a protocol,0
We only handle the ConnectException.,0
This is the exception we can't handle.,0
check if timed out,0
wait for retry,0
IGNORE,0
return the RpcEngine that handles a proxy object,0
The default implementation works synchronously,0
punt: allocate a new buffer & copy into it,0
Parse cmd parameters,0
load hadoop configuration,0
load angel system configuration,0
load user configuration:,0
load user config file,0
load command line parameters,0
load user job resource files,0
load ml conf file for graph based algorithm,0
load user job jar if it exist,0
Expand the environment variable,0
Add default fs(local fs) for lib jars.,0
"LOG.info(System.getProperty(""user.dir""));",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
a simple hdfs copy function assume src path and dest path are in same hdfs,0
and FileSystem object has same schema,0
"If out path exist , just remove it first",0
Create parent directory if not exist,0
Rename,0
"LOG.warn(""interrupted while sleeping"", ie);",0
public static String getHostname() {,0
try {,0
"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();",0
} catch (UnknownHostException uhe) {,0
},0
"return new StringBuilder().append("""").append(uhe).toString();",0
},0
"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {",0
String hostname = getHostname();,0
String classname = clazz.getSimpleName();,0
"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new",0
"StringBuilder().append(""Starting "").append(classname).toString(), new",0
"StringBuilder().append(""  host = "").append(hostname).toString(), new",0
"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new",0
"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new",0
"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));",0
,0
"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {",0
public void run() {,0
"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +",0
"this.val$classname + "" at "" + this.val$hostname}));",0
},0
});,0
},0
"We we interrupted because we're meant to stop? If not, just",0
continue ignoring the interruption,0
Recalculate waitTime.,0
// Begin delegation to Thread,0
// End delegation to Thread,0
instance submitter class,0
Obtain filename from path,0
Split filename to prexif and suffix (extension),0
Check if the filename is okay,0
Prepare temporary file,0
Prepare buffer for data copying,0
Open and check input stream,0
Open output stream and copy data between source file in JAR and the temporary file,0
"If read/write fails, close streams safely before throwing an exception",0
"Finally, load the library",0
little endian load order,0
tail,0
fallthrough,0
fallthrough,0
finalization,0
fmix(h1);,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
throw new AngelException(e);,0
JobStateProto jobState = report.getJobState();,0
Check need load matrices,0
Used for java code to get a AngelClient instance,0
Used for python code to get a AngelClient instance,0
load user job resource files,0
the leaf level file should be readable by others,0
the subdirs in the path should have execute permissions for,0
others,0
2.get job id,0
Credentials credentials = new Credentials();,0
4.copy resource files to hdfs,0
5.write configuration to a xml file,0
6.create am container context,0
7.Submit to ResourceManager,0
8.get app master client,0
Create a number of filenames in the JobTracker's fs namespace,0
add all the command line files/ jars and archive,0
first copy them to jobtrackers filesystem,0
should not throw a uri exception,0
should not throw an uri excpetion,0
set the timestamps of the archives and files,0
set the public/private visibility of the archives and files,0
get DelegationToken for each cached file,0
check if we do not need to copy the files,0
is jt using the same file system.,0
just checking for uri strings... doing no dns lookups,0
to see if the filesystems are the same. This is not optimal.,0
but avoids name resolution.,0
this might have name collisions. copy will throw an exception,0
parse the original path to create new path,0
check for ports,0
Write job file to JobTracker's fs,0
Setup resource requirements,0
Setup LocalResources,0
Setup security tokens,0
Setup the command to run the AM,0
Add AM user command opts,0
Final command,0
Setup the CLASSPATH in environment,0
"i.e. add { Hadoop jars, job jar, CWD } to classpath.",0
Setup the environment variables for Admin first,0
"Setup the environment variables (LD_LIBRARY_PATH, etc)",0
Parse distributed cache,0
Setup ContainerLaunchContext for AM container,0
Set up the ApplicationSubmissionContext,0
private volatile PS2PSPusherImpl ps2PSPusher;,0
TODO,1
Add tokens to new user so that it may execute its task correctly.,0
TODO,1
to exit,0
TODO,1
TODO,1
ps2PSPusher.recover(ProtobufUtil.convert(needRecoverParts.get(i)));,0
context.getSnapshotManager().processRecovery();,0
Recover PS from snapshot or load path,0
1. First check old snapshot,0
2. Check new checkpoints,0
3. Check load path setting and old save result,0
Just init it again,0
TODO,1
if(ps2PSPusher != null) {,0
ps2PSPusher.start();,0
},0
public PS2PSPusherImpl getPs2PSPusher() {,0
return ps2PSPusher;,0
},0
"LOG.error(""send response of request "" + requestToString(clientId, seqId) + "" failed "");",0
"LOG.error(""send response of request failed, request seqId="" + seqId + "", channel="" + ch, ex);",0
Release the input buffer,0
Release the input buffer,0
"1. handle the rpc, get the response",0
Release the input buffer,0
2. Serialize the response,0
Send the serialized response,0
Exception happened,0
write seq id,0
Just serialize the head,0
Exception happened,0
Allocate result buffer,0
Exception happened,0
Just serialize the head,0
Exception happened,0
runningContext.printToken();,0
Reset the response and allocate buffer again,0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this location, only master ps can accept the update",0
Check the partition state again,0
Start to put the update to the slave pss,0
TODO,1
"context.getPS2PSPusher().put(request, in, partLoc);",0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this partition, if not, just return failed",0
Start to put the update to the slave pss,0
TODO,1
return ServerState.GENERAL;,0
Use Epoll for linux,0
public String uuid;,0
TODO:,1
part = new ServerPartition();,0
TODO:,1
public void setChannelPool(GenericObjectPool<Channel> channelPool) {,0
this.channelPool = channelPool;,0
},0
private final ParameterServer psServer;,0
Create and start workers,0
Set workers,0
Create and start workers,0
Set workers,0
"If matrix checkpoint path not exist, just return null",0
Return the path with maximum checkpoint id,0
Rename temp to item path,0
Checkpoint base path = Base dir/matrix name,0
Path for this checkpoint,0
Generate tmp path,0
Delete old checkpoints,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"////// network io method, for model transform",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
Serailize the head,0
Serialize the storage,0
Deserailze the head,0
Deseralize the storage,0
Serailize the head,0
Serialize the storage,0
Deserailze the head,0
Deseralize the storage,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"Notice: Basic type ServerRow only support Vector as inner storage now, so you can use methods",0
to get inner vector for basic type ServerRow.,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
Just update the exist element now!!,0
Just update the exist element now!!,0
TODO: just check the value is 0 or not now,1
TODO: just check the value is zero or not now,1
"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low.",0
Attention: Only update the exist values for sorted storage method,0
Attention: Only update exist element,0
Attention: Only update the exist values for sorted storage method,0
Attention: Only update exist element,0
TODO: just check the value is zero or not now,1
"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low.",0
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
Element data,0
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
Attention: Only update the exist values for sorted storage method,0
Attention: Only update exist element,0
TODO: just check the value is zero or not now,1
"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low.",0
TODO: just check the value is 0 or not now,1
Just update the exist element now!!,0
Just update the exist element now!!,0
"Use sparse storage method, as some elements in the array maybe null",0
Array length,0
Valid element number,0
Element data,0
Array len,0
Valid element number,0
"Use sparse storage method, as some elements in the array maybe null",0
Array length,0
Valid element number,0
Element data,0
Element data,0
Array len,0
Valid element number,0
Attention: Only update the exist values for sorted storage method,0
Attention: Only update exist element,0
TODO: just check the value is zero or not now,1
"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low.",0
Row type,0
Storage method,0
Key type,0
Value type,0
Vector dim,0
Vector length,0
Vector data,0
Row type,0
Storage method,0
Key type,0
Value type,0
Vector dim,0
Vector length,0
Init the vector,0
Vector data,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
Impossible now,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
Impossible now,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
Impossible now,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
Impossible now,0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
Just update the exist element now!!,0
Just update the exist element now!!,0
TODO: just check the value is 0 or not now,1
Just update the exist element now!!,0
Just update the exist element now!!,0
TODO: just check the value is 0 or not now,1
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
Element data,0
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
private final List<PartitionKey> partitionKeys;,0
Get server partition class,0
"If partition class is not set, just use the default partition class",0
Get server partition storage class type,0
Get value class,0
"if col == -1, we use the start/end index to calculate range,",0
we use double to store the range value since two long minus might exceed the,0
range of long.,0
Serialize the head,0
Serialize the storage,0
Deserialize the head,0
Deseralize the storage,0
Row base partition,0
"If storage class is not set, use default DenseServerRowsStorage",0
Serialize values,0
Deserialize values,0
Array size,0
Actual write size,0
Rows data,0
Row id,0
Row type,0
Row data,0
Array size,0
Actual write row number,0
Rows data,0
Row id,0
Create empty server row,0
Row data,0
Rows data,0
TODO,1
Serialize row offsets,0
Serialize column offsets,0
Deserialize row offset,0
Deserialize row offset,0
"If storage is set, just get a instance",0
"If storage is not set, use default",0
"If storage is set, just get a instance",0
"If storage is not set, use default",0
Map size,0
Actual write size,0
Rows data,0
Row id,0
Row type,0
Row data,0
Array size,0
Actual write row number,0
Rows data,0
Row id,0
Create empty server row,0
Row data,0
Rows data,0
Use Epoll for linux,0
find the partition request context from cache,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
channelManager.removeChannelPool(loc);,0
Generate seq id,0
Create a RecoverPartRequest,0
Serialize the request,0
Change the seqId for the request,0
Serialize the request,0
"First check the state of the channels in the pool, if a channel is unused, just return",0
"If all channels are in use, create a new channel or wait",0
Create a new channel,0
"add the PSAgentContext,need fix",0
If col == -1 and start/end not set,0
start/end set,0
"for dense type, we need to set the colNum to set dim for vectors",0
"colNum set, start/end not set",0
Row number must > 0,0
"both set, check its valid",0
public static final int T_INT_ARBITRARY_VALUE = 28;,0
public static final int T_INVALID_VALUE = 29;,0
TODO:add more vector type,1
TODO : subDim set,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Split updates,0
Shuffle update splits,0
Generate part update parameters,0
"Set split context: partition key, use int key for long key vector or not ect",0
write the max abs,0
---------------------------------------------------,0
---------------------------------------------------,0
---------------------------------------------------------------,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
-----------------,0
-----------------,0
-----------------,0
-----------------,0
-----------------,0
KeepStorage is guaranteed,0
"ignore the isInplace option, since v2 is dense",0
"the value in old storage can be changed safe, so switch a storage",0
"but user required keep storage, we can prevent rehash",0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
KeepStorage is guaranteed,0
we gauss dense storage is more efficient,0
v1Size < v2Size * Constant.sparseThreshold,0
KeepStorage is guaranteed,0
"ignore the isInplace option, since v2 is dense",0
"the value in old storage can be changed safe, so switch a storage",0
"but user required keep storage, we can prevent rehash",0
we gauss dense storage is more efficient,0
prevent rehash,0
KeepStorage is guaranteed,0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
dense preferred,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
sorted preferred,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"Transform mat1, generate a new matrix",0
Split the row indices of mat1Trans,0
Parallel execute use fork-join,0
"Get the sub-matrix of left matrix, split by row",0
"Transform mat1, generate a new matrix",0
Split the row indices of mat1Trans,0
Parallel execute use fork-join,0
"Get the sub-matrix of left matrix, split by row",0
"mat1 trans true, mat trans true",0
"mat1 trans true, mat trans false",0
"mat1 trans false, mat trans true, important",0
"mat1 trans false, mat trans false",0
"mat1 trans true, mat trans true",0
"mat1 trans true, mat trans false",0
"mat1 trans false, mat trans true, important",0
"mat1 trans false, mat trans false",0
"dger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)",0
"sger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)",0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
add dense double matrix,0
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
get configuration from config file,0
set localDir with enviroment set by nm.,0
get master location,0
init task manager and start tasks,0
start heartbeat thread,0
taskManager.assignTaskIds(response.getTaskidsList());,0
todo,1
"if worker timeout, it may be knocked off.",0
"SUCCESS, do nothing",0
heartbeatFailedTime = 0;,0
private KEY currentKey;,0
will be created,0
TODO Auto-generated method stub,1
Bitmap bitmap = new Bitmap();,0
int max = indexArray[size - 1];,0
byte [] bitIndexArray = new byte[max / 8 + 1];,0
for(int i = 0; i < size; i++){,0
int bitIndex = indexArray[i] >> 3;,0
int bitOffset = indexArray[i] - (bitIndex << 3);,0
switch(bitOffset){,0
case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;,0
case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;,0
case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;,0
case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;,0
case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;,0
case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;,0
case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;,0
case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;,0
},0
},0
"true, false",0
//////////////////////////////,0
Application Configs,0
//////////////////////////////,0
//////////////////////////////,0
Master Configs,0
//////////////////////////////,0
//////////////////////////////,0
Worker Configs,0
//////////////////////////////,0
//////////////////////////////,0
Task Configs,0
//////////////////////////////,0
//////////////////////////////,0
ParameterServer Configs,0
//////////////////////////////,0
////////////////// IPC //////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
Configs used to ANGEL_PS_PSAGENT running mode future.,0
model parse,0
Mark whether use pyangel or not.,0
private Configuration conf;,0
"Configuration that should be used in python environment, there should only be one",0
configuration instance in each Angel context.,0
Use private access means jconf should not be changed or modified in this way.,0
Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:,0
Do nothing,0
To-DO: add other ways to justify different value types,0
"This is so ugly, must re-implement by more elegance way",1
"Create python path which include angel's jars, the python directory in ANGEL_HOME,",0
and other files submitted by user.,0
Launch python process,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);",0
get a angel client,0
add sparse float matrix,0
siMat.setPartitionStorageClass(LongElementMapStorage.class);,0
siMat.setPartitionClass(CSRPartition.class);,0
Start PS,0
Start to run application,0
Init node neighbors,0
client.asyncUpdate(new InitNeighborOver(new InitNeighborOverParam(matrixId))).get();,0
Sample the neighbors,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);",0
get a angel client,0
add sparse float matrix,0
siMat.setValidIndexNum(100);,0
siMat.setColNum(10000000000L);,0
siMat.setPartitionStorageClass(LongElementMapStorage.class);,0
siMat.setPartitionClass(CSRPartition.class);,0
Start PS,0
Start to run application,0
Init node neighbors and feats,0
Sample the neighbors,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);",0
get a angel client,0
add sparse float matrix,0
Start PS,0
Start to run application,0
Init node neighbors,0
Sample the neighbors,0
sample continuously beginning from a random index,0
Get node neighbor number,0
"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random",0
Get node neighbor number,0
"If the neighbor number is 0, just return a int[0]",0
"If count <= 0 or the neighbor number is less or equal then count, just copy all neighbors to the result array",0
"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random",0
"System.out.println(""serialize size="" + (endIndex - startIndex));",0
"System.out.println(""deserialize size="" + nodesToNeighbors.size());",0
Store the total neighbor number of all nodes in rowOffsets,0
"Put the node ids, node neighbor number, node neighbors to the cache",0
No data in this partition,0
Get total neighbor number,0
Final matrix column indices: neighbors node ids,0
Write positions in cloumnIndices for nodes,0
Copy all cached sub column indices to final column indices,0
Read position for a sub column indices,0
Copy column indices for a node to final column indices,0
Update write position for this node in final column indices,0
Update the read position in sub column indices,0
Clear all temp data,0
Get node neighbor number,0
"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random",0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Data format,0
Feature number of train data,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Ratio of validation,0
Learning rate,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"Set angel resource, #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Set training data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set predict data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model type,0
String modelType = String.valueOf(RowType.T_FLOAT_DENSE);,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
"conf.set(MLConf.ML_MODEL_TYPE(), modelType);",0
"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);",0
predictTest();,0
"Data format, libsvm or dummy",0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set data format,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
class number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set log path,0
Set trainning data path,0
Set save model path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set training data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log save path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set testing data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Data is classification,0
Model is classification,0
Train batch number per epoch.,0
loss delta,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Data is classification,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
TODO: optimize int key indices,1
"System.out.println(""deserialize cols.length="" + nCols);",0
"System.out.print(""deserialize "");",0
"System.out.print(cols[c] + "" "");",0
System.out.println();,0
TODO Auto-generated method stub,1
"LOG.info(""original float length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));",0
"LOG.info(""original double length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));",0
"LOG.info(""parsed float length: "" + length + Arrays.toString(arr));",0
"LOG.info(""parsed double length: "" + length + Arrays.toString(arr));",0
"ground truth: positive, precision: positive",0
start row index for words,0
start row index for docs,0
doc ids,0
topic assignments,0
word to docs reverse index,0
count word,0
build word start index,0
build word to doc reverse idx,0
build dks,0
dks = new TraverseHashMap[n_docs];,0
for (int d = 0; d < n_docs; d++) {,0
if (K < Short.MAX_VALUE) {,0
if (docs.get(d).len < Byte.MAX_VALUE),0
dks[d] = new S2BTraverseMap(docs.get(d).len);,0
if (docs.get(d).len < Short.MAX_VALUE),0
"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));",0
else,0
"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));",0
} else {,0
"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));",0
},0
},0
build dks,0
allocate update maps,0
Skip if no token for this word,0
Check whether error when fetching word-topic,0
Build FTree for current word,0
current doc,0
old topic assignment,0
"Check if error happens. if this happen, it's probably that failures happen to servers.",0
We need to adjust the memory settings or network fetching parameters.,0
Update statistics if needed,0
Calculate psum and sample new topic,0
Update statistics if needed,0
Assign new topic,0
Skip if no token for this word,0
if (u >= p[end]) {,0
"System.out.println(""u="" + u + "" p[end]="" + p[end] + "" start="" + start + "" end="" + end);",0
return end;,0
},0
,0
if (u < p[start]) {,0
"System.out.println(""u="" + u + "" p[start]="" + p[start] + "" start="" + start + "" end="" + end);",0
return start;,0
},0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
Write #rows,0
Write each row,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
dense,0
sparse,0
calculate columns,0
reset(row);,0
loss function,0
gradient and hessian,0
"categorical feature set, null: none, empty: all, else: partial",0
"node's end index in instancePos, instances in [start, end] belong to a tree node",0
initialize the phase,0
current tree and depth,0
create loss function,0
calculate grad info of each instance,0
"create data sketch, push candidate split value to PS",0
1. calculate candidate split value,0
categorical features,0
2. push local sketch to PS,0
the leader worker,0
merge categorical features,0
create updates,0
"pull the global sketch from PS, only called once by each worker",0
number of categorical feature,0
sample feature,0
push sampled feature set to the current tree,0
create new tree,0
"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,",0
calculate gradient,0
"1. create new tree, initialize tree nodes and node stats",0
"2. initialize feature set, if sampled, get from PS, otherwise use all the features",0
2.1. pull the sampled features of the current tree,0
this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();,0
"2.2. if use all the features, only called one",0
"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle",0
4. set root node to active,0
"5. reset instance position, set the root node's span",0
6. calculate gradient,0
1. decide nodes that should be calculated,0
2. decide calculated and subtracted tree nodes,0
3. calculate threads,0
wait until all threads finish,0
4. subtract threads,0
wait until all threads finish,0
5. send histograms to PS,0
6. update histogram cache,0
clock,0
find split,0
"1. find responsible tree node, using RR scheme",0
2. pull gradient histogram,0
2.1. get the name of this node's gradient histogram on PS,0
2.2. pull the histogram,0
2.3. find best split result of this tree node,0
2.3.1 using server split,0
"update the grad stats of the root node on PS, only called once by leader worker",0
update the grad stats of children node,0
update the left child,0
update the right child,0
"2.3.2 the updated split result (tree node/feature/value/gain) on PS,",0
"2.3.3 otherwise, the returned histogram contains the gradient info",0
"2.3.4 the updated split result (tree node/feature/value/gain) on PS,",0
2.3.5 reset this tree node's gradient histogram to 0,0
3. push split feature to PS,0
4. push split value to PS,0
5. push split gain to PS,0
6. set phase to AFTER_SPLIT,0
this.phase = GBDTPhase.AFTER_SPLIT;,0
clock,0
1. get split feature,0
2. get split value,0
3. get split gain,0
4. get node weight,0
5. split node,0
update local replica,0
create AfterSplit task,0
"2. check thread stats, if all threads finish, return",0
6. clock,0
"split the span of one node, reset the instance position",0
in case this worker has no instance on this node,0
set the span of left child,0
set the span of right child,0
"1. left to right, find the first instance that should be in the right child",0
"2. right to left, find the first instance that should be in the left child",0
3. swap two instances,0
4. find the cut pos,0
5. set the span of left child,0
6. set the span of right child,0
set tree node to active,0
set node to leaf,0
set node to inactive,0
finish current depth,0
finish current tree,0
set the tree phase,0
check if there is active node,0
check if finish all the tree,0
update node's grad stats on PS,0
"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split",0
the root node's stats is updated by leader worker,0
1. create the update,0
2. push the update to PS,0
1. update predictions of training data,0
2. update predictions of validation data,0
the leader task adds node prediction to flush list,0
1. name of this node's grad histogram on PS,0
2. build the grad histogram of this node,0
3. push the histograms to PS,0
4. reset thread stats to finished,0
5.1. set the children nodes of this node,0
5.2. set split info and grad stats to this node,0
5.2. create children nodes,0
"5.3. create node stats for children nodes, and add them to the tree",0
5.4. reset instance position,0
"5.5. add new active nodes if possible, inc depth, otherwise finish this tree",0
5.6. set children nodes to leaf nodes,0
5.7. set nid to leaf node,0
5.8. deactivate active node,0
"get feature type, 0:empty 1:all equal 2:real",0
"if not -1, sufficient space will be allocated at once",0
copy the highest levels,0
copy baseBuffer,0
merge two non-empty quantile sketches,0
left child <= split value; right child > split value,0
"the first: minimal, the last: maximal",0
categorical features,0
continuous features,0
left child <= split value; right child > split value,0
feature index used to split,0
feature value used to split,0
loss change after split this node,0
grad stats of the left child,0
grad stats of the right child,0
"LOG.info(""Constructor with fid = -1"");",0
fid = -1: no split currently,0
the minimal split value is the minimal value of feature,0
the splits do not include the maximal value of feature,0
"1. the average distance, (maxValue - minValue) / splitNum",0
2. calculate the candidate split value,0
1. new feature's histogram (grad + hess),0
size: sampled_featureNum * (2 * splitNum),0
"in other words, concatenate each feature's histogram",0
2. get the span of this node,0
------ 3. using sparse-aware method to build histogram ---,0
"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances",0
3.1. get the instance index,0
3.2. get the grad and hess of the instance,0
3.3. add to the sum,0
3.4. loop the non-zero entries,0
3.4.1. get feature value,0
3.4.2. current feature's position in the sampled feature set,0
"int fPos = findFidPlace(this.controller.fSet, fid);",0
3.4.3. find the position of feature value in a histogram,0
"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]",0
3.4.4. add the grad and hess to the corresponding bin,0
3.4.5. add the reverse to the bin that contains 0.0f,0
4. add the grad and hess sum to the zero bin of all features,0
find the best split result of the histogram of a tree node,0
1. calculate the gradStats of the root node,0
"1.1. update the grad stats of the root node on PS, only called once by leader worker",0
2. loop over features,0
2.1. get the ture feature id in the sampled feature set,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"update the grad stats of the root node on PS, only called once by leader worker",0
3. update the grad stats of children node,0
3.1. update the left child,0
3.2. update the right child,0
find the best split result of one feature,0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
split value = sketches[splitIdx],0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
partition number,0
cols of each partition,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
"loop all the possible split value, start from split[0], the first item is the minimal feature value",0
assert fvalue >= sketch[start] && fvalue <= sketch[end];,0
if (left > end) return end - start;,0
find the best split result of the histogram of a tree node,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
find the best split result of one feature,0
1. set the feature id,0
splitEntry.setFid(fid);,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]",0
the task use index to find fvalue,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting",0
"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking.",0
max and min of each feature,0
clear all the information,0
calculate the sum of gradient and hess,0
"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,",0
ridx),0
check if necessary information is ready,0
"same as add, reduce is used in All Reduce",0
"features used in this tree, if equals null, means use all the features without sampling",0
node in the tree,0
the gradient info of each instances,0
initialize nodes,0
gradient,0
second order gradient,0
int sendStartCol = (int) row.getStartCol();,0
logistic loss for binary classification task.,0
"logistic loss, but predict un-transformed margin",0
check if label in range,0
return the default evaluation metric for the objective,0
"task type: classification, regression, or ranking",0
"quantile sketch, size = featureNum * splitNum",0
"gradient histograms, size = treeNodeNum * featureNum * splitNum",0
"active tree nodes, size = pow(2, treeDepth) -1",0
sampled features. size = treeNum * sampleRatio * featureNum,0
categorical feature. size = workerNum * cateFeatNum * splitNum,0
"split features, size = treeNum * treeNodeNum",0
"split values, size = treeNum * treeNodeNum",0
"split gains, size = treeNum * treeNodeNum",0
"node weights, size = treeNum * treeNodeNum",0
"node preds, size = treeNum * treeNodeNum",0
if using PS to perform split,0
step size for a tree,0
number of class,0
minimum loss change required for a split,0
maximum depth of a tree,0
number of features,0
number of nonzero,0
number of candidates split value,0
----- the rest parameters are less important ----,0
base instance weight,0
minimum amount of hessian(weight) allowed in a child,0
L2 regularization factor,0
L1 regularization factor,0
default direction choice,0
maximum delta update we can add in weight estimation,0
this parameter can be used to stabilize update,0
default=0 means no constraint on weight delta,0
whether we want to do subsample for row,0
whether to subsample columns for each tree,0
accuracy of sketch,0
accuracy of sketch,0
leaf vector size,0
option for parallelization,0
option to open cacheline optimization,0
whether to not print info during training.,0
maximum depth of the tree,0
number of features used for tree construction,0
"minimum loss change required for a split, otherwise stop split",0
----- the rest parameters are less important ----,0
default direction choice,0
whether we want to do sample data,0
whether to sample columns during tree construction,0
whether to use histogram for split,0
number of histogram units,0
whether to print info during training.,0
----- the rest parameters are obtained after training ----,0
total number of nodes,0
number of deleted nodes */,0
@maxIndex: this variable contains the max index of node/word,0
values[b + offset] = (random.nextFloat() - 0.5f) / dimension;,0
some params,0
max index for node/word,0
compute number of nodes for one row,0
check the length of dot values,0
merge dot values from all partitions,0
Skip-Gram model,0
Negative sampling,0
used to accumulate the updates for input vectors,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
Skip-Gram model,0
Negative sampling,0
used to accumulate the updates for input vectors,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
update output,0
Some params,0
compute number of nodes for one row,0
window size,0
Skip-Gram model,0
Accumulate the input vectors from context,0
Negative sampling,0
used to accumulate the updates for input vectors,0
window size,0
skip-gram model,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
update output,0
some params,0
batch sentences,0
max index for node/word,0
compute number of nodes for one row,0
check the length of dot values,0
merge dot values from all partitions,0
locates the input vectors to local array to prevent randomly access,0
on the large server row.,0
fill 0 for context vector,0
window size,0
Continuous bag-of-words Models,0
Accumulate the input vectors from context,0
Calculate the partial dot values,0
We should guarantee here that the sample would not equal the ``word``,0
used to accumulate the context input vectors,0
locates the input vector into local arrays to prevent randomly access for,0
the large server row.,0
window size,0
while true to prevent sampling out a positive target,0
how to prevent the randomly access to the output vectors??,0
accumulate gradients for the input vectors,0
update output vectors,0
update input,0
update output,0
Some params,0
compute number of nodes for one row,0
// calculate bias,0
if (param.getPartKey().getStartCol() <= 0 && param.getPartKey().getEndCol() > 0) {,0
"double zVal = VectorUtils.getDouble(z, 0);",0
"double nVal = VectorUtils.getDouble(n, 0);",0
"VectorUtils.setFloat(w, 0, (float) (-1.0 * alpha * zVal / (beta + Math.sqrt(nVal))));",0
},0
Do nothing.,0
split updates,0
shuffle update splits,0
generate part update splits,0
"set split context: partition key, use int key for long key vector or net",0
how to do intersection for two dense vector with a given indices ??,0
compress the neighbor IDs,0
write out edges,0
write out tags,0
Get node neighbors,0
current word,0
neu1 stores the average value of input vectors in the context (CBOW),0
Continuous Bag-of-Words Model,0
Accumulate the input vectors from context,0
negative sampling,0
Using the sigmoid value from the pre-computed table,0
accumulate for the hidden layer,0
update output layer,0
add the counter for target,0
update hidden layer,0
Update the input vector for each word in the context,0
add the counter to input,0
update input layers,0
update output layers,0
for (int a = 0; a < layers.length; a++) deltas[a] = layers[a] - deltas[a];,0
copy the highest levels,0
copy baseBuffer,0
merge two non-empty quantile sketches,0
"if not -1, sufficient space will be allocated at once",0
InstanceRow ins = instanceRows[insId];,0
int[] indices = ins.indices();,0
int[] bins = ins.bins();,0
int nnz = indices.length;,0
for (int j = 0; j < nnz; j++) {,0
int fid = indices[j];,0
if (isFeatUsed[fid - featLo]) {,0
"histograms[fid - featLo].accumulate(bins[j], gradPairs[insId]);",0
},0
},0
1. allocate histogram,0
"2. loop non-zero instances, accumulate to histogram",0
if (nnz <= nodeEnd - nodeStart + 1) { // loop all nnz of current feature,0
3. add remaining grad and hess to default bin,0
"return param.calcWeights(grad, hess);",0
"numClass is usually small, so we do not use arraycopy here",0
"numClass is usually small, so we do not use arraycopy here",0
TODO: use more schema on default bin,1
1. set default bin to left child,0
"2. for other bins, find its location",0
3. create split set,0
this.nodeStats = new GBTNodeStat[numClass == 2 ? 1 : numClass];,0
predict sparse instance with indices and values,0
predict libsvm data,0
"Preconditions.checkArgument(preds.length == labels.length,",0
"""LogLossMetric should be used for binary-label classification"");",0
double loss = 0.0;,0
for (int i = 0; i < preds.length; i++) {,0
"loss += evalOne(preds[i], labels[i]);",0
},0
return loss / labels.length;,0
double error = 0.0;,0
if (preds.length == labels.length) {,0
for (int i = 0; i < preds.length; i++) {,0
"error += evalOne(preds[i], labels[i]);",0
},0
} else {,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"error += evalOne(pred, labels[i]);",0
},0
},0
return error / labels.length;,0
Preconditions.checkArgument(preds.length != labels.length,0
"&& preds.length % labels.length == 0,",0
"""CrossEntropyMetric should be used for multi-label classification"");",0
double loss = 0.0;,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"loss += evalOne(pred, labels[i]);",0
},0
return loss / labels.length;,0
double correct = 0.0;,0
if (preds.length == labels.length) {,0
for (int i = 0; i < preds.length; i++) {,0
"correct += evalOne(preds[i], labels[i]);",0
},0
} else {,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"correct += evalOne(pred, labels[i]);",0
},0
},0
return (float) (correct / labels.length);,0
double errSum = 0.0f;,0
if (preds.length == labels.length) {,0
for (int i = 0; i < preds.length; i++) {,0
"errSum += evalOne(preds[i], labels[i]);",0
},0
} else {,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"errSum += evalOne(pred, labels[i]);",0
},0
},0
return Math.sqrt(errSum / labels.length);,0
"System.out.println(""----------"");",0
"System.out.println(""read buffer = "" + Integer.toBinaryString(readBufferT & 0b11111111)",0
"+ "", mask = "" + Integer.toBinaryString(readMaskT));",0
readMaskT <<= 1;,0
"boolean[] bits = new boolean[]{true, true, false, false, true, false, false, false, true, true, false, true};",0
int n = bits.length;,0
BufferedBitSet writeBitSet = new BufferedBitSet(n);,0
"BufferedBitSet readBitSet = new BufferedBitSet(writeBitSet.getBytes(), n);",0
if (bitSet.get(i) != bits[i]) {,0
"throw new RuntimeException("""" + i);",0
},0
private final ByteBuffer bytes;,0
"public BufferedBitSetReader(ByteBuffer bytes, int numBits) {",0
int capacity = bytes.capacity() * 8;,0
readIndexT = bytes.capacity() - 1;,0
return bytes.get(index);,0
TODO: use arraycopy to make it faster,1
assert from >= this.from && to <= this.to;,0
"LOG.debug(String.format(""Create subset: [%d-%d]"", newFrom, newTo));",0
"LOG.debug(String.format(""Get overlap: [%d-%d]"", newFrom, newTo));",0
return bits.clone();,0
private final SerializableBuffer bytes;,0
private final ByteBuffer bytes;,0
this.bytes = ByteBuffer.allocate(numBytes);,0
public BufferedBitSetWriter(ByteBuffer bytes) {,0
this.bytes = bytes;,0
},0
"bytes.put(writeIndex++, (byte) writeBuffer);",0
public ByteBuffer getBytes() {,0
return bytes;,0
},0
ML TreeConf,0
GBDT TreeConf,0
"edges=[x,...] firstFlow=1 => go to right if < x and go to left if > x",0
"edges=[x,...] firstFlow=0 => go to left if < x and go to right if > x",0
"different types of tree node splits, enumerated by their complexity",0
"in order to reduce model size, we give priority to split point",0
"comparison between two split points, we give priority to lower feature index",0
TODO: comparison between two split sets,0
"public boolean leafwise;  // true if leaf-wise training, false if level-wise training",0
TODO: regularization,1
TODO: regularization,1
public float insSampleRatio;  // subsample ratio for instances,0
Use by line with weight,0
evict entry with the smallest degree,0
set basic configuration keys,0
use local deploy mode and dummy data spliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighbor;,0
import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighborParam;,0
import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighborResult;,0
import com.tencent.angel.ml.matrix.psf.graph.adjacency.initneighbor.InitNeighbor;,0
import com.tencent.angel.ml.matrix.psf.graph.adjacency.initneighbor.InitNeighborParam;,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
@Test,0
"public void testInitAndGet() throws ExecutionException, InterruptedException {",0
Worker worker = LocalClusterContext.get().getWorker(worker0Attempt0Id).getWorker();,0
"MatrixClient client1 = worker.getPSAgent().getMatrixClient(""w2"", 0);",0
int matrixW1Id = client1.getMatrixId();,0
// Generate graph data,0
"Map<Integer, int []> adjMap = generateAdjTable(nodeNum, maxNeighborNum);",0
,0
// Init graph adj table,0
"InitNeighbor func = new InitNeighbor(new InitNeighborParam(matrixW1Id, adjMap));",0
client1.update(func);,0
,0
int [] nodeIds = new int[adjMap.size()];,0
int i = 0;,0
for(int nodeId : adjMap.keySet()) {,0
nodeIds[i++] = nodeId;,0
},0
,0
// Get graph adj table from PS,0
"GetNeighbor getFunc = new GetNeighbor(new GetNeighborParam(matrixW1Id, nodeIds, maxNeighborNum));",0
"Map<Integer, int[]> getResults = ((GetNeighborResult) (client1.get(getFunc)))",0
.getNodeIdToNeighborIndices();,0
,0
// Check the result,0
"for(Entry<Integer, int[]> entry : getResults.entrySet()) {",0
"Assert.assertArrayEquals(entry.getValue(), adjMap.get(entry.getKey()));",0
},0
},0
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
import jdk.nashorn.internal.runtime.regexp.joni.Config;,0
"paras[1] = ""abc"";",0
"paras[2] = ""123"";",0
Add standard Hadoop classes,0
Feature number of train data,0
Total iteration number,0
Learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set input data path,0
Set save model path,0
Set actionType train,0
QSLRRunner runner = new QSLRRunner();,0
runner.train(conf);,0
"conf.setInt(AngelConf.ANGEL_STALENESS, -1);",0
Dataset,0
"Data format, libsvm or dummy",0
class number,0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set Softmax algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
Feature number of train data,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Ratio of validation,0
Learning rate,0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set DeepFM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
Model type,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set FM algorithm parameters,0
Set model class,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set WideAndDeep algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
"Set LDA parameters #V, #K",0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set LDA algorithm parameters,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set SVM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Load model meta,0
Convert model,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Convert model,0
load hadoop configuration,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model,0
load hadoop configuration,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
worker register,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
attempt 0,0
attempt1,0
attempt1,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
set basic configuration keys,0
"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());",0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
Thread.sleep(5000);,0
"response = master.getJobReport(null, request);",0
"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);",0
"assertEquals(response.getJobReport().getCurIteration(), jobIteration);",0
"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
testDenseDoubleUDF();,0
testSparseDoubleUDF();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
testDenseDoubleUDF();,0
testSparseDoubleUDF();,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add sparse double matrix,0
add comp dense double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
testDenseDoubleUDF();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"LOG.info(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntFloatVector row = (IntFloatVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntLongVector row = (IntLongVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add sparse float matrix,0
siMat.setPartitionClass(CSRPartition.class);,0
siMat.setPartitionStorageClass(IntCSRStorage.class);,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);",0
get a angel client,0
add sparse float matrix,0
MatrixContext siMat = new MatrixContext();,0
siMat.setName(SPARSE_INT_MAT);,0
siMat.setRowType(RowType.T_ANY_INTKEY_SPARSE);,0
siMat.setRowNum(1);,0
siMat.setValidIndexNum(100);,0
siMat.setColNum(10000000000L);,0
siMat.setValueType(Node.class);,0
siMat.setPartitionStorageClass(LongElementMapStorage.class);,0
siMat.setPartitionClass(CSRPartition.class);,0
angelClient.addMatrix(siMat);,0
add sparse long-key double matrix,0
Start PS,0
Start to run application,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
import com.tencent.angel.psagent.consistency.SSPConsistencyController;,0
@RunWith(MockitoJUnitRunner.class),0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);",0
get a angel client,0
add matrix,0
psAgent.initAndStart();,0
test conf,0
test master location,0
test app id,0
test user,0
test ps agent attempt id,0
test connection,0
test master client,0
test ip,0
test loc,0
test master location,0
test ps location,0
"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));",0
test all ps ids,0
test all matrix ids,0
test all matrix names,0
test matrix attribute,0
test matrix meta,0
test ps location,0
test partitions,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
https://blog.csdn.net/cocoonyang/article/details/63068108,0
v1[i] = v1[i] + da * v2[i];,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
v1[i] = v1[i] + da * v2[i];,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
Default does nothing.,0
The app injection is optional,0
"renderText(""hello world"");",0
"user choose a workerGroupID from the workergroups page,",0
now we should change the AngelApp params and render the workergroup page;,0
"static final String WORKER_ID = ""worker.id"";",0
"div(""#logo"").",0
"img(""/static/hadoop-st.png"")._().",0
import org.apache.hadoop.yarn.webapp.view.FooterBlock;,0
import org.apache.hadoop.yarn.webapp.view.HeaderBlock;,0
JQueryUI.jsnotice(html);,0
import org.apache.hadoop.conf.Configuration;,0
import java.lang.reflect.Field;,0
all the files in input set,0
Shuffle the file,0
Get the blocks for all files,0
Adjust the maxSize to make the split more balanced,0
Handle the splittable files,0
Handle the unsplittable files,0
Split the blocks,0
"If the remaining size of the current block is smaller than the required size,",0
the remaining blocks are divided into the current split,0
Update current split length and move to next block,0
Clear the current block offset,0
"Current split length is > maxSize, split the block and generate a new split",0
Clear blocks list for next split,0
Clear the current split length,0
"If splitBlocks is not empty, just genetate a split for it",0
get block locations from file system,0
create an input split,0
get block locations from file system,0
create a list of all block and their locations,0
"if the file is not splitable, just create the one block with",0
full file length,0
each split can be a maximum of maxSize,0
if remainder is between max and 2*max - then,0
"instead of creating splits of size max, left-max we",0
create splits of size left/2 and left/2. This is,0
a heuristic to avoid creating really really small,0
splits.,0
add this block to the block --> node locations map,0
"For blocks that do not have host/rack information,",0
assign to default  rack.,0
add this block to the rack --> block map,0
Add this host to rackToNodes map,0
add this block to the node --> block map,0
"if the file system does not have any rack information, then",0
use dummy rack location.,0
The topology paths have the host name included as the last,0
component. Strip it.,0
get tokens for all the required FileSystems..,0
"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,",0
job.getConfiguration());,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
all the files in input set,0
it is allowed for maxSize to be 0. Disable smoothing load for such cases,0
process all nodes and create splits that are local to a node. Generate,0
"one split per node iteration, and walk over nodes multiple times to",0
distribute the splits across nodes.,0
Skip the node if it has previously been marked as completed.,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
Remove all blocks which may already have been assigned to other,0
splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
Remove entries from blocksInNode so that we don't walk these,0
again.,0
Done creating a single split for this node. Move on to the next,0
node so that splits are distributed across nodes.,0
This implies that the last few blocks (or all in case maxSize=0),0
were not part of a split. The node is complete.,0
if there were any blocks left over and their combined size is,0
"larger than minSplitNode, then combine them into one split.",0
Otherwise add them back to the unprocessed pool. It is likely,0
that they will be combined with other blocks from the,0
same rack later on.,0
This condition also kicks in when max split size is not set. All,0
blocks on a node will be grouped together into a single split.,0
haven't created any split on this machine. so its ok to add a,0
smaller one for parallelism. Otherwise group it in the rack for,0
balanced size create an input split and add it to the splits,0
array,0
Remove entries from blocksInNode so that we don't walk this again.,0
The node is done. This was the last set of blocks for this node.,0
Put the unplaced blocks back into the pool for later rack-allocation.,0
Node is done. All blocks were fit into node-local splits.,0
Check if node-local assignments are complete.,0
All nodes have been walked over and marked as completed or all blocks,0
have been assigned. The rest should be handled via rackLock assignment.,0
"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """,0
"+ completedNodes.size() + "", size left: "" + totalLength);",0
"if blocks in a rack are below the specified minimum size, then keep them",0
"in 'overflow'. After the processing of all racks is complete, these",0
overflow blocks will be combined into splits.,0
Process all racks over and over again until there is no more work to do.,0
Create one split for this rack before moving over to the next rack.,0
Come back to this rack after creating a single split for each of the,0
remaining racks.,0
"Process one rack location at a time, Combine all possible blocks that",0
reside on this rack as one split. (constrained by minimum and maximum,0
split size).,0
iterate over all racks,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"if we created a split, then just go to the next rack",0
"if there is a minimum size specified, then create a single split",0
"otherwise, store these blocks into overflow data structure",0
There were a few blocks in this rack that,0
remained to be processed. Keep them in 'overflow' block list.,0
These will be combined later.,0
Process all overflow blocks,0
"This might cause an exiting rack location to be re-added,",0
but it should be ok.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"Process any remaining blocks, if any.",0
create an input split,0
add this split to the list that is returned,0
long num = totLength / maxSize;,0
all blocks for all the files in input set,0
mapping from a rack name to the list of blocks it has,0
mapping from a block to the nodes on which it has replicas,0
mapping from a node to the list of blocks that it contains,0
populate all the blocks for all files,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
"Security framework already loaded the tokens into current UGI, just use",0
them,0
Now remove the AM->RM token so tasks don't have it,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init container allocator,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
Init Client manager,0
Init PS Client manager,0
init parameter server manager,0
recover task information if needed,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
Init model saver & loader,0
start a web service if use yarn deploy mode,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
Check Workers,0
Check PSS,0
Check Clients,0
Check PS Clients,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
Init Client manager,0
Init PS Client manager,0
init parameter server manager,0
recover task information if needed,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
Init model saver & loader,0
k8sClusterManager = new KubernetesClusterManager(appContext);,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
parse parameter server counters,0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
refresh last heartbeat timestamp,0
send a state update event to the specific PSAttempt,0
Check is there save request,0
"LOG.info(""PS "" + psAttemptId + "" need save "" + subSaveContext);",0
Check is there load request,0
"LOG.info(""PS "" + psAttemptId + "" need load "" + subLoadContext);",0
check matrix metadata inconsistencies between master and parameter server.,0
"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix",0
"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix",0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
choose a unused port,0
start RPC server,0
remove this parameter server attempt from monitor set,0
remove this parameter server attempt from monitor set,0
"if worker attempt id is not in monitor set, we should shutdown it",0
find workergroup in worker manager,0
"if this worker group does not initialized, just return WORKERGROUP_NOTREADY",0
"if this worker group run over, just return WORKERGROUP_EXITED",0
"if this worker group is running now, return tasks, workers, data splits for it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update the clock for this matrix,0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update task iteration,0
"LOG.info(""error happened in psAttempt "" + psAttemptId + "" error msg="" + request.getMsg());",0
remove this parameter server attempt from monitor set,0
"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);",0
"Calculate how many splits we need. As each task handles a separate split of data, so we want",0
the number of splits equal to the number of tasks,0
split data,0
dispatch the splits to workergroups,0
split data,0
dispatch the splits to workergroups,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
write meta data to a temporary file,0
rename the temporary file to final file,0
"if the file exists, read from file and deserialize it",0
write task meta,0
write ps meta,0
generate a temporary file,0
write task meta to the temporary file first,0
rename the temporary file to the final file,0
"if last final task file exist, remove it",0
find task meta file which has max timestamp,0
"if the file does not exist, just return null",0
read task meta from file and deserialize it,0
generate a temporary file,0
write ps meta to the temporary file first.,0
rename the temporary file to the final file,0
"if the old final file exist, just remove it",0
find ps meta file,0
"if ps meta file does not exist, just return null",0
read ps meta from file and deserialize it,0
"Path tmpPath = new Path(new Path(context.getConf().get(AngelConf.ANGEL_JOB_TMP_OUTPUT_PATH)),",0
String.valueOf(requestId));,0
Path tmpPath = HdfsUtil.toTmpPath(new Path(saveContext.getSavePath()));,0
saveContext.setTmpSavePath(tmpPath.toString());,0
Filter old epoch trigger first,0
Split the user request to sub-requests to pss,0
Init matrix files meta,0
Move output files,0
Write the meta file,0
Split the user request to sub-requests to pss,0
check whether psagent heartbeat timeout,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
LocalStorageToken is needed irrespective of whether security is enabled,0
or not.,0
"TokenCache.setJobToken(jobToken, taskCredentials);",0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Construct the actual Container,0
The null fields are per-container and will be constructed for each,0
container separately.,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
"a * in the classpath will only find a .jar, so we need to filter out",0
all .jars and add everything else,0
Propagate the system classpath when using the mini cluster,0
Add standard Hadoop classes,0
Add mr,0
Cache archives,0
Cache files,0
Sanity check,0
Add URI fragment or just the filename,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Old parameter name,0
Parallel GC parameters,0
G1 params,0
Parallel Scavenge + Parallel Old,0
G1,0
".append("" -XX:G1NewSizePercent="").append(minNewRatio)",0
".append("" -XX:G1MaxNewSizePercent="").append(maxNewRatio)",0
CMS,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
G1 params,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Final commmand,0
"if amTask is not null, we should clone task state from it",0
"if all parameter server complete commit, master can commit now",0
restartPS(psLoc);,0
check whether parameter server heartbeat timeout,0
Transitions from the NEW state.,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event",0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event dispatched before",0
PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.COMMITTING state,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
reqeuest resource:send a resource request to the resource allocator,0
"Once the resource is applied, build and send the launch request to the container launcher",0
deallocator the resource of the ps attempt:send a resource deallocator request to the,0
resource allocator,0
set the launch time,0
add the ps attempt to the heartbeat timeout monitoring list,0
parse ps attempt location and put it to location manager,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt",0
or failed,0
remove ps attempt id from heartbeat timeout monitor list,0
release container:send a release request to container launcher,0
set the finish time only if launch time is set,0
private long scheduledTime;,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
Refresh ps location & matrix meta,0
start a new attempt for this ps,0
notify ps manager,0
"getContext().getLocationManager().setPsLocation(id, null);",0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
notify the event handler of state change,0
"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor",0
"if forcedState is set, just return",0
else get state from state machine,0
add this worker group to the success set,0
check if all worker group run or run over,0
add this worker group to the success set,0
check if all worker group run over,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
calculate the actual number of worker groups and the total number of tasks based on the number of data split,0
"init all tasks , workers and worker groups and put them to the corresponding maps",0
just return the total task number now,0
TODO,1
check whether worker heartbeat timeout,0
"if workerAttempt is not null, we should clone task state from it",0
from NEW state,0
from SCHEDULED state,0
get data splits location for data locality,0
reqeuest resource:send a resource request to the resource allocator,0
"once the resource is applied, build and send the launch request to the container launcher",0
notify failed message to the worker,0
notify killed message to the worker,0
release the allocated container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
release the allocated container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
clean the container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
record the finish time,0
clean the container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listening list,0
record the finish time,0
"if the worker attempt launch successfully, add it to heartbeat timeout listening list",0
set worker attempt location,0
notify the register message to the worker,0
record the launch time,0
update worker attempt metrics,0
update tasks metrics,0
clean the container,0
notify the worker attempt run successfully message to the worker,0
record the finish time,0
todo,1
init a worker attempt for the worker,0
schedule the worker attempt,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
"If we need Yarn to restart a new application master, we should not unregister from Yarn RM",0
register to Yarn RM,0
send heartbeat to Yarn RM every rmPollInterval milliseconds,0
"catch YarnException or YarnRuntimeException, we should exit and need not retry",0
build heartbeat request,0
send heartbeat request to rm,0
"This can happen if the RM has been restarted. If it is in that state,",0
this application must clean itself up.,0
Setting NMTokens,0
assgin containers,0
"if some container is not assigned, release them",0
handle finish containers,0
dispatch container exit message to corresponding components,0
killed by framework,0
killed by framework,0
get application finish state,0
build application diagnostics,0
TODO:add a job history for angel,1
build unregister request,0
send unregister request to rm,0
Note this down for next interaction with ResourceManager,0
based on blacklisting comments above we can end up decrementing more,0
than requested. so guard for that.,0
send the updated resource request to RM,0
send 0 container count requests also to cancel previous requests,0
Update resource requests,0
try to assign to all nodes first to match node local,0
try to match all rack local,0
assign remaining,0
Update resource requests,0
send the container-assigned event to task attempt,0
build the start container request use launch context,0
send the start request to Yarn nm,0
send the message that the container starts successfully to the corresponding component,0
"after launching, send launched event to task attempt to move",0
it from ASSIGNED to RUNNING state,0
send the message that the container starts failed to the corresponding component,0
kill the remote container if already launched,0
start a thread pool to startup the container,0
See if we need up the pool size only if haven't reached the,0
maximum limit yet.,0
nodes where containers will run at *this* point of time. This is,0
*not* the cluster size and doesn't need to be.,0
"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the",0
later is just a buffer so we are not always increasing the,0
pool-size,0
the events from the queue are handled in parallel,0
using a thread pool,0
return if already stopped,0
shutdown any containers that might be left running,0
Add one sync matrix,0
addSyncMatrix();,0
"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
"matrixContext.set(MatrixConf.MATRIX_LOAD_PATH, """");",0
"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {",0
"LOG.info(""ps id = "" + psEntry.getKey());",0
"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();",0
"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {",0
"LOG.info(""matrix id = "" + metaEntry.getKey());",0
"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());",0
},0
},0
get matrix ids in the parameter server report,0
get the matrices parameter server need to create and delete,0
"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix",0
"if a matrix exists on master but not exist on parameter server, this parameter server need build it.",0
Init control connection manager,0
Get ps locations from master and put them to the location cache.,0
Build and initialize rpc client to master,0
Get psagent id,0
Build PS control rpc client manager,0
Build local location,0
Initialize matrix meta information,0
Start all services,0
Stop all modules,0
Stop all modules,0
clock first,0
wait,0
Update generic resource counters,0
Updating resources specified in ResourceCalculatorProcessTree,0
Remove the CPU time consumed previously by JVM reuse,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Plus a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Update a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get values from pss use row/column indices,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"/ PSF get/update, use can implement their own psf",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get a row or a batch of rows,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
Just return,0
Just return,0
Just return,0
Just return,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
Sort the partitions by start column index,0
Generate a flush request and put it to request queue,0
Generate a clock request and put it to request queue,0
Generate a merge request and put it to request queue,0
Generate a merge request and put it to request queue,0
"If the matrix op log cache does not exist for the matrix, create a new one for the",0
matrix,0
and add it to cache maps,0
Add the message to the tree map,0
"If there are flush / clock requests blocked, we need to put this merge request into",0
the waiting queue,0
Launch a merge worker to merge the update to matrix op log cache,0
Remove the message from the tree map,0
Wake up blocked flush/clock request,0
Add flush/clock request to listener list to waiting for all the existing,0
updates are merged,0
Wake up blocked flush/clock request,0
"If all updates are merged for this matrix, we need wake up flush/clock requests which are",0
blocked.,0
Get next merge message sequence id,0
Wake up listeners(flush/clock requests) that have little sequence id than current merge,0
position,0
Wake up blocked merge requests,0
Get minimal sequence id from listeners,0
"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we",0
should flush updates to local matrix storage,0
Doing average or not,0
Filter un-important update,0
Split this row according the matrix partitions,0
Set split context,0
Remove the row from matrix,0
buf.writeDouble(0.0);,0
TODO:,1
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());",0
"LOG.error(""put response message queue failed "", e);",0
Use Epoll for linux,0
Update location table,0
Remove the server from failed list,0
Notify refresh success message to request dispatcher,0
Check PS exist or not,0
Check heartbeat timeout,0
getPSState(entry.getKey());,0
Check PS restart or not,0
private final HashSet<ParameterServerId> refreshingServerSet;,0
Add it to failed rpc list,0
Add the server to gray server list,0
Add it to failed rpc list,0
Add the server to gray server list,0
Move from gray server list to failed server list,0
Handle the RPCS to this server,0
Submit the schedulable failed get RPCS,0
Submit new get RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"If the queue is empty, just return 0",0
"If request is not over limit, just submit it",0
Submit the schedulable failed get RPCS,0
Submit new put RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"LOG.info(""choose put server "" + psIds[index]);",0
Check all pending RPCS,0
Check get channel context,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
channelManager.printPools();,0
"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {",0
if(ts - entry.getValue() > requestTimeOut * 2)  {,0
"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())",0
"+ "" milliseconds, close all channels to it"");",0
closeChannels(entry.getKey());,0
"psLocToLastChannelTsMap.put(entry.getKey(), ts);",0
},0
},0
"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);",0
Remove all pending RPCS,0
Close all channel to this PS,0
private Channel getChannel(Location loc) throws Exception {,0
"return channelManager.getOrCreateChannel(new Location(loc.getIp(), loc.getPort() + 1));",0
},0
private GenericObjectPool<Channel> getChannelPool(Location loc) throws InterruptedException {,0
"return channelManager.getOrCreateChannelPool(new Location(loc.getIp(), loc.getPort() + 1), PSAgentContext",0
.get(),0
.getConf(),0
".getInt(AngelConf.ANGEL_WORKER_TASK_NUMBER,",0
AngelConf.DEFAULT_ANGEL_WORKER_TASK_NUMBER));,0
},0
Get server id and location for this request,0
"If location is null, means that the server is not ready",0
Get the channel for the location,0
Check if need get token first,0
Serialize the request,0
Send the request,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
request.getContext().setChannelPool(pool);,0
Allocate the bytebuf and serialize the request,0
find the partition request context from cache,0
"updateMatrixCache(request.getPartKey(), response.getPartition());",0
"updateMatrixCache(request.getPartKey(), response.getRowsSplit());",0
TODO,1
ServerRow rowSplit = PSAgentContext.get().getMatricesCache(),0
".getRowSplit(request.getPartKey().getMatrixId(), request.getPartKey(),",0
request.getRowIndex());,0
response.setRowSplit(rowSplit);,0
"updateMatrixCache(request.getPartKey(), response.getRowSplit());",0
"LOG.info(""user request id "" + request.getUserRequestId());",0
"LOG.info(""user request id "" + request.getUserRequestId());",0
TODO,1
"private void updateMatrixCache(PartitionKey partKey, ServerPartition partition) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, partition);",0
},0
"private void updateMatrixCache(PartitionKey partKey, ServerRow rowSplit) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowSplit);",0
},0
"private void updateMatrixCache(PartitionKey partKey, List<ServerRow> rowsSplit) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowsSplit);",0
},0
Get partitions for this row,0
Distinct get row requests,0
Get row splits of this row from the matrix cache first,0
responseCache.addSubResponse(rowSplit);,0
"If the row split does not exist in cache, get it from parameter server",0
Split the param use matrix partitions,0
Send request to PSS,0
Split the matrix oplog according to the matrix partitions,0
"If need update clock, we should send requests to all partitions",0
Send request to PSS,0
Filter the rowIds which are fetching now,0
Send the rowIndex to rpc dispatcher and return immediately,0
"LOG.info(""get row split use time="" + (System.currentTimeMillis() - startTs));",0
"LOG.info(""start to request "" + requestId);",0
"LOG.info(""start to request "" + requestId);",0
Split param use matrix partitons,0
"If all sub-results are received, just remove request and result cache",0
Split this row according the matrix partitions,0
Set split context,0
Split this row according the matrix partitions,0
Set split context,0
long startTs = System.currentTimeMillis();,0
"LOG.error(""combine use time = "" + (System.currentTimeMillis() - startTs));",0
Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition,0
Generate dispatch items and add them to the corresponding queues,0
Filter the rowIds which are fetching now,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {",0
return key1.getStartCol() < key2.getStartCol() ? -1 : 1;,0
});,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {",0
return key1.getStartCol() < key2.getStartCol() ? -1 : 1;,0
});,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Put the row split to the cache(row index to row splits map),0
"If all splits of the row are received, means this row can be merged",0
TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage,1
TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage,1
TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage,1
TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage,1
TODO,1
TODO,1
/////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO,1
buf.writeDouble(0);,0
TODO,1
///////////////////////////////////////////////////////////////////////////////////////////////,0
Now we just support pipelined row splits merging for dense type row,0
Pre-fetching is disable default,0
matrix id to clock map,0
"task index, it must be unique for whole application",0
Deserialize data splits meta,0
Get workers,0
Send request to every ps,0
Wait the responses,0
Update clock cache,0
if(syncNum % 1024 == 0) {,0
},0
"Use simple flow, do not use any cache",0
Get row from cache.,0
"if row clock is satisfy ssp staleness limit, just return.",0
Get row from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC mode, just get from pss.",0
"For BSP/SSP, get rows from storage/cache first",0
Get from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC, just get rows from pss.",0
no more retries.,0
calculate sleep time and return.,0
parse the i-th sleep-time,0
parse the i-th number-of-retries,0
calculateSleepTime may overflow.,0
"A few common retry policies, with no delays.",0
Read matrix meta from meta file,0
Save partitions to files use fork-join,0
Write the ps matrix meta to the meta file,0
matrix.startServering();,0
return;,0
Read matrix meta from meta file,0
Load partitions from file use fork-join,0
Read matrix meta from meta file,0
Sort partitions,0
TODO:,1
int size = rows.length;,0
int size = rows.length;,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
close is a local operation and should finish within milliseconds; timeout just to be safe,0
response will be null for one way messages.,0
maxFrameLength = 2G,0
lengthFieldOffset = 0,0
lengthFieldLength = 8,0
"lengthAdjustment = -8, i.e. exclude the 8 byte length itself",0
"initialBytesToStrip = 8, i.e. strip out the length field itself",0
"return Objects.toStringHelper(this).add(""serial"", serial).add(""listSize"", datas.size())",0
.toString();,0
indicates whether this connection's life cycle is managed,0
See if we already have a connection (common case),0
create a unique lock for this RS + protocol (if necessary),0
get the RS lock,0
do one more lookup in case we were stalled above,0
Only create isa when we need to.,0
definitely a cache miss. establish an RPC for,0
this RS,0
Throw what the RemoteException was carrying.,0
check,0
every,0
minutes,0
TODO,1
failoverHandler,0
"The number of times this invocation handler has ever been failed over,",0
before this method invocation attempt. Used to prevent concurrent,0
failed method invocations from triggering multiple failover attempts.,0
Make sure that concurrent failed method invocations,0
only cause a,0
single actual fail over.,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
"LOG.info(""method "" + method.getName() + ""construct request time = """,0
+ (System.currentTimeMillis() - beforeConstructTs));,0
get an instance of the method arg type,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
Message (hand written code usually has only a single,0
argument),0
log any RPC responses that are slower than the configured,0
warn,0
response time or larger than configured warning size,0
"when tagging, we let TooLarge trump TooSmall to keep",0
output simple,0
note that large responses will often also be slow.,0
provides a count of log-reported slow responses,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
unexpected,0
"in the protobuf methods, args[1] is the only significant argument",0
for JSON encoding,0
base information that is reported regardless of type of call,0
Disable Nagle's Algorithm since we don't want packets to wait,0
Configure the event pipeline factory.,0
Make a new connection.,0
Remove all pending requests (will be canceled after relinquishing,0
write lock).,0
Cancel any pending requests by sending errors to the callbacks:,0
Close the channel:,0
Close the connection:,0
Shut down all thread pools to exit.,0
"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());",0
See NettyServer.prepareResponse for where we write out the response.,0
"It writes the call.id (int), a boolean signifying any error (and if",0
"so the exception name/trace), and the response bytes",0
Read the call id.,0
"When the stream is closed, protobuf doesn't raise an EOFException,",0
"instead, it returns a null message object.",0
"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +",0
System.currentTimeMillis());,0
"It would be good widen this to just Throwable, but IOException is what we",0
allow now,0
not implemented,1
not implemented,1
"track what RpcEngine is used by a proxy class, for stopProxy()",0
cache of RpcEngines by protocol,0
return the RpcEngine configured to handle a protocol,0
We only handle the ConnectException.,0
This is the exception we can't handle.,0
check if timed out,0
wait for retry,0
IGNORE,0
return the RpcEngine that handles a proxy object,0
The default implementation works synchronously,0
punt: allocate a new buffer & copy into it,0
Parse cmd parameters,0
load hadoop configuration,0
load angel system configuration,0
load user configuration:,0
load user config file,0
load command line parameters,0
load user job resource files,0
load ml conf file for graph based algorithm,0
load user job jar if it exist,0
Expand the environment variable,0
Add default fs(local fs) for lib jars.,0
"LOG.info(System.getProperty(""user.dir""));",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
a simple hdfs copy function assume src path and dest path are in same hdfs,0
and FileSystem object has same schema,0
"If out path exist , just remove it first",0
Create parent directory if not exist,0
Rename,0
"LOG.warn(""interrupted while sleeping"", ie);",0
public static String getHostname() {,0
try {,0
"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();",0
} catch (UnknownHostException uhe) {,0
},0
"return new StringBuilder().append("""").append(uhe).toString();",0
},0
"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {",0
String hostname = getHostname();,0
String classname = clazz.getSimpleName();,0
"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new",0
"StringBuilder().append(""Starting "").append(classname).toString(), new",0
"StringBuilder().append(""  host = "").append(hostname).toString(), new",0
"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new",0
"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new",0
"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));",0
,0
"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {",0
public void run() {,0
"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +",0
"this.val$classname + "" at "" + this.val$hostname}));",0
},0
});,0
},0
"We we interrupted because we're meant to stop? If not, just",0
continue ignoring the interruption,0
Recalculate waitTime.,0
// Begin delegation to Thread,0
// End delegation to Thread,0
instance submitter class,0
Obtain filename from path,0
Split filename to prexif and suffix (extension),0
Check if the filename is okay,0
Prepare temporary file,0
Prepare buffer for data copying,0
Open and check input stream,0
Open output stream and copy data between source file in JAR and the temporary file,0
"If read/write fails, close streams safely before throwing an exception",0
"Finally, load the library",0
little endian load order,0
tail,0
fallthrough,0
fallthrough,0
finalization,0
fmix(h1);,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
throw new AngelException(e);,0
JobStateProto jobState = report.getJobState();,0
Check need load matrices,0
Used for java code to get a AngelClient instance,0
Used for python code to get a AngelClient instance,0
load user job resource files,0
setLocalAddr();,0
2.get job id,0
5.write configuration to a xml file,0
8.get app master client,0
Write job file to JobTracker's fs,0
the leaf level file should be readable by others,0
the subdirs in the path should have execute permissions for,0
others,0
2.get job id,0
Credentials credentials = new Credentials();,0
4.copy resource files to hdfs,0
5.write configuration to a xml file,0
6.create am container context,0
7.Submit to ResourceManager,0
8.get app master client,0
Create a number of filenames in the JobTracker's fs namespace,0
add all the command line files/ jars and archive,0
first copy them to jobtrackers filesystem,0
should not throw a uri exception,0
should not throw an uri excpetion,0
set the timestamps of the archives and files,0
set the public/private visibility of the archives and files,0
get DelegationToken for each cached file,0
check if we do not need to copy the files,0
is jt using the same file system.,0
just checking for uri strings... doing no dns lookups,0
to see if the filesystems are the same. This is not optimal.,0
but avoids name resolution.,0
this might have name collisions. copy will throw an exception,0
parse the original path to create new path,0
check for ports,0
Write job file to JobTracker's fs,0
Setup resource requirements,0
Setup LocalResources,0
Setup security tokens,0
Setup the command to run the AM,0
Add AM user command opts,0
Final command,0
Setup the CLASSPATH in environment,0
"i.e. add { Hadoop jars, job jar, CWD } to classpath.",0
Setup the environment variables for Admin first,0
"Setup the environment variables (LD_LIBRARY_PATH, etc)",0
Parse distributed cache,0
Setup ContainerLaunchContext for AM container,0
Set up the ApplicationSubmissionContext,0
private volatile PS2PSPusherImpl ps2PSPusher;,0
TODO,1
Add tokens to new user so that it may execute its task correctly.,0
TODO,1
to exit,0
TODO,1
TODO,1
ps2PSPusher.recover(ProtobufUtil.convert(needRecoverParts.get(i)));,0
context.getSnapshotManager().processRecovery();,0
Recover PS from snapshot or load path,0
1. First check old snapshot,0
2. Check new checkpoints,0
3. Check load path setting and old save result,0
Just init it again,0
TODO,1
if(ps2PSPusher != null) {,0
ps2PSPusher.start();,0
},0
public PS2PSPusherImpl getPs2PSPusher() {,0
return ps2PSPusher;,0
},0
Filter the head,0
Get the RPC destination,0
Get and init the queue,0
"If the queue is empty, activate the processor",0
Just put it to the rpc queue,0
if(useInDepWorkers) {,0
Use independent rpc workers,0
if (method == TransportMethod.GET_CLOCKS || method == TransportMethod.UPDATE_CLOCK) {,0
"Small rpc request, use sync to avoid thread switch",0
return false;,0
},0
return true;,0
} else {,0
return false;,0
},0
if (!useSync && useAyncHandler) {,0
"senderPool.execute(new Sender(clientId, seqId, method, ctx, result));",0
} else {,0
"send(clientId, seqId, method, ctx, result);",0
},0
Release the input buffer,0
Release the input buffer,0
"1. handle the rpc, get the response",0
Release the input buffer,0
2. Serialize the response,0
Send the serialized response,0
Exception happened,0
write seq id,0
Just serialize the head,0
Exception happened,0
Allocate result buffer,0
Exception happened,0
Just serialize the head,0
Exception happened,0
runningContext.printToken();,0
Reset the response and allocate buffer again,0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this location, only master ps can accept the update",0
Check the partition state again,0
Start to put the update to the slave pss,0
TODO,1
"context.getPS2PSPusher().put(request, in, partLoc);",0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this partition, if not, just return failed",0
Start to put the update to the slave pss,0
TODO,1
"int maxRPCCounter = Math.max(estSize, (int) (workerNum * factor));",0
"for (Map.Entry<Integer, ClientRunningContext> clientEntry : clientRPCCounters.entrySet()) {",0
"LOG.info(""client "" + clientEntry.getKey() + "" running context:"");",0
clientEntry.getValue().printToken();,0
},0
return ServerState.GENERAL;,0
Use Epoll for linux,0
public String uuid;,0
TODO:,1
part = new ServerPartition();,0
TODO:,1
public void setChannelPool(GenericObjectPool<Channel> channelPool) {,0
this.channelPool = channelPool;,0
},0
private final ParameterServer psServer;,0
Create and start workers,0
Set workers,0
Create and start workers,0
Set workers,0
"If matrix checkpoint path not exist, just return null",0
Return the path with maximum checkpoint id,0
Rename temp to item path,0
Checkpoint base path = Base dir/matrix name,0
Path for this checkpoint,0
Generate tmp path,0
Delete old checkpoints,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"////// network io method, for model transform",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
Serailize the head,0
Serialize the storage,0
Deserailze the head,0
Deseralize the storage,0
Serailize the head,0
Serialize the storage,0
Deserailze the head,0
Deseralize the storage,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"Notice: Basic type ServerRow only support Vector as inner storage now, so you can use methods",0
to get inner vector for basic type ServerRow.,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
Just update the exist element now!!,0
Just update the exist element now!!,0
TODO: just check the value is 0 or not now,1
TODO: just check the value is zero or not now,1
"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low.",0
Attention: Only update the exist values for sorted storage method,0
Attention: Only update exist element,0
Attention: Only update the exist values for sorted storage method,0
Attention: Only update exist element,0
TODO: just check the value is zero or not now,1
"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low.",0
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
Element data,0
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
Attention: Only update the exist values for sorted storage method,0
Attention: Only update exist element,0
TODO: just check the value is zero or not now,1
"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low.",0
TODO: just check the value is 0 or not now,1
Just update the exist element now!!,0
Just update the exist element now!!,0
"Use sparse storage method, as some elements in the array maybe null",0
Array length,0
Valid element number,0
Element data,0
Array len,0
Valid element number,0
"Use sparse storage method, as some elements in the array maybe null",0
Array length,0
Valid element number,0
Element data,0
Element data,0
Array len,0
Valid element number,0
Attention: Only update the exist values for sorted storage method,0
Attention: Only update exist element,0
TODO: just check the value is zero or not now,1
"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low.",0
Row type,0
Storage method,0
Key type,0
Value type,0
Vector dim,0
Vector length,0
Vector data,0
Row type,0
Storage method,0
Key type,0
Value type,0
Vector dim,0
Vector length,0
Init the vector,0
Vector data,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
Impossible now,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
Impossible now,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
Impossible now,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
Impossible now,0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
Just update the exist element now!!,0
Just update the exist element now!!,0
TODO: just check the value is 0 or not now,1
Just update the exist element now!!,0
Just update the exist element now!!,0
TODO: just check the value is 0 or not now,1
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
Element data,0
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
private final List<PartitionKey> partitionKeys;,0
Get server partition class,0
"If partition class is not set, just use the default partition class",0
Get server partition storage class type,0
Get value class,0
"if col == -1, we use the start/end index to calculate range,",0
we use double to store the range value since two long minus might exceed the,0
range of long.,0
Serialize the head,0
Serialize the storage,0
Deserialize the head,0
Deseralize the storage,0
Serialize the head,0
Serialize the storage,0
Deserialize the head,0
Deseralize the storage,0
Row base partition,0
"If storage class is not set, use default DenseServerRowsStorage",0
Serialize values,0
Deserialize values,0
Array size,0
Actual write size,0
Rows data,0
Row id,0
Row type,0
Row data,0
Array size,0
Actual write row number,0
Rows data,0
Row id,0
Create empty server row,0
Row data,0
Rows data,0
TODO,1
Serialize row offsets,0
Serialize column offsets,0
Deserialize row offset,0
Deserialize row offset,0
"If storage is set, just get a instance",0
"If storage is not set, use default",0
"If storage is set, just get a instance",0
"If storage is not set, use default",0
Map size,0
Actual write size,0
Rows data,0
Row id,0
Row type,0
Row data,0
Array size,0
Actual write row number,0
Rows data,0
Row id,0
Create empty server row,0
Row data,0
Rows data,0
Use Epoll for linux,0
find the partition request context from cache,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
channelManager.removeChannelPool(loc);,0
Generate seq id,0
Create a RecoverPartRequest,0
Serialize the request,0
Change the seqId for the request,0
Serialize the request,0
"First check the state of the channels in the pool, if a channel is unused, just return",0
"If all channels are in use, create a new channel or wait",0
Create a new channel,0
"add the PSAgentContext,need fix",0
If col == -1 and start/end not set,0
start/end set,0
"for dense type, we need to set the colNum to set dim for vectors",0
"colNum set, start/end not set",0
Row number must > 0,0
"both set, check its valid",0
public static final int T_INT_ARBITRARY_VALUE = 28;,0
public static final int T_INVALID_VALUE = 29;,0
TODO:add more vector type,1
TODO : subDim set,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Split updates,0
Shuffle update splits,0
Generate part update parameters,0
"Set split context: partition key, use int key for long key vector or not ect",0
write the max abs,0
---------------------------------------------------,0
---------------------------------------------------,0
---------------------------------------------------------------,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
-----------------,0
-----------------,0
-----------------,0
-----------------,0
-----------------,0
KeepStorage is guaranteed,0
"ignore the isInplace option, since v2 is dense",0
"the value in old storage can be changed safe, so switch a storage",0
"but user required keep storage, we can prevent rehash",0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
KeepStorage is guaranteed,0
we gauss dense storage is more efficient,0
v1Size < v2Size * Constant.sparseThreshold,0
KeepStorage is guaranteed,0
"ignore the isInplace option, since v2 is dense",0
"the value in old storage can be changed safe, so switch a storage",0
"but user required keep storage, we can prevent rehash",0
we gauss dense storage is more efficient,0
prevent rehash,0
KeepStorage is guaranteed,0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
dense preferred,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
sorted preferred,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"Transform mat1, generate a new matrix",0
Split the row indices of mat1Trans,0
Parallel execute use fork-join,0
"Get the sub-matrix of left matrix, split by row",0
"Transform mat1, generate a new matrix",0
Split the row indices of mat1Trans,0
Parallel execute use fork-join,0
"Get the sub-matrix of left matrix, split by row",0
"mat1 trans true, mat trans true",0
"mat1 trans true, mat trans false",0
"mat1 trans false, mat trans true, important",0
"mat1 trans false, mat trans false",0
"mat1 trans true, mat trans true",0
"mat1 trans true, mat trans false",0
"mat1 trans false, mat trans true, important",0
"mat1 trans false, mat trans false",0
"dger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)",0
"sger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)",0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
get configuration from envs,0
get master location,0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
add dense double matrix,0
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
get configuration from config file,0
set localDir with enviroment set by nm.,0
get master location,0
init task manager and start tasks,0
start heartbeat thread,0
taskManager.assignTaskIds(response.getTaskidsList());,0
todo,1
"if worker timeout, it may be knocked off.",0
"SUCCESS, do nothing",0
heartbeatFailedTime = 0;,0
private KEY currentKey;,0
will be created,0
TODO Auto-generated method stub,1
Bitmap bitmap = new Bitmap();,0
int max = indexArray[size - 1];,0
byte [] bitIndexArray = new byte[max / 8 + 1];,0
for(int i = 0; i < size; i++){,0
int bitIndex = indexArray[i] >> 3;,0
int bitOffset = indexArray[i] - (bitIndex << 3);,0
switch(bitOffset){,0
case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;,0
case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;,0
case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;,0
case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;,0
case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;,0
case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;,0
case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;,0
case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;,0
},0
},0
"true, false",0
//////////////////////////////,0
Application Configs,0
//////////////////////////////,0
//////////////////////////////,0
Master Configs,0
//////////////////////////////,0
//////////////////////////////,0
Worker Configs,0
//////////////////////////////,0
//////////////////////////////,0
Task Configs,0
//////////////////////////////,0
//////////////////////////////,0
ParameterServer Configs,0
//////////////////////////////,0
//////////////////////////////,0
Kubernetes Configs.,0
//////////////////////////////,0
////////////////// IPC //////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
Configs used to ANGEL_PS_PSAGENT running mode future.,0
model parse,0
Mark whether use pyangel or not.,0
private Configuration conf;,0
"Configuration that should be used in python environment, there should only be one",0
configuration instance in each Angel context.,0
Use private access means jconf should not be changed or modified in this way.,0
Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:,0
Do nothing,0
To-DO: add other ways to justify different value types,0
"This is so ugly, must re-implement by more elegance way",1
"Create python path which include angel's jars, the python directory in ANGEL_HOME,",0
and other files submitted by user.,0
Launch python process,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);",0
get a angel client,0
add sparse float matrix,0
siMat.setPartitionStorageClass(LongElementMapStorage.class);,0
siMat.setPartitionClass(CSRPartition.class);,0
Start PS,0
Start to run application,0
Init node neighbors,0
client.asyncUpdate(new InitNeighborOver(new InitNeighborOverParam(matrixId))).get();,0
Sample the neighbors,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);",0
get a angel client,0
add sparse float matrix,0
siMat.setValidIndexNum(100);,0
siMat.setColNum(10000000000L);,0
siMat.setPartitionStorageClass(LongElementMapStorage.class);,0
siMat.setPartitionClass(CSRPartition.class);,0
Start PS,0
Start to run application,0
Init node neighbors and feats,0
Sample the neighbors,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);",0
get a angel client,0
add sparse float matrix,0
Start PS,0
Start to run application,0
Init node neighbors,0
Sample the neighbors,0
sample continuously beginning from a random index,0
Get node neighbor number,0
"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random",0
Get node neighbor number,0
"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random",0
"ServerLongAnyRow row = (ServerLongAnyRow) psContext.getMatrixStorageManager().getRow(pparam.getPartKey(), 0);",0
ObjectIterator<Long2ObjectMap.Entry<IElement>> iter = row.iterator();,0
while (iter.hasNext()) {,0
Long2ObjectMap.Entry<IElement> entry = iter.next();,0
long key = entry.getLongKey() + pparam.getPartKey().getStartCol();,0
WalkPath value = (WalkPath) entry.getValue();,0
,0
if (workerPartitionId == value.getNextPartitionIdx()) {,0
"result.put(key, value.getTail2());",0
},0
},0
"int matrixId, PartitionKey partKey, long[] keyIds, int startIdx, int endIdx",0
"System.out.println(""PathQueue: put data to queue"");",0
"System.out.println(""queue.size: "" + queue.size());",0
"System.out.println(""CurrPathIdx of "" + wPath.getHead() + "" is "" + wPath.getCurrPathIdx());",0
if (numRetry == retry) {,0
"System.out.println(""retried 3 time, got : "" + result.size());",0
},0
"System.out.println(""popBatch: "" + result.size() +"" | ""+ count);",0
"getRow(partKey.getMatrixId(), rowId, partKey.getPartitionId())",0
StringBuilder sb = new StringBuilder();,0
"sb.append(key).append("" -> {"");",0
for (long n: neighbor) {,0
"sb.append(n).append("", "");",0
},0
"sb.append(""} : "").append(neigh);",0
System.out.println(sb.toString());,0
"System.out.println(""pushed size: "" + pathTail.size());",0
List<LinkedBlockingQueue<WalkPath>> queueList = PathQueue.getQueueList(partKey.getPartitionId());,0
int p = 0;,0
for (LinkedBlockingQueue<WalkPath> queue: queueList) {,0
"System.out.println(""partition "" + p + "", size1 = ""+ pathTail.size() +  "" size2 = "" + queue.size());",0
p++;,0
},0
"System.out.println(""pushed batch finished!"");",0
Get node neighbor number,0
"If the neighbor number is 0, just return a int[0]",0
"If count <= 0 or the neighbor number is less or equal then count, just copy all neighbors to the result array",0
"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random",0
Store the total neighbor number of all nodes in rowOffsets,0
"Put the node ids, node neighbor number, node neighbors to the cache",0
No data in this partition,0
Get total neighbor number,0
Final matrix column indices: neighbors node ids,0
Write positions in cloumnIndices for nodes,0
Copy all cached sub column indices to final column indices,0
Read position for a sub column indices,0
Copy column indices for a node to final column indices,0
Update write position for this node in final column indices,0
Update the read position in sub column indices,0
Clear all temp data,0
Get node neighbor number,0
"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random",0
sample happens here to avoid memory copy on servers,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Data format,0
Feature number of train data,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Ratio of validation,0
Learning rate,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"Set angel resource, #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Set training data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set predict data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model type,0
String modelType = String.valueOf(RowType.T_FLOAT_DENSE);,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
"conf.set(MLConf.ML_MODEL_TYPE(), modelType);",0
"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);",0
predictTest();,0
"Data format, libsvm or dummy",0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set data format,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
class number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set log path,0
Set trainning data path,0
Set save model path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set training data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log save path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set testing data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
TODO: optimize int key indices,1
"System.out.println(""deserialize cols.length="" + nCols);",0
"System.out.print(""deserialize "");",0
"System.out.print(cols[c] + "" "");",0
System.out.println();,0
TODO Auto-generated method stub,1
"LOG.info(""original float length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));",0
"LOG.info(""original double length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));",0
"LOG.info(""parsed float length: "" + length + Arrays.toString(arr));",0
"LOG.info(""parsed double length: "" + length + Arrays.toString(arr));",0
"ground truth: positive, precision: positive",0
start row index for words,0
start row index for docs,0
doc ids,0
topic assignments,0
word to docs reverse index,0
count word,0
build word start index,0
build word to doc reverse idx,0
build dks,0
dks = new TraverseHashMap[n_docs];,0
for (int d = 0; d < n_docs; d++) {,0
if (K < Short.MAX_VALUE) {,0
if (docs.get(d).len < Byte.MAX_VALUE),0
dks[d] = new S2BTraverseMap(docs.get(d).len);,0
if (docs.get(d).len < Short.MAX_VALUE),0
"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));",0
else,0
"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));",0
} else {,0
"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));",0
},0
},0
build dks,0
allocate update maps,0
Skip if no token for this word,0
Check whether error when fetching word-topic,0
Build FTree for current word,0
current doc,0
old topic assignment,0
"Check if error happens. if this happen, it's probably that failures happen to servers.",0
We need to adjust the memory settings or network fetching parameters.,0
Update statistics if needed,0
Calculate psum and sample new topic,0
Update statistics if needed,0
Assign new topic,0
Skip if no token for this word,0
if (u >= p[end]) {,0
"System.out.println(""u="" + u + "" p[end]="" + p[end] + "" start="" + start + "" end="" + end);",0
return end;,0
},0
,0
if (u < p[start]) {,0
"System.out.println(""u="" + u + "" p[start]="" + p[start] + "" start="" + start + "" end="" + end);",0
return start;,0
},0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
Write #rows,0
Write each row,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
dense,0
sparse,0
calculate columns,0
reset(row);,0
loss function,0
gradient and hessian,0
"categorical feature set, null: none, empty: all, else: partial",0
"node's end index in instancePos, instances in [start, end] belong to a tree node",0
initialize the phase,0
current tree and depth,0
create loss function,0
calculate grad info of each instance,0
"create data sketch, push candidate split value to PS",0
1. calculate candidate split value,0
categorical features,0
2. push local sketch to PS,0
the leader worker,0
merge categorical features,0
create updates,0
"pull the global sketch from PS, only called once by each worker",0
number of categorical feature,0
sample feature,0
push sampled feature set to the current tree,0
create new tree,0
"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,",0
calculate gradient,0
"1. create new tree, initialize tree nodes and node stats",0
"2. initialize feature set, if sampled, get from PS, otherwise use all the features",0
2.1. pull the sampled features of the current tree,0
this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();,0
"2.2. if use all the features, only called one",0
"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle",0
4. set root node to active,0
"5. reset instance position, set the root node's span",0
6. calculate gradient,0
1. decide nodes that should be calculated,0
2. decide calculated and subtracted tree nodes,0
3. calculate threads,0
wait until all threads finish,0
4. subtract threads,0
wait until all threads finish,0
5. send histograms to PS,0
6. update histogram cache,0
clock,0
find split,0
"1. find responsible tree node, using RR scheme",0
2. pull gradient histogram,0
2.1. get the name of this node's gradient histogram on PS,0
2.2. pull the histogram,0
2.3. find best split result of this tree node,0
2.3.1 using server split,0
"update the grad stats of the root node on PS, only called once by leader worker",0
update the grad stats of children node,0
update the left child,0
update the right child,0
"2.3.2 the updated split result (tree node/feature/value/gain) on PS,",0
"2.3.3 otherwise, the returned histogram contains the gradient info",0
"2.3.4 the updated split result (tree node/feature/value/gain) on PS,",0
2.3.5 reset this tree node's gradient histogram to 0,0
3. push split feature to PS,0
4. push split value to PS,0
5. push split gain to PS,0
6. set phase to AFTER_SPLIT,0
this.phase = GBDTPhase.AFTER_SPLIT;,0
clock,0
1. get split feature,0
2. get split value,0
3. get split gain,0
4. get node weight,0
5. split node,0
update local replica,0
create AfterSplit task,0
"2. check thread stats, if all threads finish, return",0
6. clock,0
"split the span of one node, reset the instance position",0
in case this worker has no instance on this node,0
set the span of left child,0
set the span of right child,0
"1. left to right, find the first instance that should be in the right child",0
"2. right to left, find the first instance that should be in the left child",0
3. swap two instances,0
4. find the cut pos,0
5. set the span of left child,0
6. set the span of right child,0
set tree node to active,0
set node to leaf,0
set node to inactive,0
finish current depth,0
finish current tree,0
set the tree phase,0
check if there is active node,0
check if finish all the tree,0
update node's grad stats on PS,0
"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split",0
the root node's stats is updated by leader worker,0
1. create the update,0
2. push the update to PS,0
1. update predictions of training data,0
2. update predictions of validation data,0
the leader task adds node prediction to flush list,0
1. name of this node's grad histogram on PS,0
2. build the grad histogram of this node,0
3. push the histograms to PS,0
4. reset thread stats to finished,0
5.1. set the children nodes of this node,0
5.2. set split info and grad stats to this node,0
5.2. create children nodes,0
"5.3. create node stats for children nodes, and add them to the tree",0
5.4. reset instance position,0
"5.5. add new active nodes if possible, inc depth, otherwise finish this tree",0
5.6. set children nodes to leaf nodes,0
5.7. set nid to leaf node,0
5.8. deactivate active node,0
"get feature type, 0:empty 1:all equal 2:real",0
"if not -1, sufficient space will be allocated at once",0
copy the highest levels,0
copy baseBuffer,0
merge two non-empty quantile sketches,0
left child <= split value; right child > split value,0
"the first: minimal, the last: maximal",0
categorical features,0
continuous features,0
left child <= split value; right child > split value,0
feature index used to split,0
feature value used to split,0
loss change after split this node,0
grad stats of the left child,0
grad stats of the right child,0
"LOG.info(""Constructor with fid = -1"");",0
fid = -1: no split currently,0
the minimal split value is the minimal value of feature,0
the splits do not include the maximal value of feature,0
"1. the average distance, (maxValue - minValue) / splitNum",0
2. calculate the candidate split value,0
1. new feature's histogram (grad + hess),0
size: sampled_featureNum * (2 * splitNum),0
"in other words, concatenate each feature's histogram",0
2. get the span of this node,0
------ 3. using sparse-aware method to build histogram ---,0
"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances",0
3.1. get the instance index,0
3.2. get the grad and hess of the instance,0
3.3. add to the sum,0
3.4. loop the non-zero entries,0
3.4.1. get feature value,0
3.4.2. current feature's position in the sampled feature set,0
"int fPos = findFidPlace(this.controller.fSet, fid);",0
3.4.3. find the position of feature value in a histogram,0
"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]",0
3.4.4. add the grad and hess to the corresponding bin,0
3.4.5. add the reverse to the bin that contains 0.0f,0
4. add the grad and hess sum to the zero bin of all features,0
find the best split result of the histogram of a tree node,0
1. calculate the gradStats of the root node,0
"1.1. update the grad stats of the root node on PS, only called once by leader worker",0
2. loop over features,0
2.1. get the ture feature id in the sampled feature set,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"update the grad stats of the root node on PS, only called once by leader worker",0
3. update the grad stats of children node,0
3.1. update the left child,0
3.2. update the right child,0
find the best split result of one feature,0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
split value = sketches[splitIdx],0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
partition number,0
cols of each partition,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
"loop all the possible split value, start from split[0], the first item is the minimal feature value",0
assert fvalue >= sketch[start] && fvalue <= sketch[end];,0
if (left > end) return end - start;,0
find the best split result of the histogram of a tree node,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
find the best split result of one feature,0
1. set the feature id,0
splitEntry.setFid(fid);,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]",0
the task use index to find fvalue,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting",0
"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking.",0
max and min of each feature,0
clear all the information,0
calculate the sum of gradient and hess,0
"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,",0
ridx),0
check if necessary information is ready,0
"same as add, reduce is used in All Reduce",0
"features used in this tree, if equals null, means use all the features without sampling",0
node in the tree,0
the gradient info of each instances,0
initialize nodes,0
gradient,0
second order gradient,0
int sendStartCol = (int) row.getStartCol();,0
logistic loss for binary classification task.,0
"logistic loss, but predict un-transformed margin",0
check if label in range,0
return the default evaluation metric for the objective,0
"task type: classification, regression, or ranking",0
"quantile sketch, size = featureNum * splitNum",0
"gradient histograms, size = treeNodeNum * featureNum * splitNum",0
"active tree nodes, size = pow(2, treeDepth) -1",0
sampled features. size = treeNum * sampleRatio * featureNum,0
categorical feature. size = workerNum * cateFeatNum * splitNum,0
"split features, size = treeNum * treeNodeNum",0
"split values, size = treeNum * treeNodeNum",0
"split gains, size = treeNum * treeNodeNum",0
"node weights, size = treeNum * treeNodeNum",0
"node preds, size = treeNum * treeNodeNum",0
if using PS to perform split,0
step size for a tree,0
number of class,0
minimum loss change required for a split,0
maximum depth of a tree,0
number of features,0
number of nonzero,0
number of candidates split value,0
----- the rest parameters are less important ----,0
base instance weight,0
minimum amount of hessian(weight) allowed in a child,0
L2 regularization factor,0
L1 regularization factor,0
default direction choice,0
maximum delta update we can add in weight estimation,0
this parameter can be used to stabilize update,0
default=0 means no constraint on weight delta,0
whether we want to do subsample for row,0
whether to subsample columns for each tree,0
accuracy of sketch,0
accuracy of sketch,0
leaf vector size,0
option for parallelization,0
option to open cacheline optimization,0
whether to not print info during training.,0
maximum depth of the tree,0
number of features used for tree construction,0
"minimum loss change required for a split, otherwise stop split",0
----- the rest parameters are less important ----,0
default direction choice,0
whether we want to do sample data,0
whether to sample columns during tree construction,0
whether to use histogram for split,0
number of histogram units,0
whether to print info during training.,0
----- the rest parameters are obtained after training ----,0
total number of nodes,0
number of deleted nodes */,0
set basic configuration keys,0
use local deploy mode and dummy data spliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighbor;,0
import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighborParam;,0
import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighborResult;,0
import com.tencent.angel.ml.matrix.psf.graph.adjacency.initneighbor.InitNeighbor;,0
import com.tencent.angel.ml.matrix.psf.graph.adjacency.initneighbor.InitNeighborParam;,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
@Test,0
"public void testInitAndGet() throws ExecutionException, InterruptedException {",0
Worker worker = LocalClusterContext.get().getWorker(worker0Attempt0Id).getWorker();,0
"MatrixClient client1 = worker.getPSAgent().getMatrixClient(""w2"", 0);",0
int matrixW1Id = client1.getMatrixId();,0
// Generate graph data,0
"Map<Integer, int []> adjMap = generateAdjTable(nodeNum, maxNeighborNum);",0
,0
// Init graph adj table,0
"InitNeighbor func = new InitNeighbor(new InitNeighborParam(matrixW1Id, adjMap));",0
client1.update(func);,0
,0
int [] nodeIds = new int[adjMap.size()];,0
int i = 0;,0
for(int nodeId : adjMap.keySet()) {,0
nodeIds[i++] = nodeId;,0
},0
,0
// Get graph adj table from PS,0
"GetNeighbor getFunc = new GetNeighbor(new GetNeighborParam(matrixW1Id, nodeIds, maxNeighborNum));",0
"Map<Integer, int[]> getResults = ((GetNeighborResult) (client1.get(getFunc)))",0
.getNodeIdToNeighborIndices();,0
,0
// Check the result,0
"for(Entry<Integer, int[]> entry : getResults.entrySet()) {",0
"Assert.assertArrayEquals(entry.getValue(), adjMap.get(entry.getKey()));",0
},0
},0
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
import jdk.nashorn.internal.runtime.regexp.joni.Config;,0
"paras[1] = ""abc"";",0
"paras[2] = ""123"";",0
Add standard Hadoop classes,0
Feature number of train data,0
Total iteration number,0
Learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set input data path,0
Set save model path,0
Set actionType train,0
QSLRRunner runner = new QSLRRunner();,0
runner.train(conf);,0
"conf.setInt(AngelConf.ANGEL_STALENESS, -1);",0
Dataset,0
"Data format, libsvm or dummy",0
class number,0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set Softmax algorithm parameters,0
Set model class,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set AFM algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
Feature number of train data,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Ratio of validation,0
Learning rate,0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set DeepFM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set FM algorithm parameters,0
Set model class,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set WideAndDeep algorithm parameters,0
Set model class,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set DCN algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
"Set LDA parameters #V, #K",0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set LDA algorithm parameters,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set SVM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Load model meta,0
Convert model,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Convert model,0
load hadoop configuration,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model,0
load hadoop configuration,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
attempt 0,0
attempt1,0
attempt1,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
set basic configuration keys,0
"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());",0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
Thread.sleep(5000);,0
"response = master.getJobReport(null, request);",0
"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);",0
"assertEquals(response.getJobReport().getCurIteration(), jobIteration);",0
"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
testDenseDoubleUDF();,0
testSparseDoubleUDF();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
testDenseDoubleUDF();,0
testSparseDoubleUDF();,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add sparse double matrix,0
add comp dense double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
testDenseDoubleUDF();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"LOG.info(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntFloatVector row = (IntFloatVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntLongVector row = (IntLongVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add sparse float matrix,0
siMat.setPartitionClass(CSRPartition.class);,0
siMat.setPartitionStorageClass(IntCSRStorage.class);,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);",0
get a angel client,0
add sparse float matrix,0
MatrixContext siMat = new MatrixContext();,0
siMat.setName(SPARSE_INT_MAT);,0
siMat.setRowType(RowType.T_ANY_INTKEY_SPARSE);,0
siMat.setRowNum(1);,0
siMat.setValidIndexNum(100);,0
siMat.setColNum(10000000000L);,0
siMat.setValueType(Node.class);,0
siMat.setPartitionStorageClass(LongElementMapStorage.class);,0
siMat.setPartitionClass(CSRPartition.class);,0
angelClient.addMatrix(siMat);,0
add sparse long-key double matrix,0
Start PS,0
Start to run application,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
import com.tencent.angel.psagent.consistency.SSPConsistencyController;,0
@RunWith(MockitoJUnitRunner.class),0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);",0
get a angel client,0
add matrix,0
psAgent.initAndStart();,0
test conf,0
test master location,0
test app id,0
test user,0
test ps agent attempt id,0
test connection,0
test master client,0
test ip,0
test loc,0
test master location,0
test ps location,0
"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));",0
test all ps ids,0
test all matrix ids,0
test all matrix names,0
test matrix attribute,0
test matrix meta,0
test ps location,0
test partitions,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
https://blog.csdn.net/cocoonyang/article/details/63068108,0
v1[i] = v1[i] + da * v2[i];,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
v1[i] = v1[i] + da * v2[i];,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
Default does nothing.,0
The app injection is optional,0
"renderText(""hello world"");",0
"user choose a workerGroupID from the workergroups page,",0
now we should change the AngelApp params and render the workergroup page;,0
"static final String WORKER_ID = ""worker.id"";",0
"div(""#logo"").",0
"img(""/static/hadoop-st.png"")._().",0
import org.apache.hadoop.yarn.webapp.view.FooterBlock;,0
import org.apache.hadoop.yarn.webapp.view.HeaderBlock;,0
JQueryUI.jsnotice(html);,0
import org.apache.hadoop.conf.Configuration;,0
import java.lang.reflect.Field;,0
all the files in input set,0
Shuffle the file,0
Get the blocks for all files,0
Adjust the maxSize to make the split more balanced,0
Handle the splittable files,0
Handle the unsplittable files,0
Split the blocks,0
"If the remaining size of the current block is smaller than the required size,",0
the remaining blocks are divided into the current split,0
Update current split length and move to next block,0
Clear the current block offset,0
"Current split length is > maxSize, split the block and generate a new split",0
Clear blocks list for next split,0
Clear the current split length,0
"If splitBlocks is not empty, just genetate a split for it",0
get block locations from file system,0
create an input split,0
get block locations from file system,0
create a list of all block and their locations,0
"if the file is not splitable, just create the one block with",0
full file length,0
each split can be a maximum of maxSize,0
if remainder is between max and 2*max - then,0
"instead of creating splits of size max, left-max we",0
create splits of size left/2 and left/2. This is,0
a heuristic to avoid creating really really small,0
splits.,0
add this block to the block --> node locations map,0
"For blocks that do not have host/rack information,",0
assign to default  rack.,0
add this block to the rack --> block map,0
Add this host to rackToNodes map,0
add this block to the node --> block map,0
"if the file system does not have any rack information, then",0
use dummy rack location.,0
The topology paths have the host name included as the last,0
component. Strip it.,0
get tokens for all the required FileSystems..,0
"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,",0
job.getConfiguration());,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
all the files in input set,0
it is allowed for maxSize to be 0. Disable smoothing load for such cases,0
process all nodes and create splits that are local to a node. Generate,0
"one split per node iteration, and walk over nodes multiple times to",0
distribute the splits across nodes.,0
Skip the node if it has previously been marked as completed.,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
Remove all blocks which may already have been assigned to other,0
splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
Remove entries from blocksInNode so that we don't walk these,0
again.,0
Done creating a single split for this node. Move on to the next,0
node so that splits are distributed across nodes.,0
This implies that the last few blocks (or all in case maxSize=0),0
were not part of a split. The node is complete.,0
if there were any blocks left over and their combined size is,0
"larger than minSplitNode, then combine them into one split.",0
Otherwise add them back to the unprocessed pool. It is likely,0
that they will be combined with other blocks from the,0
same rack later on.,0
This condition also kicks in when max split size is not set. All,0
blocks on a node will be grouped together into a single split.,0
haven't created any split on this machine. so its ok to add a,0
smaller one for parallelism. Otherwise group it in the rack for,0
balanced size create an input split and add it to the splits,0
array,0
Remove entries from blocksInNode so that we don't walk this again.,0
The node is done. This was the last set of blocks for this node.,0
Put the unplaced blocks back into the pool for later rack-allocation.,0
Node is done. All blocks were fit into node-local splits.,0
Check if node-local assignments are complete.,0
All nodes have been walked over and marked as completed or all blocks,0
have been assigned. The rest should be handled via rackLock assignment.,0
"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """,0
"+ completedNodes.size() + "", size left: "" + totalLength);",0
"if blocks in a rack are below the specified minimum size, then keep them",0
"in 'overflow'. After the processing of all racks is complete, these",0
overflow blocks will be combined into splits.,0
Process all racks over and over again until there is no more work to do.,0
Create one split for this rack before moving over to the next rack.,0
Come back to this rack after creating a single split for each of the,0
remaining racks.,0
"Process one rack location at a time, Combine all possible blocks that",0
reside on this rack as one split. (constrained by minimum and maximum,0
split size).,0
iterate over all racks,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"if we created a split, then just go to the next rack",0
"if there is a minimum size specified, then create a single split",0
"otherwise, store these blocks into overflow data structure",0
There were a few blocks in this rack that,0
remained to be processed. Keep them in 'overflow' block list.,0
These will be combined later.,0
Process all overflow blocks,0
"This might cause an exiting rack location to be re-added,",0
but it should be ok.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"Process any remaining blocks, if any.",0
create an input split,0
add this split to the list that is returned,0
long num = totLength / maxSize;,0
all blocks for all the files in input set,0
mapping from a rack name to the list of blocks it has,0
mapping from a block to the nodes on which it has replicas,0
mapping from a node to the list of blocks that it contains,0
populate all the blocks for all files,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
"Security framework already loaded the tokens into current UGI, just use",0
them,0
Now remove the AM->RM token so tasks don't have it,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init container allocator,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
Init Client manager,0
Init PS Client manager,0
init parameter server manager,0
recover task information if needed,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
Init model saver & loader,0
start a web service if use yarn deploy mode,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
Check Workers,0
Check PSS,0
Check Clients,0
Check PS Clients,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
Init Client manager,0
Init PS Client manager,0
init parameter server manager,0
recover task information if needed,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
Init model saver & loader,0
k8sClusterManager = new KubernetesClusterManager(appContext);,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
parse parameter server counters,0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
refresh last heartbeat timestamp,0
send a state update event to the specific PSAttempt,0
Check is there save request,0
"LOG.info(""PS "" + psAttemptId + "" need save "" + subSaveContext);",0
Check is there load request,0
"LOG.info(""PS "" + psAttemptId + "" need load "" + subLoadContext);",0
check matrix metadata inconsistencies between master and parameter server.,0
"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix",0
"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix",0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
choose a unused port,0
start RPC server,0
remove this parameter server attempt from monitor set,0
remove this parameter server attempt from monitor set,0
"if worker attempt id is not in monitor set, we should shutdown it",0
find workergroup in worker manager,0
"if this worker group does not initialized, just return WORKERGROUP_NOTREADY",0
"if this worker group run over, just return WORKERGROUP_EXITED",0
"if this worker group is running now, return tasks, workers, data splits for it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update the clock for this matrix,0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update task iteration,0
"LOG.info(""error happened in psAttempt "" + psAttemptId + "" error msg="" + request.getMsg());",0
remove this parameter server attempt from monitor set,0
"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);",0
"Calculate how many splits we need. As each task handles a separate split of data, so we want",0
the number of splits equal to the number of tasks,0
split data,0
dispatch the splits to workergroups,0
split data,0
dispatch the splits to workergroups,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
write meta data to a temporary file,0
rename the temporary file to final file,0
"if the file exists, read from file and deserialize it",0
write task meta,0
write ps meta,0
generate a temporary file,0
write task meta to the temporary file first,0
rename the temporary file to the final file,0
"if last final task file exist, remove it",0
find task meta file which has max timestamp,0
"if the file does not exist, just return null",0
read task meta from file and deserialize it,0
generate a temporary file,0
write ps meta to the temporary file first.,0
rename the temporary file to the final file,0
"if the old final file exist, just remove it",0
find ps meta file,0
"if ps meta file does not exist, just return null",0
read ps meta from file and deserialize it,0
"Path tmpPath = new Path(new Path(context.getConf().get(AngelConf.ANGEL_JOB_TMP_OUTPUT_PATH)),",0
String.valueOf(requestId));,0
Path tmpPath = HdfsUtil.toTmpPath(new Path(saveContext.getSavePath()));,0
saveContext.setTmpSavePath(tmpPath.toString());,0
Filter old epoch trigger first,0
Split the user request to sub-requests to pss,0
Init matrix files meta,0
Move output files,0
Write the meta file,0
Split the user request to sub-requests to pss,0
check whether psagent heartbeat timeout,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
LocalStorageToken is needed irrespective of whether security is enabled,0
or not.,0
"TokenCache.setJobToken(jobToken, taskCredentials);",0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Construct the actual Container,0
The null fields are per-container and will be constructed for each,0
container separately.,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
"a * in the classpath will only find a .jar, so we need to filter out",0
all .jars and add everything else,0
Propagate the system classpath when using the mini cluster,0
Add standard Hadoop classes,0
Add mr,0
Cache archives,0
Cache files,0
Sanity check,0
Add URI fragment or just the filename,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
G1 params,0
".append("" -XX:G1NewSizePercent="").append(minNewRatio)",0
".append("" -XX:G1MaxNewSizePercent="").append(maxNewRatio)",0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
G1 params,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Final commmand,0
"if amTask is not null, we should clone task state from it",0
"if all parameter server complete commit, master can commit now",0
restartPS(psLoc);,0
check whether parameter server heartbeat timeout,0
Transitions from the NEW state.,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event",0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event dispatched before",0
PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.COMMITTING state,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
reqeuest resource:send a resource request to the resource allocator,0
"Once the resource is applied, build and send the launch request to the container launcher",0
deallocator the resource of the ps attempt:send a resource deallocator request to the,0
resource allocator,0
set the launch time,0
add the ps attempt to the heartbeat timeout monitoring list,0
parse ps attempt location and put it to location manager,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt",0
or failed,0
remove ps attempt id from heartbeat timeout monitor list,0
release container:send a release request to container launcher,0
TODO: 2019/5/5,1
set the finish time only if launch time is set,0
private long scheduledTime;,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
Refresh ps location & matrix meta,0
start a new attempt for this ps,0
notify ps manager,0
"getContext().getLocationManager().setPsLocation(id, null);",0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
notify the event handler of state change,0
"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor",0
"if forcedState is set, just return",0
else get state from state machine,0
add this worker group to the success set,0
check if all worker group run or run over,0
add this worker group to the success set,0
check if all worker group run over,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
calculate the actual number of worker groups and the total number of tasks based on the number of data split,0
"init all tasks , workers and worker groups and put them to the corresponding maps",0
just return the total task number now,0
TODO,1
check whether worker heartbeat timeout,0
"if workerAttempt is not null, we should clone task state from it",0
from NEW state,0
from SCHEDULED state,0
get data splits location for data locality,0
reqeuest resource:send a resource request to the resource allocator,0
"once the resource is applied, build and send the launch request to the container launcher",0
notify failed message to the worker,0
notify killed message to the worker,0
release the allocated container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
release the allocated container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
clean the container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
record the finish time,0
clean the container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listening list,0
record the finish time,0
"if the worker attempt launch successfully, add it to heartbeat timeout listening list",0
set worker attempt location,0
notify the register message to the worker,0
record the launch time,0
update worker attempt metrics,0
update tasks metrics,0
clean the container,0
notify the worker attempt run successfully message to the worker,0
record the finish time,0
todo,1
init a worker attempt for the worker,0
schedule the worker attempt,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
"If we need Yarn to restart a new application master, we should not unregister from Yarn RM",0
register to Yarn RM,0
send heartbeat to Yarn RM every rmPollInterval milliseconds,0
"catch YarnException or YarnRuntimeException, we should exit and need not retry",0
build heartbeat request,0
send heartbeat request to rm,0
"This can happen if the RM has been restarted. If it is in that state,",0
this application must clean itself up.,0
Setting NMTokens,0
assgin containers,0
"if some container is not assigned, release them",0
handle finish containers,0
dispatch container exit message to corresponding components,0
killed by framework,0
killed by framework,0
get application finish state,0
build application diagnostics,0
TODO:add a job history for angel,1
build unregister request,0
send unregister request to rm,0
Note this down for next interaction with ResourceManager,0
based on blacklisting comments above we can end up decrementing more,0
than requested. so guard for that.,0
send the updated resource request to RM,0
send 0 container count requests also to cancel previous requests,0
Update resource requests,0
try to assign to all nodes first to match node local,0
try to match all rack local,0
assign remaining,0
Update resource requests,0
send the container-assigned event to task attempt,0
build the start container request use launch context,0
send the start request to Yarn nm,0
send the message that the container starts successfully to the corresponding component,0
"after launching, send launched event to task attempt to move",0
it from ASSIGNED to RUNNING state,0
send the message that the container starts failed to the corresponding component,0
kill the remote container if already launched,0
start a thread pool to startup the container,0
See if we need up the pool size only if haven't reached the,0
maximum limit yet.,0
nodes where containers will run at *this* point of time. This is,0
*not* the cluster size and doesn't need to be.,0
"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the",0
later is just a buffer so we are not always increasing the,0
pool-size,0
the events from the queue are handled in parallel,0
using a thread pool,0
return if already stopped,0
shutdown any containers that might be left running,0
Add one sync matrix,0
addSyncMatrix();,0
"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {",0
"LOG.info(""ps id = "" + psEntry.getKey());",0
"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();",0
"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {",0
"LOG.info(""matrix id = "" + metaEntry.getKey());",0
"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());",0
},0
},0
get matrix ids in the parameter server report,0
get the matrices parameter server need to create and delete,0
"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix",0
"if a matrix exists on master but not exist on parameter server, this parameter server need build it.",0
Init control connection manager,0
Get ps locations from master and put them to the location cache.,0
Build and initialize rpc client to master,0
Get psagent id,0
Build PS control rpc client manager,0
Build local location,0
Initialize matrix meta information,0
Start all services,0
Stop all modules,0
Stop all modules,0
clock first,0
wait,0
Update generic resource counters,0
Updating resources specified in ResourceCalculatorProcessTree,0
Remove the CPU time consumed previously by JVM reuse,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Plus a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Update a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get values from pss use row/column indices,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"/ PSF get/update, use can implement their own psf",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get a row or a batch of rows,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
Just return,0
Just return,0
Just return,0
Just return,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
Sort the partitions by start column index,0
Generate a flush request and put it to request queue,0
Generate a clock request and put it to request queue,0
Generate a merge request and put it to request queue,0
Generate a merge request and put it to request queue,0
"If the matrix op log cache does not exist for the matrix, create a new one for the",0
matrix,0
and add it to cache maps,0
Add the message to the tree map,0
"If there are flush / clock requests blocked, we need to put this merge request into",0
the waiting queue,0
Launch a merge worker to merge the update to matrix op log cache,0
Remove the message from the tree map,0
Wake up blocked flush/clock request,0
Add flush/clock request to listener list to waiting for all the existing,0
updates are merged,0
Wake up blocked flush/clock request,0
"If all updates are merged for this matrix, we need wake up flush/clock requests which are",0
blocked.,0
Get next merge message sequence id,0
Wake up listeners(flush/clock requests) that have little sequence id than current merge,0
position,0
Wake up blocked merge requests,0
Get minimal sequence id from listeners,0
"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we",0
should flush updates to local matrix storage,0
Doing average or not,0
Filter un-important update,0
Split this row according the matrix partitions,0
Set split context,0
Remove the row from matrix,0
buf.writeDouble(0.0);,0
TODO:,1
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"LOG.error(""channel "" + ctx.channel() + "" inactive"");",0
"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());",0
"LOG.error(""put response message queue failed "", e);",0
Use Epoll for linux,0
Update location table,0
Remove the server from failed list,0
Notify refresh success message to request dispatcher,0
Check PS exist or not,0
Check heartbeat timeout,0
getPSState(entry.getKey());,0
Check PS restart or not,0
private final HashSet<ParameterServerId> refreshingServerSet;,0
Add it to failed rpc list,0
Add the server to gray server list,0
Add it to failed rpc list,0
Add the server to gray server list,0
Move from gray server list to failed server list,0
Handle the RPCS to this server,0
Submit the schedulable failed get RPCS,0
Submit new get RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"If the queue is empty, just return 0",0
"If request is not over limit, just submit it",0
Submit the schedulable failed get RPCS,0
Submit new put RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"LOG.info(""choose put server "" + psIds[index]);",0
Check all pending RPCS,0
Check get channel context,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
channelManager.printPools();,0
"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {",0
if(ts - entry.getValue() > requestTimeOut * 2)  {,0
"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())",0
"+ "" milliseconds, close all channels to it"");",0
closeChannels(entry.getKey());,0
"psLocToLastChannelTsMap.put(entry.getKey(), ts);",0
},0
},0
"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);",0
Remove all pending RPCS,0
Close all channel to this PS,0
private Channel getChannel(Location loc) throws Exception {,0
"return channelManager.getOrCreateChannel(new Location(loc.getIp(), loc.getPort() + 1));",0
},0
private GenericObjectPool<Channel> getChannelPool(Location loc) throws InterruptedException {,0
"return channelManager.getOrCreateChannelPool(new Location(loc.getIp(), loc.getPort() + 1), PSAgentContext",0
.get(),0
.getConf(),0
".getInt(AngelConf.ANGEL_WORKER_TASK_NUMBER,",0
AngelConf.DEFAULT_ANGEL_WORKER_TASK_NUMBER));,0
},0
Get server id and location for this request,0
"If location is null, means that the server is not ready",0
Get the channel for the location,0
Check if need get token first,0
Serialize the request,0
Send the request,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
request.getContext().setChannelPool(pool);,0
Allocate the bytebuf and serialize the request,0
find the partition request context from cache,0
"updateMatrixCache(request.getPartKey(), response.getPartition());",0
"updateMatrixCache(request.getPartKey(), response.getRowsSplit());",0
TODO,1
ServerRow rowSplit = PSAgentContext.get().getMatricesCache(),0
".getRowSplit(request.getPartKey().getMatrixId(), request.getPartKey(),",0
request.getRowIndex());,0
response.setRowSplit(rowSplit);,0
"updateMatrixCache(request.getPartKey(), response.getRowSplit());",0
"LOG.info(""user request id "" + request.getUserRequestId());",0
"LOG.info(""user request id "" + request.getUserRequestId());",0
TODO,1
"private void updateMatrixCache(PartitionKey partKey, ServerPartition partition) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, partition);",0
},0
"private void updateMatrixCache(PartitionKey partKey, ServerRow rowSplit) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowSplit);",0
},0
"private void updateMatrixCache(PartitionKey partKey, List<ServerRow> rowsSplit) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowsSplit);",0
},0
Get partitions for this row,0
Distinct get row requests,0
Get row splits of this row from the matrix cache first,0
responseCache.addSubResponse(rowSplit);,0
"If the row split does not exist in cache, get it from parameter server",0
Split the param use matrix partitions,0
Send request to PSS,0
Split the matrix oplog according to the matrix partitions,0
"If need update clock, we should send requests to all partitions",0
Send request to PSS,0
Filter the rowIds which are fetching now,0
Send the rowIndex to rpc dispatcher and return immediately,0
"LOG.info(""get row split use time="" + (System.currentTimeMillis() - startTs));",0
"LOG.info(""start to request "" + requestId);",0
"LOG.info(""start to request "" + requestId);",0
Split param use matrix partitons,0
"If all sub-results are received, just remove request and result cache",0
Split this row according the matrix partitions,0
Set split context,0
Split this row according the matrix partitions,0
Set split context,0
long startTs = System.currentTimeMillis();,0
"LOG.error(""combine use time = "" + (System.currentTimeMillis() - startTs));",0
Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition,0
Generate dispatch items and add them to the corresponding queues,0
Filter the rowIds which are fetching now,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {",0
return key1.getStartCol() < key2.getStartCol() ? -1 : 1;,0
});,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {",0
return key1.getStartCol() < key2.getStartCol() ? -1 : 1;,0
});,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Put the row split to the cache(row index to row splits map),0
"If all splits of the row are received, means this row can be merged",0
TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage,1
TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage,1
TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage,1
TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage,1
TODO,1
TODO,1
/////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO,1
buf.writeDouble(0);,0
TODO,1
///////////////////////////////////////////////////////////////////////////////////////////////,0
Now we just support pipelined row splits merging for dense type row,0
Pre-fetching is disable default,0
matrix id to clock map,0
"task index, it must be unique for whole application",0
Deserialize data splits meta,0
Get workers,0
Send request to every ps,0
Wait the responses,0
Update clock cache,0
if(syncNum % 1024 == 0) {,0
},0
"Use simple flow, do not use any cache",0
Get row from cache.,0
"if row clock is satisfy ssp staleness limit, just return.",0
Get row from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC mode, just get from pss.",0
"For BSP/SSP, get rows from storage/cache first",0
Get from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC, just get rows from pss.",0
no more retries.,0
calculate sleep time and return.,0
parse the i-th sleep-time,0
parse the i-th number-of-retries,0
calculateSleepTime may overflow.,0
"A few common retry policies, with no delays.",0
int size = rows.length;,0
int size = rows.length;,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
row.getStorage().serialize(dataOutputStream);,0
row.getStorage().deserialize(input);,0
Save partitions to files use fork-join,0
Write the ps matrix meta to the meta file,0
matrix.startServering();,0
return;,0
Read matrix meta from meta file,0
Load partitions from file use fork-join,0
close is a local operation and should finish within milliseconds; timeout just to be safe,0
response will be null for one way messages.,0
maxFrameLength = 2G,0
lengthFieldOffset = 0,0
lengthFieldLength = 8,0
"lengthAdjustment = -8, i.e. exclude the 8 byte length itself",0
"initialBytesToStrip = 8, i.e. strip out the length field itself",0
"return Objects.toStringHelper(this).add(""serial"", serial).add(""listSize"", datas.size())",0
.toString();,0
indicates whether this connection's life cycle is managed,0
See if we already have a connection (common case),0
create a unique lock for this RS + protocol (if necessary),0
get the RS lock,0
do one more lookup in case we were stalled above,0
Only create isa when we need to.,0
definitely a cache miss. establish an RPC for,0
this RS,0
Throw what the RemoteException was carrying.,0
check,0
every,0
minutes,0
TODO,1
failoverHandler,0
"The number of times this invocation handler has ever been failed over,",0
before this method invocation attempt. Used to prevent concurrent,0
failed method invocations from triggering multiple failover attempts.,0
Make sure that concurrent failed method invocations,0
only cause a,0
single actual fail over.,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
"LOG.info(""method "" + method.getName() + ""construct request time = """,0
+ (System.currentTimeMillis() - beforeConstructTs));,0
get an instance of the method arg type,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
Message (hand written code usually has only a single,0
argument),0
log any RPC responses that are slower than the configured,0
warn,0
response time or larger than configured warning size,0
"when tagging, we let TooLarge trump TooSmall to keep",0
output simple,0
note that large responses will often also be slow.,0
provides a count of log-reported slow responses,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
unexpected,0
"in the protobuf methods, args[1] is the only significant argument",0
for JSON encoding,0
base information that is reported regardless of type of call,0
Disable Nagle's Algorithm since we don't want packets to wait,0
Configure the event pipeline factory.,0
Make a new connection.,0
Remove all pending requests (will be canceled after relinquishing,0
write lock).,0
Cancel any pending requests by sending errors to the callbacks:,0
Close the channel:,0
Close the connection:,0
Shut down all thread pools to exit.,0
"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());",0
See NettyServer.prepareResponse for where we write out the response.,0
"It writes the call.id (int), a boolean signifying any error (and if",0
"so the exception name/trace), and the response bytes",0
Read the call id.,0
"When the stream is closed, protobuf doesn't raise an EOFException,",0
"instead, it returns a null message object.",0
"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +",0
System.currentTimeMillis());,0
"It would be good widen this to just Throwable, but IOException is what we",0
allow now,0
not implemented,1
not implemented,1
"track what RpcEngine is used by a proxy class, for stopProxy()",0
cache of RpcEngines by protocol,0
return the RpcEngine configured to handle a protocol,0
We only handle the ConnectException.,0
This is the exception we can't handle.,0
check if timed out,0
wait for retry,0
IGNORE,0
return the RpcEngine that handles a proxy object,0
The default implementation works synchronously,0
punt: allocate a new buffer & copy into it,0
Parse cmd parameters,0
load hadoop configuration,0
load angel system configuration,0
load user configuration:,0
load user config file,0
load command line parameters,0
load user job resource files,0
load ml conf file for graph based algorithm,0
load user job jar if it exist,0
Expand the environment variable,0
Add default fs(local fs) for lib jars.,0
"LOG.info(System.getProperty(""user.dir""));",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
a simple hdfs copy function assume src path and dest path are in same hdfs,0
and FileSystem object has same schema,0
"If out path exist , just remove it first",0
Create parent directory if not exist,0
Rename,0
"LOG.warn(""interrupted while sleeping"", ie);",0
public static String getHostname() {,0
try {,0
"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();",0
} catch (UnknownHostException uhe) {,0
},0
"return new StringBuilder().append("""").append(uhe).toString();",0
},0
"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {",0
String hostname = getHostname();,0
String classname = clazz.getSimpleName();,0
"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new",0
"StringBuilder().append(""Starting "").append(classname).toString(), new",0
"StringBuilder().append(""  host = "").append(hostname).toString(), new",0
"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new",0
"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new",0
"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));",0
,0
"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {",0
public void run() {,0
"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +",0
"this.val$classname + "" at "" + this.val$hostname}));",0
},0
});,0
},0
"We we interrupted because we're meant to stop? If not, just",0
continue ignoring the interruption,0
Recalculate waitTime.,0
// Begin delegation to Thread,0
// End delegation to Thread,0
instance submitter class,0
Obtain filename from path,0
Split filename to prexif and suffix (extension),0
Check if the filename is okay,0
Prepare temporary file,0
Prepare buffer for data copying,0
Open and check input stream,0
Open output stream and copy data between source file in JAR and the temporary file,0
"If read/write fails, close streams safely before throwing an exception",0
"Finally, load the library",0
little endian load order,0
tail,0
fallthrough,0
fallthrough,0
finalization,0
fmix(h1);,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
throw new AngelException(e);,0
JobStateProto jobState = report.getJobState();,0
Check need load matrices,0
Used for java code to get a AngelClient instance,0
Used for python code to get a AngelClient instance,0
load user job resource files,0
setLocalAddr();,0
2.get job id,0
5.write configuration to a xml file,0
8.get app master client,0
Write job file to JobTracker's fs,0
the leaf level file should be readable by others,0
the subdirs in the path should have execute permissions for,0
others,0
2.get job id,0
Credentials credentials = new Credentials();,0
4.copy resource files to hdfs,0
5.write configuration to a xml file,0
6.create am container context,0
7.Submit to ResourceManager,0
8.get app master client,0
Create a number of filenames in the JobTracker's fs namespace,0
add all the command line files/ jars and archive,0
first copy them to jobtrackers filesystem,0
should not throw a uri exception,0
should not throw an uri excpetion,0
set the timestamps of the archives and files,0
set the public/private visibility of the archives and files,0
get DelegationToken for each cached file,0
check if we do not need to copy the files,0
is jt using the same file system.,0
just checking for uri strings... doing no dns lookups,0
to see if the filesystems are the same. This is not optimal.,0
but avoids name resolution.,0
this might have name collisions. copy will throw an exception,0
parse the original path to create new path,0
check for ports,0
Write job file to JobTracker's fs,0
Setup resource requirements,0
Setup LocalResources,0
Setup security tokens,0
Setup the command to run the AM,0
Add AM user command opts,0
Final command,0
Setup the CLASSPATH in environment,0
"i.e. add { Hadoop jars, job jar, CWD } to classpath.",0
Setup the environment variables for Admin first,0
"Setup the environment variables (LD_LIBRARY_PATH, etc)",0
Parse distributed cache,0
Setup ContainerLaunchContext for AM container,0
Set up the ApplicationSubmissionContext,0
private volatile PS2PSPusherImpl ps2PSPusher;,0
TODO,1
Add tokens to new user so that it may execute its task correctly.,0
TODO,1
to exit,0
TODO,1
TODO,1
ps2PSPusher.recover(ProtobufUtil.convert(needRecoverParts.get(i)));,0
context.getSnapshotManager().processRecovery();,0
Recover PS from snapshot or load path,0
1. First check old snapshot,0
2. Check new checkpoints,0
3. Check load path setting and old save result,0
Just init it again,0
TODO,1
if(ps2PSPusher != null) {,0
ps2PSPusher.start();,0
},0
public PS2PSPusherImpl getPs2PSPusher() {,0
return ps2PSPusher;,0
},0
"LOG.error(""send response of request "" + requestToString(clientId, seqId) + "" failed "");",0
"LOG.error(""send response of request failed, request seqId="" + seqId + "", channel="" + ch, ex);",0
Release the input buffer,0
Release the input buffer,0
"1. handle the rpc, get the response",0
Release the input buffer,0
2. Serialize the response,0
Send the serialized response,0
Exception happened,0
write seq id,0
Just serialize the head,0
Exception happened,0
Allocate result buffer,0
Exception happened,0
Just serialize the head,0
Exception happened,0
runningContext.printToken();,0
Reset the response and allocate buffer again,0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this location, only master ps can accept the update",0
Check the partition state again,0
Start to put the update to the slave pss,0
TODO,1
"context.getPS2PSPusher().put(request, in, partLoc);",0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this partition, if not, just return failed",0
Start to put the update to the slave pss,0
TODO,1
return ServerState.GENERAL;,0
Use Epoll for linux,0
public String uuid;,0
TODO:,1
part = new ServerPartition();,0
TODO:,1
public void setChannelPool(GenericObjectPool<Channel> channelPool) {,0
this.channelPool = channelPool;,0
},0
private final ParameterServer psServer;,0
Create and start workers,0
Set workers,0
Create and start workers,0
Set workers,0
"If matrix checkpoint path not exist, just return null",0
Return the path with maximum checkpoint id,0
Rename temp to item path,0
Checkpoint base path = Base dir/matrix name,0
Path for this checkpoint,0
Generate tmp path,0
Delete old checkpoints,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"////// network io method, for model transform",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
Serailize the head,0
Serialize the storage,0
Deserailze the head,0
Deseralize the storage,0
Serailize the head,0
Serialize the storage,0
Deserailze the head,0
Deseralize the storage,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"Notice: Basic type ServerRow only support Vector as inner storage now, so you can use methods",0
to get inner vector for basic type ServerRow.,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
Just update the exist element now!!,0
Just update the exist element now!!,0
TODO: just check the value is 0 or not now,1
TODO: just check the value is zero or not now,1
"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low.",0
Attention: Only update the exist values for sorted storage method,0
Attention: Only update exist element,0
Attention: Only update the exist values for sorted storage method,0
Attention: Only update exist element,0
TODO: just check the value is zero or not now,1
"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low.",0
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
Element data,0
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
Attention: Only update the exist values for sorted storage method,0
Attention: Only update exist element,0
TODO: just check the value is zero or not now,1
"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low.",0
TODO: just check the value is 0 or not now,1
Just update the exist element now!!,0
Just update the exist element now!!,0
"Use sparse storage method, as some elements in the array maybe null",0
Array length,0
Valid element number,0
Element data,0
Array len,0
Valid element number,0
"Use sparse storage method, as some elements in the array maybe null",0
Array length,0
Valid element number,0
Element data,0
Element data,0
Array len,0
Valid element number,0
Attention: Only update the exist values for sorted storage method,0
Attention: Only update exist element,0
TODO: just check the value is zero or not now,1
"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low.",0
Row type,0
Storage method,0
Key type,0
Value type,0
Vector dim,0
Vector length,0
Vector data,0
Row type,0
Storage method,0
Key type,0
Value type,0
Vector dim,0
Vector length,0
Init the vector,0
Vector data,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
Impossible now,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
Impossible now,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
Impossible now,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
Impossible now,0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
Just update the exist element now!!,0
Just update the exist element now!!,0
TODO: just check the value is 0 or not now,1
Just update the exist element now!!,0
Just update the exist element now!!,0
TODO: just check the value is 0 or not now,1
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
Element data,0
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
private final List<PartitionKey> partitionKeys;,0
Get server partition class,0
"If partition class is not set, just use the default partition class",0
Get server partition storage class type,0
Get value class,0
"if col == -1, we use the start/end index to calculate range,",0
we use double to store the range value since two long minus might exceed the,0
range of long.,0
Serialize the head,0
Serialize the storage,0
Deserialize the head,0
Deseralize the storage,0
Row base partition,0
"If storage class is not set, use default DenseServerRowsStorage",0
Serialize values,0
Deserialize values,0
Array size,0
Actual write size,0
Rows data,0
Row id,0
Row type,0
Row data,0
Array size,0
Actual write row number,0
Rows data,0
Row id,0
Create empty server row,0
Row data,0
Rows data,0
TODO,1
Serialize row offsets,0
Serialize column offsets,0
Deserialize row offset,0
Deserialize row offset,0
"If storage is set, just get a instance",0
"If storage is not set, use default",0
"If storage is set, just get a instance",0
"If storage is not set, use default",0
Map size,0
Actual write size,0
Rows data,0
Row id,0
Row type,0
Row data,0
Array size,0
Actual write row number,0
Rows data,0
Row id,0
Create empty server row,0
Row data,0
Rows data,0
Use Epoll for linux,0
find the partition request context from cache,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
channelManager.removeChannelPool(loc);,0
Generate seq id,0
Create a RecoverPartRequest,0
Serialize the request,0
Change the seqId for the request,0
Serialize the request,0
"First check the state of the channels in the pool, if a channel is unused, just return",0
"If all channels are in use, create a new channel or wait",0
Create a new channel,0
"add the PSAgentContext,need fix",0
If col == -1 and start/end not set,0
start/end set,0
"for dense type, we need to set the colNum to set dim for vectors",0
"colNum set, start/end not set",0
Row number must > 0,0
"both set, check its valid",0
TODO:add more vector type,1
TODO : subDim set,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Split updates,0
Shuffle update splits,0
Generate part update parameters,0
"Set split context: partition key, use int key for long key vector or not ect",0
write the max abs,0
get configuration from envs,0
get master location,0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
add dense double matrix,0
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
get configuration from config file,0
set localDir with enviroment set by nm.,0
get master location,0
init task manager and start tasks,0
start heartbeat thread,0
taskManager.assignTaskIds(response.getTaskidsList());,0
todo,1
"if worker timeout, it may be knocked off.",0
"SUCCESS, do nothing",0
heartbeatFailedTime = 0;,0
private KEY currentKey;,0
will be created,0
TODO Auto-generated method stub,1
Bitmap bitmap = new Bitmap();,0
int max = indexArray[size - 1];,0
byte [] bitIndexArray = new byte[max / 8 + 1];,0
for(int i = 0; i < size; i++){,0
int bitIndex = indexArray[i] >> 3;,0
int bitOffset = indexArray[i] - (bitIndex << 3);,0
switch(bitOffset){,0
case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;,0
case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;,0
case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;,0
case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;,0
case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;,0
case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;,0
case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;,0
case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;,0
},0
},0
"true, false",0
//////////////////////////////,0
Application Configs,0
//////////////////////////////,0
//////////////////////////////,0
Master Configs,0
//////////////////////////////,0
//////////////////////////////,0
Worker Configs,0
//////////////////////////////,0
//////////////////////////////,0
Task Configs,0
//////////////////////////////,0
//////////////////////////////,0
ParameterServer Configs,0
//////////////////////////////,0
//////////////////////////////,0
Kubernetes Configs.,0
//////////////////////////////,0
////////////////// IPC //////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
Configs used to ANGEL_PS_PSAGENT running mode future.,0
model parse,0
Mark whether use pyangel or not.,0
private Configuration conf;,0
"Configuration that should be used in python environment, there should only be one",0
configuration instance in each Angel context.,0
Use private access means jconf should not be changed or modified in this way.,0
Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:,0
Do nothing,0
To-DO: add other ways to justify different value types,0
"This is so ugly, must re-implement by more elegance way",1
"Create python path which include angel's jars, the python directory in ANGEL_HOME,",0
and other files submitted by user.,0
Launch python process,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);",0
get a angel client,0
add sparse float matrix,0
siMat.setPartitionStorageClass(LongElementMapStorage.class);,0
siMat.setPartitionClass(CSRPartition.class);,0
Start PS,0
Start to run application,0
Init node neighbors,0
client.asyncUpdate(new InitNeighborOver(new InitNeighborOverParam(matrixId))).get();,0
Sample the neighbors,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);",0
get a angel client,0
add sparse float matrix,0
siMat.setValidIndexNum(100);,0
siMat.setColNum(10000000000L);,0
siMat.setPartitionStorageClass(LongElementMapStorage.class);,0
siMat.setPartitionClass(CSRPartition.class);,0
Start PS,0
Start to run application,0
Init node neighbors and feats,0
Sample the neighbors,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);",0
get a angel client,0
add sparse float matrix,0
Start PS,0
Start to run application,0
Init node neighbors,0
Sample the neighbors,0
sample continuously beginning from a random index,0
Get node neighbor number,0
"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random",0
Get node neighbor number,0
"If the neighbor number is 0, just return a int[0]",0
"If count <= 0 or the neighbor number is less or equal then count, just copy all neighbors to the result array",0
"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random",0
Store the total neighbor number of all nodes in rowOffsets,0
"Put the node ids, node neighbor number, node neighbors to the cache",0
No data in this partition,0
Get total neighbor number,0
Final matrix column indices: neighbors node ids,0
Write positions in cloumnIndices for nodes,0
Copy all cached sub column indices to final column indices,0
Read position for a sub column indices,0
Copy column indices for a node to final column indices,0
Update write position for this node in final column indices,0
Update the read position in sub column indices,0
Clear all temp data,0
Get node neighbor number,0
"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random",0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Data format,0
Feature number of train data,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Ratio of validation,0
Learning rate,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"Set angel resource, #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Set training data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set predict data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model type,0
String modelType = String.valueOf(RowType.T_FLOAT_DENSE);,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
"conf.set(AngelMLConf.ML_MODEL_TYPE(), modelType);",0
"conf.setDouble(AngelMLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);",0
"Data format, libsvm or dummy",0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set data format,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
class number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
"conf.setStrings(AngelConf.ANGEL_ML_CONF, jsonFile);",0
Set log path,0
Set trainning data path,0
Set save model path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"String savePath = LOCAL_FS + TMP_PATH + ""/model/wideDeep"";",0
Set save model path,0
Set log path,0
predictTest();,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set training data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"String savePath = LOCAL_FS + TMP_PATH + ""/model/wideDeep"";",0
Set save model path,0
Set log path,0
predictTest();,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"String savePath = LOCAL_FS + TMP_PATH + ""/model/wideDeep"";",0
Set save model path,0
Set log path,0
predictTest();,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Model type,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);",0
"conf.setStrings(AngelConf.ANGEL_ML_CONF, jsonFile);",0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log save path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set testing data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
incTrain();,0
predictTest();,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Data is classification,0
Model is classification,0
Train batch number per epoch.,0
loss delta,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Data is classification,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Model type,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
"conf.setStrings(AngelConf.ANGEL_ML_CONF, jsonFile);",0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
TODO: optimize int key indices,1
"System.out.println(""deserialize cols.length="" + nCols);",0
"System.out.print(""deserialize "");",0
"System.out.print(cols[c] + "" "");",0
System.out.println();,0
TODO Auto-generated method stub,1
"LOG.info(""original float length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));",0
"LOG.info(""original double length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));",0
"LOG.info(""parsed float length: "" + length + Arrays.toString(arr));",0
"LOG.info(""parsed double length: "" + length + Arrays.toString(arr));",0
start row index for words,0
start row index for docs,0
doc ids,0
topic assignments,0
word to docs reverse index,0
count word,0
build word start index,0
build word to doc reverse idx,0
build dks,0
dks = new TraverseHashMap[n_docs];,0
for (int d = 0; d < n_docs; d++) {,0
if (K < Short.MAX_VALUE) {,0
if (docs.get(d).len < Byte.MAX_VALUE),0
dks[d] = new S2BTraverseMap(docs.get(d).len);,0
if (docs.get(d).len < Short.MAX_VALUE),0
"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));",0
else,0
"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));",0
} else {,0
"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));",0
},0
},0
build dks,0
allocate update maps,0
Skip if no token for this word,0
Check whether error when fetching word-topic,0
Build FTree for current word,0
current doc,0
old topic assignment,0
"Check if error happens. if this happen, it's probably that failures happen to servers.",0
We need to adjust the memory settings or network fetching parameters.,0
Update statistics if needed,0
Calculate psum and sample new topic,0
Update statistics if needed,0
Assign new topic,0
Skip if no token for this word,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
Write #rows,0
Write each row,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
dense,0
sparse,0
calculate columns,0
loss function,0
gradient and hessian,0
"categorical feature set, null: none, empty: all, else: partial",0
"node's end index in instancePos, instances in [start, end] belong to a tree node",0
initialize the phase,0
current tree and depth,0
create loss function,0
calculate grad info of each instance,0
"create data sketch, push candidate split value to PS",0
1. calculate candidate split value,0
categorical features,0
2. push local sketch to PS,0
the leader worker,0
merge categorical features,0
create updates,0
"pull the global sketch from PS, only called once by each worker",0
number of categorical feature,0
sample feature,0
push sampled feature set to the current tree,0
create new tree,0
"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,",0
calculate gradient,0
"1. create new tree, initialize tree nodes and node stats",0
"2. initialize feature set, if sampled, get from PS, otherwise use all the features",0
2.1. pull the sampled features of the current tree,0
this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();,0
"2.2. if use all the features, only called one",0
"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle",0
4. set root node to active,0
"5. reset instance position, set the root node's span",0
6. calculate gradient,0
1. decide nodes that should be calculated,0
2. decide calculated and subtracted tree nodes,0
3. calculate threads,0
wait until all threads finish,0
4. subtract threads,0
wait until all threads finish,0
5. send histograms to PS,0
6. update histogram cache,0
clock,0
find split,0
"1. find responsible tree node, using RR scheme",0
2. pull gradient histogram,0
2.1. get the name of this node's gradient histogram on PS,0
2.2. pull the histogram,0
2.3. find best split result of this tree node,0
2.3.1 using server split,0
"update the grad stats of the root node on PS, only called once by leader worker",0
update the grad stats of children node,0
update the left child,0
update the right child,0
"2.3.2 the updated split result (tree node/feature/value/gain) on PS,",0
"2.3.3 otherwise, the returned histogram contains the gradient info",0
"2.3.4 the updated split result (tree node/feature/value/gain) on PS,",0
2.3.5 reset this tree node's gradient histogram to 0,0
3. push split feature to PS,0
4. push split value to PS,0
5. push split gain to PS,0
6. set phase to AFTER_SPLIT,0
this.phase = GBDTPhase.AFTER_SPLIT;,0
clock,0
1. get split feature,0
2. get split value,0
3. get split gain,0
4. get node weight,0
5. split node,0
update local replica,0
create AfterSplit task,0
"2. check thread stats, if all threads finish, return",0
6. clock,0
"split the span of one node, reset the instance position",0
in case this worker has no instance on this node,0
set the span of left child,0
set the span of right child,0
"1. left to right, find the first instance that should be in the right child",0
"2. right to left, find the first instance that should be in the left child",0
3. swap two instances,0
4. find the cut pos,0
5. set the span of left child,0
6. set the span of right child,0
set tree node to active,0
set node to leaf,0
set node to inactive,0
finish current depth,0
finish current tree,0
set the tree phase,0
check if there is active node,0
check if finish all the tree,0
update node's grad stats on PS,0
"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split",0
the root node's stats is updated by leader worker,0
1. create the update,0
2. push the update to PS,0
1. update predictions of training data,0
2. update predictions of validation data,0
the leader task adds node prediction to flush list,0
1. name of this node's grad histogram on PS,0
2. build the grad histogram of this node,0
3. push the histograms to PS,0
4. reset thread stats to finished,0
5.1. set the children nodes of this node,0
5.2. set split info and grad stats to this node,0
5.2. create children nodes,0
"5.3. create node stats for children nodes, and add them to the tree",0
5.4. reset instance position,0
"5.5. add new active nodes if possible, inc depth, otherwise finish this tree",0
5.6. set children nodes to leaf nodes,0
5.7. set nid to leaf node,0
5.8. deactivate active node,0
"get feature type, 0:empty 1:all equal 2:real",0
"if not -1, sufficient space will be allocated at once",0
copy the highest levels,0
copy baseBuffer,0
merge two non-empty quantile sketches,0
left child <= split value; right child > split value,0
"the first: minimal, the last: maximal",0
categorical features,0
continuous features,0
left child <= split value; right child > split value,0
feature index used to split,0
feature value used to split,0
loss change after split this node,0
grad stats of the left child,0
grad stats of the right child,0
"LOG.info(""Constructor with fid = -1"");",0
fid = -1: no split currently,0
the minimal split value is the minimal value of feature,0
the splits do not include the maximal value of feature,0
"1. the average distance, (maxValue - minValue) / splitNum",0
2. calculate the candidate split value,0
1. new feature's histogram (grad + hess),0
size: sampled_featureNum * (2 * splitNum),0
"in other words, concatenate each feature's histogram",0
2. get the span of this node,0
------ 3. using sparse-aware method to build histogram ---,0
"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances",0
3.1. get the instance index,0
3.2. get the grad and hess of the instance,0
3.3. add to the sum,0
3.4. loop the non-zero entries,0
3.4.1. get feature value,0
3.4.2. current feature's position in the sampled feature set,0
"int fPos = findFidPlace(this.controller.fSet, fid);",0
3.4.3. find the position of feature value in a histogram,0
"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]",0
3.4.4. add the grad and hess to the corresponding bin,0
3.4.5. add the reverse to the bin that contains 0.0f,0
4. add the grad and hess sum to the zero bin of all features,0
find the best split result of the histogram of a tree node,0
1. calculate the gradStats of the root node,0
"1.1. update the grad stats of the root node on PS, only called once by leader worker",0
2. loop over features,0
2.1. get the ture feature id in the sampled feature set,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"update the grad stats of the root node on PS, only called once by leader worker",0
3. update the grad stats of children node,0
3.1. update the left child,0
3.2. update the right child,0
find the best split result of one feature,0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
split value = sketches[splitIdx],0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
partition number,0
cols of each partition,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
"loop all the possible split value, start from split[0], the first item is the minimal feature value",0
assert fvalue >= sketch[start] && fvalue <= sketch[end];,0
if (left > end) return end - start;,0
find the best split result of the histogram of a tree node,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
find the best split result of one feature,0
1. set the feature id,0
splitEntry.setFid(fid);,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]",0
the task use index to find fvalue,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting",0
"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking.",0
max and min of each feature,0
clear all the information,0
calculate the sum of gradient and hess,0
"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,",0
ridx),0
check if necessary information is ready,0
"same as add, reduce is used in All Reduce",0
"features used in this tree, if equals null, means use all the features without sampling",0
node in the tree,0
the gradient info of each instances,0
initialize nodes,0
gradient,0
second order gradient,0
int sendStartCol = (int) row.getStartCol();,0
logistic loss for binary classification task.,0
"logistic loss, but predict un-transformed margin",0
check if label in range,0
return the default evaluation metric for the objective,0
"task type: classification, regression, or ranking",0
"quantile sketch, size = featureNum * splitNum",0
"gradient histograms, size = treeNodeNum * featureNum * splitNum",0
"active tree nodes, size = pow(2, treeDepth) -1",0
sampled features. size = treeNum * sampleRatio * featureNum,0
categorical feature. size = workerNum * cateFeatNum * splitNum,0
"split features, size = treeNum * treeNodeNum",0
"split values, size = treeNum * treeNodeNum",0
"split gains, size = treeNum * treeNodeNum",0
"node weights, size = treeNum * treeNodeNum",0
"node preds, size = treeNum * treeNodeNum",0
if using PS to perform split,0
step size for a tree,0
number of class,0
minimum loss change required for a split,0
maximum depth of a tree,0
number of features,0
number of nonzero,0
number of candidates split value,0
----- the rest parameters are less important ----,0
base instance weight,0
minimum amount of hessian(weight) allowed in a child,0
L2 regularization factor,0
L1 regularization factor,0
default direction choice,0
maximum delta update we can add in weight estimation,0
this parameter can be used to stabilize update,0
default=0 means no constraint on weight delta,0
whether we want to do subsample for row,0
whether to subsample columns for each tree,0
accuracy of sketch,0
accuracy of sketch,0
leaf vector size,0
option for parallelization,0
option to open cacheline optimization,0
whether to not print info during training.,0
maximum depth of the tree,0
number of features used for tree construction,0
"minimum loss change required for a split, otherwise stop split",0
----- the rest parameters are less important ----,0
default direction choice,0
whether we want to do sample data,0
whether to sample columns during tree construction,0
whether to use histogram for split,0
number of histogram units,0
whether to print info during training.,0
----- the rest parameters are obtained after training ----,0
total number of nodes,0
number of deleted nodes */,0
Long type node id,0
"TODO: support String, Int, and Any type node id",1
@maxIndex: this variable contains the max index of node/word,0
some params,0
max index for node/word,0
compute number of nodes for one row,0
check the length of dot values,0
merge dot values from all partitions,0
Skip-Gram model,0
Negative sampling,0
used to accumulate the updates for input vectors,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
Skip-Gram model,0
Negative sampling,0
used to accumulate the updates for input vectors,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
update output,0
Some params,0
compute number of nodes for one row,0
get nodeId,0
get  data the nodeId mapped 1.neighbors 2.tags 3.attrs,0
write out edges,0
write out tags,0
write out attrs,0
Get node neighbor number,0
start sampling by alias table for count times,0
Long type node id,0
Get node neighbor number,0
start sampling by alias table for count times,0
int index = Math.abs(r.nextInt()) % nodeNeighbors.length;,0
float ac = Math.abs(r.nextFloat());,0
Get matrix meta,0
Split,0
Generate Part psf get param,0
Get nodes and features,0
return null;,0
compress the neighbor IDs,0
write out edges,0
write out tags,0
Get node neighbors,0
Get nodes and features,0
Get nodes and features,0
Get nodes and features,0
Get matrix meta,0
Split,0
Generate Part psf get param,0
Get nodes and features,0
Long type node id,0
Get data,0
Get matrix meta,0
Split,0
Generate Part psf get param,0
Get matrix meta,0
Split,0
Generate Part psf get param,0
Just return original features,0
Use by line with weight,0
get nodeId,0
get  data the nodeId mapped 1.neighbors 2.tags 3.attrs,0
write out edges,0
write out tags,0
write out attrs,0
clear();,0
evict entry with the smallest degree,0
// calculate bias,0
if (param.getPartKey().getStartCol() <= 0 && param.getPartKey().getEndCol() > 0) {,0
"double zVal = VectorUtils.getDouble(z, 0);",0
"double nVal = VectorUtils.getDouble(n, 0);",0
"VectorUtils.setFloat(w, 0, (float) (-1.0 * alpha * zVal / (beta + Math.sqrt(nVal))));",0
},0
Do nothing.,0
split updates,0
shuffle update splits,0
generate part update splits,0
"set split context: partition key, use int key for long key vector or net",0
how to do intersection for two dense vector with a given indices ??,0
copy the highest levels,0
copy baseBuffer,0
merge two non-empty quantile sketches,0
"if not -1, sufficient space will be allocated at once",0
InstanceRow ins = instanceRows[insId];,0
int[] indices = ins.indices();,0
int[] bins = ins.bins();,0
int nnz = indices.length;,0
for (int j = 0; j < nnz; j++) {,0
int fid = indices[j];,0
if (isFeatUsed[fid - featLo]) {,0
"histograms[fid - featLo].accumulate(bins[j], gradPairs[insId]);",0
},0
},0
1. allocate histogram,0
"2. loop non-zero instances, accumulate to histogram",0
if (nnz <= nodeEnd - nodeStart + 1) { // loop all nnz of current feature,0
3. add remaining grad and hess to default bin,0
"return param.calcWeights(grad, hess);",0
"numClass is usually small, so we do not use arraycopy here",0
"numClass is usually small, so we do not use arraycopy here",0
TODO: use more schema on default bin,1
1. set default bin to left child,0
"2. for other bins, find its location",0
3. create split set,0
this.nodeStats = new GBTNodeStat[numClass == 2 ? 1 : numClass];,0
predict sparse instance with indices and values,0
predict libsvm data,0
"Preconditions.checkArgument(preds.length == labels.length,",0
"""LogLossMetric should be used for binary-label classification"");",0
double loss = 0.0;,0
for (int i = 0; i < preds.length; i++) {,0
"loss += evalOne(preds[i], labels[i]);",0
},0
return loss / labels.length;,0
double error = 0.0;,0
if (preds.length == labels.length) {,0
for (int i = 0; i < preds.length; i++) {,0
"error += evalOne(preds[i], labels[i]);",0
},0
} else {,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"error += evalOne(pred, labels[i]);",0
},0
},0
return error / labels.length;,0
Preconditions.checkArgument(preds.length != labels.length,0
"&& preds.length % labels.length == 0,",0
"""CrossEntropyMetric should be used for multi-label classification"");",0
double loss = 0.0;,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"loss += evalOne(pred, labels[i]);",0
},0
return loss / labels.length;,0
double correct = 0.0;,0
if (preds.length == labels.length) {,0
for (int i = 0; i < preds.length; i++) {,0
"correct += evalOne(preds[i], labels[i]);",0
},0
} else {,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"correct += evalOne(pred, labels[i]);",0
},0
},0
return (float) (correct / labels.length);,0
double errSum = 0.0f;,0
if (preds.length == labels.length) {,0
for (int i = 0; i < preds.length; i++) {,0
"errSum += evalOne(preds[i], labels[i]);",0
},0
} else {,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"errSum += evalOne(pred, labels[i]);",0
},0
},0
return Math.sqrt(errSum / labels.length);,0
"System.out.println(""----------"");",0
"System.out.println(""read buffer = "" + Integer.toBinaryString(readBufferT & 0b11111111)",0
"+ "", mask = "" + Integer.toBinaryString(readMaskT));",0
readMaskT <<= 1;,0
"boolean[] bits = new boolean[]{true, true, false, false, true, false, false, false, true, true, false, true};",0
int n = bits.length;,0
BufferedBitSet writeBitSet = new BufferedBitSet(n);,0
"BufferedBitSet readBitSet = new BufferedBitSet(writeBitSet.getBytes(), n);",0
if (bitSet.get(i) != bits[i]) {,0
"throw new RuntimeException("""" + i);",0
},0
private final ByteBuffer bytes;,0
"public BufferedBitSetReader(ByteBuffer bytes, int numBits) {",0
int capacity = bytes.capacity() * 8;,0
readIndexT = bytes.capacity() - 1;,0
return bytes.get(index);,0
TODO: use arraycopy to make it faster,1
assert from >= this.from && to <= this.to;,0
"LOG.debug(String.format(""Create subset: [%d-%d]"", newFrom, newTo));",0
"LOG.debug(String.format(""Get overlap: [%d-%d]"", newFrom, newTo));",0
return bits.clone();,0
private final SerializableBuffer bytes;,0
private final ByteBuffer bytes;,0
this.bytes = ByteBuffer.allocate(numBytes);,0
public BufferedBitSetWriter(ByteBuffer bytes) {,0
this.bytes = bytes;,0
},0
"bytes.put(writeIndex++, (byte) writeBuffer);",0
public ByteBuffer getBytes() {,0
return bytes;,0
},0
ML TreeConf,0
GBDT TreeConf,0
"edges=[x,...] firstFlow=1 => go to right if < x and go to left if > x",0
"edges=[x,...] firstFlow=0 => go to left if < x and go to right if > x",0
"different types of tree node splits, enumerated by their complexity",0
"in order to reduce model size, we give priority to split point",0
"comparison between two split points, we give priority to lower feature index",0
TODO: comparison between two split sets,0
"public boolean leafwise;  // true if leaf-wise training, false if level-wise training",0
TODO: regularization,1
TODO: regularization,1
public float insSampleRatio;  // subsample ratio for instances,0
set basic configuration keys,0
use local deploy mode and dummy data spliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighbor;,0
import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighborParam;,0
import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighborResult;,0
import com.tencent.angel.ml.matrix.psf.graph.adjacency.initneighbor.InitNeighbor;,0
import com.tencent.angel.ml.matrix.psf.graph.adjacency.initneighbor.InitNeighborParam;,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
@Test,0
"public void testInitAndGet() throws ExecutionException, InterruptedException {",0
Worker worker = LocalClusterContext.get().getWorker(worker0Attempt0Id).getWorker();,0
"MatrixClient client1 = worker.getPSAgent().getMatrixClient(""w2"", 0);",0
int matrixW1Id = client1.getMatrixId();,0
// Generate graph data,0
"Map<Integer, int []> adjMap = generateAdjTable(nodeNum, maxNeighborNum);",0
,0
// Init graph adj table,0
"InitNeighbor func = new InitNeighbor(new InitNeighborParam(matrixW1Id, adjMap));",0
client1.update(func);,0
,0
int [] nodeIds = new int[adjMap.size()];,0
int i = 0;,0
for(int nodeId : adjMap.keySet()) {,0
nodeIds[i++] = nodeId;,0
},0
,0
// Get graph adj table from PS,0
"GetNeighbor getFunc = new GetNeighbor(new GetNeighborParam(matrixW1Id, nodeIds, maxNeighborNum));",0
"Map<Integer, int[]> getResults = ((GetNeighborResult) (client1.get(getFunc)))",0
.getNodeIdToNeighborIndices();,0
,0
// Check the result,0
"for(Entry<Integer, int[]> entry : getResults.entrySet()) {",0
"Assert.assertArrayEquals(entry.getValue(), adjMap.get(entry.getKey()));",0
},0
},0
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
import jdk.nashorn.internal.runtime.regexp.joni.Config;,0
"paras[1] = ""abc"";",0
"paras[2] = ""123"";",0
Add standard Hadoop classes,0
Feature number of train data,0
Total iteration number,0
Learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set input data path,0
Set save model path,0
Set actionType train,0
QSLRRunner runner = new QSLRRunner();,0
runner.train(conf);,0
"conf.setInt(AngelConf.ANGEL_STALENESS, -1);",0
Dataset,0
"Data format, libsvm or dummy",0
class number,0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set Softmax algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
Feature number of train data,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Ratio of validation,0
Learning rate,0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set DeepFM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
Model type,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"conf.set(MLConf.ML_EMBEDDING_MATRIX_OUTPUT_FORMAT(),""TextColumnFormat"");",0
"System.out.println(""-------------"");",0
System.out.println(predictInput);,0
"System.out.println(LOCAL_FS + TMP_PATH + ""/predict/kmeans"");",0
"System.out.println(""-------------"");",0
"Set angel resource, #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
"conf.setBoolean(MLConf.KMEANS_SILHOUETTE_FLAG(),true);",0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set FM algorithm parameters,0
Set model class,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set WideAndDeep algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
"Set LDA parameters #V, #K",0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set LDA algorithm parameters,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set SVM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Load model meta,0
Convert model,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Convert model,0
load hadoop configuration,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model,0
load hadoop configuration,0
"partitioner.init(mMatrix, conf);",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
worker register,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
attempt 0,0
attempt1,0
attempt1,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
set basic configuration keys,0
"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());",0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
Thread.sleep(5000);,0
"response = master.getJobReport(null, request);",0
"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);",0
"assertEquals(response.getJobReport().getCurIteration(), jobIteration);",0
"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add sparse double matrix,0
add dense float matrix,0
add sparse float matrix,0
add dense float matrix,0
add sparse float matrix,0
add dense long matrix,0
add sparse long matrix,0
add sparse long-key double matrix,0
add sparse long-key float matrix,0
add sparse long-key int matrix,0
add sparse long-key long matrix,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add sparse double matrix,0
add sparse float matrix,0
add sparse float matrix,0
add sparse long matrix,0
add sparse long-key double matrix,0
add sparse long-key float matrix,0
add sparse long-key int matrix,0
add sparse long-key long matrix,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add sparse double matrix,0
add dense float matrix,0
add sparse float matrix,0
add dense float matrix,0
add sparse float matrix,0
add dense long matrix,0
add sparse long matrix,0
add sparse long-key double matrix,0
add sparse long-key float matrix,0
add sparse long-key int matrix,0
add sparse long-key long matrix,0
Start PS,0
Start to run application,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add sparse double matrix,0
add dense float matrix,0
add sparse float matrix,0
add dense float matrix,0
add sparse float matrix,0
add dense long matrix,0
add sparse long matrix,0
add sparse long-key float matrix,0
add sparse long-key float matrix,0
add sparse long-key int matrix,0
add sparse long-key long matrix,0
Start PS,0
Start to run application,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add sparse double matrix,0
add dense float matrix,0
add sparse float matrix,0
add dense float matrix,0
add sparse float matrix,0
add dense long matrix,0
add sparse long matrix,0
add sparse long-key double matrix,0
add sparse long-key float matrix,0
add sparse long-key int matrix,0
add sparse long-key long matrix,0
Start PS,0
Start to run application,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add sparse double matrix,0
add sparse float matrix,0
add sparse float matrix,0
add sparse long matrix,0
add sparse long-key double matrix,0
add sparse long-key float matrix,0
add sparse long-key int matrix,0
add sparse long-key long matrix,0
Start PS,0
Start to run application,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add sparse double matrix,0
add sparse float matrix,0
add sparse float matrix,0
add sparse long matrix,0
add sparse long-key float matrix,0
add sparse long-key float matrix,0
add sparse long-key int matrix,0
add sparse long-key long matrix,0
Start PS,0
Start to run application,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();,0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntFloatVector row = (IntFloatVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntFloatVector row = (IntFloatVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntLongVector row = (IntLongVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add sparse double matrix,0
add sparse float matrix,0
add sparse float matrix,0
add sparse long matrix,0
add sparse long-key double matrix,0
add sparse long-key float matrix,0
add sparse long-key int matrix,0
add sparse long-key long matrix,0
Start PS,0
Start to run application,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add sparse double matrix,0
add dense float matrix,0
add sparse float matrix,0
add dense float matrix,0
add sparse float matrix,0
add dense long matrix,0
add sparse long matrix,0
add sparse long-key float matrix,0
add sparse long-key float matrix,0
add sparse long-key int matrix,0
add sparse long-key long matrix,0
Start PS,0
Start to run application,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();,0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntFloatVector row = (IntFloatVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntFloatVector row = (IntFloatVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntLongVector row = (IntLongVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add sparse double matrix,0
add sparse float matrix,0
add sparse float matrix,0
add sparse long matrix,0
add sparse long-key double matrix,0
add sparse long-key float matrix,0
add sparse long-key int matrix,0
add sparse long-key long matrix,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add sparse double matrix,0
add sparse float matrix,0
add sparse float matrix,0
add sparse long matrix,0
add sparse long-key double matrix,0
add sparse long-key float matrix,0
add sparse long-key int matrix,0
add sparse long-key long matrix,0
Start PS,0
Start to run application,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add sparse double matrix,0
add dense float matrix,0
add sparse float matrix,0
add dense float matrix,0
add sparse float matrix,0
add dense long matrix,0
add sparse long matrix,0
add sparse long-key double matrix,0
add sparse long-key float matrix,0
add sparse long-key int matrix,0
add sparse long-key long matrix,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add sparse double matrix,0
add dense float matrix,0
add sparse float matrix,0
add dense float matrix,0
add sparse float matrix,0
add dense long matrix,0
add sparse long matrix,0
add sparse long-key double matrix,0
add sparse long-key float matrix,0
add sparse long-key int matrix,0
add sparse long-key long matrix,0
Start PS,0
Start to run application,0
@RunWith(MockitoJUnitRunner.class),0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);",0
get a angel client,0
add matrix,0
psAgent.initAndStart();,0
test conf,0
test master location,0
test app id,0
test user,0
test ps agent attempt id,0
test connection,0
test master client,0
test ip,0
test loc,0
test master location,0
test ps location,0
"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));",0
test all ps ids,0
test all matrix ids,0
test all matrix names,0
test matrix attribute,0
test matrix meta,0
test ps location,0
test partitions,0
assertTrue(psAgentContext.getOpLogCache() != null);,0
ConsistencyController consistControl = psAgent.getConsistencyController();,0
assertTrue(consistControl != null);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
https://blog.csdn.net/cocoonyang/article/details/63068108,0
v1[i] = v1[i] + da * v2[i];,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
v1[i] = v1[i] + da * v2[i];,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
Default does nothing.,0
The app injection is optional,0
"renderText(""hello world"");",0
"user choose a workerGroupID from the workergroups page,",0
now we should change the AngelApp params and render the workergroup page;,0
"static final String WORKER_ID = ""worker.id"";",0
"div(""#logo"").",0
"img(""/static/hadoop-st.png"")._().",0
import org.apache.hadoop.yarn.webapp.view.FooterBlock;,0
import org.apache.hadoop.yarn.webapp.view.HeaderBlock;,0
JQueryUI.jsnotice(html);,0
import org.apache.hadoop.conf.Configuration;,0
import java.lang.reflect.Field;,0
all the files in input set,0
Shuffle the file,0
Get the blocks for all files,0
Adjust the maxSize to make the split more balanced,0
Handle the splittable files,0
Handle the unsplittable files,0
Split the blocks,0
"If the remaining size of the current block is smaller than the required size,",0
the remaining blocks are divided into the current split,0
Update current split length and move to next block,0
Clear the current block offset,0
"Current split length is > maxSize, split the block and generate a new split",0
Clear blocks list for next split,0
Clear the current split length,0
"If splitBlocks is not empty, just genetate a split for it",0
get block locations from file system,0
create an input split,0
get block locations from file system,0
create a list of all block and their locations,0
"if the file is not splitable, just create the one block with",0
full file length,0
each split can be a maximum of maxSize,0
if remainder is between max and 2*max - then,0
"instead of creating splits of size max, left-max we",0
create splits of size left/2 and left/2. This is,0
a heuristic to avoid creating really really small,0
splits.,0
add this block to the block --> node locations map,0
"For blocks that do not have host/rack information,",0
assign to default  rack.,0
add this block to the rack --> block map,0
Add this host to rackToNodes map,0
add this block to the node --> block map,0
"if the file system does not have any rack information, then",0
use dummy rack location.,0
The topology paths have the host name included as the last,0
component. Strip it.,0
get tokens for all the required FileSystems..,0
"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,",0
job.getConfiguration());,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
all the files in input set,0
it is allowed for maxSize to be 0. Disable smoothing load for such cases,0
process all nodes and create splits that are local to a node. Generate,0
"one split per node iteration, and walk over nodes multiple times to",0
distribute the splits across nodes.,0
Skip the node if it has previously been marked as completed.,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
Remove all blocks which may already have been assigned to other,0
splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
Remove entries from blocksInNode so that we don't walk these,0
again.,0
Done creating a single split for this node. Move on to the next,0
node so that splits are distributed across nodes.,0
This implies that the last few blocks (or all in case maxSize=0),0
were not part of a split. The node is complete.,0
if there were any blocks left over and their combined size is,0
"larger than minSplitNode, then combine them into one split.",0
Otherwise add them back to the unprocessed pool. It is likely,0
that they will be combined with other blocks from the,0
same rack later on.,0
This condition also kicks in when max split size is not set. All,0
blocks on a node will be grouped together into a single split.,0
haven't created any split on this machine. so its ok to add a,0
smaller one for parallelism. Otherwise group it in the rack for,0
balanced size create an input split and add it to the splits,0
array,0
Remove entries from blocksInNode so that we don't walk this again.,0
The node is done. This was the last set of blocks for this node.,0
Put the unplaced blocks back into the pool for later rack-allocation.,0
Node is done. All blocks were fit into node-local splits.,0
Check if node-local assignments are complete.,0
All nodes have been walked over and marked as completed or all blocks,0
have been assigned. The rest should be handled via rackLock assignment.,0
"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """,0
"+ completedNodes.size() + "", size left: "" + totalLength);",0
"if blocks in a rack are below the specified minimum size, then keep them",0
"in 'overflow'. After the processing of all racks is complete, these",0
overflow blocks will be combined into splits.,0
Process all racks over and over again until there is no more work to do.,0
Create one split for this rack before moving over to the next rack.,0
Come back to this rack after creating a single split for each of the,0
remaining racks.,0
"Process one rack location at a time, Combine all possible blocks that",0
reside on this rack as one split. (constrained by minimum and maximum,0
split size).,0
iterate over all racks,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"if we created a split, then just go to the next rack",0
"if there is a minimum size specified, then create a single split",0
"otherwise, store these blocks into overflow data structure",0
There were a few blocks in this rack that,0
remained to be processed. Keep them in 'overflow' block list.,0
These will be combined later.,0
Process all overflow blocks,0
"This might cause an exiting rack location to be re-added,",0
but it should be ok.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"Process any remaining blocks, if any.",0
create an input split,0
add this split to the list that is returned,0
long num = totLength / maxSize;,0
all blocks for all the files in input set,0
mapping from a rack name to the list of blocks it has,0
mapping from a block to the nodes on which it has replicas,0
mapping from a node to the list of blocks that it contains,0
populate all the blocks for all files,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
"Security framework already loaded the tokens into current UGI, just use",0
them,0
Now remove the AM->RM token so tasks don't have it,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init container allocator,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
Init Client manager,0
Init PS Client manager,0
init parameter server manager,0
recover task information if needed,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
Init model saver & loader,0
start a web service if use yarn deploy mode,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
Check Workers,0
Check PSS,0
Check Clients,0
Check PS Clients,0
parse parameter server counters,0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
refresh last heartbeat timestamp,0
send a state update event to the specific PSAttempt,0
Check is there save request,0
"LOG.info(""PS "" + psAttemptId + "" need save "" + subSaveContext);",0
Check is there load request,0
"LOG.info(""PS "" + psAttemptId + "" need load "" + subLoadContext);",0
check matrix metadata inconsistencies between master and parameter server.,0
"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix",0
"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix",0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
choose a unused port,0
start RPC server,0
remove this parameter server attempt from monitor set,0
remove this parameter server attempt from monitor set,0
"if worker attempt id is not in monitor set, we should shutdown it",0
find workergroup in worker manager,0
"if this worker group does not initialized, just return WORKERGROUP_NOTREADY",0
"if this worker group run over, just return WORKERGROUP_EXITED",0
"if this worker group is running now, return tasks, workers, data splits for it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update the clock for this matrix,0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update task iteration,0
"LOG.info(""error happened in psAttempt "" + psAttemptId + "" error msg="" + request.getMsg());",0
remove this parameter server attempt from monitor set,0
"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);",0
"Calculate how many splits we need. As each task handles a separate split of data, so we want",0
the number of splits equal to the number of tasks,0
split data,0
dispatch the splits to workergroups,0
split data,0
dispatch the splits to workergroups,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
write meta data to a temporary file,0
rename the temporary file to final file,0
"if the file exists, read from file and deserialize it",0
write task meta,0
write ps meta,0
generate a temporary file,0
write task meta to the temporary file first,0
rename the temporary file to the final file,0
"if last final task file exist, remove it",0
find task meta file which has max timestamp,0
"if the file does not exist, just return null",0
read task meta from file and deserialize it,0
generate a temporary file,0
write ps meta to the temporary file first.,0
rename the temporary file to the final file,0
"if the old final file exist, just remove it",0
find ps meta file,0
"if ps meta file does not exist, just return null",0
read ps meta from file and deserialize it,0
"Path tmpPath = new Path(new Path(context.getConf().get(AngelConf.ANGEL_JOB_TMP_OUTPUT_PATH)),",0
String.valueOf(requestId));,0
Path tmpPath = HdfsUtil.toTmpPath(new Path(saveContext.getSavePath()));,0
saveContext.setTmpSavePath(tmpPath.toString());,0
Filter old epoch trigger first,0
Split the user request to sub-requests to pss,0
Init matrix files meta,0
Move output files,0
Write the meta file,0
Split the user request to sub-requests to pss,0
check whether psagent heartbeat timeout,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
LocalStorageToken is needed irrespective of whether security is enabled,0
or not.,0
"TokenCache.setJobToken(jobToken, taskCredentials);",0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Construct the actual Container,0
The null fields are per-container and will be constructed for each,0
container separately.,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
"a * in the classpath will only find a .jar, so we need to filter out",0
all .jars and add everything else,0
Propagate the system classpath when using the mini cluster,0
Add standard Hadoop classes,0
Add mr,0
Cache archives,0
Cache files,0
Sanity check,0
Add URI fragment or just the filename,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Old parameter name,0
Parallel GC parameters,0
G1 params,0
Parallel Scavenge + Parallel Old,0
G1,0
".append("" -XX:G1NewSizePercent="").append(minNewRatio)",0
".append("" -XX:G1MaxNewSizePercent="").append(maxNewRatio)",0
CMS,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
G1 params,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Final commmand,0
"if amTask is not null, we should clone task state from it",0
"if all parameter server complete commit, master can commit now",0
restartPS(psLoc);,0
check whether parameter server heartbeat timeout,0
Transitions from the NEW state.,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event",0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event dispatched before",0
PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.COMMITTING state,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
reqeuest resource:send a resource request to the resource allocator,0
"Once the resource is applied, build and send the launch request to the container launcher",0
deallocator the resource of the ps attempt:send a resource deallocator request to the,0
resource allocator,0
set the launch time,0
add the ps attempt to the heartbeat timeout monitoring list,0
parse ps attempt location and put it to location manager,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt",0
or failed,0
remove ps attempt id from heartbeat timeout monitor list,0
release container:send a release request to container launcher,0
set the finish time only if launch time is set,0
private long scheduledTime;,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
Refresh ps location & matrix meta,0
start a new attempt for this ps,0
notify ps manager,0
"getContext().getLocationManager().setPsLocation(id, null);",0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
notify the event handler of state change,0
"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor",0
"if forcedState is set, just return",0
else get state from state machine,0
add this worker group to the success set,0
check if all worker group run or run over,0
add this worker group to the success set,0
check if all worker group run over,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
calculate the actual number of worker groups and the total number of tasks based on the number of data split,0
"init all tasks , workers and worker groups and put them to the corresponding maps",0
just return the total task number now,0
TODO,1
check whether worker heartbeat timeout,0
"if workerAttempt is not null, we should clone task state from it",0
from NEW state,0
from SCHEDULED state,0
get data splits location for data locality,0
reqeuest resource:send a resource request to the resource allocator,0
"once the resource is applied, build and send the launch request to the container launcher",0
notify failed message to the worker,0
notify killed message to the worker,0
release the allocated container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
release the allocated container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
clean the container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
record the finish time,0
clean the container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listening list,0
record the finish time,0
"if the worker attempt launch successfully, add it to heartbeat timeout listening list",0
set worker attempt location,0
notify the register message to the worker,0
record the launch time,0
update worker attempt metrics,0
update tasks metrics,0
clean the container,0
notify the worker attempt run successfully message to the worker,0
record the finish time,0
todo,1
init a worker attempt for the worker,0
schedule the worker attempt,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
"If we need Yarn to restart a new application master, we should not unregister from Yarn RM",0
register to Yarn RM,0
send heartbeat to Yarn RM every rmPollInterval milliseconds,0
"catch YarnException or YarnRuntimeException, we should exit and need not retry",0
build heartbeat request,0
send heartbeat request to rm,0
"This can happen if the RM has been restarted. If it is in that state,",0
this application must clean itself up.,0
Setting NMTokens,0
assgin containers,0
"if some container is not assigned, release them",0
handle finish containers,0
dispatch container exit message to corresponding components,0
killed by framework,0
killed by framework,0
get application finish state,0
build application diagnostics,0
TODO:add a job history for angel,1
build unregister request,0
send unregister request to rm,0
Note this down for next interaction with ResourceManager,0
based on blacklisting comments above we can end up decrementing more,0
than requested. so guard for that.,0
send the updated resource request to RM,0
send 0 container count requests also to cancel previous requests,0
Update resource requests,0
try to assign to all nodes first to match node local,0
try to match all rack local,0
assign remaining,0
Update resource requests,0
send the container-assigned event to task attempt,0
build the start container request use launch context,0
send the start request to Yarn nm,0
send the message that the container starts successfully to the corresponding component,0
"after launching, send launched event to task attempt to move",0
it from ASSIGNED to RUNNING state,0
send the message that the container starts failed to the corresponding component,0
kill the remote container if already launched,0
start a thread pool to startup the container,0
See if we need up the pool size only if haven't reached the,0
maximum limit yet.,0
nodes where containers will run at *this* point of time. This is,0
*not* the cluster size and doesn't need to be.,0
"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the",0
later is just a buffer so we are not always increasing the,0
pool-size,0
the events from the queue are handled in parallel,0
using a thread pool,0
return if already stopped,0
shutdown any containers that might be left running,0
Add one sync matrix,0
addSyncMatrix();,0
"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
"matrixContext.set(MatrixConf.MATRIX_LOAD_PATH, """");",0
"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {",0
"LOG.info(""ps id = "" + psEntry.getKey());",0
"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();",0
"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {",0
"LOG.info(""matrix id = "" + metaEntry.getKey());",0
"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());",0
},0
},0
get matrix ids in the parameter server report,0
get the matrices parameter server need to create and delete,0
"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix",0
"if a matrix exists on master but not exist on parameter server, this parameter server need build it.",0
private volatile MatrixOpLogCache opLogCache;,0
Init control connection manager,0
Get ps locations from master and put them to the location cache.,0
Build and initialize rpc client to master,0
Get psagent id,0
Build PS control rpc client manager,0
Build local location,0
Initialize matrix meta information,0
clockCache = new ClockCache();,0
opLogCache = new MatrixOpLogCache();,0
"int staleness = conf.getInt(AngelConf.ANGEL_STALENESS, AngelConf.DEFAULT_ANGEL_STALENESS);",0
consistencyController = new ConsistencyController(staleness);,0
consistencyController.init();,0
Start all services,0
clockCache.start();,0
opLogCache.start();,0
Stop all modules,0
Stop all modules,0
public MatrixOpLogCache getOpLogCache() {,0
return opLogCache;,0
},0
clock first,0
"if (cache.getClock(matrixId, pkeys.get(0)) < clock) {",0
},0
Update generic resource counters,0
Updating resources specified in ResourceCalculatorProcessTree,0
Remove the CPU time consumed previously by JVM reuse,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Plus a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Update a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get values from pss use row/column indices,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"/ PSF get/update, use can implement their own psf",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get a row or a batch of rows,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
Just return,0
Just return,0
Just return,0
Just return,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"return PSAgentContext.get().getMatrixOpLogCache().flush(taskContext, matrixId);",0
"return PSAgentContext.get().getConsistencyController().clock(taskContext, matrixId, flushFirst);",0
this.partClockCache = partClockCache;,0
Generate a flush request and put it to request queue,0
Generate a clock request and put it to request queue,0
Generate a merge request and put it to request queue,0
Generate a merge request and put it to request queue,0
"If the matrix op log cache does not exist for the matrix, create a new one for the",0
matrix,0
and add it to cache maps,0
Add the message to the tree map,0
"If there are flush / clock requests blocked, we need to put this merge request into",0
the waiting queue,0
Launch a merge worker to merge the update to matrix op log cache,0
Remove the message from the tree map,0
Wake up blocked flush/clock request,0
Add flush/clock request to listener list to waiting for all the existing,0
updates are merged,0
Wake up blocked flush/clock request,0
"If all updates are merged for this matrix, we need wake up flush/clock requests which are",0
blocked.,0
Get next merge message sequence id,0
Wake up listeners(flush/clock requests) that have little sequence id than current merge,0
position,0
Wake up blocked merge requests,0
Get minimal sequence id from listeners,0
Future<VoidResult> flushFuture = adapter,0
".flush(message.getMatrixId(), message.getContext(), matrixOpLog,",0
message.getType() == OpLogMessageType.CLOCK);,0
VoidResult result = flushFuture.get();,0
((FutureResult<VoidResult>) messageToFutureMap.remove(message)).set(result);,0
"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we",0
should flush updates to local matrix storage,0
Doing average or not,0
Filter un-important update,0
Split this row according the matrix partitions,0
Set split context,0
Remove the row from matrix,0
buf.writeDouble(0.0);,0
TODO:,1
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Init response handler,0
Init network parameters,0
Use Epoll for linux,0
Request header,0
Request body,0
Request,0
Send the request,0
Request header,0
Request body,0
Request,0
Send the request,0
Update location table,0
Remove the server from failed list,0
Notify refresh success message to request dispatcher,0
Check PS exist or not,0
Check heartbeat timeout,0
getPSState(entry.getKey());,0
Check PS restart or not,0
private final HashSet<ParameterServerId> refreshingServerSet;,0
Add it to failed rpc list,0
Add the server to gray server list,0
Add it to failed rpc list,0
Add the server to gray server list,0
Move from gray server list to failed server list,0
Handle the RPCS to this server,0
Submit the schedulable failed get RPCS,0
Submit new get RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"If the queue is empty, just return 0",0
"If request is not over limit, just submit it",0
Submit the schedulable failed get RPCS,0
Submit new put RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"LOG.info(""choose put server "" + psIds[index]);",0
Check all pending RPCS,0
Check get channel context,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
channelManager.printPools();,0
"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {",0
if(ts - entry.getValue() > requestTimeOut * 2)  {,0
"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())",0
"+ "" milliseconds, close all channels to it"");",0
closeChannels(entry.getKey());,0
"psLocToLastChannelTsMap.put(entry.getKey(), ts);",0
},0
},0
Check need retry or not,0
Clear the cache for this request,0
"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);",0
Remove all pending RPCS,0
Close all channel to this PS,0
Get server id and location for this request,0
"If location is null, means that the server is not ready",0
Get the channel for the location,0
Check if need get token first,0
"LOG.info(""Send request "" + request.getHeader());",0
Serialize the request,0
Send the request,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
request.getContext().setChannelPool(pool);,0
Allocate the bytebuf and serialize the request,0
Parse response,0
"LOG.info(""ResponseHeader="" + header);",0
Get Request,0
"TODO: for LDA, will be remove future",1
Update Server state,0
"LOG.info(""Handle request "" + request.getHeader() + "", response "" + response.getHeader());",0
Get user request and result cache,0
"LOG.info(""userRequest="" + userRequest + "", responseCache="" + responseCache + "", futureResult="" + futureResult);",0
"Some error happens, just return",0
"LOG.info(""responseCache "" + responseCache.getExpectedResponseNum());",0
Get response handler,0
Add the response to the cache,0
Check can merge or not,0
Merge,0
Clear response cache,0
Remove the response cache,0
Handle success,0
"Server is busy now, retry",0
"Server is not ready, retry",0
"Handle failed, just return error",0
Parse response msg failed,0
Merge the sub-response,0
Set matrix/row information,0
Set result,0
Merge,0
Set final result,0
Check update result,0
Set the final result,0
Merge,0
Set matrix/row meta,0
Set final result,0
Just,0
Check update result,0
Set the final result,0
Get matrix meta,0
//////////////////////////////////////////////////////////////////////////////,0
Combine Int key Double value vector,0
//////////////////////////////////////////////////////////////////////////////,0
//////////////////////////////////////////////////////////////////////////////,0
Combine Int key Float value vector,0
//////////////////////////////////////////////////////////////////////////////,0
//////////////////////////////////////////////////////////////////////////////,0
Combine Int key Long value vector,0
//////////////////////////////////////////////////////////////////////////////,0
//////////////////////////////////////////////////////////////////////////////,0
Combine Int key Int value vector,0
//////////////////////////////////////////////////////////////////////////////,0
//////////////////////////////////////////////////////////////////////////////,0
Combine Long key Double value vector,0
//////////////////////////////////////////////////////////////////////////////,0
//////////////////////////////////////////////////////////////////////////////,0
Combine Long key Float value vector,0
//////////////////////////////////////////////////////////////////////////////,0
//////////////////////////////////////////////////////////////////////////////,0
Combine Long key Int value vector,0
//////////////////////////////////////////////////////////////////////////////,0
//////////////////////////////////////////////////////////////////////////////,0
Combine Long key Long value vector,0
//////////////////////////////////////////////////////////////////////////////,0
Merge the sub-response,0
Set the result,0
Adaptor to Get PSF merge,0
Merge the sub-results,0
Split the user request to partition requests,0
filter empty partition requests,0
Create partition results cache,0
Send all the partition requests,0
Request header,0
Request body,0
Request,0
Send the request,0
Only support column-partitioned matrix now!!,0
Split the user request to partition requests,0
filter empty partition requests,0
Create partition results cache,0
Request header,0
Request body,0
Request,0
Send the request,0
Split param use matrix partitons,0
Request header,0
Request body,0
Request,0
Send the request,0
Get partitions for this row,0
Request header,0
Request body,0
Request,0
Send the request,0
Get partitions for this row,0
Request header,0
Request body,0
Request,0
Send the request,0
Split the param use matrix partitions,0
Send request to PSS,0
Request header,0
Request body,0
Request,0
Send the request,0
Request header,0
Request body,0
Request,0
Send the request,0
Get matrix meta,0
//////////////////////////////////////////////////////////////////////////////,0
Combine Int key Double value vector,0
//////////////////////////////////////////////////////////////////////////////,0
//////////////////////////////////////////////////////////////////////////////,0
Combine Int key Float value vector,0
//////////////////////////////////////////////////////////////////////////////,0
//////////////////////////////////////////////////////////////////////////////,0
Combine Int key Long value vector,0
//////////////////////////////////////////////////////////////////////////////,0
//////////////////////////////////////////////////////////////////////////////,0
Combine Int key Int value vector,0
//////////////////////////////////////////////////////////////////////////////,0
//////////////////////////////////////////////////////////////////////////////,0
Combine Long key Double value vector,0
//////////////////////////////////////////////////////////////////////////////,0
//////////////////////////////////////////////////////////////////////////////,0
Combine Long key Float value vector,0
//////////////////////////////////////////////////////////////////////////////,0
//////////////////////////////////////////////////////////////////////////////,0
Combine Long key Int value vector,0
//////////////////////////////////////////////////////////////////////////////,0
//////////////////////////////////////////////////////////////////////////////,0
Combine Long key Long value vector,0
//////////////////////////////////////////////////////////////////////////////,0
Need not call super.serialize,0
Valid sub data part number,0
Just use for stream model,0
Need not call super.serialize,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
Keys split,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Keys and values split: int/long/string/object key, float value",0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Keys and values split: int/long/string/object key, double value",0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Keys and values split: int/long/string/object key, long value",0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Keys and values split: int/long/string/object key, object value",0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
Keys and values split: vector,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////,0
keys split: range split only support int/long key,0
///////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////,0
"keys/values pair split: int key, float/double/int/long/any values",0
///////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////,0
"keys/values pair split: long key, float/double/int/long/any values",0
///////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////,0
values pair split: float/double/int/long/any values,0
///////////////////////////////////////////////////////////////////////////////////////,0
Get keys and values,0
Get values,0
Key and value array pair,0
Get keys and values,0
Get values,0
Key and value array pair,0
Get keys and values,0
Get values,0
Key and value array pair,0
Get keys and values,0
Get values,0
Key and value array pair,0
Get keys and values,0
Key and value array pair,0
Get keys and values,0
Key and value array pair,0
Get keys and values,0
Key and value array pair,0
Get keys and values,0
Key and value array pair,0
IElement class name,0
IElement class name,0
IElement class name,0
IElement class name,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
Use comp key value part,0
Split each vector,0
Combine sub data part,0
Use iterator,0
Get values,0
Key and value array pair,0
Use iterator,0
Get values,0
Key and value array pair,0
Use iterator,0
Get values,0
Key and value array pair,0
Use iterator,0
Get values,0
Key and value array pair,0
Use iterator,0
Get values,0
Key and value array pair,0
Use iterator,0
Get values,0
Key and value array pair,0
Use iterator,0
Get values,0
Key and value array pair,0
Use iterator,0
Get values,0
Key and value array pair,0
Use iterator,0
Key and value array pair,0
Use iterator,0
Key and value array pair,0
Use iterator,0
Key and value array pair,0
Use iterator,0
Key and value array pair,0
Use iterator,0
Key and value array pair,0
Use iterator,0
Key and value array pair,0
Use iterator,0
Key and value array pair,0
Use iterator,0
Key and value array pair,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
It is recommended not to call this method on the client,0
"task index, it must be unique for whole application",0
Deserialize data splits meta,0
Get workers,0
no more retries.,0
calculate sleep time and return.,0
parse the i-th sleep-time,0
parse the i-th number-of-retries,0
calculateSleepTime may overflow.,0
"A few common retry policies, with no delays.",0
Read matrix meta from meta file,0
Save partitions to files use fork-join,0
Write the ps matrix meta to the meta file,0
matrix.startServering();,0
return;,0
Read matrix meta from meta file,0
Load partitions from file use fork-join,0
Read matrix meta from meta file,0
Sort partitions,0
TODO:,1
int size = rows.length;,0
int size = rows.length;,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
close is a local operation and should finish within milliseconds; timeout just to be safe,0
response will be null for one way messages.,0
maxFrameLength = 2G,0
lengthFieldOffset = 0,0
lengthFieldLength = 8,0
"lengthAdjustment = -8, i.e. exclude the 8 byte length itself",0
"initialBytesToStrip = 8, i.e. strip out the length field itself",0
"return Objects.toStringHelper(this).add(""serial"", serial).add(""listSize"", datas.size())",0
.toString();,0
indicates whether this connection's life cycle is managed,0
See if we already have a connection (common case),0
create a unique lock for this RS + protocol (if necessary),0
get the RS lock,0
do one more lookup in case we were stalled above,0
Only create isa when we need to.,0
definitely a cache miss. establish an RPC for,0
this RS,0
Throw what the RemoteException was carrying.,0
check,0
every,0
minutes,0
TODO,1
failoverHandler,0
"The number of times this invocation handler has ever been failed over,",0
before this method invocation attempt. Used to prevent concurrent,0
failed method invocations from triggering multiple failover attempts.,0
Make sure that concurrent failed method invocations,0
only cause a,0
single actual fail over.,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
"LOG.info(""method "" + method.getName() + ""construct request time = """,0
+ (System.currentTimeMillis() - beforeConstructTs));,0
get an instance of the method arg type,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
Message (hand written code usually has only a single,0
argument),0
log any RPC responses that are slower than the configured,0
warn,0
response time or larger than configured warning size,0
"when tagging, we let TooLarge trump TooSmall to keep",0
output simple,0
note that large responses will often also be slow.,0
provides a count of log-reported slow responses,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
unexpected,0
"in the protobuf methods, args[1] is the only significant argument",0
for JSON encoding,0
base information that is reported regardless of type of call,0
Disable Nagle's Algorithm since we don't want packets to wait,0
Configure the event pipeline factory.,0
Make a new connection.,0
Remove all pending requests (will be canceled after relinquishing,0
write lock).,0
Cancel any pending requests by sending errors to the callbacks:,0
Close the channel:,0
Close the connection:,0
Shut down all thread pools to exit.,0
"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());",0
See NettyServer.prepareResponse for where we write out the response.,0
"It writes the call.id (int), a boolean signifying any error (and if",0
"so the exception name/trace), and the response bytes",0
Read the call id.,0
"When the stream is closed, protobuf doesn't raise an EOFException,",0
"instead, it returns a null message object.",0
"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +",0
System.currentTimeMillis());,0
"It would be good widen this to just Throwable, but IOException is what we",0
allow now,0
not implemented,1
not implemented,1
"track what RpcEngine is used by a proxy class, for stopProxy()",0
cache of RpcEngines by protocol,0
return the RpcEngine configured to handle a protocol,0
We only handle the ConnectException.,0
This is the exception we can't handle.,0
check if timed out,0
wait for retry,0
IGNORE,0
return the RpcEngine that handles a proxy object,0
The default implementation works synchronously,0
punt: allocate a new buffer & copy into it,0
Parse cmd parameters,0
load hadoop configuration,0
load angel system configuration,0
load user configuration:,0
load user config file,0
load command line parameters,0
load user job resource files,0
load ml conf file for graph based algorithm,0
load user job jar if it exist,0
Expand the environment variable,0
Add default fs(local fs) for lib jars.,0
"LOG.info(System.getProperty(""user.dir""));",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
a simple hdfs copy function assume src path and dest path are in same hdfs,0
and FileSystem object has same schema,0
"If out path exist , just remove it first",0
Create parent directory if not exist,0
Rename,0
"LOG.warn(""interrupted while sleeping"", ie);",0
Release the input buffer,0
"If is stream response, just return bytebuf",0
Reset the response and allocate buffer again,0
Release response data,0
public static String getHostname() {,0
try {,0
"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();",0
} catch (UnknownHostException uhe) {,0
},0
"return new StringBuilder().append("""").append(uhe).toString();",0
},0
"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {",0
String hostname = getHostname();,0
String classname = clazz.getSimpleName();,0
"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new",0
"StringBuilder().append(""Starting "").append(classname).toString(), new",0
"StringBuilder().append(""  host = "").append(hostname).toString(), new",0
"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new",0
"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new",0
"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));",0
,0
"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {",0
public void run() {,0
"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +",0
"this.val$classname + "" at "" + this.val$hostname}));",0
},0
});,0
},0
for (int i = 0; i < indices.length; i++) {,0
"System.out.println("""" + i + "", index = "" + indices[i] + "", value = "" + values[i]);",0
},0
"We we interrupted because we're meant to stop? If not, just",0
continue ignoring the interruption,0
Recalculate waitTime.,0
// Begin delegation to Thread,0
// End delegation to Thread,0
instance submitter class,0
Obtain filename from path,0
Split filename to prexif and suffix (extension),0
Check if the filename is okay,0
Prepare temporary file,0
Prepare buffer for data copying,0
Open and check input stream,0
Open output stream and copy data between source file in JAR and the temporary file,0
"If read/write fails, close streams safely before throwing an exception",0
"Finally, load the library",0
little endian load order,0
tail,0
fallthrough,0
fallthrough,0
finalization,0
fmix(h1);,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
throw new AngelException(e);,0
JobStateProto jobState = report.getJobState();,0
Check need load matrices,0
Used for java code to get a AngelClient instance,0
Used for python code to get a AngelClient instance,0
load user job resource files,0
setLocalAddr();,0
2.get job id,0
5.write configuration to a xml file,0
8.get app master client,0
Write job file to JobTracker's fs,0
the leaf level file should be readable by others,0
the subdirs in the path should have execute permissions for,0
others,0
2.get job id,0
Credentials credentials = new Credentials();,0
4.copy resource files to hdfs,0
5.write configuration to a xml file,0
6.create am container context,0
7.Submit to ResourceManager,0
8.get app master client,0
Create a number of filenames in the JobTracker's fs namespace,0
add all the command line files/ jars and archive,0
first copy them to jobtrackers filesystem,0
should not throw a uri exception,0
should not throw an uri excpetion,0
set the timestamps of the archives and files,0
set the public/private visibility of the archives and files,0
get DelegationToken for each cached file,0
check if we do not need to copy the files,0
is jt using the same file system.,0
just checking for uri strings... doing no dns lookups,0
to see if the filesystems are the same. This is not optimal.,0
but avoids name resolution.,0
this might have name collisions. copy will throw an exception,0
parse the original path to create new path,0
check for ports,0
Write job file to JobTracker's fs,0
Setup resource requirements,0
Setup LocalResources,0
Setup security tokens,0
Setup the command to run the AM,0
Add AM user command opts,0
Final command,0
Setup the CLASSPATH in environment,0
"i.e. add { Hadoop jars, job jar, CWD } to classpath.",0
Setup the environment variables for Admin first,0
"Setup the environment variables (LD_LIBRARY_PATH, etc)",0
Parse distributed cache,0
Setup ContainerLaunchContext for AM container,0
Set up the ApplicationSubmissionContext,0
private volatile PS2PSPusherImpl ps2PSPusher;,0
TODO,1
Add tokens to new user so that it may execute its task correctly.,0
to exit,0
calculate data size of all partitions of ps,0
totalRPC,0
request size,0
data size,0
Recover PS from snapshot or load path,0
1. First check old snapshot,0
2. Check new checkpoints,0
3. Check load path setting and old save result,0
Just init it again,0
TODO,1
if(ps2PSPusher != null) {,0
ps2PSPusher.start();,0
},0
public PS2PSPusherImpl getPs2PSPusher() {,0
return ps2PSPusher;,0
},0
Deserialize head,0
Get method type,0
Check method,0
Check if data request,0
Use async handler or not,0
Get and init the queue,0
"LOG.info(""Request header = "" + requestHeader);",0
"Idle, handle the request",0
Parse head success,0
Handle request,0
"Handle error, generate response",0
Release the input buffer,0
Create response,0
"LOG.info(""Response header = "" + responseHeader);",0
Serialize the response,0
2. Serialize the response,0
Send the serialized response,0
"int maxRPCCounter = Math.max(estSize, (int) (workerNum * factor));",0
"for (Map.Entry<Integer, ClientRunningContext> clientEntry : clientRPCCounters.entrySet()) {",0
"LOG.info(""client "" + clientEntry.getKey() + "" running context:"");",0
clientEntry.getValue().printToken();,0
},0
return ServerState.GENERAL;,0
Use Epoll for linux,0
Read router type,0
Key type,0
Row id,0
Key number,0
Calculate final output buffer len,0
Response header,0
Data flag,0
Value type,0
Row id,0
Size,0
Data,0
Allocate result buffer,0
Header,0
Serialize Value part head,0
response.setOutputBuffer(resultBuf);,0
Read router type,0
Key type,0
Row id,0
Key number,0
Calculate final output buffer len,0
Response header,0
Data flag,0
Value type,0
Row id,0
Size,0
Data,0
Allocate final result byte buffer,0
Write response header,0
Data,0
Value part flag,0
Value type,0
Row id,0
Values number,0
Result data,0
Filter comp key value,0
"LOG.info(""Get PSF func = "" + func.getClass().getName());",0
Data is not de-serialized first,0
public void setChannelPool(GenericObjectPool<Channel> channelPool) {,0
this.channelPool = channelPool;,0
},0
private final ParameterServer psServer;,0
Create and start workers,0
Set workers,0
Create and start workers,0
Set workers,0
"If matrix checkpoint path not exist, just return null",0
Return the path with maximum checkpoint id,0
Rename temp to item path,0
Checkpoint base path = Base dir/matrix name,0
Path for this checkpoint,0
Generate tmp path,0
Delete old checkpoints,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"////// network io method, for model transform",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
Serailize the head,0
Serialize the storage,0
Deserailze the head,0
Deseralize the storage,0
Serailize the head,0
Serialize the storage,0
Deserailze the head,0
Deseralize the storage,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"Notice: Basic type ServerRow only support Vector as inner storage now, so you can use methods",0
to get inner vector for basic type ServerRow.,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
Index is int,0
Index is long,0
Just update the exist element now!!,0
Just update the exist element now!!,0
TODO: just check the value is 0 or not now,1
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
Element data,0
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
Element data,0
Attention: Only update exist element,0
Attention: Only update exist element,0
"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low.",0
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
Element data,0
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
Element data,0
Attention: Only update exist element,0
TODO: just check the value is 0 or not now,1
Just update the exist element now!!,0
Just update the exist element now!!,0
"Use sparse storage method, as some elements in the array maybe null",0
Array length,0
Valid element number,0
Element data,0
Array len,0
Valid element number,0
"Use sparse storage method, as some elements in the array maybe null",0
Array length,0
Valid element number,0
Element data,0
Element data,0
Array len,0
Valid element number,0
Attention: Only update exist element,0
Row type,0
Storage method,0
Key type,0
Value type,0
Vector dim,0
Vector length,0
Vector data,0
Row type,0
Storage method,0
Key type,0
Value type,0
Vector dim,0
Vector length,0
Init the vector,0
Vector data,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
Impossible now,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
Impossible now,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
Impossible now,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
Impossible now,0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
Just update the exist element now!!,0
Just update the exist element now!!,0
TODO: just check the value is 0 or not now,1
Just update the exist element now!!,0
Just update the exist element now!!,0
TODO: just check the value is 0 or not now,1
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
Element data,0
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
Element data,0
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
Element data,0
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
Element data,0
private final List<PartitionKey> partitionKeys;,0
Get server partition class,0
"If partition class is not set, just use the default partition class",0
Get server partition storage class type,0
Get value class,0
Partition number use set,0
Use total partition number if set,0
Use partition number per server,0
"if col == -1, we use the start/end index to calculate range,",0
we use double to store the range value since two long minus might exceed the,0
range of long.,0
"partitioner.init(matrix1, conf);",0
Serialize the head,0
Serialize the storage,0
Deserialize the head,0
Deseralize the storage,0
Serialize the head,0
Serialize the storage,0
Deserialize the head,0
Deseralize the storage,0
Row base partition,0
"If storage class is not set, use default DenseServerRowsStorage",0
Serialize values,0
Deserialize values,0
Filter head,0
Array size,0
Actual write size,0
Rows data,0
Row id,0
Row type,0
Row data,0
Array size,0
Actual write row number,0
Rows data,0
Row id,0
Create empty server row,0
Row data,0
Rows data,0
Rows data,0
TODO,1
Serialize row offsets,0
Serialize column offsets,0
Deserialize row offset,0
Deserialize row offset,0
"If storage is set, just get a instance",0
"If storage is not set, use default",0
"If storage is set, just get a instance",0
"If storage is not set, use default",0
Map size,0
Actual write size,0
Rows data,0
Row id,0
Row type,0
Row data,0
Array size,0
Actual write row number,0
Rows data,0
Row id,0
Create empty server row,0
Row data,0
Rows data,0
Rows data,0
=======================================================,0
Boolean,0
=======================================================,0
Byte,0
=======================================================,0
Int,0
=======================================================,0
Int,0
=======================================================,0
Long,0
=======================================================,0
Float,0
=======================================================,0
String,0
=======================================================,0
Double,0
=======================================================,0
Byte array,0
=======================================================,0
Int array,0
=======================================================,0
Long array,0
=======================================================,0
Float array,0
=======================================================,0
Double array,0
=======================================================,0
String array,0
=======================================================,0
2-D Int array,0
=======================================================,0
2-D Long array,0
=======================================================,0
2-D Float array,0
=======================================================,0
2-D Double array,0
=======================================================,0
Vector,0
Meta data,0
TODO: other vector type,1
TODO: other vector type,1
TODO: other vector type,1
IntDoubleVector,0
IntFloatVector,0
IntFloatVector array,0
"serializeInt(out, vector.getType().getNumber()); // no need to record type",0
RowType type = RowType.valueOf(deserializeInt(in));,0
LongFloatVector,0
LongFloatVector array,0
"serializeInt(out, vector.getType().getNumber()); // no need to record type",0
RowType type = RowType.valueOf(deserializeInt(in));,0
LongFloatVector,0
LongFloatVector array,0
"serializeInt(out, vector.getType().getNumber()); // no need to record type",0
RowType type = RowType.valueOf(deserializeInt(in));,0
////////////////////////////////////////////////////////////////////////////,0
=======================================================,0
Boolean,0
=======================================================,0
Byte,0
=======================================================,0
Int,0
=======================================================,0
Int,0
=======================================================,0
Long,0
=======================================================,0
Float,0
=======================================================,0
String,0
=======================================================,0
Double,0
=======================================================,0
Byte array,0
=======================================================,0
Int array,0
=======================================================,0
Long array,0
=======================================================,0
Float array,0
=======================================================,0
Double array,0
=======================================================,0
String array,0
=======================================================,0
2-D Int array,0
=======================================================,0
2-D Long array,0
=======================================================,0
2-D Float array,0
=======================================================,0
2-D Double array,0
=======================================================,0
Vector,0
Meta data,0
TODO: other vector type,1
TODO: other vector type,1
TODO: other vector type,1
IntDoubleVector,0
IntFloatVector,0
IntFloatVector array,0
"serializeInt(out, vector.getType().getNumber()); // no need to record type",0
RowType type = RowType.valueOf(deserializeInt(in));,0
LongFloatVector,0
LongFloatVector array,0
"serializeInt(out, vector.getType().getNumber()); // no need to record type",0
RowType type = RowType.valueOf(deserializeInt(in));,0
LongFloatVector,0
LongFloatVector array,0
"serializeInt(out, vector.getType().getNumber()); // no need to record type",0
RowType type = RowType.valueOf(deserializeInt(in));,0
"First check the state of the channels in the pool, if a channel is unused, just return",0
"If all channels are in use, create a new channel or wait",0
Create a new channel,0
checker.start();,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
Keys and values,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
Keys,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
Keys,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
Keys,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
Keys,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
Keys,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
Key size,0
Key class name,0
Keys,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
Values,0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
Keys,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
Keys,0
"[0, currentBatchIndex - 1] batchs",0
Last batch,0
"add the PSAgentContext,need fix",0
If col == -1 and start/end not set,0
start/end set,0
"for dense type, we need to set the colNum to set dim for vectors",0
"colNum set, start/end not set",0
Hash partition does not support dense type row,0
Row number must > 0,0
"both set, check its valid",0
Sort the partitions by start column index,0
public static final int T_INT_ARBITRARY_VALUE = 28;,0
public static final int T_INVALID_VALUE = 29;,0
TODO:add more vector type,1
TODO : subDim set,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Split updates,0
Shuffle update splits,0
Generate part update parameters,0
"Set split context: partition key, use int key for long key vector or not ect",0
write the max abs,0
---------------------------------------------------,0
---------------------------------------------------,0
---------------------------------------------------------------,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
-----------------,0
-----------------,0
-----------------,0
-----------------,0
-----------------,0
KeepStorage is guaranteed,0
"ignore the isInplace option, since v2 is dense",0
"the value in old storage can be changed safe, so switch a storage",0
"but user required keep storage, we can prevent rehash",0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
KeepStorage is guaranteed,0
we gauss dense storage is more efficient,0
v1Size < v2Size * Constant.sparseThreshold,0
KeepStorage is guaranteed,0
"ignore the isInplace option, since v2 is dense",0
"the value in old storage can be changed safe, so switch a storage",0
"but user required keep storage, we can prevent rehash",0
we gauss dense storage is more efficient,0
prevent rehash,0
KeepStorage is guaranteed,0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
dense preferred,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
sorted preferred,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"slower but memory efficient, for small vector only",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"Transform mat1, generate a new matrix",0
Split the row indices of mat1Trans,0
Parallel execute use fork-join,0
"Get the sub-matrix of left matrix, split by row",0
"Transform mat1, generate a new matrix",0
Split the row indices of mat1Trans,0
Parallel execute use fork-join,0
"Get the sub-matrix of left matrix, split by row",0
"mat1 trans true, mat trans true",0
"mat1 trans true, mat trans false",0
"mat1 trans false, mat trans true, important",0
"mat1 trans false, mat trans false",0
"mat1 trans true, mat trans true",0
"mat1 trans true, mat trans false",0
"mat1 trans false, mat trans true, important",0
"mat1 trans false, mat trans false",0
"dger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)",0
"sger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)",0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
get configuration from envs,0
get master location,0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
add dense double matrix,0
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
public volatile static int colNum = 1000000;,0
public volatile static int nnz = 10000;,0
public volatile static int blockColNum = 20000;,0
"test the colNum is 1000000, 1 million",0
"rowNum = conf.getInt(""row"", 1);",0
"colNum = conf.getInt(""col"", 1000000);",0
"nnz = conf.getInt(""nnz"", 10000);",0
"blockRowNum = conf.getInt(""blockRow"", 1);",0
"blockColNum = conf.getInt(""blockCol"", 20000);",0
"rowNum = conf.getInt(""row"", 1);",0
"colNum = conf.getInt(""col"", 10);",0
"nnz = conf.getInt(""nnz"", 2);",0
"blockRowNum = conf.getInt(""blockRow"", 1);",0
"blockColNum = conf.getInt(""blockCol"", 10);",0
add sparse longkey float matrix of 1 million,0
MatrixContext dMat = new MatrixContext();,0
dMat.setName(SPARSE_LONGKEY_FLOAT_MAT);,0
dMat.setRowNum(rowNum);,0
dMat.setColNum(colNum);,0
dMat.setMaxRowNumInBlock(blockRowNum);,0
dMat.setMaxColNumInBlock(blockColNum);,0
dMat.setRowType(RowType.T_FLOAT_SPARSE_LONGKEY);,0
"dMat.set(MatrixConf.MATRIX_SAVE_PATH, conf.get(""angel.save.model.path""));",0
angelClient.addMatrix(dMat);,0
test the colNum is Long.MAX_VALUE,0
add sparse longkey float matrix of Long.MAX_VALUE,0
get configuration from config file,0
set localDir with enviroment set by nm.,0
get master location,0
init task manager and start tasks,0
start heartbeat thread,0
taskManager.assignTaskIds(response.getTaskidsList());,0
todo,1
"if worker timeout, it may be knocked off.",0
"SUCCESS, do nothing",0
heartbeatFailedTime = 0;,0
private KEY currentKey;,0
will be created,0
TODO Auto-generated method stub,1
Bitmap bitmap = new Bitmap();,0
int max = indexArray[size - 1];,0
byte [] bitIndexArray = new byte[max / 8 + 1];,0
for(int i = 0; i < size; i++){,0
int bitIndex = indexArray[i] >> 3;,0
int bitOffset = indexArray[i] - (bitIndex << 3);,0
switch(bitOffset){,0
case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;,0
case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;,0
case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;,0
case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;,0
case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;,0
case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;,0
case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;,0
case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;,0
},0
},0
"true, false",0
//////////////////////////////,0
Application Configs,0
//////////////////////////////,0
//////////////////////////////,0
Master Configs,0
//////////////////////////////,0
//////////////////////////////,0
Worker Configs,0
//////////////////////////////,0
//////////////////////////////,0
Task Configs,0
//////////////////////////////,0
//////////////////////////////,0
ParameterServer Configs,0
//////////////////////////////,0
//////////////////////////////,0
Kubernetes Configs.,0
//////////////////////////////,0
////////////////// IPC //////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
Configs used to ANGEL_PS_PSAGENT running mode future.,0
model parse,0
Mark whether use pyangel or not.,0
private Configuration conf;,0
"Configuration that should be used in python environment, there should only be one",0
configuration instance in each Angel context.,0
Use private access means jconf should not be changed or modified in this way.,0
Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:,0
Do nothing,0
To-DO: add other ways to justify different value types,0
"This is so ugly, must re-implement by more elegance way",1
"Create python path which include angel's jars, the python directory in ANGEL_HOME,",0
and other files submitted by user.,0
Launch python process,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);",0
get a angel client,0
add sparse float matrix,0
siMat.setPartitionStorageClass(LongElementMapStorage.class);,0
siMat.setPartitionClass(CSRPartition.class);,0
Start PS,0
Start to run application,0
Init node neighbors,0
client.asyncUpdate(new InitNeighborOver(new InitNeighborOverParam(matrixId))).get();,0
Sample the neighbors,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);",0
get a angel client,0
add sparse float matrix,0
siMat.setValidIndexNum(100);,0
siMat.setColNum(10000000000L);,0
siMat.setPartitionStorageClass(LongElementMapStorage.class);,0
siMat.setPartitionClass(CSRPartition.class);,0
Start PS,0
Start to run application,0
Init node neighbors and feats,0
Sample the neighbors,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);",0
get a angel client,0
add sparse float matrix,0
Start PS,0
Start to run application,0
Init node neighbors,0
Sample the neighbors,0
flags len for init value: 7*4,0
Just return original features,0
clear();,0
Long type node id,0
"If node not exist, just skip",0
"TODO: support String, Int, and Any type node id",1
Get matrix meta,0
Long type node id,0
"If node not exist, just skip",0
"TODO: support String, Int, and Any type node id",1
Long type node id,0
"If node not exist, just skip",0
"TODO: support String, Int, and Any type node id",1
Long type node id,0
"If node not exist, just skip",0
"TODO: support String, Int, and Any type node id",1
Get matrix meta,0
Split,0
Generate Part psf get param,0
Long type node id,0
"If node not exist, just skip",0
"TODO: support String, Int, and Any type node id",1
Long type node id,0
"If node not exist, just skip",0
"TODO: support String, Int, and Any type node id",1
Long type node id,0
"If node not exist, just skip",0
"TODO: support String, Int, and Any type node id",1
Get node neighbor number,0
"If the neighbor number > count, just copy a range of neighbors to",0
"the result array, the copy position is random",0
Get node neighbor number,0
"If the neighbor number > count, just copy a range of neighbors to",0
"the result array, the copy position is random",0
sample types,0
Get node neighbor number,0
Get node neighbor number,0
"If the neighbor number > count, just copy a range of neighbors to",0
"the result array, the copy position is random",0
sample node types,0
sample edge types,0
Get node neighbor number,0
"If the neighbor number > count, just copy a range of neighbors to",0
"the result array, the copy position is random",0
sample types,0
Get matrix meta,0
sample (count / partNum + 1) in every partition randomly,0
Generate rpc params,0
Sample part result,0
Neighbors,0
Neighbors type,0
Edge type,0
Sample part result,0
Neighbors,0
Get matrix meta,0
Split nodeIds,0
Generate node ids,0
Get matrix meta,0
Split nodeIds,0
Generate rpc params,0
Get matrix meta,0
Split nodeIds,0
Generate node ids,0
Sample part result,0
Neighbors,0
EdgeFeatures,0
Get matrix meta,0
Split nodeIds,0
Generate rpc params,0
Get node ids and features,0
Get nodes and features,0
Get nodes and features,0
sample continuously beginning from a random index,0
Get node neighbor number,0
"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random",0
Get node neighbor number,0
"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random",0
"int matrixId, PartitionKey partKey, long[] keyIds, int startIdx, int endIdx",0
clear();,0
int completeCount = 0;,0
ArrayList<WalkPath> paths = new ArrayList<>(pathTail.size());,0
,0
row.startWrite();,0
try {,0
"ObjectIterator<Map.Entry<Long, Long>> iter = pathTail.entrySet().iterator();",0
while (iter.hasNext()) {,0
"Map.Entry<Long, Long> entry = iter.next();",0
long key = entry.getKey();,0
long tail = entry.getValue();,0
,0
WalkPath wPath = (WalkPath) row.get(key);,0
wPath.add2Path(tail);,0
,0
if (wPath.isComplete()) {,0
completeCount += 1;,0
} else {,0
paths.add(wPath);,0
},0
},0
} finally {,0
row.endWrite();,0
},0
,0
if (completeCount != 0) {,0
"PathQueue.progress(partKey.getPartitionId(), completeCount);",0
},0
,0
if (!paths.isEmpty()) {,0
"PathQueue.pushBatch(partKey.getPartitionId(), paths);",0
},0
"System.out.println(Thread.currentThread().getId() + ""\t serialize -> "" + ());",0
clear();,0
"System.out.println(Thread.currentThread().getId() + ""\t deserialize -> "" + size);",0
Get node neighbor number,0
"If the neighbor number is 0, just return a int[0]",0
"If count <= 0 or the neighbor number is less or equal then count, just copy all neighbors to the result array",0
"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random",0
Store the total neighbor number of all nodes in rowOffsets,0
"Put the node ids, node neighbor number, node neighbors to the cache",0
No data in this partition,0
Get total neighbor number,0
Final matrix column indices: neighbors node ids,0
Write positions in cloumnIndices for nodes,0
Copy all cached sub column indices to final column indices,0
Read position for a sub column indices,0
Copy column indices for a node to final column indices,0
Update write position for this node in final column indices,0
Update the read position in sub column indices,0
Clear all temp data,0
Get node neighbor number,0
"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random",0
sample happens here to avoid memory copy on servers,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Data format,0
Feature number of train data,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Ratio of validation,0
Learning rate,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"Set angel resource, #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Set training data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set predict data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model type,0
String modelType = String.valueOf(RowType.T_FLOAT_DENSE);,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
"conf.set(MLConf.ML_MODEL_TYPE(), modelType);",0
"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);",0
predictTest();,0
"Data format, libsvm or dummy",0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set data format,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
class number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set log path,0
Set trainning data path,0
Set save model path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set training data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log save path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set testing data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Data is classification,0
Model is classification,0
Train batch number per epoch.,0
loss delta,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Data is classification,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
TODO: optimize int key indices,1
"System.out.println(""deserialize cols.length="" + nCols);",0
"System.out.print(""deserialize "");",0
"System.out.print(cols[c] + "" "");",0
System.out.println();,0
TODO Auto-generated method stub,1
"LOG.info(""original float length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));",0
"LOG.info(""original double length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));",0
"LOG.info(""parsed float length: "" + length + Arrays.toString(arr));",0
"LOG.info(""parsed double length: "" + length + Arrays.toString(arr));",0
"ground truth: positive, precision: positive",0
start row index for words,0
start row index for docs,0
doc ids,0
topic assignments,0
word to docs reverse index,0
count word,0
build word start index,0
build word to doc reverse idx,0
build dks,0
dks = new TraverseHashMap[n_docs];,0
for (int d = 0; d < n_docs; d++) {,0
if (K < Short.MAX_VALUE) {,0
if (docs.get(d).len < Byte.MAX_VALUE),0
dks[d] = new S2BTraverseMap(docs.get(d).len);,0
if (docs.get(d).len < Short.MAX_VALUE),0
"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));",0
else,0
"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));",0
} else {,0
"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));",0
},0
},0
build dks,0
allocate update maps,0
Skip if no token for this word,0
Check whether error when fetching word-topic,0
Build FTree for current word,0
current doc,0
old topic assignment,0
"Check if error happens. if this happen, it's probably that failures happen to servers.",0
We need to adjust the memory settings or network fetching parameters.,0
Update statistics if needed,0
Calculate psum and sample new topic,0
Update statistics if needed,0
Assign new topic,0
Skip if no token for this word,0
if (u >= p[end]) {,0
"System.out.println(""u="" + u + "" p[end]="" + p[end] + "" start="" + start + "" end="" + end);",0
return end;,0
},0
,0
if (u < p[start]) {,0
"System.out.println(""u="" + u + "" p[start]="" + p[start] + "" start="" + start + "" end="" + end);",0
return start;,0
},0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
Write #rows,0
Write each row,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
dense,0
sparse,0
dense,0
sparse,0
calculate columns,0
reset(row);,0
loss function,0
gradient and hessian,0
"categorical feature set, null: none, empty: all, else: partial",0
"node's end index in instancePos, instances in [start, end] belong to a tree node",0
initialize the phase,0
current tree and depth,0
create loss function,0
calculate grad info of each instance,0
"create data sketch, push candidate split value to PS",0
1. calculate candidate split value,0
categorical features,0
2. push local sketch to PS,0
the leader worker,0
merge categorical features,0
create updates,0
"pull the global sketch from PS, only called once by each worker",0
number of categorical feature,0
sample feature,0
push sampled feature set to the current tree,0
create new tree,0
"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,",0
calculate gradient,0
"1. create new tree, initialize tree nodes and node stats",0
"2. initialize feature set, if sampled, get from PS, otherwise use all the features",0
2.1. pull the sampled features of the current tree,0
this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();,0
"2.2. if use all the features, only called one",0
"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle",0
4. set root node to active,0
"5. reset instance position, set the root node's span",0
6. calculate gradient,0
1. decide nodes that should be calculated,0
2. decide calculated and subtracted tree nodes,0
3. calculate threads,0
wait until all threads finish,0
4. subtract threads,0
wait until all threads finish,0
5. send histograms to PS,0
6. update histogram cache,0
clock,0
find split,0
"1. find responsible tree node, using RR scheme",0
2. pull gradient histogram,0
2.1. get the name of this node's gradient histogram on PS,0
2.2. pull the histogram,0
2.3. find best split result of this tree node,0
2.3.1 using server split,0
"update the grad stats of the root node on PS, only called once by leader worker",0
update the grad stats of children node,0
update the left child,0
update the right child,0
"2.3.2 the updated split result (tree node/feature/value/gain) on PS,",0
"2.3.3 otherwise, the returned histogram contains the gradient info",0
"2.3.4 the updated split result (tree node/feature/value/gain) on PS,",0
2.3.5 reset this tree node's gradient histogram to 0,0
3. push split feature to PS,0
4. push split value to PS,0
5. push split gain to PS,0
6. set phase to AFTER_SPLIT,0
this.phase = GBDTPhase.AFTER_SPLIT;,0
clock,0
1. get split feature,0
2. get split value,0
3. get split gain,0
4. get node weight,0
5. split node,0
update local replica,0
create AfterSplit task,0
"2. check thread stats, if all threads finish, return",0
6. clock,0
"split the span of one node, reset the instance position",0
in case this worker has no instance on this node,0
set the span of left child,0
set the span of right child,0
"1. left to right, find the first instance that should be in the right child",0
"2. right to left, find the first instance that should be in the left child",0
3. swap two instances,0
4. find the cut pos,0
5. set the span of left child,0
6. set the span of right child,0
set tree node to active,0
set node to leaf,0
set node to inactive,0
finish current depth,0
finish current tree,0
set the tree phase,0
check if there is active node,0
check if finish all the tree,0
update node's grad stats on PS,0
"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split",0
the root node's stats is updated by leader worker,0
1. create the update,0
2. push the update to PS,0
1. update predictions of training data,0
2. update predictions of validation data,0
the leader task adds node prediction to flush list,0
1. name of this node's grad histogram on PS,0
2. build the grad histogram of this node,0
3. push the histograms to PS,0
4. reset thread stats to finished,0
5.1. set the children nodes of this node,0
5.2. set split info and grad stats to this node,0
5.2. create children nodes,0
"5.3. create node stats for children nodes, and add them to the tree",0
5.4. reset instance position,0
"5.5. add new active nodes if possible, inc depth, otherwise finish this tree",0
5.6. set children nodes to leaf nodes,0
5.7. set nid to leaf node,0
5.8. deactivate active node,0
"get feature type, 0:empty 1:all equal 2:real",0
"if not -1, sufficient space will be allocated at once",0
copy the highest levels,0
copy baseBuffer,0
merge two non-empty quantile sketches,0
left child <= split value; right child > split value,0
"the first: minimal, the last: maximal",0
categorical features,0
continuous features,0
left child <= split value; right child > split value,0
feature index used to split,0
feature value used to split,0
loss change after split this node,0
grad stats of the left child,0
grad stats of the right child,0
"LOG.info(""Constructor with fid = -1"");",0
fid = -1: no split currently,0
the minimal split value is the minimal value of feature,0
the splits do not include the maximal value of feature,0
"1. the average distance, (maxValue - minValue) / splitNum",0
2. calculate the candidate split value,0
1. new feature's histogram (grad + hess),0
size: sampled_featureNum * (2 * splitNum),0
"in other words, concatenate each feature's histogram",0
2. get the span of this node,0
------ 3. using sparse-aware method to build histogram ---,0
"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances",0
3.1. get the instance index,0
3.2. get the grad and hess of the instance,0
3.3. add to the sum,0
3.4. loop the non-zero entries,0
3.4.1. get feature value,0
3.4.2. current feature's position in the sampled feature set,0
"int fPos = findFidPlace(this.controller.fSet, fid);",0
3.4.3. find the position of feature value in a histogram,0
"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]",0
3.4.4. add the grad and hess to the corresponding bin,0
3.4.5. add the reverse to the bin that contains 0.0f,0
4. add the grad and hess sum to the zero bin of all features,0
find the best split result of the histogram of a tree node,0
1. calculate the gradStats of the root node,0
"1.1. update the grad stats of the root node on PS, only called once by leader worker",0
2. loop over features,0
2.1. get the ture feature id in the sampled feature set,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"update the grad stats of the root node on PS, only called once by leader worker",0
3. update the grad stats of children node,0
3.1. update the left child,0
3.2. update the right child,0
find the best split result of one feature,0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
split value = sketches[splitIdx],0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
partition number,0
cols of each partition,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
"loop all the possible split value, start from split[0], the first item is the minimal feature value",0
assert fvalue >= sketch[start] && fvalue <= sketch[end];,0
if (left > end) return end - start;,0
find the best split result of the histogram of a tree node,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
find the best split result of one feature,0
1. set the feature id,0
splitEntry.setFid(fid);,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]",0
the task use index to find fvalue,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting",0
"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking.",0
max and min of each feature,0
clear all the information,0
calculate the sum of gradient and hess,0
"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,",0
ridx),0
check if necessary information is ready,0
"same as add, reduce is used in All Reduce",0
"features used in this tree, if equals null, means use all the features without sampling",0
node in the tree,0
the gradient info of each instances,0
initialize nodes,0
gradient,0
second order gradient,0
int sendStartCol = (int) row.getStartCol();,0
logistic loss for binary classification task.,0
"logistic loss, but predict un-transformed margin",0
check if label in range,0
return the default evaluation metric for the objective,0
"task type: classification, regression, or ranking",0
"quantile sketch, size = featureNum * splitNum",0
"gradient histograms, size = treeNodeNum * featureNum * splitNum",0
"active tree nodes, size = pow(2, treeDepth) -1",0
sampled features. size = treeNum * sampleRatio * featureNum,0
categorical feature. size = workerNum * cateFeatNum * splitNum,0
"split features, size = treeNum * treeNodeNum",0
"split values, size = treeNum * treeNodeNum",0
"split gains, size = treeNum * treeNodeNum",0
"node weights, size = treeNum * treeNodeNum",0
"node preds, size = treeNum * treeNodeNum",0
if using PS to perform split,0
step size for a tree,0
number of class,0
minimum loss change required for a split,0
maximum depth of a tree,0
number of features,0
number of nonzero,0
number of candidates split value,0
----- the rest parameters are less important ----,0
base instance weight,0
minimum amount of hessian(weight) allowed in a child,0
L2 regularization factor,0
L1 regularization factor,0
default direction choice,0
maximum delta update we can add in weight estimation,0
this parameter can be used to stabilize update,0
default=0 means no constraint on weight delta,0
whether we want to do subsample for row,0
whether to subsample columns for each tree,0
accuracy of sketch,0
accuracy of sketch,0
leaf vector size,0
option for parallelization,0
option to open cacheline optimization,0
whether to not print info during training.,0
maximum depth of the tree,0
number of features used for tree construction,0
"minimum loss change required for a split, otherwise stop split",0
----- the rest parameters are less important ----,0
default direction choice,0
whether we want to do sample data,0
whether to sample columns during tree construction,0
whether to use histogram for split,0
number of histogram units,0
whether to print info during training.,0
----- the rest parameters are obtained after training ----,0
total number of nodes,0
number of deleted nodes */,0
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
set basic configuration keys,0
use local deploy mode and dummy data spliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
"set input, output path",0
"set angel resource parameters #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Load Model from HDFS.,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"Set trainning data, and save model path",0
Set actionType train,0
Set MF algorithm parameters,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set input data path,0
Set save model path,0
Set actionType train,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
worker register,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
attempt 0,0
attempt 1,0
attempt 2,0
attempt 3,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
attempt 0,0
attempt1,0
attempt1,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
set basic configuration keys,0
"conf.set(AngelConfiguration.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());",0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
"DenseIntVector deltaVec = new DenseIntVector(100000, delta);",0
deltaVec.setMatrixId(matrixW1Id);,0
deltaVec.setRowId(0);,0
TODO Auto-generated constructor stub,1
import com.tencent.angel.psagent.consistency.SSPConsistencyController;,0
@RunWith(MockitoJUnitRunner.class),0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConfiguration.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);",0
get a angel client,0
add matrix,0
test create matrix,0
Int2IntOpenHashMap matrix1Clocks = task1.getMatrixClocks();,0
"assertEquals(matrix1Clocks.size(), 2);",0
"assertEquals(matrix1Clocks.get(w1Id), 1);",0
"assertEquals(matrix1Clocks.get(w2Id), 1);",0
psAgent.initAndStart();,0
test conf,0
test master location,0
test app id,0
test user,0
test ps agent attempt id,0
test ps agent id,0
test connection,0
test master client,0
test ip,0
test loc,0
test master location,0
test ps location,0
"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));",0
test all ps ids,0
test all matrix ids,0
test all matrix names,0
test matrix attribute,0
test matrix meta,0
test ps location,0
test partitions,0
"PartitionKey part1 = matrixPartitionRouter.getPartitionKey(1, 0);",0
assertTrue(part1 != null);,0
"assertEquals(part1, partition1Keys.get(0));",0
"PartitionKey part2 = matrixPartitionRouter.getPartitionKey(2, 0);",0
assertTrue(part2 != null);,0
"assertEquals(part2, partition2Keys.get(1));",0
"assertEquals(((SSPConsistencyController) consistControl).getStaleness(), staleness);",0
"PartitionKey part1 = psAgent.getMatrixPartitionRouter().getPartitionKey(1, 0);",0
"int part1Clock = clockCache.getClock(1, part1);",0
"assertEquals(part1Clock, 0);",0
,0
"PartitionKey part2 = psAgent.getMatrixPartitionRouter().getPartitionKey(2, 0);",0
"int part2Clock = clockCache.getClock(2, part2);",0
"assertEquals(part2Clock, 0);",0
"Note:[startRow,endRow)",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
test this func in testWriteMatrix,0
test this func in testClock,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
group0Id = new WorkerGroupId(0);,0
"worker0Id = new WorkerId(group0Id, 0);",0
"worker0Attempt0Id = new WorkerAttemptId(worker0Id, 0);",0
task0Id = new TaskId(0);,0
task1Id = new TaskId(1);,0
LOG.info(serverArbitraryIntRow1.getSparseRep());,0
LOG.info(serverArbitraryIntRow1.getSparseRep());,0
test this func in testWriteTo,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
"LOG.info(index[0] + "" "" + value[0]);",0
"LOG.info(index[1] + "" "" + value[1]);",0
"LOG.info(index[2] + "" "" + value[2]);",0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
dot,0
plusBy,0
@Test,0
public void dotDenseFloatVector() throws Exception {,0
int dim = 1000;,0
Random random = new Random(System.currentTimeMillis());,0
,0
double[] values = new double[dim];,0
float[] values_1 = new float[dim];,0
for (int i = 0; i < dim; i++) {,0
values[i] = random.nextDouble();,0
values_1[i] = random.nextFloat();,0
},0
,0
"DenseDoubleVector vec = new DenseDoubleVector(dim, values);",0
"TDoubleVector vec_1 = new DenseFloatVector(dim, values_1);",0
,0
double sum = 0.0;,0
for (int i = 0; i < dim; i++) {,0
sum += values[i] * values_1[i];,0
},0
,0
"assertEquals(sum, vec.dot(vec_1));",0
,0
},0
@Test,0
public void plusDenseFlaotVector() throws Exception {,0
"double[] value_1 = new double[]{0.1, 0.2, 0.3, 0.4, 0.5};",0
"double[] value_2 = new double[]{0.1f, 0.2f, 0.3f, 0.4f, 0.5f};",0
"DenseDoubleVector vec = new DenseDoubleVector(5, value_1);",0
"TDoubleVector vec_1 = new DenseFloatVector(5, value_2);",0
,0
TDoubleVector vec_2 = vec.plus(vec_1);,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(value_1[i] + value_2[i], vec_2.get(i));",0
,0
,0
"TDoubleVector vec_3 = vec.plus(vec_1, 2.0);",0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec_3.get(i), value_1[i] + 2 * value_2[i]);",0
,0
double[] oldValues = vec.getValues().clone();,0
,0
vec.plusBy(vec_1);,0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec.get(i), oldValues[i] + vec_1.get(i));",0
,0
oldValues = vec.getValues().clone();,0
,0
"vec.plusBy(vec_1, 3);",0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec.get(i), oldValues[i] + 3 * vec_1.get(i));",0
},0
@Test,0
public void plusByArrayTest() {,0
DenseFloatVector vec = new DenseFloatVector(dim);,0
"int[] index = genSortedIndexs(nnz, dim);",0
float[] deltF = genFloatArray(nnz);,0
double[] deltD = genDoubleArray(nnz);,0
,0
float[] oldVal = vec.getValues().clone();,0
"vec.plusBy(index, deltF);",0
for (int i = 0; i < nnz; i++) {,0
int idx = index[i];,0
"assertEquals(oldVal[idx] + deltF[i], vec.get(idx));",0
},0
,0
oldVal = vec.getValues().clone();,0
"vec.plusBy(index, deltD);",0
for (int i = 0; i < nnz; i++) {,0
int idx = index[i];,0
"assertEquals(oldVal[idx] + (float) deltD[i], vec.get(idx));",0
},0
,0
oldVal = vec.getValues().clone();,0
"vec.plusBy(index, deltF);",0
for (int i = 0; i < nnz; i++) {,0
int idx = index[i];,0
"assertEquals(oldVal[idx] + deltF[i], vec.get(idx));",0
},0
,0
oldVal = vec.getValues().clone();,0
"vec.plusBy(index, deltD);",0
for (int i = 0; i < nnz; i++) {,0
int idx = index[i];,0
"assertEquals(oldVal[idx] + (float) deltD[i], vec.get(idx));",0
},0
},0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
@Test,0
public void plusBy3() throws Exception {,0
"float[][] value = {{1.0f, 2.0f}, {3.0f, 4.0f}};",0
"DenseFloatMatrix mat = new DenseFloatMatrix(2, 2,value);",0
"TFloatVector vec = new DenseFloatVector(2, new float[]{1.0f, 1.0f});",0
vec.setRowId(0);,0
"TDoubleVector vec_1 = new DenseDoubleVector(2, new double[]{1.0f, 1.0f});",0
vec_1.setRowId(1);,0
TDoubleVector vec_2 = new SparseDoubleVector(2);,0
"vec_2.set(1, 1.0);",0
vec_2.setRowId(0);,0
,0
mat.plusBy(vec);,0
mat.plusBy(vec_1);,0
mat.plusBy(vec_2);,0
,0
"assertEquals(2.0f, mat.get(0, 0));",0
"assertEquals(4.0f, mat.get(0, 1));",0
"assertEquals(4.0f, mat.get(1, 0));",0
"assertEquals(5.0f, mat.get(1, 1));",0
},0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
test worker getActiveTaskNum,0
test worker getTaskNum,0
test worker getTaskManager,0
test workerId,0
test workerAttemptId,0
tet worker initFinished,0
test worker getInitMinclock,0
test worker loacation,0
test AppId,0
test Conf,0
test UserName,0
master location,0
masterClient,0
test psAgent,0
test worker get dataBlockManager,0
workerGroup.getSplits();,0
application,0
lcation,0
workerGroup info,0
worker info,0
task,0
Matrix parameters,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
Create an Angel client,0
Add different types of matrix,0
using mock object,0
verification,0
Stubbing,0
Default does nothing.,0
The app injection is optional,0
"renderText(""hello world"");",0
"user choose a workerGroupID from the workergroups page,",0
now we should change the AngelApp params and render the workergroup page;,0
"static final String WORKER_ID = ""worker.id"";",0
"div(""#logo"").",0
"img(""/static/hadoop-st.png"")._().",0
import org.apache.hadoop.yarn.webapp.view.FooterBlock;,0
import org.apache.hadoop.yarn.webapp.view.HeaderBlock;,0
JQueryUI.jsnotice(html);,0
import org.apache.hadoop.conf.Configuration;,0
import java.lang.reflect.Field;,0
get block locations from file system,0
create a list of all block and their locations,0
"if the file is not splitable, just create the one block with",0
full file length,0
each split can be a maximum of maxSize,0
if remainder is between max and 2*max - then,0
"instead of creating splits of size max, left-max we",0
create splits of size left/2 and left/2. This is,0
a heuristic to avoid creating really really small,0
splits.,0
add this block to the block --> node locations map,0
"For blocks that do not have host/rack information,",0
assign to default  rack.,0
add this block to the rack --> block map,0
Add this host to rackToNodes map,0
add this block to the node --> block map,0
"if the file system does not have any rack information, then",0
use dummy rack location.,0
The topology paths have the host name included as the last,0
component. Strip it.,0
get tokens for all the required FileSystems..,0
"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,",0
job.getConfiguration());,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
all the files in input set,0
it is allowed for maxSize to be 0. Disable smoothing load for such cases,0
process all nodes and create splits that are local to a node. Generate,0
"one split per node iteration, and walk over nodes multiple times to",0
distribute the splits across nodes.,0
Skip the node if it has previously been marked as completed.,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
Remove all blocks which may already have been assigned to other,0
splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
Remove entries from blocksInNode so that we don't walk these,0
again.,0
Done creating a single split for this node. Move on to the next,0
node so that splits are distributed across nodes.,0
This implies that the last few blocks (or all in case maxSize=0),0
were not part of a split. The node is complete.,0
if there were any blocks left over and their combined size is,0
"larger than minSplitNode, then combine them into one split.",0
Otherwise add them back to the unprocessed pool. It is likely,0
that they will be combined with other blocks from the,0
same rack later on.,0
This condition also kicks in when max split size is not set. All,0
blocks on a node will be grouped together into a single split.,0
haven't created any split on this machine. so its ok to add a,0
smaller one for parallelism. Otherwise group it in the rack for,0
balanced size create an input split and add it to the splits,0
array,0
Remove entries from blocksInNode so that we don't walk this again.,0
The node is done. This was the last set of blocks for this node.,0
Put the unplaced blocks back into the pool for later rack-allocation.,0
Node is done. All blocks were fit into node-local splits.,0
Check if node-local assignments are complete.,0
All nodes have been walked over and marked as completed or all blocks,0
have been assigned. The rest should be handled via rackLock assignment.,0
"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """,0
"+ completedNodes.size() + "", size left: "" + totalLength);",0
"if blocks in a rack are below the specified minimum size, then keep them",0
"in 'overflow'. After the processing of all racks is complete, these",0
overflow blocks will be combined into splits.,0
Process all racks over and over again until there is no more work to do.,0
Create one split for this rack before moving over to the next rack.,0
Come back to this rack after creating a single split for each of the,0
remaining racks.,0
"Process one rack location at a time, Combine all possible blocks that",0
reside on this rack as one split. (constrained by minimum and maximum,0
split size).,0
iterate over all racks,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"if we created a split, then just go to the next rack",0
"if there is a minimum size specified, then create a single split",0
"otherwise, store these blocks into overflow data structure",0
There were a few blocks in this rack that,0
remained to be processed. Keep them in 'overflow' block list.,0
These will be combined later.,0
Process all overflow blocks,0
"This might cause an exiting rack location to be re-added,",0
but it should be ok.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"Process any remaining blocks, if any.",0
create an input split,0
add this split to the list that is returned,0
long num = totLength / maxSize;,0
all blocks for all the files in input set,0
mapping from a rack name to the list of blocks it has,0
mapping from a block to the nodes on which it has replicas,0
mapping from a node to the list of blocks that it contains,0
populate all the blocks for all files,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init container allocator,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
init parameter server manager,0
recover task information if needed,0
init psagent manager and register psagent manager event,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
start a web service if use yarn deploy mode,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
parse parameter server counters,0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
refresh last heartbeat timestamp,0
send a state update event to the specific PSAttempt,0
check if parameter server can commit now.,0
check matrix metadata inconsistencies between master and parameter server.,0
"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix",0
"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix",0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
check whether psagent heartbeat timeout,0
check whether parameter server heartbeat timeout,0
check whether worker heartbeat timeout,0
choose a unused port,0
start RPC server,0
find matrix partitions from master matrix meta manager for this parameter server,0
remove this parameter server attempt from monitor set,0
remove this parameter server attempt from monitor set,0
"if worker attempt id is not in monitor set, we should shutdown it",0
find workergroup in worker manager,0
"if this worker group does not initialized, just return WORKERGROUP_NOTREADY",0
"if this worker group run over, just return WORKERGROUP_EXITED",0
"if this worker group is running now, return tasks, workers, data splits for it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update the clock for this matrix,0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update task iteration,0
private boolean matrixInited;,0
"check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
dispatch matrix partitions to parameter servers,0
update matrix id generator,0
"check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
dispatch matrix partitions to parameter servers,0
get matrix ids in the parameter server report,0
get the matrices parameter server need to create and delete,0
"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix",0
"if a matrix exists on master but not exist on parameter server, this parameter server need build it.",0
waitForMatrixReleaseOnPS(matrixId);,0
"Calculate how many splits we need. As each task handles a separate split of data, so we want",0
the number of splits equal to the number of tasks,0
split data,0
dispatch the splits to workergroups,0
split data,0
dispatch the splits to workergroups,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
write meta data to a temporary file,0
rename the temporary file to final file,0
"if the file exists, read from file and deserialize it",0
write task meta,0
write ps meta,0
generate a temporary file,0
write task meta to the temporary file first,0
rename the temporary file to the final file,0
"if last final task file exist, remove it",0
find task meta file which has max timestamp,0
"if the file does not exist, just return null",0
read task meta from file and deserialize it,0
generate a temporary file,0
write ps meta to the temporary file first.,0
rename the temporary file to the final file,0
"if the old final file exist, just remove it",0
find ps meta file,0
"if ps meta file does not exist, just return null",0
read ps meta from file and deserialize it,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
Transitions from the NEW state.,0
PA_FAILMSG,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG",0
event,0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event",0
dispatched before PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will",0
retry another attempt or failed,0
release container,0
TODO,1
set the launch time,0
"set tarckerName,httpPort, which used by webserver",0
added to psManager so psManager can monitor it;,0
psAttempt.getContext().getParameterServerManager.registerPSAttempt(psAttempt.attemptId);,0
set the finish time only if launch time is set,0
"ParameterServerJVM.setVMEnv(myEnv, conf);",0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Add the env variables passed by the user,0
Set logging level in the environment.,0
"This is so that, if the child forks another ""bin/hadoop"" (common in",0
streaming) it will have the correct loglevel.,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
Transitions from the NEW state.,0
Transitions from the RUNNING state.,0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
LocalStorageToken is needed irrespective of whether security is enabled,0
or not.,0
"TokenCache.setJobToken(jobToken, taskCredentials);",0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Construct the actual Container,0
The null fields are per-container and will be constructed for each,0
container separately.,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
"a * in the classpath will only find a .jar, so we need to filter out",0
all .jars and add everything else,0
Propagate the system classpath when using the mini cluster,0
Add standard Hadoop classes,0
Cache archives,0
Cache files,0
Sanity check,0
Add URI fragment or just the filename,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Final commmand,0
"if amTask is not null, we should clone task state from it",0
"if all parameter server complete commit, master can commit now",0
init and start master committer,0
Transitions from the NEW state.,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event",0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event dispatched before",0
PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.COMMITTING state,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
reqeuest resource:send a resource request to the resource allocator,0
"Once the resource is applied, build and send the launch request to the container launcher",0
deallocator the resource of the ps attempt:send a resource deallocator request to the,0
resource allocator,0
set the launch time,0
add the ps attempt to the heartbeat timeout monitoring list,0
parse ps attempt location and put it to location manager,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt",0
or failed,0
remove ps attempt id from heartbeat timeout monitor list,0
release container:send a release request to container launcher,0
set the finish time only if launch time is set,0
private long scheduledTime;,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
notify the event handler of state change,0
"If new state is not RUNNING and COMMITTING, add it to state timeout monitor",0
"if forcedState is set, just return",0
else get state from state machine,0
add this worker group to the success set,0
check if all worker group run over,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
calculate the actual number of worker groups and the total number of tasks based on the number of data split,0
"init all tasks , workers and worker groups and put them to the corresponding maps",0
just return the total task number now,0
TODO,1
"if workerAttempt is not null, we should clone task state from it",0
from NEW state,0
from SCHEDULED state,0
get data splits location for data locality,0
reqeuest resource:send a resource request to the resource allocator,0
"once the resource is applied, build and send the launch request to the container launcher",0
notify failed message to the worker,0
notify killed message to the worker,0
release the allocated container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
release the allocated container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
clean the container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
record the finish time,0
clean the container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listening list,0
record the finish time,0
"if the worker attempt launch successfully, add it to heartbeat timeout listening list",0
set worker attempt location,0
notify the register message to the worker,0
record the launch time,0
update worker attempt metrics,0
update tasks metrics,0
clean the container,0
notify the worker attempt run successfully message to the worker,0
record the finish time,0
init a worker attempt for the worker,0
schedule the worker attempt,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
"If we need Yarn to restart a new application master, we should not unregister from Yarn RM",0
register to Yarn RM,0
send heartbeat to Yarn RM every rmPollInterval milliseconds,0
"catch YarnRuntimeException, we should exit and need not retry",0
build heartbeat request,0
send heartbeat request to rm,0
"This can happen if the RM has been restarted. If it is in that state,",0
this application must clean itself up.,0
Setting NMTokens,0
assgin containers,0
"if some container is not assigned, release them",0
handle finish containers,0
dispatch container exit message to corresponding components,0
killed by framework,0
killed by framework,0
killed by framework,0
get application finish state,0
build application diagnostics,0
TODO:add a job history for angel,1
build unregister request,0
send unregister request to rm,0
Note this down for next interaction with ResourceManager,0
based on blacklisting comments above we can end up decrementing more,0
than requested. so guard for that.,0
send the updated resource request to RM,0
send 0 container count requests also to cancel previous requests,0
Update resource requests,0
try to assign to all nodes first to match node local,0
try to match all rack local,0
assign remaining,0
Update resource requests,0
send the container-assigned event to task attempt,0
build the start container request use launch context,0
send the start request to Yarn nm,0
send the message that the container starts successfully to the corresponding component,0
"after launching, send launched event to task attempt to move",0
it from ASSIGNED to RUNNING state,0
send the message that the container starts failed to the corresponding component,0
kill the remote container if already launched,0
start a thread pool to startup the container,0
See if we need up the pool size only if haven't reached the,0
maximum limit yet.,0
nodes where containers will run at *this* point of time. This is,0
*not* the cluster size and doesn't need to be.,0
"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the",0
later is just a buffer so we are not always increasing the,0
pool-size,0
the events from the queue are handled in parallel,0
using a thread pool,0
return if already stopped,0
shutdown any containers that might be left running,0
Build and initialize rpc client to master,0
Build local location,0
"Initialize matrix info, this method will wait until master accepts the information from",0
client,0
Get ps locations from master and put them to the location cache.,0
Initialize matrix meta information,0
Start heartbeat thread if need,0
Start all services,0
Register to master first,0
Report state to master every specified time,0
Notify run success to master only on ANGEL_PS_PSAGENT running mode,0
Stop all modules,0
Exit the process if on ANGEL_PS_PSAGENT mode,0
Notify run success to master only on ANGEL_PS_PSAGENT running mode,0
Stop all modules,0
Exit the process if on ANGEL_PS_PSAGENT mode,0
get configuration from config file,0
set localDir with enviroment set by nm.,0
Update generic resource counters,0
Updating resources specified in ResourceCalculatorProcessTree,0
Remove the CPU time consumed previously by JVM reuse,0
array stores clock for each row and clock,0
local task num,0
mapping from task index to taskId,0
mapping from taskId to task index,0
TODO Auto-generated method stub,1
Generate a flush request and put it to request queue,0
Generate a clock request and put it to request queue,0
Generate a merge request and put it to request queue,0
"If the matrix op log cache does not exist for the matrix, create a new one for the",0
matrix,0
and add it to cache maps,0
Add the message to the tree map,0
"If there are flush / clock requests blocked, we need to put this merge request into",0
the waiting queue,0
Launch a merge worker to merge the update to matrix op log cache,0
Remove the message from the tree map,0
Wake up blocked flush/clock request,0
Add flush/clock request to listener list to waiting for all the existing,0
updates are merged,0
Wake up blocked flush/clock request,0
"If all updates are merged for this matrix, we need wake up flush/clock requests which are",0
blocked.,0
Get next merge message sequence id,0
Wake up listeners(flush/clock requests) that have little sequence id than current merge,0
position,0
Wake up blocked merge requests,0
Get minimal sequence id from listeners,0
"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we",0
should flush updates to local matrix storage,0
unused now,1
Get partitions for the matrix,0
"Filter it, removing zero values",0
Doing average or not,0
Split this row according the matrix partitions,0
Add the splits to the result container,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
Then submit normal task until reach upper limit of flow control or all tasks are submit,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"LOG.info(""choose put server "" + psIds[index]);",0
allocate the bytebuf,0
"check the location of server is ready, if not, we should wait",0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
channelManager.removeChannelPool(loc);,0
find the partition request context from cache,0
Check if the result of the sub-request is received,0
Update received result number,0
Get row splits received,0
Put the row split to the cache(row index to row splits map),0
"If all splits of the row are received, means this row can be merged",0
TODO Auto-generated method stub,1
"Check futures, if the result of a sub-request is received, put it to the result queue",0
Now we just support pipelined row splits merging for dense type row,0
Wait until the clock value of this row is greater than or equal to the value,0
Get partitions for this row,0
First get this row from matrix storage,0
"If the row exists in the matrix storage and the clock value meets the requirements, just",0
return,0
Get row splits of this row from the matrix cache first,0
"If the row split does not exist in cache, get it from parameter server",0
Wait the final result,0
Put it to the matrix cache,0
Split the matrix oplog according to the matrix partitions,0
"If need update clock, we should send requests to all partitions",0
use update index if exist,0
Filter the rowIds which are fetching now,0
Send the rowIndex to rpc dispatcher and return immediately,0
Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition,0
Generate dispatch items and add them to the corresponding queues,0
Pre-fetching is disable default,0
matrix id to clock map,0
"task index, it must be unique for whole application",0
matrix id to task update index map. each task may only update some specific part for a matrix,0
if (matrixManager.getMatrixMeta(createResponse.getMatrixId()) != null) {,0
return matrixManager.getMatrixMeta(createResponse.getMatrixId());,0
},0
Deserialize data splits meta,0
Get workers,0
Send request to every ps,0
Wait the responses,0
Update clock cache,0
Get row from cache.,0
"if row clock is satisfy ssp staleness limit, just return.",0
Get row from ps.,0
"For ASYNC mode, just get from pss.",0
"For BSP/SSP, get rows from storage/cache first",0
Get from ps.,0
"For ASYNC, just get rows from pss.",0
no more retries.,0
calculate sleep time and return.,0
parse the i-th sleep-time,0
parse the i-th number-of-retries,0
calculateSleepTime may overflow.,0
"A few common retry policies, with no delays.",0
response will be null for one way messages.,0
serial number and list size,0
indicates whether this connection's life cycle is managed,0
See if we already have a connection (common case),0
create a unique lock for this RS + protocol (if necessary),0
get the RS lock,0
do one more lookup in case we were stalled above,0
Only create isa when we need to.,0
definitely a cache miss. establish an RPC for,0
this RS,0
Throw what the RemoteException was carrying.,0
check,0
every,0
minutes,0
TODO,1
failoverHandler,0
"The number of times this invocation handler has ever been failed over,",0
before this method invocation attempt. Used to prevent concurrent,0
failed method invocations from triggering multiple failover attempts.,0
Make sure that concurrent failed method invocations,0
only cause a,0
single actual fail over.,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
"LOG.info(""method "" + method.getName() + ""construct request time = """,0
+ (System.currentTimeMillis() - beforeConstructTs));,0
get an instance of the method arg type,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
Message (hand written code usually has only a single,0
argument),0
log any RPC responses that are slower than the configured,0
warn,0
response time or larger than configured warning size,0
"when tagging, we let TooLarge trump TooSmall to keep",0
output simple,0
note that large responses will often also be slow.,0
provides a count of log-reported slow responses,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
unexpected,0
"in the protobuf methods, args[1] is the only significant argument",0
for JSON encoding,0
base information that is reported regardless of type of call,0
DefaultChannelFuture.setUseDeadLockChecker(false);,0
Set up.,0
Configure the event pipeline factory.,0
Make a new connection.,0
Need to reconnect,0
Upgrade to write lock,0
Downgrade to read lock:,0
(TODO: why use writeLock? why not use this.channel instead of channel?,1
Remove all pending requests (will be canceled after relinquishing,0
write lock).,0
Cancel any pending requests by sending errors to the callbacks:,0
Close the channel:,0
Close the connection:,0
Shut down all thread pools to exit.,0
"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());",0
See NettyServer.prepareResponse for where we write out the response.,0
"It writes the call.id (int), a boolean signifying any error (and if",0
"so the exception name/trace), and the response bytes",0
Read the call id.,0
"When the stream is closed, protobuf doesn't raise an EOFException,",0
"instead, it returns a null message object.",0
channel = e.getChannel();,0
channel = e.getChannel();,0
"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +",0
System.currentTimeMillis());,0
Make an ml rpc client.,0
"It would be good widen this to just Throwable, but IOException is what we",0
allow now,0
not implemented,1
not implemented,1
"track what RpcEngine is used by a proxy class, for stopProxy()",0
cache of RpcEngines by protocol,0
return the RpcEngine configured to handle a protocol,0
We only handle the ConnectException.,0
This is the exception we can't handle.,0
check if timed out,0
wait for retry,0
IGNORE,0
return the RpcEngine that handles a proxy object,0
The default implementation works synchronously,0
punt: allocate a new buffer & copy into it,0
"LOG.info(System.getProperty(""user.dir""));",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
a simple hdfs copy function assume src path and dest path are in same hdfs,0
and FileSystem object has same schema,0
"LOG.warn(""interrupted while sleeping"", ie);",0
private static UnpooledByteBufAllocator allocator = new UnpooledByteBufAllocator(false);,0
public static String getHostname() {,0
try {,0
"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();",0
} catch (UnknownHostException uhe) {,0
},0
"return new StringBuilder().append("""").append(uhe).toString();",0
},0
"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {",0
String hostname = getHostname();,0
String classname = clazz.getSimpleName();,0
"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new",0
"StringBuilder().append(""Starting "").append(classname).toString(), new",0
"StringBuilder().append(""  host = "").append(hostname).toString(), new",0
"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new",0
"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new",0
"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));",0
,0
"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {",0
public void run() {,0
"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +",0
"this.val$classname + "" at "" + this.val$hostname}));",0
},0
});,0
},0
"We we interrupted because we're meant to stop? If not, just",0
continue ignoring the interruption,0
Recalculate waitTime.,0
// Begin delegation to Thread,0
// End delegation to Thread,0
load angel system configuration,0
load user configuration:,0
1. user config file,0
2. command lines,0
"add user resource files to ""angel.lib.jars"" to upload to hdfs",0
load user job jar if it exist,0
Expand the environment variable,0
instance submitter class,0
Add default fs(local fs) for lib jars.,0
Obtain filename from path,0
Split filename to prexif and suffix (extension),0
Check if the filename is okay,0
Prepare temporary file,0
Prepare buffer for data copying,0
Open and check input stream,0
Open output stream and copy data between source file in JAR and the temporary file,0
"If read/write fails, close streams safely before throwing an exception",0
"Finally, load the library",0
little endian load order,0
tail,0
fallthrough,0
fallthrough,0
finalization,0
fmix(h1);,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
JobStateProto jobState = report.getJobState();,0
generate tmp output directory,0
the leaf level file should be readable by others,0
the subdirs in the path should have execute permissions for,0
others,0
2.get job id,0
Credentials credentials = new Credentials();,0
4.copy resource files to hdfs,0
5.write configuration to a xml file,0
6.create am container context,0
7.Submit to ResourceManager,0
8.get app master client,0
Create a number of filenames in the JobTracker's fs namespace,0
add all the command line files/ jars and archive,0
first copy them to jobtrackers filesystem,0
should not throw a uri exception,0
should not throw an uri excpetion,0
set the timestamps of the archives and files,0
set the public/private visibility of the archives and files,0
get DelegationToken for each cached file,0
check if we do not need to copy the files,0
is jt using the same file system.,0
just checking for uri strings... doing no dns lookups,0
to see if the filesystems are the same. This is not optimal.,0
but avoids name resolution.,0
this might have name collisions. copy will throw an exception,0
parse the original path to create new path,0
check for ports,0
Write job file to JobTracker's fs,0
Setup resource requirements,0
Setup LocalResources,0
Setup security tokens,0
Setup the command to run the AM,0
Add AM user command opts,0
Final command,0
Setup the CLASSPATH in environment,0
"i.e. add { Hadoop jars, job jar, CWD } to classpath.",0
Setup the environment variables for Admin first,0
"Setup the environment variables (LD_LIBRARY_PATH, etc)",0
Parse distributed cache,0
Setup ContainerLaunchContext for AM container,0
Set up the ApplicationSubmissionContext,0
resposne.encode(buf);,0
TODO:,1
resposne.encode(buf);,0
TODO:,1
resposne.encode(buf);,0
TODO:,1
resposne.encode(buf);,0
TODO:,1
private final ParameterServerId serverId;,0
private final PSIdProto idProto;,0
to exit,0
mkdir does not throw exception if path exits,0
commitTaskPool.shutdown();,0
private final ParameterServer psServer;,0
TODO,1
"when we should write snapshot to hdfs? clearly, we have two methods:",0
"1. write snapshot at regular time, if there are updates, just write them.",0
"2. write snapshot every N iterations, this method depends on notification of master",0
"FSDataOutputStream output = fileContext.create(snapshotsTempFilePath,",0
EnumSet.of(CreateFlag.CREATE));,0
@brief get filename of the old snapshot written before,0
"no snapshotFile write before, maybe write snapshots the first time",0
start end,0
rowtype,0
data.rewind();,0
data.rewind();,0
data.rewind();,0
Pass the matrix and partition number field,0
Mapping from taskId to clock value.,0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
nnz = 0;,0
for (int i = 0; i < keys.length; i++),0
if (used[i]) {,0
"denseRep.put(keys[i], values[i]);",0
nnz++;,0
},0
sparseRep = null;,0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
for (int i = 0; i < keys.length; i++),0
if (used[i]) {,0
"denseRep.put(keys[i], values[i]);",0
},0
sparseRep = null;,0
output.writeInt(data.length);,0
@Override,0
public void serialize(ByteBuf buf) {,0
if (sparseRep != null),0
return serializeSparse();,0
else if (denseRep != null),0
return serializeDense();,0
return serializeEmpty();,0
},0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
int idx = 0;,0
for (int i = 0; i < keys.length; i++),0
if (used[i]) {,0
"keysBuf.put(idx, keys[i]);",0
"valuesBuf.put(idx, values[i]);",0
idx++;,0
},0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
"int ov, k, v;",0
for (int i = 0; i < keys.length; i++) {,0
if (used[i]) {,0
k = keys[i];,0
ov = denseRep.get(k);,0
v = ov + values[i];,0
"denseRep.put(k, v);",0
if (ov != 0 && v == 0),0
nnz--;,0
},0
},0
"add the PSAgentContext,need fix",0
set MatrixPartitionLocation,0
set attribute,0
TODO Auto-generated method stub,1
@brief Sorted index for non-zero items,0
@brief Number of non-zero items in this vector,0
@brief Array to store values.,0
@brief sum of the square of all of element,0
"LOG.error(""Cannot perform plus operation on SparseDoubleSortedVector"");",0
TODO Auto-generated method stub,1
TODO:,1
TODO:,1
write the max abs,0
"Thread.currentThread().getContextClassLoader().getResourceAsStream(""feature_conf.xml"");",0
this.matchList = new ArrayList<Match>();,0
"LOG.debug(String.format(""index: %d, value: %s"", index, value));",0
"LOG.debug(String.format(""index: %d, value: %s"", index, value));",0
"LOG.debug(String.format(""index: %d, value: %s"", index, value));",0
"LOG.debug(String.format(""index: %d, value: %s"", index, value));",0
"LOG.debug(""target="" + target);",0
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
get configuration from config file,0
set localDir with enviroment set by nm.,0
get master location,0
init task manager and start tasks,0
start heartbeat thread,0
taskManager.assignTaskIds(response.getTaskidsList());,0
todo,1
"if worker timeout, it may be knocked off.",0
"SUCCESS, do nothing",0
heartbeatFailedTime = 0;,0
private KEY currentKey;,0
will be created,0
TODO Auto-generated method stub,1
Bitmap bitmap = new Bitmap();,0
int max = indexArray[size - 1];,0
byte [] bitIndexArray = new byte[max / 8 + 1];,0
for(int i = 0; i < size; i++){,0
int bitIndex = indexArray[i] >> 3;,0
int bitOffset = indexArray[i] - (bitIndex << 3);,0
switch(bitOffset){,0
case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;,0
case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;,0
case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;,0
case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;,0
case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;,0
case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;,0
case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;,0
case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;,0
},0
},0
//////////////////////////////,0
Application Configs,0
//////////////////////////////,0
//////////////////////////////,0
Master Configs,0
//////////////////////////////,0
//////////////////////////////,0
Worker Configs,0
//////////////////////////////,0
//////////////////////////////,0
Task Configs,0
//////////////////////////////,0
//////////////////////////////,0
ParameterServer Configs,0
//////////////////////////////,0
////////////////// IPC //////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
Configs used to ANGEL_PS_PSAGENT running mode future.,0
model parse,0
private Configuration conf;,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConfiguration.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);",0
"conf.set(AngelConfiguration.ANGEL_MODEL_PATH, LOCAL_FS + TMP_PATH + ""/out"");",0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
get a angel client,0
add matrix,0
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
"set input, output path",0
"set angel resource parameters #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Submit GBDT Train Task,0
Load Model from HDFS.,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"Set trainning data, save model, log path",0
Set actionType train,0
Set MF algorithm parameters,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
LOG.info(sigmoid(data[i]));,0
LOG.info(Math.exp(-data[i]));,0
when b is a negative number,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Set trainning data path,0
Set data format,0
Set save model path,0
Set log sava path,0
Set actionType train,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Set log sava path,0
Feature number of train data,0
Total iteration number,0
Validation Ratio,0
Data format,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set basic configuration keys,0
Set data format,0
Use local deploy mode,0
"set angel resource parameters #worker, #task, #PS",0
set sgd SVM algorithm parameters,0
"set input, output path",0
Set save model path,0
Set actionType train,0
Set log path,0
Submit LR Train Task,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log sava path,0
Set actionType prediction,0
double z=pre*y;,0
if(z<=0) return 0.5-z;,0
"else if(z>0 && z<1) return 0.5*Math.pow(1-z,2);",0
return 0.0;,0
if (pre * y <= 0),0
return y;,0
else if (pre * y > 0 && pre * y < 1),0
return (1 - pre * y) * y;,0
return 0.0;,0
feature index used to split,0
feature value used to split,0
loss change after split this node,0
grad stats of the left child,0
grad stats of the right child,0
"LOG.info(""Constructor with fid = -1"");",0
fid = -1: no split currently,0
the minimal split value is the minimal value of feature,0
the splits do not include the maximal value of feature,0
"1. the average distance, (maxValue - minValue) / splitNum",0
2. calculate the candidate split value,0
insIdx is the index of instances,0
loop over features,0
"int[] counts = {10, 2, 1, 2, 0, 3, 0, 5};",0
setting,0
ranking.,0
clear all the information,0
map feature id to the position in units,0
add hist unit of each feature,0
"loop instance's position, find those belong to nid",0
calculate the sum of gradient and hess,0
"queue of nodes to be expanded, -1 means no work",0
map active node to its working index offset in qexpand,0
"can be -1, which means the node is not actively expanding",0
position of each instances in the tree,0
"can be negative, which means this ins2Node is no longer expanding",0
used to candidate split cut value,0
"HistSet of all nodes, use node2Work to find the position",0
loss function,0
gradient statistics,0
create loss function,0
calculate gradient info,0
"add root node, including split entry",0
add root node work,0
create split value helper,0
init instance position to root,0
init histogram,0
calculate grad info of each instance,0
add active work(node) to queue,0
add new work to the queue,0
init histogram,0
"get one work(node) from queue, -1 means no active work",0
get candidate split values,0
build gradient histogram,0
get candidate split value,0
"find the best split from candidates, add new node to tree",0
"add new node,",0
set node's left and right children,0
"create left and right children node, add them to regtree",0
create node stats for children nodes add them to regtree,0
job after splits,0
"update work queue, add new work to queue, update node to work",0
"update instance pos,",0
no extra work for leaf node,0
"update work queue, set finished node to inactive",0
"set node's work to inactive, set node2Work to inactivate",0
add children node's work to queue,0
update instance ins2Node,0
update instance's corresponding node,0
"LOG.info(String.format(""Move ins[%d] fid[%d] fvalue[%f] to node[%d]"",",0
"insIdx, fid, fvalue, 2 * nid + 1));",0
"LOG.info(String.format(""Move ins[%d] fid[%d] fvalue[%f] to node[%d]"",",0
"insIdx, fid, fvalue, 2 * nid + 2));",0
check whether there exist active works in the queue,0
change the node to leaf,0
change to leaf,0
update the preds of instances,0
evaluate the pre result,0
build the regression tree,0
"if reach the max depth, set it to leaf",0
1. new feature's histogram (grad + hess),0
size: sampled_featureNum * (2 * splitNum),0
"in other words, concatenate each feature's histogram",0
2. get the span of this node,0
------ 3. using sparse-aware method to build histogram ---,0
"first add grads of all instances to the first bin of all features, then loop the non-zero",0
entries,0
the grad sum and hess sum of all the instances,0
3.1. get the instance index,0
3.2. get the grad and hess of the instance,0
3.3. add to the sum,0
"LOG.info(String.format(""Instance[%d]: indices size[%d], indices%s"",",0
"insIdx, instance.getIndices().length, Arrays.toString(instance.getIndices())));",0
"LOG.info(String.format(""Instance[%d]: values size[%d], values%s "",",0
"insIdx, instance.getValues().length, Arrays.toString(instance.getValues())));",0
3.4. loop the non-zero entries,0
3.4.1. get feature value,0
3.4.2. current feature's position in the sampled feature set,0
3.4.3. find the position of feature value in a histogram,0
"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]",0
3.4.4. add the grad and hess to the corresponding bin,0
3.4.5. add the reverse to the bin that contains 0.0f,0
int gradStartIdx = 2 * splitNum * fPos;,0
int hessStartIdx = gradStartIdx + splitNum;,0
if (idx % 50000 == 0 && fPos % 5 == 0) {,0
"LOG.info(String.format(""instance:%d, feature pos: %d, index of 0.0f: %d, "" +",0
"""grad index: %d, hess index: %d"",",0
"idx, fPos, fZeroValueIdx, gradZeroIdx, hessZeroIdx));",0
},0
"LOG.info(String.format(""Add negative grad to index[%d] value[%f], "" +",0
"""add negative hess to index[%d] value[%f]"",",0
"gradStartIdx, curGrad, hessStartIdx, curHess));",0
"LOG.info(String.format(""Update 0-th bin grad to %f, 0-th bin hess to %f"",",0
"histogram.get(gradStartIdx), histogram.get(hessStartIdx)));",0
4. add the grad and hess sum to the first bin of all features,0
int startIdx = fid * 2 * splitNum;,0
if (fid % 5000 == 0) {,0
"LOG.info(String.format(""feature pos: %d, index of 0.0f: %d, "" +",0
"""grad index: %d, hess index: %d"",",0
"fid, fZeroValueIdx, gradZeroIdx, hessZeroIdx));",0
},0
// 3. loop over all the features of all the instances on this node,0
for (int idx = nodeStart; idx <= nodeEnd; idx++) {,0
// 3.1. get the instance index,0
int insIdx = this.controller.instancePos[idx]; // the instance index,0
SparseDoubleSortedVector instance = this.controller.dataMeta.instances.get(insIdx);,0
// 3.2. get instance indices and values,0
//int[] indices = instance.getIndices();,0
//double[] values = instance.getValues();,0
//for (int i = 0; i < indices.length; i++) {,0
for(int fid = 0; fid < instance.getDimension(); fid++) {,0
// 3.3. get feature id,0
//int fid = indices[i];,0
// 3.4. current feature's position in the sampled feature set,0
"int fPos = findFidPlace(this.controller.fset, fid);",0
if (fPos == -1) {,0
continue;,0
},0
// 3.5. get feature value,0
//float fv = (float) values[i];,0
float fv = (float) instance.get(fid);,0
// 3.6. find the position of feature value in a histogram,0
"// the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]",0
"int fvalueIdx = findFvaluePlace(this.controller.sketches, fv,",0
"fid * this.controller.param.numSplit, (fid + 1) * this.controller.param.numSplit - 1);",0
// 3.7. get the grad and hess of the instance,0
GradPair gradPair = this.controller.gradPairs.get(insIdx);,0
// 3.8. the updated position in the histogram,0
// since the siz of histogram = grad(# splitNum) + hess(# splitNum),0
// the hessIndex = gradIndex + numSplit,0
//if (insIdx % 100000 == 0) {,0
//,0
"LOG.info(String.format(""Instance[%d], fid: %d, fpos: %d, fvalueIdx: %d, grad: %f, hess: %f"",",0
"// insIdx, fid, fPos, fvalueIdx, gradPair.getGrad(), gradPair.getHess()));",0
//},0
int gradIdx = 2 * this.controller.param.numSplit * fPos + fvalueIdx;,0
int hessIdx = 2 * this.controller.param.numSplit * fPos + fvalueIdx + this.controller.param.numSplit;,0
// 3.9. add grad and hess to the corresponding histogram,0
"histogram.set(gradIdx, histogram.get(gradIdx) + gradPair.getGrad());",0
"histogram.set(hessIdx, histogram.get(hessIdx) + gradPair.getHess());",0
},0
},0
find the best split result of the histogram of a tree node,0
1. calculate the gradStats of the root node,0
"1.1. update the grad stats of the root node on PS, only called once by leader worker",0
2. loop over features,0
2.1. get the ture feature id in the sampled feature set,0
2.2. get the indexes of histogram of this feature,0
"LOG.info(String.format(""Histogram of feature[%d]: %s"", trueFid,",0
Arrays.toString(curHistogram.getValues())));,0
2.3. find the best split of current feature,0
"LOG.info(String.format(""Best split of feature[%d]: value[%f], gain[%f]"",",0
"trueFid, curSplit.getFvalue(), curSplit.getLossChg()));",0
2.4. update the best split result if possible,0
"update the grad stats of the root node on PS, only called once by leader worker",0
3. update the grad stats of children node,0
3.1. update the left child,0
3.2. update the right child,0
find the best split result of one feature,0
"LOG.info(String.format(""Find best split for fid[%d] in histogram[size:%d], startIdx[%d]"",",0
"fid, histogram.getDimension(), startIdx));",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
"LOG.info(String.format(""Node[%d] feature[%d]: sumGrad[%f], sumHess[%f], gain[%f]"",",0
"this.nid, fid, rootStats.sumGrad, rootStats.sumHess, rootGain));",0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
split value = sketches[splitIdx+1],0
"LOG.info(String.format(""Find new best split: fid[%d], fvalue[%f], lossChg[%f]"",",0
"splitEntry.fid, splitEntry.fvalue, splitEntry.lossChg));",0
"LOG.info(String.format(""Left child of node[%d]: sumGrad[%f], sumHess[%f]; "" +",0
"""right child of node[%d]: sumGrad[%f], sumHess[%f]; lossChg[%f]"",",0
"this.nid, leftStats.sumGrad, leftStats.sumHess,",0
"this.nid, rightStats.sumGrad, rightStats.sumHess, lossChg));",0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
"LOG.info(String.format(""Best left node grad: sumGrad[%f], sumHess[%f]"",",0
"bestLeftStat.sumGrad, bestLeftStat.sumHess));",0
"LOG.info(String.format(""Best right node grad: sumGrad[%f], sumHess[%f]"",",0
"bestRightStat.sumGrad, bestRightStat.sumHess));",0
partition number,0
cols of each partition,0
"update the grad stats of the root node on PS, only called once by leader worker",0
3. update the grad stats of children node,0
3.1. update the left child,0
3.2. update the right child,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
"loop all the possible split value, start from split[1], since the first item is the minimal",0
feature value,0
find the best split result of the histogram of a tree node,0
2.2. get the indexes of histogram of this feature,0
"LOG.info(String.format(""Histogram of feature[%d]: %s"", trueFid,",0
Arrays.toString(curHistogram.getValues())));,0
2.3. find the best split of current feature,0
"LOG.info(String.format(""Best split of feature[%d]: value[%f], gain[%f]"",",0
"trueFid, curSplit.getFvalue(), curSplit.getLossChg()));",0
2.4. update the best split result if possible,0
find the best split result of one feature,0
1. set the feature id,0
splitEntry.setFid(fid);,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"LOG.info(String.format(""The current split: fid[%d], split index[%f], lossChg[%f]"",",0
"fid, (float) splitIdx, lossChg));",0
split value = sketches[splitIdx+1],0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
"LOG.info(String.format(""Histogram of feature[%d]: %s"", trueFid,",0
Arrays.toString(curHistogram.getValues())));,0
2.3. find the best split of current feature,0
"LOG.info(String.format(""Best split of feature[%d]: value[%f], gain[%f]"",",0
"trueFid, curSplit.getFvalue(), curSplit.getLossChg()));",0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
"LOG.info(String.format(""Node[%d] feature[%d]: sumGrad[%f], sumHess[%f], gain[%f]"",",0
"this.nid, fid, rootStats.sumGrad, rootStats.sumHess, rootGain));",0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"here we set the fvalue=splitIndex, split value = sketches[splitIdx+1], the task use",0
index to find fvalue,0
"LOG.info(String.format(""Find new best split: fid[%d], fvalue[%f], lossChg[%f]"",",0
"splitEntry.fid, splitEntry.fvalue, splitEntry.lossChg));",0
"LOG.info(String.format(""Left child of node[%d]: sumGrad[%f], sumHess[%f]; "" +",0
"""right child of node[%d]: sumGrad[%f], sumHess[%f]; lossChg[%f]"",",0
"this.nid, leftStats.sumGrad, leftStats.sumHess,",0
"this.nid, rightStats.sumGrad, rightStats.sumHess, lossChg));",0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
"LOG.info(String.format(""Best left node grad: sumGrad[%f], sumHess[%f]"",",0
"bestLeftStat.sumGrad, bestLeftStat.sumHess));",0
"LOG.info(String.format(""Best right node grad: sumGrad[%f], sumHess[%f]"",",0
"bestRightStat.sumGrad, bestRightStat.sumHess));",0
"features used in this tree, if equals null, means use all the features without sampling",0
node in the tree,0
the gradient info of each instances,0
initialize nodes,0
"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,",0
ridx),0
check if necessary information is ready,0
"same as add, reduce is used in All Reduce",0
features used in this tree,0
node in the tree,0
the gradient info of each instances,0
initialize feature id,0
initialize nodes,0
"add root node, creete split entry",0
"initialize statistic of the root, including gradient stats",0
"LOG.info(String.format(""Add fid[%d] fvalue[%f] to [%d]-bin"", this.fid, fv, i));",0
loop over all the data in hist,0
whether we can split with current hessian,0
right = root -left,0
whether we can split with current hessian,0
"LOG.info(String.format(""Left child of node[%d]: sumGrad[%f], sumHess[%f]; "" +",0
"""right child of node[%d]: sumGrad[%f], sumHess[%f]; lossChg[%f]"",",0
"this.nid, leftStats.sumGrad, leftStats.sumHess,",0
"this.nid, rightStats.sumGrad, rightStats.sumHess, lossChg));",0
"LOG.info(String.format(""The best split of node[%d]: fid[%d], fvalue[%f], lossChg[%f]"",",0
"this.nid, splitEntry.fid, splitEntry.fvalue, splitEntry.lossChg));",0
gradient,0
second order gradient,0
logistic loss for binary classification task.,0
"logistic loss, but predict un-transformed margin",0
check if label in range,0
if (!prob) {,0
preds.clear();,0
preds.addAll(rec);,0
},0
return the default evaluation metric for the objective,0
read partition header,0
deal with row according the rowType,0
TODO Auto-generated method stub,1
tree.tree[topic + K] = (row.get(topic) + beta) / (n_k.get(topic) + vbeta);,0
Inc update to local buffers,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
loss function,0
gradient and hessian,0
tree node,0
initialize the phase,0
current tree and depth,0
create loss function,0
calculate grad info of each instance,0
"create data sketch, push candidate split value to PS",0
1. calculate candidate split value,0
"float[][] splits = TAvgDisSplit.getSplitValue(this.dataMeta, this.param.numSplit);",0
"LOG.info(String.format(""Local sketch size[%d]: %s"",",0
"sketchVec.getDimension(), Arrays.toString(sketchVec.getValues())));",0
2. push local sketch to PS,0
sketchClient.activateOpLog(new String[] {this.param.sketchName});,0
3. set phase to GET_SKETCH,0
"pull the global sketch from PS, only called once by each worker",0
"LOG.info(String.format(""Sketch vector: %s"", Arrays.toString(sketchVector.getValues())));",0
this.sketches = Floats.toArray(Doubles.asList(sketchVector.getValues()));,0
sample feature,0
push sampled feature set to the current tree,0
this.taskContext.activateOpLog(new String[] {this.param.sampledFeaturesName});,0
create new tree,0
"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,",0
calculate gradient,0
"1. create new tree, initialize tree nodes and node stats",0
"2. initialize feature set, if sampled, get from PS, otherwise use all the features",0
2.1. pull the sampled features of the current tree,0
"2.2. if use all the featues, only called one",0
"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle",0
4. set root node to active,0
"5. reset instance position, set the root node's span",0
6. calculate gradient,0
7. set phase to run active,0
1. start threads of active tree nodes,0
1.1. start threads for active nodes to generate histogram,0
1.2. set thread status to running,0
1.3. set the oplog to active,0
activeOpLogSet.add(histParaName);,0
"2. check thread stats, if all threads finish, return",0
this.taskContext.activateOpLog(activeOpLogSet);,0
clock,0
find split,0
"1. find responsible tree node, using RR scheme",0
2. pull gradient histogram,0
2.1. get the name of this node's gradient histogram on PS,0
2.2. pull the histogram,0
find best split result of this tree node,0
"2.4. using server split, each partition of the histogram contains its best split result",0
find the best split from all partitions,0
"2.5. the updated split result (tree node/feature/value/gain) on PS,",0
"2.4. otherwise, the returned histogram contains the gradient info",0
"2.5. the updated split result (tree node/feature/value/gain) on PS,",0
2.6. reset this tree node's gradient histogram to 0,0
3. push split feature to PS,0
4. push split value to PS,0
5. push split gain to PS,0
6. set phase to AFTER_SPLIT,0
clock,0
1. get split feature,0
2. get split value,0
3. get split gain,0
4. get node weight,0
"LOG.info(String.format(""Node grad stats: %s"", Arrays.toString(nodeGradStatsVec.getValues())));",0
5. split node,0
5.1. set the children nodes of this node,0
5.2. set split info and grad stats to this node,0
5.2. create children nodes,0
"5.3. create node stats for children nodes, and add them to the tree",0
5.4. reset instance position,0
"5.5. add new active nodes if possible, inc depth, otherwise finish this tree",0
"LOG.info(String.format(""Current depth: %d, max depth: %d"",",0
"this.currentDepth, this.param.maxDepth));",0
5.6. set children nodes to leaf nodes,0
5.7. set nid to leaf node,0
5.8. deactivate active node,0
"6. check if there is active node, if not, finish current tree",0
boolean hasActive = hasActiveTNode();,0
if (!hasActive) {,0
finishCurrentTree();,0
this.phase = GBDTPhase.NEW_TREE;,0
} else {,0
finishCurrentDepth();,0
this.phase = GBDTPhase.RUN_ACTIVE;,0
},0
clock,0
"split the span of one node, reset the instance position",0
in case this worker has no instance on this node,0
set the span of left child,0
set the span of right child,0
"1. left to right, find the first instance that should be in the right child",0
"2. right to left, find the first instance that should be in the left child",0
3. swap two instances,0
4. find the cut pos,0
than the split value,0
5. set the span of left child,0
6. set the span of right child,0
set tree node to active,0
set node to leaf,0
set node to inactive,0
finish current tree,0
calculate the error,0
predict();,0
finish current depth,0
set the tree phase,0
check if there is active node,0
check if finish all the tree,0
update node's grad stats on PS,0
"called during splitting in DistributedHistHelper, update the grad stats of children nodes after",0
find the best split,0
the root node's stats is updated by leader worker by one time,0
// 1.1. get the index of the grad and hess,0
"int[] nodeIndice = { nid, nid + this.activeNode.length };",0
// 1.2. get the grad sum and hess sum,0
"double[] weightValue = { gradStats.sumGrad, gradStats.sumHess };",0
1.3. create the update,0
for (int i = 0; i < nodeIndice.length; i++) {,0
"vec.set(nodeIndice[i], weightValue[i]);",0
},0
1.4. push the update to PS,0
"LOG.info(String.format(""Update the prediction of instance[%d] to %f, label[%f]"",",0
"insIdx, this.dataMeta.preds[insIdx], this.dataMeta.labels[insIdx]));",0
the leader task adds node prediction to flush list,0
1. name of this node's grad histogram on PS,0
2. build the grad histogram of this node,0
3. push the histograms to PS,0
"LOG.info(String.format(""Histogram: size[%d] %s"", histogram.getDimension(),",0
Arrays.toString(histogram.getValues())));,0
4. reset thread stats to finished,0
int sendStartCol = startFid * 7; // each split contains 7 doubles,0
step size for a tree,0
number of class,0
minimum loss change required for a split,0
maximum depth of a tree,0
number of features,0
number of nonzero,0
number of candidates split value,0
----- the rest parameters are less important ----,0
base instance weight,0
minimum amount of hessian(weight) allowed in a child,0
L2 regularization factor,0
L1 regularization factor,0
default direction choice,0
maximum delta update we can add in weight estimation,0
this parameter can be used to stabilize update,0
default=0 means no constraint on weight delta,0
whether we want to do subsample for row,0
whether to subsample columns for each tree,0
accuracy of sketch,0
accuracy of sketch,0
leaf vector size,0
option for parallelization,0
option to open cacheline optimization,0
whether to not print info during training.,0
return MathUtils.sqr(sumGrad) / (sumHess + regLambda);,0
feature index used to split,0
feature value used to split,0
loss change after split this node,0
grad stats of the left child,0
grad stats of the right child,0
"LOG.info(""Constructor with fid = -1"");",0
fid = -1: no split currently,0
"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,",0
ridx),0
"same as add, reduce is used in All Reduce",0
maximum depth of the tree,0
number of features used for tree construction,0
"minimum loss change required for a split, otherwise stop split",0
----- the rest parameters are less important ----,0
default direction choice,0
whether we want to do sample data,0
whether to sample columns during tree construction,0
whether to use histogram for split,0
number of histogram units,0
whether to print info during training.,0
----- the rest parameters are obtained after training ----,0
total number of nodes,0
number of deleted nodes */,0
gradient,0
second order gradient,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
"LOG.info(String.format(""Histogram of feature[%d]: %s"", trueFid,",0
Arrays.toString(curHistogram.getValues())));,0
2.3. find the best split of current feature,0
"LOG.info(String.format(""Best split of feature[%d]: value[%f], gain[%f]"",",0
"trueFid, curSplit.getFvalue(), curSplit.getLossChg()));",0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
"LOG.info(String.format(""Find the best split for fid[%d] in server row, size[%d], startIdx[%d]"",",0
"fid, row.size(), startIdx));",0
StringBuilder sb = new StringBuilder();,0
for (int i = startIdx; i < startIdx + 2 * splitNum; i++) {,0
"sb.append(row.getData().get(i) + "", "");",0
},0
"LOG.info(""Server row: "" + sb.toString());",0
1. set the feature id,0
splitEntry.setFid(fid);,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
"LOG.info(String.format(""Feature[%d]: sumGrad[%f], sumHess[%f], gain[%f]"",",0
"fid, rootStats.sumGrad, rootStats.sumHess, rootGain));",0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"LOG.info(String.format(""The current split: fid[%d], split index[%f], lossChg[%f]"",",0
"fid, (float) splitIdx, lossChg));",0
"here we set the fvalue=splitIndex, split value = sketches[splitIdx+1], the task use",0
index to find fvalue,0
"LOG.info(String.format(""Find new best split: fid[%d], fvalue[%f], lossChg[%f]"",",0
"splitEntry.fid, splitEntry.fvalue, splitEntry.lossChg));",0
"LOG.info(String.format(""Left child of node[%d]: sumGrad[%f], sumHess[%f]; "" +",0
"""right child of node[%d]: sumGrad[%f], sumHess[%f]; lossChg[%f]"",",0
"this.nid, leftStats.sumGrad, leftStats.sumHess,",0
"this.nid, rightStats.sumGrad, rightStats.sumHess, lossChg));",0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
"LOG.info(String.format(""The best split for fid[%d], split feature[%d]: split index[%f], lossChg[%f], """,0
+,0
"""leftSumGrad[%f], leftSumHess[%f], rightSumGrad[%f], rightSumHess[%f]"",",0
"fid, splitEntry.fid, splitEntry.fvalue, splitEntry.lossChg,",0
"splitEntry.leftGradStat.sumGrad, splitEntry.leftGradStat.sumHess,",0
"splitEntry.rightGradStat.sumGrad, splitEntry.rightGradStat.sumHess));",0
grad.timesBy(-1.0 * lr);,0
System.out.println(Arrays.toString(distArr));,0
"System.out.println(String.format(""Epoch[%d] batch[%d], loss[%f]"", epoch, batch, loss));",0
"System.out.println(""Start calculate loss and auc, sample number: "" + totalNum);",0
"System.out.println(""Sort cost "" + (System.currentTimeMillis() - sortStartTime) + ""ms, Scores list size: """,0
"+ scoresArray.length + "", sorted values:"" + scoresArray[0] + "",""",0
"+ scoresArray[scoresArray.length / 5] + "","" + scoresArray[scoresArray.length / 3] + "",""",0
"+ scoresArray[scoresArray.length / 2] + "","" + scoresArray[scoresArray.length - 1]);",0
"System.out.println(""M = "" + M + "", N = "" + N + "", sigma = "" + sigma + "", AUC = "" + aucResult);",0
"System.out.println(String.format(""Validation TP=%d, TN=%d, FP=%d, FN=%d"", truePos, trueNeg, falsePos,falseNeg));",0
"System.out.println(input + "" | "" + hashFunc.encode(input));",0
step size for a tree,0
number of class,0
minimum loss change required for a split,0
maximum depth of a tree,0
number of features,0
number of nonzero,0
number of candidates split value,0
----- the rest parameters are less important ----,0
base instance weight,0
minimum amount of hessian(weight) allowed in a child,0
L2 regularization factor,0
L1 regularization factor,0
default direction choice,0
maximum delta update we can add in weight estimation,0
this parameter can be used to stabilize update,0
default=0 means no constraint on weight delta,0
whether we want to do subsample for row,0
whether to subsample columns for each tree,0
accuracy of sketch,0
accuracy of sketch,0
leaf vector size,0
option for parallelization,0
option to open cacheline optimization,0
whether to not print info during training.,0
return MathUtils.sqr(sumGrad) / (sumHess + regLambda);,0
"get feature type, 0:empty 1:all equal 2:real",0
"quantile sketch, size = featureNum * splitNum",0
"gradient histograms, size = treeNodeNum * featureNum * splitNum",0
"active tree nodes, size = pow(2, treeDepth) -1",0
sampled features. size = treeNum * sampleRatio * featureNum,0
"split features, size = treeNum * treeNodeNum",0
"split values, size = treeNum * treeNodeNum",0
"split gains, size = treeNum * treeNodeNum",0
"node weights, size = treeNum * treeNodeNum",0
"node preds, size = treeNum * treeNodeNum",0
if using PS to perform split,0
maximum depth of the tree,0
number of features used for tree construction,0
"minimum loss change required for a split, otherwise stop split",0
----- the rest parameters are less important ----,0
default direction choice,0
whether we want to do sample data,0
whether to sample columns during tree construction,0
whether to use histogram for split,0
number of histogram units,0
whether to print info during training.,0
----- the rest parameters are obtained after training ----,0
total number of nodes,0
number of deleted nodes */,0
set basic configuration keys,0
use local deploy mode and dummy data spliter,0
get a angel client,0
add matrix,0
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
TODO Auto-generated constructor stub,1
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
"set input, output path",0
"set angel resource parameters #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Load Model from HDFS.,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"Set trainning data, and save model path",0
Set actionType train,0
Set MF algorithm parameters,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set input data path,0
Set save model path,0
Set actionType train,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
worker register,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
attempt 0,0
attempt1,0
attempt1,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
set basic configuration keys,0
"conf.set(AngelConfiguration.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());",0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
Thread.sleep(5000);,0
"response = master.getJobReport(null, request);",0
"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);",0
"assertEquals(response.getJobReport().getCurIteration(), jobIteration);",0
"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
"DenseIntVector deltaVec = new DenseIntVector(100000, delta);",0
deltaVec.setMatrixId(matrixW1Id);,0
deltaVec.setRowId(0);,0
TODO Auto-generated constructor stub,1
import com.tencent.angel.psagent.consistency.SSPConsistencyController;,0
@RunWith(MockitoJUnitRunner.class),0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConfiguration.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);",0
get a angel client,0
add matrix,0
psAgent.initAndStart();,0
test conf,0
test master location,0
test app id,0
test user,0
test ps agent attempt id,0
test ps agent id,0
test connection,0
test master client,0
test ip,0
test loc,0
test master location,0
test ps location,0
"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));",0
test all ps ids,0
test all matrix ids,0
test all matrix names,0
test matrix attribute,0
test matrix meta,0
test ps location,0
test partitions,0
"Note:[startRow,endRow)",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
group0Id = new WorkerGroupId(0);,0
"worker0Id = new WorkerId(group0Id, 0);",0
"worker0Attempt0Id = new WorkerAttemptId(worker0Id, 0);",0
task0Id = new TaskId(0);,0
task1Id = new TaskId(1);,0
LOG.info(serverArbitraryIntRow1.getSparseRep());,0
LOG.info(serverArbitraryIntRow1.getSparseRep());,0
test this func in testWriteTo,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
"LOG.info(index[0] + "" "" + value[0]);",0
"LOG.info(index[1] + "" "" + value[1]);",0
"LOG.info(index[2] + "" "" + value[2]);",0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
dot,0
plusBy,0
@Test,0
public void dotDenseFloatVector() throws Exception {,0
int dim = 1000;,0
Random random = new Random(System.currentTimeMillis());,0
,0
double[] values = new double[dim];,0
float[] values_1 = new float[dim];,0
for (int i = 0; i < dim; i++) {,0
values[i] = random.nextDouble();,0
values_1[i] = random.nextFloat();,0
},0
,0
"DenseDoubleVector vec = new DenseDoubleVector(dim, values);",0
"TDoubleVector vec_1 = new DenseFloatVector(dim, values_1);",0
,0
double sum = 0.0;,0
for (int i = 0; i < dim; i++) {,0
sum += values[i] * values_1[i];,0
},0
,0
"assertEquals(sum, vec.dot(vec_1));",0
,0
},0
@Test,0
public void plusDenseFlaotVector() throws Exception {,0
"double[] value_1 = new double[]{0.1, 0.2, 0.3, 0.4, 0.5};",0
"double[] value_2 = new double[]{0.1f, 0.2f, 0.3f, 0.4f, 0.5f};",0
"DenseDoubleVector vec = new DenseDoubleVector(5, value_1);",0
"TDoubleVector vec_1 = new DenseFloatVector(5, value_2);",0
,0
TDoubleVector vec_2 = vec.plus(vec_1);,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(value_1[i] + value_2[i], vec_2.get(i));",0
,0
,0
"TDoubleVector vec_3 = vec.plus(vec_1, 2.0);",0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec_3.get(i), value_1[i] + 2 * value_2[i]);",0
,0
double[] oldValues = vec.getValues().clone();,0
,0
vec.plusBy(vec_1);,0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec.get(i), oldValues[i] + vec_1.get(i));",0
,0
oldValues = vec.getValues().clone();,0
,0
"vec.plusBy(vec_1, 3);",0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec.get(i), oldValues[i] + 3 * vec_1.get(i));",0
},0
@Test,0
public void plusByArrayTest() {,0
DenseFloatVector vec = new DenseFloatVector(dim);,0
"int[] index = genSortedIndexs(nnz, dim);",0
float[] deltF = genFloatArray(nnz);,0
double[] deltD = genDoubleArray(nnz);,0
,0
float[] oldVal = vec.getValues().clone();,0
"vec.plusBy(index, deltF);",0
for (int i = 0; i < nnz; i++) {,0
int idx = index[i];,0
"assertEquals(oldVal[idx] + deltF[i], vec.get(idx));",0
},0
,0
oldVal = vec.getValues().clone();,0
"vec.plusBy(index, deltD);",0
for (int i = 0; i < nnz; i++) {,0
int idx = index[i];,0
"assertEquals(oldVal[idx] + (float) deltD[i], vec.get(idx));",0
},0
,0
oldVal = vec.getValues().clone();,0
"vec.plusBy(index, deltF);",0
for (int i = 0; i < nnz; i++) {,0
int idx = index[i];,0
"assertEquals(oldVal[idx] + deltF[i], vec.get(idx));",0
},0
,0
oldVal = vec.getValues().clone();,0
"vec.plusBy(index, deltD);",0
for (int i = 0; i < nnz; i++) {,0
int idx = index[i];,0
"assertEquals(oldVal[idx] + (float) deltD[i], vec.get(idx));",0
},0
},0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
@Test,0
public void plusBy3() throws Exception {,0
"float[][] value = {{1.0f, 2.0f}, {3.0f, 4.0f}};",0
"DenseFloatMatrix mat = new DenseFloatMatrix(2, 2,value);",0
"TFloatVector vec = new DenseFloatVector(2, new float[]{1.0f, 1.0f});",0
vec.setRowId(0);,0
"TDoubleVector vec_1 = new DenseDoubleVector(2, new double[]{1.0f, 1.0f});",0
vec_1.setRowId(1);,0
TDoubleVector vec_2 = new SparseDoubleVector(2);,0
"vec_2.set(1, 1.0);",0
vec_2.setRowId(0);,0
,0
mat.plusBy(vec);,0
mat.plusBy(vec_1);,0
mat.plusBy(vec_2);,0
,0
"assertEquals(2.0f, mat.get(0, 0));",0
"assertEquals(4.0f, mat.get(0, 1));",0
"assertEquals(4.0f, mat.get(1, 0));",0
"assertEquals(5.0f, mat.get(1, 1));",0
},0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
test worker getActiveTaskNum,0
test worker getTaskNum,0
test worker getTaskManager,0
test workerId,0
test workerAttemptId,0
tet worker initFinished,0
test worker getInitMinclock,0
test worker loacation,0
test AppId,0
test Conf,0
test UserName,0
master location,0
masterClient,0
test psAgent,0
test worker get dataBlockManager,0
workerGroup.getSplits();,0
application,0
lcation,0
workerGroup info,0
worker info,0
task,0
Matrix parameters,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
Create an Angel client,0
Add different types of matrix,0
using mock object,0
verification,0
Stubbing,0
Default does nothing.,0
The app injection is optional,0
"renderText(""hello world"");",0
"user choose a workerGroupID from the workergroups page,",0
now we should change the AngelApp params and render the workergroup page;,0
"static final String WORKER_ID = ""worker.id"";",0
"div(""#logo"").",0
"img(""/static/hadoop-st.png"")._().",0
import org.apache.hadoop.yarn.webapp.view.FooterBlock;,0
import org.apache.hadoop.yarn.webapp.view.HeaderBlock;,0
JQueryUI.jsnotice(html);,0
import org.apache.hadoop.conf.Configuration;,0
import java.lang.reflect.Field;,0
get block locations from file system,0
create a list of all block and their locations,0
"if the file is not splitable, just create the one block with",0
full file length,0
each split can be a maximum of maxSize,0
if remainder is between max and 2*max - then,0
"instead of creating splits of size max, left-max we",0
create splits of size left/2 and left/2. This is,0
a heuristic to avoid creating really really small,0
splits.,0
add this block to the block --> node locations map,0
"For blocks that do not have host/rack information,",0
assign to default  rack.,0
add this block to the rack --> block map,0
Add this host to rackToNodes map,0
add this block to the node --> block map,0
"if the file system does not have any rack information, then",0
use dummy rack location.,0
The topology paths have the host name included as the last,0
component. Strip it.,0
get tokens for all the required FileSystems..,0
"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,",0
job.getConfiguration());,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
all the files in input set,0
it is allowed for maxSize to be 0. Disable smoothing load for such cases,0
process all nodes and create splits that are local to a node. Generate,0
"one split per node iteration, and walk over nodes multiple times to",0
distribute the splits across nodes.,0
Skip the node if it has previously been marked as completed.,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
Remove all blocks which may already have been assigned to other,0
splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
Remove entries from blocksInNode so that we don't walk these,0
again.,0
Done creating a single split for this node. Move on to the next,0
node so that splits are distributed across nodes.,0
This implies that the last few blocks (or all in case maxSize=0),0
were not part of a split. The node is complete.,0
if there were any blocks left over and their combined size is,0
"larger than minSplitNode, then combine them into one split.",0
Otherwise add them back to the unprocessed pool. It is likely,0
that they will be combined with other blocks from the,0
same rack later on.,0
This condition also kicks in when max split size is not set. All,0
blocks on a node will be grouped together into a single split.,0
haven't created any split on this machine. so its ok to add a,0
smaller one for parallelism. Otherwise group it in the rack for,0
balanced size create an input split and add it to the splits,0
array,0
Remove entries from blocksInNode so that we don't walk this again.,0
The node is done. This was the last set of blocks for this node.,0
Put the unplaced blocks back into the pool for later rack-allocation.,0
Node is done. All blocks were fit into node-local splits.,0
Check if node-local assignments are complete.,0
All nodes have been walked over and marked as completed or all blocks,0
have been assigned. The rest should be handled via rackLock assignment.,0
"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """,0
"+ completedNodes.size() + "", size left: "" + totalLength);",0
"if blocks in a rack are below the specified minimum size, then keep them",0
"in 'overflow'. After the processing of all racks is complete, these",0
overflow blocks will be combined into splits.,0
Process all racks over and over again until there is no more work to do.,0
Create one split for this rack before moving over to the next rack.,0
Come back to this rack after creating a single split for each of the,0
remaining racks.,0
"Process one rack location at a time, Combine all possible blocks that",0
reside on this rack as one split. (constrained by minimum and maximum,0
split size).,0
iterate over all racks,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"if we created a split, then just go to the next rack",0
"if there is a minimum size specified, then create a single split",0
"otherwise, store these blocks into overflow data structure",0
There were a few blocks in this rack that,0
remained to be processed. Keep them in 'overflow' block list.,0
These will be combined later.,0
Process all overflow blocks,0
"This might cause an exiting rack location to be re-added,",0
but it should be ok.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"Process any remaining blocks, if any.",0
create an input split,0
add this split to the list that is returned,0
long num = totLength / maxSize;,0
all blocks for all the files in input set,0
mapping from a rack name to the list of blocks it has,0
mapping from a block to the nodes on which it has replicas,0
mapping from a node to the list of blocks that it contains,0
populate all the blocks for all files,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
"Security framework already loaded the tokens into current UGI, just use",0
them,0
Now remove the AM->RM token so tasks don't have it,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init container allocator,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
init parameter server manager,0
recover task information if needed,0
init psagent manager and register psagent manager event,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
start a web service if use yarn deploy mode,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
parse parameter server counters,0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
refresh last heartbeat timestamp,0
send a state update event to the specific PSAttempt,0
check if parameter server can commit now.,0
check matrix metadata inconsistencies between master and parameter server.,0
"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix",0
"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix",0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
check whether psagent heartbeat timeout,0
check whether parameter server heartbeat timeout,0
check whether worker heartbeat timeout,0
choose a unused port,0
start RPC server,0
find matrix partitions from master matrix meta manager for this parameter server,0
remove this parameter server attempt from monitor set,0
remove this parameter server attempt from monitor set,0
"if worker attempt id is not in monitor set, we should shutdown it",0
find workergroup in worker manager,0
"if this worker group does not initialized, just return WORKERGROUP_NOTREADY",0
"if this worker group run over, just return WORKERGROUP_EXITED",0
"if this worker group is running now, return tasks, workers, data splits for it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update the clock for this matrix,0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update task iteration,0
private boolean matrixInited;,0
"check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
dispatch matrix partitions to parameter servers,0
update matrix id generator,0
"check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
dispatch matrix partitions to parameter servers,0
get matrix ids in the parameter server report,0
get the matrices parameter server need to create and delete,0
"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix",0
"if a matrix exists on master but not exist on parameter server, this parameter server need build it.",0
waitForMatrixReleaseOnPS(matrixId);,0
"Calculate how many splits we need. As each task handles a separate split of data, so we want",0
the number of splits equal to the number of tasks,0
split data,0
dispatch the splits to workergroups,0
split data,0
dispatch the splits to workergroups,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
write meta data to a temporary file,0
rename the temporary file to final file,0
"if the file exists, read from file and deserialize it",0
write task meta,0
write ps meta,0
generate a temporary file,0
write task meta to the temporary file first,0
rename the temporary file to the final file,0
"if last final task file exist, remove it",0
find task meta file which has max timestamp,0
"if the file does not exist, just return null",0
read task meta from file and deserialize it,0
generate a temporary file,0
write ps meta to the temporary file first.,0
rename the temporary file to the final file,0
"if the old final file exist, just remove it",0
find ps meta file,0
"if ps meta file does not exist, just return null",0
read ps meta from file and deserialize it,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
Transitions from the NEW state.,0
PA_FAILMSG,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG",0
event,0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event",0
dispatched before PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will",0
retry another attempt or failed,0
release container,0
TODO,1
set the launch time,0
"set tarckerName,httpPort, which used by webserver",0
added to psManager so psManager can monitor it;,0
psAttempt.getContext().getParameterServerManager.registerPSAttempt(psAttempt.attemptId);,0
set the finish time only if launch time is set,0
"ParameterServerJVM.setVMEnv(myEnv, conf);",0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Add the env variables passed by the user,0
Set logging level in the environment.,0
"This is so that, if the child forks another ""bin/hadoop"" (common in",0
streaming) it will have the correct loglevel.,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
Transitions from the NEW state.,0
Transitions from the RUNNING state.,0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
LocalStorageToken is needed irrespective of whether security is enabled,0
or not.,0
"TokenCache.setJobToken(jobToken, taskCredentials);",0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Construct the actual Container,0
The null fields are per-container and will be constructed for each,0
container separately.,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
"a * in the classpath will only find a .jar, so we need to filter out",0
all .jars and add everything else,0
Propagate the system classpath when using the mini cluster,0
Add standard Hadoop classes,0
Cache archives,0
Cache files,0
Sanity check,0
Add URI fragment or just the filename,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Final commmand,0
"if amTask is not null, we should clone task state from it",0
"if all parameter server complete commit, master can commit now",0
init and start master committer,0
Transitions from the NEW state.,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event",0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event dispatched before",0
PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.COMMITTING state,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
reqeuest resource:send a resource request to the resource allocator,0
"Once the resource is applied, build and send the launch request to the container launcher",0
deallocator the resource of the ps attempt:send a resource deallocator request to the,0
resource allocator,0
set the launch time,0
add the ps attempt to the heartbeat timeout monitoring list,0
parse ps attempt location and put it to location manager,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt",0
or failed,0
remove ps attempt id from heartbeat timeout monitor list,0
release container:send a release request to container launcher,0
set the finish time only if launch time is set,0
private long scheduledTime;,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
notify the event handler of state change,0
"If new state is not RUNNING and COMMITTING, add it to state timeout monitor",0
"if forcedState is set, just return",0
else get state from state machine,0
add this worker group to the success set,0
check if all worker group run over,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
calculate the actual number of worker groups and the total number of tasks based on the number of data split,0
"init all tasks , workers and worker groups and put them to the corresponding maps",0
just return the total task number now,0
TODO,1
"if workerAttempt is not null, we should clone task state from it",0
from NEW state,0
from SCHEDULED state,0
get data splits location for data locality,0
reqeuest resource:send a resource request to the resource allocator,0
"once the resource is applied, build and send the launch request to the container launcher",0
notify failed message to the worker,0
notify killed message to the worker,0
release the allocated container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
release the allocated container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
clean the container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
record the finish time,0
clean the container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listening list,0
record the finish time,0
"if the worker attempt launch successfully, add it to heartbeat timeout listening list",0
set worker attempt location,0
notify the register message to the worker,0
record the launch time,0
update worker attempt metrics,0
update tasks metrics,0
clean the container,0
notify the worker attempt run successfully message to the worker,0
record the finish time,0
init a worker attempt for the worker,0
schedule the worker attempt,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
"If we need Yarn to restart a new application master, we should not unregister from Yarn RM",0
register to Yarn RM,0
send heartbeat to Yarn RM every rmPollInterval milliseconds,0
"catch YarnRuntimeException, we should exit and need not retry",0
build heartbeat request,0
send heartbeat request to rm,0
"This can happen if the RM has been restarted. If it is in that state,",0
this application must clean itself up.,0
Setting NMTokens,0
assgin containers,0
"if some container is not assigned, release them",0
handle finish containers,0
dispatch container exit message to corresponding components,0
killed by framework,0
killed by framework,0
killed by framework,0
get application finish state,0
build application diagnostics,0
TODO:add a job history for angel,1
build unregister request,0
send unregister request to rm,0
Note this down for next interaction with ResourceManager,0
based on blacklisting comments above we can end up decrementing more,0
than requested. so guard for that.,0
send the updated resource request to RM,0
send 0 container count requests also to cancel previous requests,0
Update resource requests,0
try to assign to all nodes first to match node local,0
try to match all rack local,0
assign remaining,0
Update resource requests,0
send the container-assigned event to task attempt,0
build the start container request use launch context,0
send the start request to Yarn nm,0
send the message that the container starts successfully to the corresponding component,0
"after launching, send launched event to task attempt to move",0
it from ASSIGNED to RUNNING state,0
send the message that the container starts failed to the corresponding component,0
kill the remote container if already launched,0
start a thread pool to startup the container,0
See if we need up the pool size only if haven't reached the,0
maximum limit yet.,0
nodes where containers will run at *this* point of time. This is,0
*not* the cluster size and doesn't need to be.,0
"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the",0
later is just a buffer so we are not always increasing the,0
pool-size,0
the events from the queue are handled in parallel,0
using a thread pool,0
return if already stopped,0
shutdown any containers that might be left running,0
Build and initialize rpc client to master,0
Build local location,0
"Initialize matrix info, this method will wait until master accepts the information from",0
client,0
Get ps locations from master and put them to the location cache.,0
Initialize matrix meta information,0
Start heartbeat thread if need,0
Start all services,0
Register to master first,0
Report state to master every specified time,0
Notify run success to master only on ANGEL_PS_PSAGENT running mode,0
Stop all modules,0
Exit the process if on ANGEL_PS_PSAGENT mode,0
Notify run success to master only on ANGEL_PS_PSAGENT running mode,0
Stop all modules,0
Exit the process if on ANGEL_PS_PSAGENT mode,0
get configuration from config file,0
set localDir with enviroment set by nm.,0
Update generic resource counters,0
Updating resources specified in ResourceCalculatorProcessTree,0
Remove the CPU time consumed previously by JVM reuse,0
array stores clock for each row and clock,0
local task num,0
mapping from task index to taskId,0
mapping from taskId to task index,0
TODO Auto-generated method stub,1
Generate a flush request and put it to request queue,0
Generate a clock request and put it to request queue,0
Generate a merge request and put it to request queue,0
"If the matrix op log cache does not exist for the matrix, create a new one for the",0
matrix,0
and add it to cache maps,0
Add the message to the tree map,0
"If there are flush / clock requests blocked, we need to put this merge request into",0
the waiting queue,0
Launch a merge worker to merge the update to matrix op log cache,0
Remove the message from the tree map,0
Wake up blocked flush/clock request,0
Add flush/clock request to listener list to waiting for all the existing,0
updates are merged,0
Wake up blocked flush/clock request,0
"If all updates are merged for this matrix, we need wake up flush/clock requests which are",0
blocked.,0
Get next merge message sequence id,0
Wake up listeners(flush/clock requests) that have little sequence id than current merge,0
position,0
Wake up blocked merge requests,0
Get minimal sequence id from listeners,0
"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we",0
should flush updates to local matrix storage,0
unused now,1
Get partitions for the matrix,0
"Filter it, removing zero values",0
Doing average or not,0
Split this row according the matrix partitions,0
Add the splits to the result container,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
Then submit normal task until reach upper limit of flow control or all tasks are submit,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"LOG.info(""choose put server "" + psIds[index]);",0
allocate the bytebuf,0
"check the location of server is ready, if not, we should wait",0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
channelManager.removeChannelPool(loc);,0
find the partition request context from cache,0
Check if the result of the sub-request is received,0
Update received result number,0
Get row splits received,0
Put the row split to the cache(row index to row splits map),0
"If all splits of the row are received, means this row can be merged",0
TODO Auto-generated method stub,1
"Check futures, if the result of a sub-request is received, put it to the result queue",0
Now we just support pipelined row splits merging for dense type row,0
Wait until the clock value of this row is greater than or equal to the value,0
Get partitions for this row,0
First get this row from matrix storage,0
"If the row exists in the matrix storage and the clock value meets the requirements, just",0
return,0
Get row splits of this row from the matrix cache first,0
"If the row split does not exist in cache, get it from parameter server",0
Wait the final result,0
Put it to the matrix cache,0
Split the matrix oplog according to the matrix partitions,0
"If need update clock, we should send requests to all partitions",0
use update index if exist,0
Filter the rowIds which are fetching now,0
Send the rowIndex to rpc dispatcher and return immediately,0
Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition,0
Generate dispatch items and add them to the corresponding queues,0
Pre-fetching is disable default,0
matrix id to clock map,0
"task index, it must be unique for whole application",0
matrix id to task update index map. each task may only update some specific part for a matrix,0
if (matrixManager.getMatrixMeta(createResponse.getMatrixId()) != null) {,0
return matrixManager.getMatrixMeta(createResponse.getMatrixId());,0
},0
Deserialize data splits meta,0
Get workers,0
Send request to every ps,0
Wait the responses,0
Update clock cache,0
Get row from cache.,0
"if row clock is satisfy ssp staleness limit, just return.",0
Get row from ps.,0
"For ASYNC mode, just get from pss.",0
"For BSP/SSP, get rows from storage/cache first",0
Get from ps.,0
"For ASYNC, just get rows from pss.",0
no more retries.,0
calculate sleep time and return.,0
parse the i-th sleep-time,0
parse the i-th number-of-retries,0
calculateSleepTime may overflow.,0
"A few common retry policies, with no delays.",0
close is a local operation and should finish within milliseconds; timeout just to be safe,0
response will be null for one way messages.,0
maxFrameLength = 2G,0
lengthFieldOffset = 0,0
lengthFieldLength = 8,0
"lengthAdjustment = -8, i.e. exclude the 8 byte length itself",0
"initialBytesToStrip = 8, i.e. strip out the length field itself",0
indicates whether this connection's life cycle is managed,0
See if we already have a connection (common case),0
create a unique lock for this RS + protocol (if necessary),0
get the RS lock,0
do one more lookup in case we were stalled above,0
Only create isa when we need to.,0
definitely a cache miss. establish an RPC for,0
this RS,0
Throw what the RemoteException was carrying.,0
check,0
every,0
minutes,0
TODO,1
failoverHandler,0
"The number of times this invocation handler has ever been failed over,",0
before this method invocation attempt. Used to prevent concurrent,0
failed method invocations from triggering multiple failover attempts.,0
Make sure that concurrent failed method invocations,0
only cause a,0
single actual fail over.,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
"LOG.info(""method "" + method.getName() + ""construct request time = """,0
+ (System.currentTimeMillis() - beforeConstructTs));,0
get an instance of the method arg type,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
Message (hand written code usually has only a single,0
argument),0
log any RPC responses that are slower than the configured,0
warn,0
response time or larger than configured warning size,0
"when tagging, we let TooLarge trump TooSmall to keep",0
output simple,0
note that large responses will often also be slow.,0
provides a count of log-reported slow responses,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
unexpected,0
"in the protobuf methods, args[1] is the only significant argument",0
for JSON encoding,0
base information that is reported regardless of type of call,0
Disable Nagle's Algorithm since we don't want packets to wait,0
Configure the event pipeline factory.,0
Make a new connection.,0
Remove all pending requests (will be canceled after relinquishing,0
write lock).,0
Cancel any pending requests by sending errors to the callbacks:,0
Close the channel:,0
Close the connection:,0
Shut down all thread pools to exit.,0
"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());",0
See NettyServer.prepareResponse for where we write out the response.,0
"It writes the call.id (int), a boolean signifying any error (and if",0
"so the exception name/trace), and the response bytes",0
Read the call id.,0
"When the stream is closed, protobuf doesn't raise an EOFException,",0
"instead, it returns a null message object.",0
"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +",0
System.currentTimeMillis());,0
"It would be good widen this to just Throwable, but IOException is what we",0
allow now,0
not implemented,1
not implemented,1
"track what RpcEngine is used by a proxy class, for stopProxy()",0
cache of RpcEngines by protocol,0
return the RpcEngine configured to handle a protocol,0
We only handle the ConnectException.,0
This is the exception we can't handle.,0
check if timed out,0
wait for retry,0
IGNORE,0
return the RpcEngine that handles a proxy object,0
The default implementation works synchronously,0
punt: allocate a new buffer & copy into it,0
"LOG.info(System.getProperty(""user.dir""));",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
a simple hdfs copy function assume src path and dest path are in same hdfs,0
and FileSystem object has same schema,0
"LOG.warn(""interrupted while sleeping"", ie);",0
private static UnpooledByteBufAllocator allocator = new UnpooledByteBufAllocator(false);,0
public static String getHostname() {,0
try {,0
"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();",0
} catch (UnknownHostException uhe) {,0
},0
"return new StringBuilder().append("""").append(uhe).toString();",0
},0
"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {",0
String hostname = getHostname();,0
String classname = clazz.getSimpleName();,0
"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new",0
"StringBuilder().append(""Starting "").append(classname).toString(), new",0
"StringBuilder().append(""  host = "").append(hostname).toString(), new",0
"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new",0
"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new",0
"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));",0
,0
"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {",0
public void run() {,0
"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +",0
"this.val$classname + "" at "" + this.val$hostname}));",0
},0
});,0
},0
"We we interrupted because we're meant to stop? If not, just",0
continue ignoring the interruption,0
Recalculate waitTime.,0
// Begin delegation to Thread,0
// End delegation to Thread,0
load angel system configuration,0
load user configuration:,0
1. user config file,0
2. command lines,0
"add user resource files to ""angel.lib.jars"" to upload to hdfs",0
load user job jar if it exist,0
Expand the environment variable,0
instance submitter class,0
Add default fs(local fs) for lib jars.,0
Obtain filename from path,0
Split filename to prexif and suffix (extension),0
Check if the filename is okay,0
Prepare temporary file,0
Prepare buffer for data copying,0
Open and check input stream,0
Open output stream and copy data between source file in JAR and the temporary file,0
"If read/write fails, close streams safely before throwing an exception",0
"Finally, load the library",0
little endian load order,0
tail,0
fallthrough,0
fallthrough,0
finalization,0
fmix(h1);,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
JobStateProto jobState = report.getJobState();,0
the leaf level file should be readable by others,0
the subdirs in the path should have execute permissions for,0
others,0
2.get job id,0
Credentials credentials = new Credentials();,0
4.copy resource files to hdfs,0
5.write configuration to a xml file,0
6.create am container context,0
7.Submit to ResourceManager,0
8.get app master client,0
Create a number of filenames in the JobTracker's fs namespace,0
add all the command line files/ jars and archive,0
first copy them to jobtrackers filesystem,0
should not throw a uri exception,0
should not throw an uri excpetion,0
set the timestamps of the archives and files,0
set the public/private visibility of the archives and files,0
get DelegationToken for each cached file,0
check if we do not need to copy the files,0
is jt using the same file system.,0
just checking for uri strings... doing no dns lookups,0
to see if the filesystems are the same. This is not optimal.,0
but avoids name resolution.,0
this might have name collisions. copy will throw an exception,0
parse the original path to create new path,0
check for ports,0
Write job file to JobTracker's fs,0
Setup resource requirements,0
Setup LocalResources,0
Setup security tokens,0
Setup the command to run the AM,0
Add AM user command opts,0
Final command,0
Setup the CLASSPATH in environment,0
"i.e. add { Hadoop jars, job jar, CWD } to classpath.",0
Setup the environment variables for Admin first,0
"Setup the environment variables (LD_LIBRARY_PATH, etc)",0
Parse distributed cache,0
Setup ContainerLaunchContext for AM container,0
Set up the ApplicationSubmissionContext,0
resposne.encode(buf);,0
TODO:,1
resposne.encode(buf);,0
TODO:,1
resposne.encode(buf);,0
TODO:,1
resposne.encode(buf);,0
TODO:,1
private final ParameterServerId serverId;,0
private final PSIdProto idProto;,0
Add tokens to new user so that it may execute its task correctly.,0
to exit,0
mkdir does not throw exception if path exits,0
commitTaskPool.shutdown();,0
private final ParameterServer psServer;,0
TODO,1
"when we should write snapshot to hdfs? clearly, we have two methods:",0
"1. write snapshot at regular time, if there are updates, just write them.",0
"2. write snapshot every N iterations, this method depends on notification of master",0
"FSDataOutputStream output = fileContext.create(snapshotsTempFilePath,",0
EnumSet.of(CreateFlag.CREATE));,0
@brief get filename of the old snapshot written before,0
"no snapshotFile write before, maybe write snapshots the first time",0
start end,0
rowtype,0
data.rewind();,0
data.rewind();,0
data.rewind();,0
Pass the matrix and partition number field,0
Mapping from taskId to clock value.,0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
nnz = 0;,0
for (int i = 0; i < keys.length; i++),0
if (used[i]) {,0
"denseRep.put(keys[i], values[i]);",0
nnz++;,0
},0
sparseRep = null;,0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
for (int i = 0; i < keys.length; i++),0
if (used[i]) {,0
"denseRep.put(keys[i], values[i]);",0
},0
sparseRep = null;,0
output.writeInt(data.length);,0
@Override,0
public void serialize(ByteBuf buf) {,0
if (sparseRep != null),0
return serializeSparse();,0
else if (denseRep != null),0
return serializeDense();,0
return serializeEmpty();,0
},0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
int idx = 0;,0
for (int i = 0; i < keys.length; i++),0
if (used[i]) {,0
"keysBuf.put(idx, keys[i]);",0
"valuesBuf.put(idx, values[i]);",0
idx++;,0
},0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
"int ov, k, v;",0
for (int i = 0; i < keys.length; i++) {,0
if (used[i]) {,0
k = keys[i];,0
ov = denseRep.get(k);,0
v = ov + values[i];,0
"denseRep.put(k, v);",0
if (ov != 0 && v == 0),0
nnz--;,0
},0
},0
"add the PSAgentContext,need fix",0
set MatrixPartitionLocation,0
set attribute,0
TODO Auto-generated method stub,1
@brief Sorted index for non-zero items,0
@brief Number of non-zero items in this vector,0
@brief Array to store values.,0
@brief sum of the square of all of element,0
"LOG.error(""Cannot perform plus operation on SparseDoubleSortedVector"");",0
TODO Auto-generated method stub,1
TODO:,1
TODO:,1
write the max abs,0
"Thread.currentThread().getContextClassLoader().getResourceAsStream(""feature_conf.xml"");",0
this.matchList = new ArrayList<Match>();,0
"LOG.debug(String.format(""index: %d, value: %s"", index, value));",0
"LOG.debug(String.format(""index: %d, value: %s"", index, value));",0
"LOG.debug(String.format(""index: %d, value: %s"", index, value));",0
"LOG.debug(String.format(""index: %d, value: %s"", index, value));",0
"LOG.debug(""target="" + target);",0
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
get configuration from config file,0
set localDir with enviroment set by nm.,0
get master location,0
Add tokens to new user so that it may execute its task correctly.,0
init task manager and start tasks,0
start heartbeat thread,0
taskManager.assignTaskIds(response.getTaskidsList());,0
todo,1
"if worker timeout, it may be knocked off.",0
"SUCCESS, do nothing",0
heartbeatFailedTime = 0;,0
private KEY currentKey;,0
will be created,0
TODO Auto-generated method stub,1
Bitmap bitmap = new Bitmap();,0
int max = indexArray[size - 1];,0
byte [] bitIndexArray = new byte[max / 8 + 1];,0
for(int i = 0; i < size; i++){,0
int bitIndex = indexArray[i] >> 3;,0
int bitOffset = indexArray[i] - (bitIndex << 3);,0
switch(bitOffset){,0
case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;,0
case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;,0
case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;,0
case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;,0
case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;,0
case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;,0
case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;,0
case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;,0
},0
},0
//////////////////////////////,0
Application Configs,0
//////////////////////////////,0
//////////////////////////////,0
Master Configs,0
//////////////////////////////,0
//////////////////////////////,0
Worker Configs,0
//////////////////////////////,0
//////////////////////////////,0
Task Configs,0
//////////////////////////////,0
//////////////////////////////,0
ParameterServer Configs,0
//////////////////////////////,0
////////////////// IPC //////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
Configs used to ANGEL_PS_PSAGENT running mode future.,0
model parse,0
private Configuration conf;,0
TODO Auto-generated constructor stub,1
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
"set input, output path",0
"set angel resource parameters #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Submit GBDT Train Task,0
Load Model from HDFS.,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"Set trainning data, save model, log path",0
Set actionType train,0
Set MF algorithm parameters,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
LOG.info(sigmoid(data[i]));,0
LOG.info(Math.exp(-data[i]));,0
when b is a negative number,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Set trainning data path,0
Set data format,0
Set save model path,0
Set log sava path,0
Set actionType train,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Set log sava path,0
Feature number of train data,0
Total iteration number,0
Validation Ratio,0
Data format,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set basic configuration keys,0
Set data format,0
Use local deploy mode,0
"set angel resource parameters #worker, #task, #PS",0
set sgd SVM algorithm parameters,0
"set input, output path",0
Set save model path,0
Set actionType train,0
Set log path,0
Submit LR Train Task,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log sava path,0
Set actionType prediction,0
double z=pre*y;,0
if(z<=0) return 0.5-z;,0
"else if(z>0 && z<1) return 0.5*Math.pow(1-z,2);",0
return 0.0;,0
if (pre * y <= 0),0
return y;,0
else if (pre * y > 0 && pre * y < 1),0
return (1 - pre * y) * y;,0
return 0.0;,0
feature index used to split,0
feature value used to split,0
loss change after split this node,0
grad stats of the left child,0
grad stats of the right child,0
"LOG.info(""Constructor with fid = -1"");",0
fid = -1: no split currently,0
the minimal split value is the minimal value of feature,0
the splits do not include the maximal value of feature,0
"1. the average distance, (maxValue - minValue) / splitNum",0
2. calculate the candidate split value,0
1. new feature's histogram (grad + hess),0
size: sampled_featureNum * (2 * splitNum),0
"in other words, concatenate each feature's histogram",0
2. get the span of this node,0
------ 3. using sparse-aware method to build histogram ---,0
"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances",0
3.1. get the instance index,0
3.2. get the grad and hess of the instance,0
3.3. add to the sum,0
3.4. loop the non-zero entries,0
3.4.1. get feature value,0
3.4.2. current feature's position in the sampled feature set,0
3.4.3. find the position of feature value in a histogram,0
"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]",0
3.4.4. add the grad and hess to the corresponding bin,0
3.4.5. add the reverse to the bin that contains 0.0f,0
4. add the grad and hess sum to the zero bin of all features,0
int startIdx = fid * 2 * splitNum;,0
find the best split result of the histogram of a tree node,0
1. calculate the gradStats of the root node,0
"1.1. update the grad stats of the root node on PS, only called once by leader worker",0
2. loop over features,0
2.1. get the ture feature id in the sampled feature set,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"update the grad stats of the root node on PS, only called once by leader worker",0
3. update the grad stats of children node,0
3.1. update the left child,0
3.2. update the right child,0
find the best split result of one feature,0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
split value = sketches[splitIdx+1],0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
partition number,0
cols of each partition,0
"update the grad stats of the root node on PS, only called once by leader worker",0
3. update the grad stats of children node,0
3.1. update the left child,0
3.2. update the right child,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
"loop all the possible split value, start from split[1], since the first item is the minimal",0
feature value,0
find the best split result of the histogram of a tree node,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
find the best split result of one feature,0
1. set the feature id,0
splitEntry.setFid(fid);,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"here we set the fvalue=splitIndex, split value = sketches[splitIdx+1], the task use",0
index to find fvalue,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
clear all the information,0
calculate the sum of gradient and hess,0
"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,",0
ridx),0
check if necessary information is ready,0
"same as add, reduce is used in All Reduce",0
"features used in this tree, if equals null, means use all the features without sampling",0
node in the tree,0
the gradient info of each instances,0
initialize nodes,0
gradient,0
second order gradient,0
logistic loss for binary classification task.,0
"logistic loss, but predict un-transformed margin",0
check if label in range,0
return the default evaluation metric for the objective,0
read partition header,0
deal with row according the rowType,0
TODO Auto-generated method stub,1
tree.tree[topic + K] = (row.get(topic) + beta) / (n_k.get(topic) + vbeta);,0
Inc update to local buffers,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
loss function,0
gradient and hessian,0
tree node,0
initialize the phase,0
current tree and depth,0
create loss function,0
calculate grad info of each instance,0
"create data sketch, push candidate split value to PS",0
1. calculate candidate split value,0
2. push local sketch to PS,0
3. set phase to GET_SKETCH,0
"pull the global sketch from PS, only called once by each worker",0
sample feature,0
push sampled feature set to the current tree,0
create new tree,0
"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,",0
calculate gradient,0
"1. create new tree, initialize tree nodes and node stats",0
"2. initialize feature set, if sampled, get from PS, otherwise use all the features",0
2.1. pull the sampled features of the current tree,0
"2.2. if use all the features, only called one",0
"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle",0
4. set root node to active,0
"5. reset instance position, set the root node's span",0
6. calculate gradient,0
7. set phase to run active,0
1. start threads of active tree nodes,0
1.1. start threads for active nodes to generate histogram,0
1.2. set thread status to running,0
1.3. set the oplog to active,0
"2. check thread stats, if all threads finish, return",0
clock,0
find split,0
"1. find responsible tree node, using RR scheme",0
2. pull gradient histogram,0
2.1. get the name of this node's gradient histogram on PS,0
2.2. pull the histogram,0
2.3. find best split result of this tree node,0
"2.3.1 using server split, each partition of the histogram contains its best split result",0
find the best split from all partitions,0
"2.3.2 the updated split result (tree node/feature/value/gain) on PS,",0
"2.3.3 otherwise, the returned histogram contains the gradient info",0
"2.3.4 the updated split result (tree node/feature/value/gain) on PS,",0
2.3.5 reset this tree node's gradient histogram to 0,0
3. push split feature to PS,0
4. push split value to PS,0
5. push split gain to PS,0
6. set phase to AFTER_SPLIT,0
clock,0
1. get split feature,0
2. get split value,0
3. get split gain,0
4. get node weight,0
5. split node,0
5.1. set the children nodes of this node,0
5.2. set split info and grad stats to this node,0
5.2. create children nodes,0
"5.3. create node stats for children nodes, and add them to the tree",0
5.4. reset instance position,0
"5.5. add new active nodes if possible, inc depth, otherwise finish this tree",0
5.6. set children nodes to leaf nodes,0
5.7. set nid to leaf node,0
5.8. deactivate active node,0
6. clock,0
"split the span of one node, reset the instance position",0
in case this worker has no instance on this node,0
set the span of left child,0
set the span of right child,0
"1. left to right, find the first instance that should be in the right child",0
"2. right to left, find the first instance that should be in the left child",0
3. swap two instances,0
4. find the cut pos,0
than the split value,0
5. set the span of left child,0
6. set the span of right child,0
set tree node to active,0
set node to leaf,0
set node to inactive,0
finish current tree,0
calculate the error,0
predict();,0
finish current depth,0
set the tree phase,0
check if there is active node,0
check if finish all the tree,0
update node's grad stats on PS,0
"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split",0
the root node's stats is updated by leader worker,0
1. create the update,0
2. push the update to PS,0
the leader task adds node prediction to flush list,0
1. name of this node's grad histogram on PS,0
2. build the grad histogram of this node,0
3. push the histograms to PS,0
"LOG.info(String.format(""Histogram: size[%d] %s"", histogram.getDimension(),",0
Arrays.toString(histogram.getValues())));,0
4. reset thread stats to finished,0
int sendStartCol = startFid * 7; // each split contains 7 doubles,0
step size for a tree,0
number of class,0
minimum loss change required for a split,0
maximum depth of a tree,0
number of features,0
number of nonzero,0
number of candidates split value,0
----- the rest parameters are less important ----,0
base instance weight,0
minimum amount of hessian(weight) allowed in a child,0
L2 regularization factor,0
L1 regularization factor,0
default direction choice,0
maximum delta update we can add in weight estimation,0
this parameter can be used to stabilize update,0
default=0 means no constraint on weight delta,0
whether we want to do subsample for row,0
whether to subsample columns for each tree,0
accuracy of sketch,0
accuracy of sketch,0
leaf vector size,0
option for parallelization,0
option to open cacheline optimization,0
whether to not print info during training.,0
return MathUtils.sqr(sumGrad) / (sumHess + regLambda);,0
feature index used to split,0
feature value used to split,0
loss change after split this node,0
grad stats of the left child,0
grad stats of the right child,0
"LOG.info(""Constructor with fid = -1"");",0
fid = -1: no split currently,0
"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,",0
ridx),0
"same as add, reduce is used in All Reduce",0
maximum depth of the tree,0
number of features used for tree construction,0
"minimum loss change required for a split, otherwise stop split",0
----- the rest parameters are less important ----,0
default direction choice,0
whether we want to do sample data,0
whether to sample columns during tree construction,0
whether to use histogram for split,0
number of histogram units,0
whether to print info during training.,0
----- the rest parameters are obtained after training ----,0
total number of nodes,0
number of deleted nodes */,0
gradient,0
second order gradient,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
"LOG.info(String.format(""Histogram of feature[%d]: %s"", trueFid,",0
Arrays.toString(curHistogram.getValues())));,0
2.3. find the best split of current feature,0
"LOG.info(String.format(""Best split of feature[%d]: value[%f], gain[%f]"",",0
"trueFid, curSplit.getFvalue(), curSplit.getLossChg()));",0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
"LOG.info(String.format(""Find the best split for fid[%d] in server row, size[%d], startIdx[%d]"",",0
"fid, row.size(), startIdx));",0
StringBuilder sb = new StringBuilder();,0
for (int i = startIdx; i < startIdx + 2 * splitNum; i++) {,0
"sb.append(row.getData().get(i) + "", "");",0
},0
"LOG.info(""Server row: "" + sb.toString());",0
1. set the feature id,0
splitEntry.setFid(fid);,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
"LOG.info(String.format(""Feature[%d]: sumGrad[%f], sumHess[%f], gain[%f]"",",0
"fid, rootStats.sumGrad, rootStats.sumHess, rootGain));",0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"LOG.info(String.format(""The current split: fid[%d], split index[%f], lossChg[%f]"",",0
"fid, (float) splitIdx, lossChg));",0
"here we set the fvalue=splitIndex, split value = sketches[splitIdx+1], the task use",0
index to find fvalue,0
"LOG.info(String.format(""Find new best split: fid[%d], fvalue[%f], lossChg[%f]"",",0
"splitEntry.fid, splitEntry.fvalue, splitEntry.lossChg));",0
"LOG.info(String.format(""Left child of node[%d]: sumGrad[%f], sumHess[%f]; "" +",0
"""right child of node[%d]: sumGrad[%f], sumHess[%f]; lossChg[%f]"",",0
"this.nid, leftStats.sumGrad, leftStats.sumHess,",0
"this.nid, rightStats.sumGrad, rightStats.sumHess, lossChg));",0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
"LOG.info(String.format(""The best split for fid[%d], split feature[%d]: split index[%f], lossChg[%f], """,0
+,0
"""leftSumGrad[%f], leftSumHess[%f], rightSumGrad[%f], rightSumHess[%f]"",",0
"fid, splitEntry.fid, splitEntry.fvalue, splitEntry.lossChg,",0
"splitEntry.leftGradStat.sumGrad, splitEntry.leftGradStat.sumHess,",0
"splitEntry.rightGradStat.sumGrad, splitEntry.rightGradStat.sumHess));",0
grad.timesBy(-1.0 * lr);,0
"System.out.println(""Quantile sketch indices: "" + Arrays.toString(qSketch.getValues()));",0
"System.out.println(""Max: "" + qSketch.max() + "", min: "" + qSketch.min());",0
"System.out.println(""Quantile sketch count: "" + Arrays.toString(qSketch.getCounts()));",0
"System.out.println(""Zero index: "" + qSketch.getZeroIndex() + "", """,0
"+ qSketch.get(qSketch.getZeroIndex()) + "", "" + qSketch.get(qSketch.getZeroIndex()-1));",0
cmSketch.distribution();,0
"write2File(cmSketch.getTable(0), ""E:\\dropbox\\code\\github\\sketchML\\table0"");",0
"write2File(cmSketch.getTable(1), ""E:\\dropbox\\code\\github\\sketchML\\table1"");",0
"write2File(cmSketch.getTable(2), ""E:\\dropbox\\code\\github\\sketchML\\table2"");",0
"System.out.println(""true freq: "" + trueFreq + "", sketch freq: "" + cmFreq);",0
"System.out.println(""true grad: "" + grad.get(i) + "", sketch grad: "" + (- qSketch.get(cmFreq)));",0
ratioArr[ratio]++;,0
"System.out.println(""true grad: "" + grad.get(i) + "", sketch grad: "" + qSketch.get(cmFreq));",0
"System.out.println(""true grad: "" + grad.get(i) + "", sketch grad: "" + qSketch.get(cmFreq));",0
ratioArr[ratio]++;,0
System.out.println(Arrays.toString(distArr));,0
System.out.println(Arrays.toString(ratioArr));,0
"System.out.println(""Nnz grad: "" + nnz +"", zero grad: "" + zeroGrad + "", negative grad: "" + negCount + "", larger grad: "" + largeCount + "", smaller grad: "" + smallCount);",0
"write2File(distArr, ""E:\\dropbox\\code\\github\\sketchML\\error_hist"");",0
for (int i = 0; i < grad.getDimension(); i++) {,0
"System.out.println(""true grad: "" + grad.get(i) + "", sketch grad: "" + qSketch.get(qSketch.indexOf(grad.get(i))));",0
"grad.set(i, qSketch.get(qSketch.indexOf(grad.get(i))));",0
},0
"System.out.println(""Cur index of rIndex: "" + Arrays.toString(curIdx));",0
change to delta store,0
"System.out.println(""Before compression: "" + Arrays.toString(rIndex[i]));",0
"System.out.println(""After compression: "" + Arrays.toString(rIndex[i]));",0
"System.out.println(""Compressed "" + nnz + "" item to "" + bytes",0
"+ "" bytes, average bytes per item: "" + (double) bytes / qSketch.totalCount()",0
"+ "", uncompressed bytes: "" + 8 * grad.getDimension());",0
for (int i = 0; i < qSketchSize; i++) {,0
int tmp = 0;,0
for (int j = 0; j < curIdx[i]; j++) {,0
tmp += rIndex[i][j];,0
"grad.set(tmp, qSketch.getSplit(i));",0
},0
},0
"System.out.println(""Start calculate loss and auc, sample number: "" + totalNum);",0
"test.trainSGD2(dataset, 47001, 20, 0.01, 0.01, 100);",0
"System.out.println(""Indices: "" + Arrays.toString(indices));",0
t[i][code]++;,0
else if (Math.random() > 0.5) {,0
t[i][code] = freq;,0
},0
"System.out.println(""Change from "" + t[i][code] + "" to "" + freq);",0
"ret = Math.min(ret, t[i][h[i].encode(key)]);",0
"quantile sketch, size = featureNum * splitNum",0
"gradient histograms, size = treeNodeNum * featureNum * splitNum",0
"active tree nodes, size = pow(2, treeDepth) -1",0
sampled features. size = treeNum * sampleRatio * featureNum,0
"split features, size = treeNum * treeNodeNum",0
"split values, size = treeNum * treeNodeNum",0
"split gains, size = treeNum * treeNodeNum",0
"node weights, size = treeNum * treeNodeNum",0
"node preds, size = treeNum * treeNodeNum",0
if using PS to perform split,0
step size for a tree,0
number of class,0
minimum loss change required for a split,0
maximum depth of a tree,0
number of features,0
number of nonzero,0
number of candidates split value,0
----- the rest parameters are less important ----,0
base instance weight,0
minimum amount of hessian(weight) allowed in a child,0
L2 regularization factor,0
L1 regularization factor,0
default direction choice,0
maximum delta update we can add in weight estimation,0
this parameter can be used to stabilize update,0
default=0 means no constraint on weight delta,0
whether we want to do subsample for row,0
whether to subsample columns for each tree,0
accuracy of sketch,0
accuracy of sketch,0
leaf vector size,0
option for parallelization,0
option to open cacheline optimization,0
whether to not print info during training.,0
"get feature type, 0:empty 1:all equal 2:real",0
maximum depth of the tree,0
number of features used for tree construction,0
"minimum loss change required for a split, otherwise stop split",0
----- the rest parameters are less important ----,0
default direction choice,0
whether we want to do sample data,0
whether to sample columns during tree construction,0
whether to use histogram for split,0
number of histogram units,0
whether to print info during training.,0
----- the rest parameters are obtained after training ----,0
total number of nodes,0
number of deleted nodes */,0
TODO: SplitNum,0
TODO: implement split method,1
SplitEntry splitEntry = GradHistHelper.findSplitOfServerRow(row);,0
int sendStartCol = startFid * 7; // each split contains 7 doubles,0
set basic configuration keys,0
use local deploy mode and dummy data spliter,0
get a angel client,0
add matrix,0
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
in different part,0
TODO Auto-generated constructor stub,1
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
find the max abs,0
compress data,0
"paras[1] = ""abc"";",0
"paras[2] = ""123"";",0
Add standard Hadoop classes,0
Feature number of train data,0
Total iteration number,0
Learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set input data path,0
Set save model path,0
Set actionType train,0
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
"set input, output path",0
"set angel resource parameters #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Load Model from HDFS.,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"Set trainning data, and save model path",0
Set actionType train,0
Set MF algorithm parameters,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
worker register,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
attempt 0,0
attempt1,0
attempt1,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
set basic configuration keys,0
"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());",0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
Thread.sleep(5000);,0
"response = master.getJobReport(null, request);",0
"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);",0
"assertEquals(response.getJobReport().getCurIteration(), jobIteration);",0
"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
"DenseIntVector deltaVec = new DenseIntVector(100000, delta);",0
deltaVec.setMatrixId(matrixW1Id);,0
deltaVec.setRowId(0);,0
TODO Auto-generated constructor stub,1
import com.tencent.angel.psagent.consistency.SSPConsistencyController;,0
@RunWith(MockitoJUnitRunner.class),0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);",0
get a angel client,0
add matrix,0
psAgent.initAndStart();,0
test conf,0
test master location,0
test app id,0
test user,0
test ps agent attempt id,0
test ps agent id,0
test connection,0
test master client,0
test ip,0
test loc,0
test master location,0
test ps location,0
"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));",0
test all ps ids,0
test all matrix ids,0
test all matrix names,0
test matrix attribute,0
test matrix meta,0
test ps location,0
test partitions,0
"Note:[startRow,endRow)",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
group0Id = new WorkerGroupId(0);,0
"worker0Id = new WorkerId(group0Id, 0);",0
"worker0Attempt0Id = new WorkerAttemptId(worker0Id, 0);",0
task0Id = new TaskId(0);,0
task1Id = new TaskId(1);,0
LOG.info(serverArbitraryIntRow1.getSparseRep());,0
LOG.info(serverArbitraryIntRow1.getSparseRep());,0
test this func in testWriteTo,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
"LOG.info(index[0] + "" "" + value[0]);",0
"LOG.info(index[1] + "" "" + value[1]);",0
"LOG.info(index[2] + "" "" + value[2]);",0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
dot,0
plusBy,0
@Test,0
public void dotDenseFloatVector() throws Exception {,0
int dim = 1000;,0
Random random = new Random(System.currentTimeMillis());,0
,0
double[] values = new double[dim];,0
float[] values_1 = new float[dim];,0
for (int i = 0; i < dim; i++) {,0
values[i] = random.nextDouble();,0
values_1[i] = random.nextFloat();,0
},0
,0
"DenseDoubleVector vec = new DenseDoubleVector(dim, values);",0
"TDoubleVector vec_1 = new DenseFloatVector(dim, values_1);",0
,0
double sum = 0.0;,0
for (int i = 0; i < dim; i++) {,0
sum += values[i] * values_1[i];,0
},0
,0
"assertEquals(sum, vec.dot(vec_1));",0
,0
},0
@Test,0
public void plusDenseFlaotVector() throws Exception {,0
"double[] value_1 = new double[]{0.1, 0.2, 0.3, 0.4, 0.5};",0
"double[] value_2 = new double[]{0.1f, 0.2f, 0.3f, 0.4f, 0.5f};",0
"DenseDoubleVector vec = new DenseDoubleVector(5, value_1);",0
"TDoubleVector vec_1 = new DenseFloatVector(5, value_2);",0
,0
TDoubleVector vec_2 = vec.plus(vec_1);,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(value_1[i] + value_2[i], vec_2.get(i));",0
,0
,0
"TDoubleVector vec_3 = vec.plus(vec_1, 2.0);",0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec_3.get(i), value_1[i] + 2 * value_2[i]);",0
,0
double[] oldValues = vec.getValues().clone();,0
,0
vec.plusBy(vec_1);,0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec.get(i), oldValues[i] + vec_1.get(i));",0
,0
oldValues = vec.getValues().clone();,0
,0
"vec.plusBy(vec_1, 3);",0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec.get(i), oldValues[i] + 3 * vec_1.get(i));",0
},0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
@Test,0
public void plusBy3() throws Exception {,0
"float[][] value = {{1.0f, 2.0f}, {3.0f, 4.0f}};",0
"DenseFloatMatrix mat = new DenseFloatMatrix(2, 2,value);",0
"TFloatVector vec = new DenseFloatVector(2, new float[]{1.0f, 1.0f});",0
vec.setRowId(0);,0
"TDoubleVector vec_1 = new DenseDoubleVector(2, new double[]{1.0f, 1.0f});",0
vec_1.setRowId(1);,0
TDoubleVector vec_2 = new SparseDoubleVector(2);,0
"vec_2.set(1, 1.0);",0
vec_2.setRowId(0);,0
,0
mat.plusBy(vec);,0
mat.plusBy(vec_1);,0
mat.plusBy(vec_2);,0
,0
"assertEquals(2.0f, mat.get(0, 0));",0
"assertEquals(4.0f, mat.get(0, 1));",0
"assertEquals(4.0f, mat.get(1, 0));",0
"assertEquals(5.0f, mat.get(1, 1));",0
},0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
test worker getActiveTaskNum,0
test worker getTaskNum,0
test worker getTaskManager,0
test workerId,0
test workerAttemptId,0
tet worker initFinished,0
test worker getInitMinclock,0
test worker loacation,0
test AppId,0
test Conf,0
test UserName,0
master location,0
masterClient,0
test psAgent,0
test worker get dataBlockManager,0
workerGroup.getSplits();,0
application,0
lcation,0
workerGroup info,0
worker info,0
task,0
Matrix parameters,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
Create an Angel client,0
Add different types of matrix,0
using mock object,0
verification,0
Stubbing,0
Default does nothing.,0
The app injection is optional,0
"renderText(""hello world"");",0
"user choose a workerGroupID from the workergroups page,",0
now we should change the AngelApp params and render the workergroup page;,0
"static final String WORKER_ID = ""worker.id"";",0
"div(""#logo"").",0
"img(""/static/hadoop-st.png"")._().",0
import org.apache.hadoop.yarn.webapp.view.FooterBlock;,0
import org.apache.hadoop.yarn.webapp.view.HeaderBlock;,0
JQueryUI.jsnotice(html);,0
import org.apache.hadoop.conf.Configuration;,0
import java.lang.reflect.Field;,0
get block locations from file system,0
create a list of all block and their locations,0
"if the file is not splitable, just create the one block with",0
full file length,0
each split can be a maximum of maxSize,0
if remainder is between max and 2*max - then,0
"instead of creating splits of size max, left-max we",0
create splits of size left/2 and left/2. This is,0
a heuristic to avoid creating really really small,0
splits.,0
add this block to the block --> node locations map,0
"For blocks that do not have host/rack information,",0
assign to default  rack.,0
add this block to the rack --> block map,0
Add this host to rackToNodes map,0
add this block to the node --> block map,0
"if the file system does not have any rack information, then",0
use dummy rack location.,0
The topology paths have the host name included as the last,0
component. Strip it.,0
get tokens for all the required FileSystems..,0
"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,",0
job.getConfiguration());,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
all the files in input set,0
it is allowed for maxSize to be 0. Disable smoothing load for such cases,0
process all nodes and create splits that are local to a node. Generate,0
"one split per node iteration, and walk over nodes multiple times to",0
distribute the splits across nodes.,0
Skip the node if it has previously been marked as completed.,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
Remove all blocks which may already have been assigned to other,0
splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
Remove entries from blocksInNode so that we don't walk these,0
again.,0
Done creating a single split for this node. Move on to the next,0
node so that splits are distributed across nodes.,0
This implies that the last few blocks (or all in case maxSize=0),0
were not part of a split. The node is complete.,0
if there were any blocks left over and their combined size is,0
"larger than minSplitNode, then combine them into one split.",0
Otherwise add them back to the unprocessed pool. It is likely,0
that they will be combined with other blocks from the,0
same rack later on.,0
This condition also kicks in when max split size is not set. All,0
blocks on a node will be grouped together into a single split.,0
haven't created any split on this machine. so its ok to add a,0
smaller one for parallelism. Otherwise group it in the rack for,0
balanced size create an input split and add it to the splits,0
array,0
Remove entries from blocksInNode so that we don't walk this again.,0
The node is done. This was the last set of blocks for this node.,0
Put the unplaced blocks back into the pool for later rack-allocation.,0
Node is done. All blocks were fit into node-local splits.,0
Check if node-local assignments are complete.,0
All nodes have been walked over and marked as completed or all blocks,0
have been assigned. The rest should be handled via rackLock assignment.,0
"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """,0
"+ completedNodes.size() + "", size left: "" + totalLength);",0
"if blocks in a rack are below the specified minimum size, then keep them",0
"in 'overflow'. After the processing of all racks is complete, these",0
overflow blocks will be combined into splits.,0
Process all racks over and over again until there is no more work to do.,0
Create one split for this rack before moving over to the next rack.,0
Come back to this rack after creating a single split for each of the,0
remaining racks.,0
"Process one rack location at a time, Combine all possible blocks that",0
reside on this rack as one split. (constrained by minimum and maximum,0
split size).,0
iterate over all racks,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"if we created a split, then just go to the next rack",0
"if there is a minimum size specified, then create a single split",0
"otherwise, store these blocks into overflow data structure",0
There were a few blocks in this rack that,0
remained to be processed. Keep them in 'overflow' block list.,0
These will be combined later.,0
Process all overflow blocks,0
"This might cause an exiting rack location to be re-added,",0
but it should be ok.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"Process any remaining blocks, if any.",0
create an input split,0
add this split to the list that is returned,0
long num = totLength / maxSize;,0
all blocks for all the files in input set,0
mapping from a rack name to the list of blocks it has,0
mapping from a block to the nodes on which it has replicas,0
mapping from a node to the list of blocks that it contains,0
populate all the blocks for all files,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
"Security framework already loaded the tokens into current UGI, just use",0
them,0
Now remove the AM->RM token so tasks don't have it,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init container allocator,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
init parameter server manager,0
recover task information if needed,0
init psagent manager and register psagent manager event,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
start a web service if use yarn deploy mode,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
parse parameter server counters,0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
refresh last heartbeat timestamp,0
send a state update event to the specific PSAttempt,0
check if parameter server can commit now.,0
check matrix metadata inconsistencies between master and parameter server.,0
"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix",0
"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix",0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
check whether psagent heartbeat timeout,0
check whether parameter server heartbeat timeout,0
check whether worker heartbeat timeout,0
choose a unused port,0
start RPC server,0
find matrix partitions from master matrix meta manager for this parameter server,0
remove this parameter server attempt from monitor set,0
remove this parameter server attempt from monitor set,0
"if worker attempt id is not in monitor set, we should shutdown it",0
find workergroup in worker manager,0
"if this worker group does not initialized, just return WORKERGROUP_NOTREADY",0
"if this worker group run over, just return WORKERGROUP_EXITED",0
"if this worker group is running now, return tasks, workers, data splits for it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update the clock for this matrix,0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update task iteration,0
private boolean matrixInited;,0
"check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
dispatch matrix partitions to parameter servers,0
update matrix id generator,0
"check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
dispatch matrix partitions to parameter servers,0
get matrix ids in the parameter server report,0
get the matrices parameter server need to create and delete,0
"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix",0
"if a matrix exists on master but not exist on parameter server, this parameter server need build it.",0
waitForMatrixReleaseOnPS(matrixId);,0
"Calculate how many splits we need. As each task handles a separate split of data, so we want",0
the number of splits equal to the number of tasks,0
split data,0
dispatch the splits to workergroups,0
split data,0
dispatch the splits to workergroups,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
write meta data to a temporary file,0
rename the temporary file to final file,0
"if the file exists, read from file and deserialize it",0
write task meta,0
write ps meta,0
generate a temporary file,0
write task meta to the temporary file first,0
rename the temporary file to the final file,0
"if last final task file exist, remove it",0
find task meta file which has max timestamp,0
"if the file does not exist, just return null",0
read task meta from file and deserialize it,0
generate a temporary file,0
write ps meta to the temporary file first.,0
rename the temporary file to the final file,0
"if the old final file exist, just remove it",0
find ps meta file,0
"if ps meta file does not exist, just return null",0
read ps meta from file and deserialize it,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
Transitions from the NEW state.,0
PA_FAILMSG,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG",0
event,0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event",0
dispatched before PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will",0
retry another attempt or failed,0
release container,0
TODO,1
set the launch time,0
"set tarckerName,httpPort, which used by webserver",0
added to psManager so psManager can monitor it;,0
psAttempt.getContext().getParameterServerManager.registerPSAttempt(psAttempt.attemptId);,0
set the finish time only if launch time is set,0
"ParameterServerJVM.setVMEnv(myEnv, conf);",0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Add the env variables passed by the user,0
Set logging level in the environment.,0
"This is so that, if the child forks another ""bin/hadoop"" (common in",0
streaming) it will have the correct loglevel.,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
Transitions from the NEW state.,0
Transitions from the RUNNING state.,0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
LocalStorageToken is needed irrespective of whether security is enabled,0
or not.,0
"TokenCache.setJobToken(jobToken, taskCredentials);",0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Construct the actual Container,0
The null fields are per-container and will be constructed for each,0
container separately.,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
"a * in the classpath will only find a .jar, so we need to filter out",0
all .jars and add everything else,0
Propagate the system classpath when using the mini cluster,0
Add standard Hadoop classes,0
Cache archives,0
Cache files,0
Sanity check,0
Add URI fragment or just the filename,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Final commmand,0
"if amTask is not null, we should clone task state from it",0
"if all parameter server complete commit, master can commit now",0
init and start master committer,0
Transitions from the NEW state.,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event",0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event dispatched before",0
PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.COMMITTING state,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
reqeuest resource:send a resource request to the resource allocator,0
"Once the resource is applied, build and send the launch request to the container launcher",0
deallocator the resource of the ps attempt:send a resource deallocator request to the,0
resource allocator,0
set the launch time,0
add the ps attempt to the heartbeat timeout monitoring list,0
parse ps attempt location and put it to location manager,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt",0
or failed,0
remove ps attempt id from heartbeat timeout monitor list,0
release container:send a release request to container launcher,0
set the finish time only if launch time is set,0
private long scheduledTime;,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
notify the event handler of state change,0
"If new state is not RUNNING and COMMITTING, add it to state timeout monitor",0
"if forcedState is set, just return",0
else get state from state machine,0
add this worker group to the success set,0
check if all worker group run over,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
calculate the actual number of worker groups and the total number of tasks based on the number of data split,0
"init all tasks , workers and worker groups and put them to the corresponding maps",0
just return the total task number now,0
TODO,1
"if workerAttempt is not null, we should clone task state from it",0
from NEW state,0
from SCHEDULED state,0
get data splits location for data locality,0
reqeuest resource:send a resource request to the resource allocator,0
"once the resource is applied, build and send the launch request to the container launcher",0
notify failed message to the worker,0
notify killed message to the worker,0
release the allocated container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
release the allocated container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
clean the container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
record the finish time,0
clean the container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listening list,0
record the finish time,0
"if the worker attempt launch successfully, add it to heartbeat timeout listening list",0
set worker attempt location,0
notify the register message to the worker,0
record the launch time,0
update worker attempt metrics,0
update tasks metrics,0
clean the container,0
notify the worker attempt run successfully message to the worker,0
record the finish time,0
init a worker attempt for the worker,0
schedule the worker attempt,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
"If we need Yarn to restart a new application master, we should not unregister from Yarn RM",0
register to Yarn RM,0
send heartbeat to Yarn RM every rmPollInterval milliseconds,0
"catch YarnRuntimeException, we should exit and need not retry",0
build heartbeat request,0
send heartbeat request to rm,0
"This can happen if the RM has been restarted. If it is in that state,",0
this application must clean itself up.,0
Setting NMTokens,0
assgin containers,0
"if some container is not assigned, release them",0
handle finish containers,0
dispatch container exit message to corresponding components,0
killed by framework,0
killed by framework,0
killed by framework,0
get application finish state,0
build application diagnostics,0
TODO:add a job history for angel,1
build unregister request,0
send unregister request to rm,0
Note this down for next interaction with ResourceManager,0
based on blacklisting comments above we can end up decrementing more,0
than requested. so guard for that.,0
send the updated resource request to RM,0
send 0 container count requests also to cancel previous requests,0
Update resource requests,0
try to assign to all nodes first to match node local,0
try to match all rack local,0
assign remaining,0
Update resource requests,0
send the container-assigned event to task attempt,0
build the start container request use launch context,0
send the start request to Yarn nm,0
send the message that the container starts successfully to the corresponding component,0
"after launching, send launched event to task attempt to move",0
it from ASSIGNED to RUNNING state,0
send the message that the container starts failed to the corresponding component,0
kill the remote container if already launched,0
start a thread pool to startup the container,0
See if we need up the pool size only if haven't reached the,0
maximum limit yet.,0
nodes where containers will run at *this* point of time. This is,0
*not* the cluster size and doesn't need to be.,0
"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the",0
later is just a buffer so we are not always increasing the,0
pool-size,0
the events from the queue are handled in parallel,0
using a thread pool,0
return if already stopped,0
shutdown any containers that might be left running,0
Build and initialize rpc client to master,0
Build local location,0
"Initialize matrix info, this method will wait until master accepts the information from",0
client,0
Get ps locations from master and put them to the location cache.,0
Initialize matrix meta information,0
Start heartbeat thread if need,0
Start all services,0
Register to master first,0
Report state to master every specified time,0
Notify run success to master only on ANGEL_PS_PSAGENT running mode,0
Stop all modules,0
Exit the process if on ANGEL_PS_PSAGENT mode,0
Notify run success to master only on ANGEL_PS_PSAGENT running mode,0
Stop all modules,0
Exit the process if on ANGEL_PS_PSAGENT mode,0
get configuration from config file,0
set localDir with enviroment set by nm.,0
Update generic resource counters,0
Updating resources specified in ResourceCalculatorProcessTree,0
Remove the CPU time consumed previously by JVM reuse,0
array stores clock for each row and clock,0
local task num,0
mapping from task index to taskId,0
mapping from taskId to task index,0
TODO Auto-generated method stub,1
Generate a flush request and put it to request queue,0
Generate a clock request and put it to request queue,0
Generate a merge request and put it to request queue,0
"If the matrix op log cache does not exist for the matrix, create a new one for the",0
matrix,0
and add it to cache maps,0
Add the message to the tree map,0
"If there are flush / clock requests blocked, we need to put this merge request into",0
the waiting queue,0
Launch a merge worker to merge the update to matrix op log cache,0
Remove the message from the tree map,0
Wake up blocked flush/clock request,0
Add flush/clock request to listener list to waiting for all the existing,0
updates are merged,0
Wake up blocked flush/clock request,0
"If all updates are merged for this matrix, we need wake up flush/clock requests which are",0
blocked.,0
Get next merge message sequence id,0
Wake up listeners(flush/clock requests) that have little sequence id than current merge,0
position,0
Wake up blocked merge requests,0
Get minimal sequence id from listeners,0
"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we",0
should flush updates to local matrix storage,0
unused now,1
Get partitions for the matrix,0
"Filter it, removing zero values",0
Doing average or not,0
Split this row according the matrix partitions,0
Add the splits to the result container,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
int seqId = ((ByteBuf) msg).readInt();,0
"LOG.info(""receive result of seqId="" + seqId);",0
((ByteBuf) msg).resetReaderIndex();,0
TODO: use Epoll for linux future,1
closeChannelForServer(request.getServerId());,0
closeChannelForServer(request.getServerId());,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
Then submit normal task until reach upper limit of flow control or all tasks are submit,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"LOG.info(""choose put server "" + psIds[index]);",0
allocate the bytebuf,0
"check the location of server is ready, if not, we should wait",0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
channelManager.removeChannelPool(loc);,0
find the partition request context from cache,0
Check if the result of the sub-request is received,0
Update received result number,0
Get row splits received,0
Put the row split to the cache(row index to row splits map),0
"If all splits of the row are received, means this row can be merged",0
TODO Auto-generated method stub,1
"Check futures, if the result of a sub-request is received, put it to the result queue",0
Now we just support pipelined row splits merging for dense type row,0
Wait until the clock value of this row is greater than or equal to the value,0
Get partitions for this row,0
First get this row from matrix storage,0
"If the row exists in the matrix storage and the clock value meets the requirements, just",0
return,0
Get row splits of this row from the matrix cache first,0
"If the row split does not exist in cache, get it from parameter server",0
Wait the final result,0
Put it to the matrix cache,0
Split the matrix oplog according to the matrix partitions,0
"If need update clock, we should send requests to all partitions",0
use update index if exist,0
Filter the rowIds which are fetching now,0
Send the rowIndex to rpc dispatcher and return immediately,0
Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition,0
Generate dispatch items and add them to the corresponding queues,0
Pre-fetching is disable default,0
matrix id to clock map,0
"task index, it must be unique for whole application",0
if (matrixManager.getMatrixMeta(createResponse.getMatrixId()) != null) {,0
return matrixManager.getMatrixMeta(createResponse.getMatrixId());,0
},0
Deserialize data splits meta,0
Get workers,0
Send request to every ps,0
Wait the responses,0
Update clock cache,0
Get row from cache.,0
"if row clock is satisfy ssp staleness limit, just return.",0
Get row from ps.,0
"For ASYNC mode, just get from pss.",0
"For BSP/SSP, get rows from storage/cache first",0
Get from ps.,0
"For ASYNC, just get rows from pss.",0
no more retries.,0
calculate sleep time and return.,0
parse the i-th sleep-time,0
parse the i-th number-of-retries,0
calculateSleepTime may overflow.,0
"A few common retry policies, with no delays.",0
close is a local operation and should finish within milliseconds; timeout just to be safe,0
response will be null for one way messages.,0
maxFrameLength = 2G,0
lengthFieldOffset = 0,0
lengthFieldLength = 8,0
"lengthAdjustment = -8, i.e. exclude the 8 byte length itself",0
"initialBytesToStrip = 8, i.e. strip out the length field itself",0
indicates whether this connection's life cycle is managed,0
See if we already have a connection (common case),0
create a unique lock for this RS + protocol (if necessary),0
get the RS lock,0
do one more lookup in case we were stalled above,0
Only create isa when we need to.,0
definitely a cache miss. establish an RPC for,0
this RS,0
Throw what the RemoteException was carrying.,0
check,0
every,0
minutes,0
TODO,1
failoverHandler,0
"The number of times this invocation handler has ever been failed over,",0
before this method invocation attempt. Used to prevent concurrent,0
failed method invocations from triggering multiple failover attempts.,0
Make sure that concurrent failed method invocations,0
only cause a,0
single actual fail over.,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
"LOG.info(""method "" + method.getName() + ""construct request time = """,0
+ (System.currentTimeMillis() - beforeConstructTs));,0
get an instance of the method arg type,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
Message (hand written code usually has only a single,0
argument),0
log any RPC responses that are slower than the configured,0
warn,0
response time or larger than configured warning size,0
"when tagging, we let TooLarge trump TooSmall to keep",0
output simple,0
note that large responses will often also be slow.,0
provides a count of log-reported slow responses,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
unexpected,0
"in the protobuf methods, args[1] is the only significant argument",0
for JSON encoding,0
base information that is reported regardless of type of call,0
Disable Nagle's Algorithm since we don't want packets to wait,0
Configure the event pipeline factory.,0
Make a new connection.,0
Remove all pending requests (will be canceled after relinquishing,0
write lock).,0
Cancel any pending requests by sending errors to the callbacks:,0
Close the channel:,0
Close the connection:,0
Shut down all thread pools to exit.,0
"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());",0
See NettyServer.prepareResponse for where we write out the response.,0
"It writes the call.id (int), a boolean signifying any error (and if",0
"so the exception name/trace), and the response bytes",0
Read the call id.,0
"When the stream is closed, protobuf doesn't raise an EOFException,",0
"instead, it returns a null message object.",0
"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +",0
System.currentTimeMillis());,0
"It would be good widen this to just Throwable, but IOException is what we",0
allow now,0
not implemented,1
not implemented,1
"track what RpcEngine is used by a proxy class, for stopProxy()",0
cache of RpcEngines by protocol,0
return the RpcEngine configured to handle a protocol,0
We only handle the ConnectException.,0
This is the exception we can't handle.,0
check if timed out,0
wait for retry,0
IGNORE,0
return the RpcEngine that handles a proxy object,0
The default implementation works synchronously,0
punt: allocate a new buffer & copy into it,0
"LOG.info(System.getProperty(""user.dir""));",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
a simple hdfs copy function assume src path and dest path are in same hdfs,0
and FileSystem object has same schema,0
"LOG.warn(""interrupted while sleeping"", ie);",0
private static UnpooledByteBufAllocator allocator = new UnpooledByteBufAllocator(false);,0
public static String getHostname() {,0
try {,0
"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();",0
} catch (UnknownHostException uhe) {,0
},0
"return new StringBuilder().append("""").append(uhe).toString();",0
},0
"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {",0
String hostname = getHostname();,0
String classname = clazz.getSimpleName();,0
"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new",0
"StringBuilder().append(""Starting "").append(classname).toString(), new",0
"StringBuilder().append(""  host = "").append(hostname).toString(), new",0
"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new",0
"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new",0
"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));",0
,0
"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {",0
public void run() {,0
"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +",0
"this.val$classname + "" at "" + this.val$hostname}));",0
},0
});,0
},0
"We we interrupted because we're meant to stop? If not, just",0
continue ignoring the interruption,0
Recalculate waitTime.,0
// Begin delegation to Thread,0
// End delegation to Thread,0
load hadoop configuration,0
load angel system configuration,0
load user configuration:,0
load user config file,0
load command line parameters,0
load user job resource files,0
load user job jar if it exist,0
Expand the environment variable,0
instance submitter class,0
Add default fs(local fs) for lib jars.,0
Obtain filename from path,0
Split filename to prexif and suffix (extension),0
Check if the filename is okay,0
Prepare temporary file,0
Prepare buffer for data copying,0
Open and check input stream,0
Open output stream and copy data between source file in JAR and the temporary file,0
"If read/write fails, close streams safely before throwing an exception",0
"Finally, load the library",0
little endian load order,0
tail,0
fallthrough,0
fallthrough,0
finalization,0
fmix(h1);,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
JobStateProto jobState = report.getJobState();,0
the leaf level file should be readable by others,0
the subdirs in the path should have execute permissions for,0
others,0
2.get job id,0
Credentials credentials = new Credentials();,0
4.copy resource files to hdfs,0
5.write configuration to a xml file,0
6.create am container context,0
7.Submit to ResourceManager,0
8.get app master client,0
Create a number of filenames in the JobTracker's fs namespace,0
add all the command line files/ jars and archive,0
first copy them to jobtrackers filesystem,0
should not throw a uri exception,0
should not throw an uri excpetion,0
set the timestamps of the archives and files,0
set the public/private visibility of the archives and files,0
get DelegationToken for each cached file,0
check if we do not need to copy the files,0
is jt using the same file system.,0
just checking for uri strings... doing no dns lookups,0
to see if the filesystems are the same. This is not optimal.,0
but avoids name resolution.,0
this might have name collisions. copy will throw an exception,0
parse the original path to create new path,0
check for ports,0
Write job file to JobTracker's fs,0
Setup resource requirements,0
Setup LocalResources,0
Setup security tokens,0
Setup the command to run the AM,0
Add AM user command opts,0
Final command,0
Setup the CLASSPATH in environment,0
"i.e. add { Hadoop jars, job jar, CWD } to classpath.",0
Setup the environment variables for Admin first,0
"Setup the environment variables (LD_LIBRARY_PATH, etc)",0
Parse distributed cache,0
Setup ContainerLaunchContext for AM container,0
Set up the ApplicationSubmissionContext,0
resposne.encode(buf);,0
TODO:,1
resposne.encode(buf);,0
TODO:,1
resposne.encode(buf);,0
TODO:,1
resposne.encode(buf);,0
TODO:,1
Add tokens to new user so that it may execute its task correctly.,0
to exit,0
mkdir does not throw exception if path exits,0
commitTaskPool.shutdown();,0
"LOG.info(""rowId = "" + rowId + "" rowType = "" + rowType + "" size = "" + size + "" request "" +",0
"""update"");",0
private final ParameterServer psServer;,0
TODO,1
"when we should write snapshot to hdfs? clearly, we have two methods:",0
"1. write snapshot at regular time, if there are updates, just write them.",0
"2. write snapshot every N iterations, this method depends on notification of master",0
"FSDataOutputStream output = fileContext.create(snapshotsTempFilePath,",0
EnumSet.of(CreateFlag.CREATE));,0
@brief get filename of the old snapshot written before,0
"no snapshotFile write before, maybe write snapshots the first time",0
start end,0
rowtype,0
data.rewind();,0
data.rewind();,0
data.rewind();,0
Pass the matrix and partition number field,0
Mapping from taskId to clock value.,0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
nnz = 0;,0
for (int i = 0; i < keys.length; i++),0
if (used[i]) {,0
"denseRep.put(keys[i], values[i]);",0
nnz++;,0
},0
sparseRep = null;,0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
for (int i = 0; i < keys.length; i++),0
if (used[i]) {,0
"denseRep.put(keys[i], values[i]);",0
},0
sparseRep = null;,0
output.writeInt(data.length);,0
@Override,0
public void serialize(ByteBuf buf) {,0
if (sparseRep != null),0
return serializeSparse();,0
else if (denseRep != null),0
return serializeDense();,0
return serializeEmpty();,0
},0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
int idx = 0;,0
for (int i = 0; i < keys.length; i++),0
if (used[i]) {,0
"keysBuf.put(idx, keys[i]);",0
"valuesBuf.put(idx, values[i]);",0
idx++;,0
},0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
"int ov, k, v;",0
for (int i = 0; i < keys.length; i++) {,0
if (used[i]) {,0
k = keys[i];,0
ov = denseRep.get(k);,0
v = ov + values[i];,0
"denseRep.put(k, v);",0
if (ov != 0 && v == 0),0
nnz--;,0
},0
},0
"add the PSAgentContext,need fix",0
set MatrixPartitionLocation,0
set attribute,0
TODO:,1
write the max abs,0
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
get configuration from config file,0
set localDir with enviroment set by nm.,0
get master location,0
init task manager and start tasks,0
start heartbeat thread,0
taskManager.assignTaskIds(response.getTaskidsList());,0
todo,1
"if worker timeout, it may be knocked off.",0
"SUCCESS, do nothing",0
heartbeatFailedTime = 0;,0
private KEY currentKey;,0
will be created,0
TODO Auto-generated method stub,1
Bitmap bitmap = new Bitmap();,0
int max = indexArray[size - 1];,0
byte [] bitIndexArray = new byte[max / 8 + 1];,0
for(int i = 0; i < size; i++){,0
int bitIndex = indexArray[i] >> 3;,0
int bitOffset = indexArray[i] - (bitIndex << 3);,0
switch(bitOffset){,0
case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;,0
case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;,0
case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;,0
case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;,0
case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;,0
case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;,0
case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;,0
case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;,0
},0
},0
//////////////////////////////,0
Application Configs,0
//////////////////////////////,0
//////////////////////////////,0
Master Configs,0
//////////////////////////////,0
//////////////////////////////,0
Worker Configs,0
//////////////////////////////,0
//////////////////////////////,0
Task Configs,0
//////////////////////////////,0
//////////////////////////////,0
ParameterServer Configs,0
//////////////////////////////,0
////////////////// IPC //////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
Configs used to ANGEL_PS_PSAGENT running mode future.,0
model parse,0
private Configuration conf;,0
TODO Auto-generated constructor stub,1
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
"set input, output path",0
"set angel resource parameters #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Submit GBDT Train Task,0
Load Model from HDFS.,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"Set trainning data, save model, log path",0
Set actionType train,0
Set MF algorithm parameters,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
LOG.info(sigmoid(data[i]));,0
LOG.info(Math.exp(-data[i]));,0
when b is a negative number,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set MLR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
Set actionType train,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
parseSparseDouble();,0
parseDenseFloat();,0
parseDenseInt();,0
parseSparseInt();,0
Set model path,0
Set model path,0
Set model path,0
Set model path,0
Set model path,0
Feature number of train data,0
Total iteration number,0
Rank,0
Regularization parameters,0
Learn rage,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set learnType,0
Set feature number,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log sava path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
"set input, output path",0
"set angel resource parameters #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Submit GBDT Train Task,0
Load Model from HDFS.,0
Feature number of train data,0
Total iteration number,0
Validation Ratio,0
Data format,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set basic configuration keys,0
Set data format,0
Use local deploy mode,0
"set angel resource parameters #worker, #task, #PS",0
set sgd SVM algorithm parameters,0
"set input, output path",0
Set save model path,0
Set actionType train,0
Set log path,0
Submit LR Train Task,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log sava path,0
Set actionType prediction,0
double z=pre*y;,0
if(z<=0) return 0.5-z;,0
"else if(z>0 && z<1) return 0.5*Math.pow(1-z,2);",0
return 0.0;,0
if (pre * y <= 0),0
return y;,0
else if (pre * y > 0 && pre * y < 1),0
return (1 - pre * y) * y;,0
return 0.0;,0
logistic loss for binary classification task.,0
"logistic loss, but predict un-transformed margin",0
check if label in range,0
return the default evaluation metric for the objective,0
TODO Auto-generated method stub,1
start row index for words,0
doc ids,0
topic assignments,0
count word,0
build word start index,0
build dks,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
Write #rows,0
Write each row,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
loss function,0
gradient and hessian,0
tree node,0
initialize the phase,0
current tree and depth,0
create loss function,0
calculate grad info of each instance,0
"create data sketch, push candidate split value to PS",0
1. calculate candidate split value,0
2. push local sketch to PS,0
3. set phase to GET_SKETCH,0
"pull the global sketch from PS, only called once by each worker",0
sample feature,0
push sampled feature set to the current tree,0
create new tree,0
"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,",0
calculate gradient,0
"1. create new tree, initialize tree nodes and node stats",0
"2. initialize feature set, if sampled, get from PS, otherwise use all the features",0
2.1. pull the sampled features of the current tree,0
"2.2. if use all the features, only called one",0
"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle",0
4. set root node to active,0
"5. reset instance position, set the root node's span",0
6. calculate gradient,0
7. set phase to run active,0
1. start threads of active tree nodes,0
1.1. start threads for active nodes to generate histogram,0
1.2. set thread status to batch num,0
1.3. set the oplog to active,0
"2. check thread stats, if all threads finish, return",0
clock,0
find split,0
"1. find responsible tree node, using RR scheme",0
2. pull gradient histogram,0
2.1. get the name of this node's gradient histogram on PS,0
2.2. pull the histogram,0
histogram = (TDoubleVector) ((GetRowResult) histMat.get(func)).getRow();,0
2.3. find best split result of this tree node,0
2.3.1 using server split,0
"update the grad stats of the root node on PS, only called once by leader worker",0
update the grad stats of children node,0
update the left child,0
update the right child,0
"2.3.2 the updated split result (tree node/feature/value/gain) on PS,",0
"2.3.3 otherwise, the returned histogram contains the gradient info",0
"2.3.4 the updated split result (tree node/feature/value/gain) on PS,",0
2.3.5 reset this tree node's gradient histogram to 0,0
3. push split feature to PS,0
4. push split value to PS,0
5. push split gain to PS,0
6. set phase to AFTER_SPLIT,0
clock,0
1. get split feature,0
2. get split value,0
3. get split gain,0
4. get node weight,0
5. split node,0
"2. check thread stats, if all threads finish, return",0
6. clock,0
"split the span of one node, reset the instance position",0
in case this worker has no instance on this node,0
set the span of left child,0
set the span of right child,0
"1. left to right, find the first instance that should be in the right child",0
"2. right to left, find the first instance that should be in the left child",0
3. swap two instances,0
4. find the cut pos,0
than the split value,0
5. set the span of left child,0
6. set the span of right child,0
set tree node to active,0
set node to leaf,0
set node to inactive,0
finish current tree,0
finish current depth,0
set the tree phase,0
check if there is active node,0
check if finish all the tree,0
update node's grad stats on PS,0
"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split",0
the root node's stats is updated by leader worker,0
1. create the update,0
2. push the update to PS,0
the leader task adds node prediction to flush list,0
1. name of this node's grad histogram on PS,0
2. build the grad histogram of this node,0
3. push the histograms to PS,0
4. reset thread stats to finished,0
5.1. set the children nodes of this node,0
5.2. set split info and grad stats to this node,0
5.2. create children nodes,0
"5.3. create node stats for children nodes, and add them to the tree",0
5.4. reset instance position,0
"5.5. add new active nodes if possible, inc depth, otherwise finish this tree",0
5.6. set children nodes to leaf nodes,0
5.7. set nid to leaf node,0
5.8. deactivate active node,0
"get feature type, 0:empty 1:all equal 2:real",0
feature index used to split,0
feature value used to split,0
loss change after split this node,0
grad stats of the left child,0
grad stats of the right child,0
"LOG.info(""Constructor with fid = -1"");",0
fid = -1: no split currently,0
the minimal split value is the minimal value of feature,0
the splits do not include the maximal value of feature,0
"1. the average distance, (maxValue - minValue) / splitNum",0
2. calculate the candidate split value,0
1. new feature's histogram (grad + hess),0
size: sampled_featureNum * (2 * splitNum),0
"in other words, concatenate each feature's histogram",0
2. get the span of this node,0
int nodeStart = this.controller.nodePosStart[nid];,0
int nodeEnd = this.controller.nodePosEnd[nid];,0
------ 3. using sparse-aware method to build histogram ---,0
"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances",0
3.1. get the instance index,0
3.2. get the grad and hess of the instance,0
3.3. add to the sum,0
3.4. loop the non-zero entries,0
3.4.1. get feature value,0
3.4.2. current feature's position in the sampled feature set,0
3.4.3. find the position of feature value in a histogram,0
"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]",0
3.4.4. add the grad and hess to the corresponding bin,0
3.4.5. add the reverse to the bin that contains 0.0f,0
4. add the grad and hess sum to the zero bin of all features,0
int startIdx = fid * 2 * splitNum;,0
find the best split result of the histogram of a tree node,0
1. calculate the gradStats of the root node,0
"1.1. update the grad stats of the root node on PS, only called once by leader worker",0
2. loop over features,0
2.1. get the ture feature id in the sampled feature set,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"update the grad stats of the root node on PS, only called once by leader worker",0
3. update the grad stats of children node,0
3.1. update the left child,0
3.2. update the right child,0
find the best split result of one feature,0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
split value = sketches[splitIdx+1],0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
partition number,0
cols of each partition,0
"// update the grad stats of the root node on PS, only called once by leader worker",0
if (this.nid == 0) {,0
GradStats rootStats = new GradStats(splitEntry.leftGradStat);,0
rootStats.add(splitEntry.rightGradStat);,0
"this.controller.updateNodeGradStats(this.nid, rootStats);",0
},0
,0
// 3. update the grad stats of children node,0
if (splitEntry.fid != -1) {,0
// 3.1. update the left child,0
"this.controller.updateNodeGradStats(2 * this.nid + 1, splitEntry.leftGradStat);",0
// 3.2. update the right child,0
"this.controller.updateNodeGradStats(2 * this.nid + 2, splitEntry.rightGradStat);",0
},0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
"loop all the possible split value, start from split[1], since the first item is the minimal",0
feature value,0
find the best split result of the histogram of a tree node,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
find the best split result of one feature,0
1. set the feature id,0
splitEntry.setFid(fid);,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]",0
the task use index to find fvalue,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
clear all the information,0
calculate the sum of gradient and hess,0
"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,",0
ridx),0
check if necessary information is ready,0
"same as add, reduce is used in All Reduce",0
"features used in this tree, if equals null, means use all the features without sampling",0
node in the tree,0
the gradient info of each instances,0
initialize nodes,0
gradient,0
second order gradient,0
int sendStartCol = startFid * 7; // each split contains 7 doubles,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"tips: here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]",0
the task use index to find fvalue,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
grad.timesBy(-1.0 * lr);,0
"System.out.println(""Quantile sketch indices: "" + Arrays.toString(qSketch.getValues()));",0
"System.out.println(""Max: "" + qSketch.max() + "", min: "" + qSketch.min());",0
"System.out.println(""Quantile sketch count: "" + Arrays.toString(qSketch.getCounts()));",0
"System.out.println(""Zero index: "" + qSketch.getZeroIndex() + "", """,0
"+ qSketch.get(qSketch.getZeroIndex()) + "", "" + qSketch.get(qSketch.getZeroIndex()-1));",0
cmSketch.distribution();,0
"write2File(cmSketch.getTable(0), ""E:\\dropbox\\code\\github\\sketchML\\table0"");",0
"write2File(cmSketch.getTable(1), ""E:\\dropbox\\code\\github\\sketchML\\table1"");",0
"write2File(cmSketch.getTable(2), ""E:\\dropbox\\code\\github\\sketchML\\table2"");",0
"System.out.println(""true freq: "" + trueFreq + "", sketch freq: "" + cmFreq);",0
"System.out.println(""true grad: "" + grad.get(i) + "", sketch grad: "" + (- qSketch.get(cmFreq)));",0
ratioArr[ratio]++;,0
"System.out.println(""true grad: "" + grad.get(i) + "", sketch grad: "" + qSketch.get(cmFreq));",0
"System.out.println(""true grad: "" + grad.get(i) + "", sketch grad: "" + qSketch.get(cmFreq));",0
ratioArr[ratio]++;,0
System.out.println(Arrays.toString(distArr));,0
System.out.println(Arrays.toString(ratioArr));,0
"System.out.println(""Nnz grad: "" + nnz +"", zero grad: "" + zeroGrad + "", negative grad: "" + negCount + "", larger grad: "" + largeCount + "", smaller grad: "" + smallCount);",0
"write2File(distArr, ""E:\\dropbox\\code\\github\\sketchML\\error_hist"");",0
for (int i = 0; i < grad.getDimension(); i++) {,0
"System.out.println(""true grad: "" + grad.get(i) + "", sketch grad: "" + qSketch.get(qSketch.indexOf(grad.get(i))));",0
"grad.set(i, qSketch.get(qSketch.indexOf(grad.get(i))));",0
},0
"System.out.println(""Cur index of rIndex: "" + Arrays.toString(curIdx));",0
change to delta store,0
"System.out.println(""Before compression: "" + Arrays.toString(rIndex[i]));",0
"System.out.println(""After compression: "" + Arrays.toString(rIndex[i]));",0
"System.out.println(""Compressed "" + nnz + "" item to "" + bytes",0
"+ "" bytes, average bytes per item: "" + (double) bytes / qSketch.totalCount()",0
"+ "", uncompressed bytes: "" + 8 * grad.getDimension());",0
for (int i = 0; i < qSketchSize; i++) {,0
int tmp = 0;,0
for (int j = 0; j < curIdx[i]; j++) {,0
tmp += rIndex[i][j];,0
"grad.set(tmp, qSketch.getSplit(i));",0
},0
},0
"System.out.println(""Start calculate loss and auc, sample number: "" + totalNum);",0
"test.trainSGD2(dataset, 47001, 20, 0.01, 0.01, 100);",0
"System.out.println(""Indices: "" + Arrays.toString(indices));",0
t[i][code]++;,0
else if (Math.random() > 0.5) {,0
t[i][code] = freq;,0
},0
"System.out.println(""Change from "" + t[i][code] + "" to "" + freq);",0
"ret = Math.min(ret, t[i][h[i].encode(key)]);",0
"quantile sketch, size = featureNum * splitNum",0
"gradient histograms, size = treeNodeNum * featureNum * splitNum",0
"active tree nodes, size = pow(2, treeDepth) -1",0
sampled features. size = treeNum * sampleRatio * featureNum,0
"split features, size = treeNum * treeNodeNum",0
"split values, size = treeNum * treeNodeNum",0
"split gains, size = treeNum * treeNodeNum",0
"node weights, size = treeNum * treeNodeNum",0
"node preds, size = treeNum * treeNodeNum",0
if using PS to perform split,0
step size for a tree,0
number of class,0
minimum loss change required for a split,0
maximum depth of a tree,0
number of features,0
number of nonzero,0
number of candidates split value,0
----- the rest parameters are less important ----,0
base instance weight,0
minimum amount of hessian(weight) allowed in a child,0
L2 regularization factor,0
L1 regularization factor,0
default direction choice,0
maximum delta update we can add in weight estimation,0
this parameter can be used to stabilize update,0
default=0 means no constraint on weight delta,0
whether we want to do subsample for row,0
whether to subsample columns for each tree,0
accuracy of sketch,0
accuracy of sketch,0
leaf vector size,0
option for parallelization,0
option to open cacheline optimization,0
whether to not print info during training.,0
"get feature type, 0:empty 1:all equal 2:real",0
maximum depth of the tree,0
number of features used for tree construction,0
"minimum loss change required for a split, otherwise stop split",0
----- the rest parameters are less important ----,0
default direction choice,0
whether we want to do sample data,0
whether to sample columns during tree construction,0
whether to use histogram for split,0
number of histogram units,0
whether to print info during training.,0
----- the rest parameters are obtained after training ----,0
total number of nodes,0
number of deleted nodes */,0
set basic configuration keys,0
use local deploy mode and dummy data spliter,0
get a angel client,0
add matrix,0
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
in different part,0
TODO Auto-generated constructor stub,1
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
find the max abs,0
compress data,0
import jdk.nashorn.internal.runtime.regexp.joni.Config;,0
"paras[1] = ""abc"";",0
"paras[2] = ""123"";",0
Add standard Hadoop classes,0
Feature number of train data,0
Total iteration number,0
Learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set input data path,0
Set save model path,0
Set actionType train,0
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
"set input, output path",0
"set angel resource parameters #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Load Model from HDFS.,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"Set trainning data, and save model path",0
Set actionType train,0
Set MF algorithm parameters,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
Load model meta,0
Convert model,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Convert model,0
load hadoop configuration,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model,0
load hadoop configuration,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
worker register,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
attempt 0,0
attempt1,0
attempt1,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
set basic configuration keys,0
"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());",0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
Thread.sleep(5000);,0
"response = master.getJobReport(null, request);",0
"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);",0
"assertEquals(response.getJobReport().getCurIteration(), jobIteration);",0
"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
"DenseIntVector deltaVec = new DenseIntVector(100000, delta);",0
deltaVec.setMatrixId(matrixW1Id);,0
deltaVec.setRowId(0);,0
TODO Auto-generated constructor stub,1
import com.tencent.angel.psagent.consistency.SSPConsistencyController;,0
@RunWith(MockitoJUnitRunner.class),0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);",0
get a angel client,0
add matrix,0
psAgent.initAndStart();,0
test conf,0
test master location,0
test app id,0
test user,0
test ps agent attempt id,0
test ps agent id,0
test connection,0
test master client,0
test ip,0
test loc,0
test master location,0
test ps location,0
"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));",0
test all ps ids,0
test all matrix ids,0
test all matrix names,0
test matrix attribute,0
test matrix meta,0
test ps location,0
test partitions,0
"Note:[startRow,endRow)",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
group0Id = new WorkerGroupId(0);,0
"worker0Id = new WorkerId(group0Id, 0);",0
"worker0Attempt0Id = new WorkerAttemptId(worker0Id, 0);",0
task0Id = new TaskId(0);,0
task1Id = new TaskId(1);,0
LOG.info(serverArbitraryIntRow1.getSparseRep());,0
LOG.info(serverArbitraryIntRow1.getSparseRep());,0
test this func in testWriteTo,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
"LOG.info(index[0] + "" "" + value[0]);",0
"LOG.info(index[1] + "" "" + value[1]);",0
"LOG.info(index[2] + "" "" + value[2]);",0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
dot,0
plusBy,0
@Test,0
public void dotDenseFloatVector() throws Exception {,0
int dim = 1000;,0
Random random = new Random(System.currentTimeMillis());,0
,0
double[] values = new double[dim];,0
float[] values_1 = new float[dim];,0
for (int i = 0; i < dim; i++) {,0
values[i] = random.nextDouble();,0
values_1[i] = random.nextFloat();,0
},0
,0
"DenseDoubleVector vec = new DenseDoubleVector(dim, values);",0
"TDoubleVector vec_1 = new DenseFloatVector(dim, values_1);",0
,0
double sum = 0.0;,0
for (int i = 0; i < dim; i++) {,0
sum += values[i] * values_1[i];,0
},0
,0
"assertEquals(sum, vec.dot(vec_1));",0
,0
},0
@Test,0
public void plusDenseFlaotVector() throws Exception {,0
"double[] value_1 = new double[]{0.1, 0.2, 0.3, 0.4, 0.5};",0
"double[] value_2 = new double[]{0.1f, 0.2f, 0.3f, 0.4f, 0.5f};",0
"DenseDoubleVector vec = new DenseDoubleVector(5, value_1);",0
"TDoubleVector vec_1 = new DenseFloatVector(5, value_2);",0
,0
TDoubleVector vec_2 = vec.plus(vec_1);,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(value_1[i] + value_2[i], vec_2.get(i));",0
,0
,0
"TDoubleVector vec_3 = vec.plus(vec_1, 2.0);",0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec_3.get(i), value_1[i] + 2 * value_2[i]);",0
,0
double[] oldValues = vec.getValues().clone();,0
,0
vec.plusBy(vec_1);,0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec.get(i), oldValues[i] + vec_1.get(i));",0
,0
oldValues = vec.getValues().clone();,0
,0
"vec.plusBy(vec_1, 3);",0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec.get(i), oldValues[i] + 3 * vec_1.get(i));",0
},0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
@Test,0
public void plusBy3() throws Exception {,0
"float[][] value = {{1.0f, 2.0f}, {3.0f, 4.0f}};",0
"DenseFloatMatrix mat = new DenseFloatMatrix(2, 2,value);",0
"TFloatVector vec = new DenseFloatVector(2, new float[]{1.0f, 1.0f});",0
vec.setRowId(0);,0
"TDoubleVector vec_1 = new DenseDoubleVector(2, new double[]{1.0f, 1.0f});",0
vec_1.setRowId(1);,0
TDoubleVector vec_2 = new SparseDoubleVector(2);,0
"vec_2.set(1, 1.0);",0
vec_2.setRowId(0);,0
,0
mat.plusBy(vec);,0
mat.plusBy(vec_1);,0
mat.plusBy(vec_2);,0
,0
"assertEquals(2.0f, mat.get(0, 0));",0
"assertEquals(4.0f, mat.get(0, 1));",0
"assertEquals(4.0f, mat.get(1, 0));",0
"assertEquals(5.0f, mat.get(1, 1));",0
},0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
test worker getActiveTaskNum,0
test worker getTaskNum,0
test worker getTaskManager,0
test workerId,0
test workerAttemptId,0
tet worker initFinished,0
test worker getInitMinclock,0
test worker loacation,0
test AppId,0
test Conf,0
test UserName,0
master location,0
masterClient,0
test psAgent,0
test worker get dataBlockManager,0
workerGroup.getSplits();,0
application,0
lcation,0
workerGroup info,0
worker info,0
task,0
Matrix parameters,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
Create an Angel client,0
Add different types of matrix,0
using mock object,0
verification,0
Stubbing,0
Default does nothing.,0
The app injection is optional,0
"renderText(""hello world"");",0
"user choose a workerGroupID from the workergroups page,",0
now we should change the AngelApp params and render the workergroup page;,0
"static final String WORKER_ID = ""worker.id"";",0
"div(""#logo"").",0
"img(""/static/hadoop-st.png"")._().",0
import org.apache.hadoop.yarn.webapp.view.FooterBlock;,0
import org.apache.hadoop.yarn.webapp.view.HeaderBlock;,0
JQueryUI.jsnotice(html);,0
import org.apache.hadoop.conf.Configuration;,0
import java.lang.reflect.Field;,0
get block locations from file system,0
create a list of all block and their locations,0
"if the file is not splitable, just create the one block with",0
full file length,0
each split can be a maximum of maxSize,0
if remainder is between max and 2*max - then,0
"instead of creating splits of size max, left-max we",0
create splits of size left/2 and left/2. This is,0
a heuristic to avoid creating really really small,0
splits.,0
add this block to the block --> node locations map,0
"For blocks that do not have host/rack information,",0
assign to default  rack.,0
add this block to the rack --> block map,0
Add this host to rackToNodes map,0
add this block to the node --> block map,0
"if the file system does not have any rack information, then",0
use dummy rack location.,0
The topology paths have the host name included as the last,0
component. Strip it.,0
get tokens for all the required FileSystems..,0
"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,",0
job.getConfiguration());,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
all the files in input set,0
it is allowed for maxSize to be 0. Disable smoothing load for such cases,0
process all nodes and create splits that are local to a node. Generate,0
"one split per node iteration, and walk over nodes multiple times to",0
distribute the splits across nodes.,0
Skip the node if it has previously been marked as completed.,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
Remove all blocks which may already have been assigned to other,0
splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
Remove entries from blocksInNode so that we don't walk these,0
again.,0
Done creating a single split for this node. Move on to the next,0
node so that splits are distributed across nodes.,0
This implies that the last few blocks (or all in case maxSize=0),0
were not part of a split. The node is complete.,0
if there were any blocks left over and their combined size is,0
"larger than minSplitNode, then combine them into one split.",0
Otherwise add them back to the unprocessed pool. It is likely,0
that they will be combined with other blocks from the,0
same rack later on.,0
This condition also kicks in when max split size is not set. All,0
blocks on a node will be grouped together into a single split.,0
haven't created any split on this machine. so its ok to add a,0
smaller one for parallelism. Otherwise group it in the rack for,0
balanced size create an input split and add it to the splits,0
array,0
Remove entries from blocksInNode so that we don't walk this again.,0
The node is done. This was the last set of blocks for this node.,0
Put the unplaced blocks back into the pool for later rack-allocation.,0
Node is done. All blocks were fit into node-local splits.,0
Check if node-local assignments are complete.,0
All nodes have been walked over and marked as completed or all blocks,0
have been assigned. The rest should be handled via rackLock assignment.,0
"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """,0
"+ completedNodes.size() + "", size left: "" + totalLength);",0
"if blocks in a rack are below the specified minimum size, then keep them",0
"in 'overflow'. After the processing of all racks is complete, these",0
overflow blocks will be combined into splits.,0
Process all racks over and over again until there is no more work to do.,0
Create one split for this rack before moving over to the next rack.,0
Come back to this rack after creating a single split for each of the,0
remaining racks.,0
"Process one rack location at a time, Combine all possible blocks that",0
reside on this rack as one split. (constrained by minimum and maximum,0
split size).,0
iterate over all racks,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"if we created a split, then just go to the next rack",0
"if there is a minimum size specified, then create a single split",0
"otherwise, store these blocks into overflow data structure",0
There were a few blocks in this rack that,0
remained to be processed. Keep them in 'overflow' block list.,0
These will be combined later.,0
Process all overflow blocks,0
"This might cause an exiting rack location to be re-added,",0
but it should be ok.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"Process any remaining blocks, if any.",0
create an input split,0
add this split to the list that is returned,0
long num = totLength / maxSize;,0
all blocks for all the files in input set,0
mapping from a rack name to the list of blocks it has,0
mapping from a block to the nodes on which it has replicas,0
mapping from a node to the list of blocks that it contains,0
populate all the blocks for all files,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
"Security framework already loaded the tokens into current UGI, just use",0
them,0
Now remove the AM->RM token so tasks don't have it,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init container allocator,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
init parameter server manager,0
recover task information if needed,0
init psagent manager and register psagent manager event,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
start a web service if use yarn deploy mode,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
parse parameter server counters,0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
refresh last heartbeat timestamp,0
send a state update event to the specific PSAttempt,0
check if parameter server can commit now.,0
check matrix metadata inconsistencies between master and parameter server.,0
"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix",0
"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix",0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
check whether psagent heartbeat timeout,0
check whether parameter server heartbeat timeout,0
check whether worker heartbeat timeout,0
choose a unused port,0
start RPC server,0
find matrix partitions from master matrix meta manager for this parameter server,0
remove this parameter server attempt from monitor set,0
remove this parameter server attempt from monitor set,0
"if worker attempt id is not in monitor set, we should shutdown it",0
find workergroup in worker manager,0
"if this worker group does not initialized, just return WORKERGROUP_NOTREADY",0
"if this worker group run over, just return WORKERGROUP_EXITED",0
"if this worker group is running now, return tasks, workers, data splits for it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update the clock for this matrix,0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update task iteration,0
private boolean matrixInited;,0
"check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
dispatch matrix partitions to parameter servers,0
update matrix id generator,0
"check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
dispatch matrix partitions to parameter servers,0
get matrix ids in the parameter server report,0
get the matrices parameter server need to create and delete,0
"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix",0
"if a matrix exists on master but not exist on parameter server, this parameter server need build it.",0
waitForMatrixReleaseOnPS(matrixId);,0
"Calculate how many splits we need. As each task handles a separate split of data, so we want",0
the number of splits equal to the number of tasks,0
split data,0
dispatch the splits to workergroups,0
split data,0
dispatch the splits to workergroups,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
write meta data to a temporary file,0
rename the temporary file to final file,0
"if the file exists, read from file and deserialize it",0
write task meta,0
write ps meta,0
generate a temporary file,0
write task meta to the temporary file first,0
rename the temporary file to the final file,0
"if last final task file exist, remove it",0
find task meta file which has max timestamp,0
"if the file does not exist, just return null",0
read task meta from file and deserialize it,0
generate a temporary file,0
write ps meta to the temporary file first.,0
rename the temporary file to the final file,0
"if the old final file exist, just remove it",0
find ps meta file,0
"if ps meta file does not exist, just return null",0
read ps meta from file and deserialize it,0
Init matrix files meta,0
Move output files,0
Write the meta file,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
Transitions from the NEW state.,0
PA_FAILMSG,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG",0
event,0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event",0
dispatched before PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will",0
retry another attempt or failed,0
release container,0
TODO,1
set the launch time,0
"set tarckerName,httpPort, which used by webserver",0
added to psManager so psManager can monitor it;,0
psAttempt.getContext().getParameterServerManager.registerPSAttempt(psAttempt.attemptId);,0
set the finish time only if launch time is set,0
"ParameterServerJVM.setVMEnv(myEnv, conf);",0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Add the env variables passed by the user,0
Set logging level in the environment.,0
"This is so that, if the child forks another ""bin/hadoop"" (common in",0
streaming) it will have the correct loglevel.,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
Transitions from the NEW state.,0
Transitions from the RUNNING state.,0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
LocalStorageToken is needed irrespective of whether security is enabled,0
or not.,0
"TokenCache.setJobToken(jobToken, taskCredentials);",0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Construct the actual Container,0
The null fields are per-container and will be constructed for each,0
container separately.,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
"a * in the classpath will only find a .jar, so we need to filter out",0
all .jars and add everything else,0
Propagate the system classpath when using the mini cluster,0
Add standard Hadoop classes,0
Cache archives,0
Cache files,0
Sanity check,0
Add URI fragment or just the filename,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Final commmand,0
"if amTask is not null, we should clone task state from it",0
"if all parameter server complete commit, master can commit now",0
init and start master committer,0
Transitions from the NEW state.,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event",0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event dispatched before",0
PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.COMMITTING state,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
reqeuest resource:send a resource request to the resource allocator,0
"Once the resource is applied, build and send the launch request to the container launcher",0
deallocator the resource of the ps attempt:send a resource deallocator request to the,0
resource allocator,0
set the launch time,0
add the ps attempt to the heartbeat timeout monitoring list,0
parse ps attempt location and put it to location manager,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt",0
or failed,0
remove ps attempt id from heartbeat timeout monitor list,0
release container:send a release request to container launcher,0
set the finish time only if launch time is set,0
private long scheduledTime;,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
notify the event handler of state change,0
"If new state is not RUNNING and COMMITTING, add it to state timeout monitor",0
"if forcedState is set, just return",0
else get state from state machine,0
add this worker group to the success set,0
check if all worker group run over,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
calculate the actual number of worker groups and the total number of tasks based on the number of data split,0
"init all tasks , workers and worker groups and put them to the corresponding maps",0
just return the total task number now,0
TODO,1
"if workerAttempt is not null, we should clone task state from it",0
from NEW state,0
from SCHEDULED state,0
get data splits location for data locality,0
reqeuest resource:send a resource request to the resource allocator,0
"once the resource is applied, build and send the launch request to the container launcher",0
notify failed message to the worker,0
notify killed message to the worker,0
release the allocated container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
release the allocated container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
clean the container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
record the finish time,0
clean the container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listening list,0
record the finish time,0
"if the worker attempt launch successfully, add it to heartbeat timeout listening list",0
set worker attempt location,0
notify the register message to the worker,0
record the launch time,0
update worker attempt metrics,0
update tasks metrics,0
clean the container,0
notify the worker attempt run successfully message to the worker,0
record the finish time,0
init a worker attempt for the worker,0
schedule the worker attempt,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
"If we need Yarn to restart a new application master, we should not unregister from Yarn RM",0
register to Yarn RM,0
send heartbeat to Yarn RM every rmPollInterval milliseconds,0
"catch YarnRuntimeException, we should exit and need not retry",0
build heartbeat request,0
send heartbeat request to rm,0
"This can happen if the RM has been restarted. If it is in that state,",0
this application must clean itself up.,0
Setting NMTokens,0
assgin containers,0
"if some container is not assigned, release them",0
handle finish containers,0
dispatch container exit message to corresponding components,0
killed by framework,0
killed by framework,0
killed by framework,0
get application finish state,0
build application diagnostics,0
TODO:add a job history for angel,1
build unregister request,0
send unregister request to rm,0
Note this down for next interaction with ResourceManager,0
based on blacklisting comments above we can end up decrementing more,0
than requested. so guard for that.,0
send the updated resource request to RM,0
send 0 container count requests also to cancel previous requests,0
Update resource requests,0
try to assign to all nodes first to match node local,0
try to match all rack local,0
assign remaining,0
Update resource requests,0
send the container-assigned event to task attempt,0
build the start container request use launch context,0
send the start request to Yarn nm,0
send the message that the container starts successfully to the corresponding component,0
"after launching, send launched event to task attempt to move",0
it from ASSIGNED to RUNNING state,0
send the message that the container starts failed to the corresponding component,0
kill the remote container if already launched,0
start a thread pool to startup the container,0
See if we need up the pool size only if haven't reached the,0
maximum limit yet.,0
nodes where containers will run at *this* point of time. This is,0
*not* the cluster size and doesn't need to be.,0
"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the",0
later is just a buffer so we are not always increasing the,0
pool-size,0
the events from the queue are handled in parallel,0
using a thread pool,0
return if already stopped,0
shutdown any containers that might be left running,0
Build and initialize rpc client to master,0
Build local location,0
"Initialize matrix info, this method will wait until master accepts the information from",0
client,0
Get ps locations from master and put them to the location cache.,0
Initialize matrix meta information,0
Start heartbeat thread if need,0
Start all services,0
Register to master first,0
Report state to master every specified time,0
Notify run success to master only on ANGEL_PS_PSAGENT running mode,0
Stop all modules,0
Exit the process if on ANGEL_PS_PSAGENT mode,0
Notify run success to master only on ANGEL_PS_PSAGENT running mode,0
Stop all modules,0
Exit the process if on ANGEL_PS_PSAGENT mode,0
get configuration from config file,0
set localDir with enviroment set by nm.,0
Update generic resource counters,0
Updating resources specified in ResourceCalculatorProcessTree,0
Remove the CPU time consumed previously by JVM reuse,0
array stores clock for each row and clock,0
local task num,0
mapping from task index to taskId,0
mapping from taskId to task index,0
TODO Auto-generated method stub,1
Generate a flush request and put it to request queue,0
Generate a clock request and put it to request queue,0
Generate a merge request and put it to request queue,0
"If the matrix op log cache does not exist for the matrix, create a new one for the",0
matrix,0
and add it to cache maps,0
Add the message to the tree map,0
"If there are flush / clock requests blocked, we need to put this merge request into",0
the waiting queue,0
Launch a merge worker to merge the update to matrix op log cache,0
Remove the message from the tree map,0
Wake up blocked flush/clock request,0
Add flush/clock request to listener list to waiting for all the existing,0
updates are merged,0
Wake up blocked flush/clock request,0
"If all updates are merged for this matrix, we need wake up flush/clock requests which are",0
blocked.,0
Get next merge message sequence id,0
Wake up listeners(flush/clock requests) that have little sequence id than current merge,0
position,0
Wake up blocked merge requests,0
Get minimal sequence id from listeners,0
"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we",0
should flush updates to local matrix storage,0
unused now,1
Get partitions for the matrix,0
"Filter it, removing zero values",0
Doing average or not,0
Split this row according the matrix partitions,0
Add the splits to the result container,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
int seqId = ((ByteBuf) msg).readInt();,0
"LOG.info(""receive result of seqId="" + seqId);",0
((ByteBuf) msg).resetReaderIndex();,0
TODO: use Epoll for linux future,1
closeChannelForServer(request.getServerId());,0
closeChannelForServer(request.getServerId());,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
Then submit normal task until reach upper limit of flow control or all tasks are submit,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"LOG.info(""choose put server "" + psIds[index]);",0
allocate the bytebuf,0
"check the location of server is ready, if not, we should wait",0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
channelManager.removeChannelPool(loc);,0
find the partition request context from cache,0
Check if the result of the sub-request is received,0
Update received result number,0
Get row splits received,0
Put the row split to the cache(row index to row splits map),0
"If all splits of the row are received, means this row can be merged",0
TODO Auto-generated method stub,1
"Check futures, if the result of a sub-request is received, put it to the result queue",0
Now we just support pipelined row splits merging for dense type row,0
Wait until the clock value of this row is greater than or equal to the value,0
Get partitions for this row,0
First get this row from matrix storage,0
"If the row exists in the matrix storage and the clock value meets the requirements, just",0
return,0
Get row splits of this row from the matrix cache first,0
"If the row split does not exist in cache, get it from parameter server",0
Wait the final result,0
Put it to the matrix cache,0
Split the matrix oplog according to the matrix partitions,0
"If need update clock, we should send requests to all partitions",0
use update index if exist,0
Filter the rowIds which are fetching now,0
Send the rowIndex to rpc dispatcher and return immediately,0
Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition,0
Generate dispatch items and add them to the corresponding queues,0
Pre-fetching is disable default,0
matrix id to clock map,0
"task index, it must be unique for whole application",0
if (matrixManager.getMatrixMeta(createResponse.getMatrixId()) != null) {,0
return matrixManager.getMatrixMeta(createResponse.getMatrixId());,0
},0
Deserialize data splits meta,0
Get workers,0
Send request to every ps,0
Wait the responses,0
Update clock cache,0
Get row from cache.,0
"if row clock is satisfy ssp staleness limit, just return.",0
Get row from ps.,0
"For ASYNC mode, just get from pss.",0
"For BSP/SSP, get rows from storage/cache first",0
Get from ps.,0
"For ASYNC, just get rows from pss.",0
no more retries.,0
calculate sleep time and return.,0
parse the i-th sleep-time,0
parse the i-th number-of-retries,0
calculateSleepTime may overflow.,0
"A few common retry policies, with no delays.",0
close is a local operation and should finish within milliseconds; timeout just to be safe,0
response will be null for one way messages.,0
maxFrameLength = 2G,0
lengthFieldOffset = 0,0
lengthFieldLength = 8,0
"lengthAdjustment = -8, i.e. exclude the 8 byte length itself",0
"initialBytesToStrip = 8, i.e. strip out the length field itself",0
indicates whether this connection's life cycle is managed,0
See if we already have a connection (common case),0
create a unique lock for this RS + protocol (if necessary),0
get the RS lock,0
do one more lookup in case we were stalled above,0
Only create isa when we need to.,0
definitely a cache miss. establish an RPC for,0
this RS,0
Throw what the RemoteException was carrying.,0
check,0
every,0
minutes,0
TODO,1
failoverHandler,0
"The number of times this invocation handler has ever been failed over,",0
before this method invocation attempt. Used to prevent concurrent,0
failed method invocations from triggering multiple failover attempts.,0
Make sure that concurrent failed method invocations,0
only cause a,0
single actual fail over.,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
"LOG.info(""method "" + method.getName() + ""construct request time = """,0
+ (System.currentTimeMillis() - beforeConstructTs));,0
get an instance of the method arg type,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
Message (hand written code usually has only a single,0
argument),0
log any RPC responses that are slower than the configured,0
warn,0
response time or larger than configured warning size,0
"when tagging, we let TooLarge trump TooSmall to keep",0
output simple,0
note that large responses will often also be slow.,0
provides a count of log-reported slow responses,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
unexpected,0
"in the protobuf methods, args[1] is the only significant argument",0
for JSON encoding,0
base information that is reported regardless of type of call,0
Disable Nagle's Algorithm since we don't want packets to wait,0
Configure the event pipeline factory.,0
Make a new connection.,0
Remove all pending requests (will be canceled after relinquishing,0
write lock).,0
Cancel any pending requests by sending errors to the callbacks:,0
Close the channel:,0
Close the connection:,0
Shut down all thread pools to exit.,0
"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());",0
See NettyServer.prepareResponse for where we write out the response.,0
"It writes the call.id (int), a boolean signifying any error (and if",0
"so the exception name/trace), and the response bytes",0
Read the call id.,0
"When the stream is closed, protobuf doesn't raise an EOFException,",0
"instead, it returns a null message object.",0
"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +",0
System.currentTimeMillis());,0
"It would be good widen this to just Throwable, but IOException is what we",0
allow now,0
not implemented,1
not implemented,1
"track what RpcEngine is used by a proxy class, for stopProxy()",0
cache of RpcEngines by protocol,0
return the RpcEngine configured to handle a protocol,0
We only handle the ConnectException.,0
This is the exception we can't handle.,0
check if timed out,0
wait for retry,0
IGNORE,0
return the RpcEngine that handles a proxy object,0
The default implementation works synchronously,0
punt: allocate a new buffer & copy into it,0
Parse cmd parameters,0
load hadoop configuration,0
load angel system configuration,0
load user configuration:,0
load user config file,0
load command line parameters,0
load user job resource files,0
load user job jar if it exist,0
Expand the environment variable,0
Add default fs(local fs) for lib jars.,0
"LOG.info(System.getProperty(""user.dir""));",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
a simple hdfs copy function assume src path and dest path are in same hdfs,0
and FileSystem object has same schema,0
"LOG.warn(""interrupted while sleeping"", ie);",0
private static UnpooledByteBufAllocator allocator = new UnpooledByteBufAllocator(false);,0
public static String getHostname() {,0
try {,0
"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();",0
} catch (UnknownHostException uhe) {,0
},0
"return new StringBuilder().append("""").append(uhe).toString();",0
},0
"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {",0
String hostname = getHostname();,0
String classname = clazz.getSimpleName();,0
"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new",0
"StringBuilder().append(""Starting "").append(classname).toString(), new",0
"StringBuilder().append(""  host = "").append(hostname).toString(), new",0
"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new",0
"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new",0
"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));",0
,0
"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {",0
public void run() {,0
"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +",0
"this.val$classname + "" at "" + this.val$hostname}));",0
},0
});,0
},0
"We we interrupted because we're meant to stop? If not, just",0
continue ignoring the interruption,0
Recalculate waitTime.,0
// Begin delegation to Thread,0
// End delegation to Thread,0
instance submitter class,0
Obtain filename from path,0
Split filename to prexif and suffix (extension),0
Check if the filename is okay,0
Prepare temporary file,0
Prepare buffer for data copying,0
Open and check input stream,0
Open output stream and copy data between source file in JAR and the temporary file,0
"If read/write fails, close streams safely before throwing an exception",0
"Finally, load the library",0
little endian load order,0
tail,0
fallthrough,0
fallthrough,0
finalization,0
fmix(h1);,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
JobStateProto jobState = report.getJobState();,0
the leaf level file should be readable by others,0
the subdirs in the path should have execute permissions for,0
others,0
2.get job id,0
Credentials credentials = new Credentials();,0
4.copy resource files to hdfs,0
5.write configuration to a xml file,0
6.create am container context,0
7.Submit to ResourceManager,0
8.get app master client,0
Create a number of filenames in the JobTracker's fs namespace,0
add all the command line files/ jars and archive,0
first copy them to jobtrackers filesystem,0
should not throw a uri exception,0
should not throw an uri excpetion,0
set the timestamps of the archives and files,0
set the public/private visibility of the archives and files,0
get DelegationToken for each cached file,0
check if we do not need to copy the files,0
is jt using the same file system.,0
just checking for uri strings... doing no dns lookups,0
to see if the filesystems are the same. This is not optimal.,0
but avoids name resolution.,0
this might have name collisions. copy will throw an exception,0
parse the original path to create new path,0
check for ports,0
Write job file to JobTracker's fs,0
Setup resource requirements,0
Setup LocalResources,0
Setup security tokens,0
Setup the command to run the AM,0
Add AM user command opts,0
Final command,0
Setup the CLASSPATH in environment,0
"i.e. add { Hadoop jars, job jar, CWD } to classpath.",0
Setup the environment variables for Admin first,0
"Setup the environment variables (LD_LIBRARY_PATH, etc)",0
Parse distributed cache,0
Setup ContainerLaunchContext for AM container,0
Set up the ApplicationSubmissionContext,0
resposne.encode(buf);,0
TODO:,1
resposne.encode(buf);,0
TODO:,1
resposne.encode(buf);,0
TODO:,1
resposne.encode(buf);,0
TODO:,1
Add tokens to new user so that it may execute its task correctly.,0
to exit,0
private final ParameterServer psServer;,0
TODO,1
"when we should write snapshot to hdfs? clearly, we have two methods:",0
"1. write snapshot at regular time, if there are updates, just write them.",0
"2. write snapshot every N iterations, this method depends on notification of master",0
"FSDataOutputStream output = fileContext.create(snapshotsTempFilePath,",0
EnumSet.of(CreateFlag.CREATE));,0
@brief get filename of the old snapshot written before,0
"no snapshotFile write before, maybe write snapshots the first time",0
data.rewind();,0
data.rewind();,0
data.rewind();,0
output.writeInt(clock);,0
clock = input.readInt();,0
Read matrix meta from meta file,0
Load partitions from file use fork-join,0
Save partitions to files use fork-join,0
Write the ps matrix meta to the meta file,0
Mapping from taskId to clock value.,0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
nnz = 0;,0
for (int i = 0; i < keys.length; i++),0
if (used[i]) {,0
"denseRep.put(keys[i], values[i]);",0
nnz++;,0
},0
sparseRep = null;,0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
for (int i = 0; i < keys.length; i++),0
if (used[i]) {,0
"denseRep.put(keys[i], values[i]);",0
},0
sparseRep = null;,0
output.writeInt(data.length);,0
@Override,0
public void serialize(ByteBuf buf) {,0
if (sparseRep != null),0
return serializeSparse();,0
else if (denseRep != null),0
return serializeDense();,0
return serializeEmpty();,0
},0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
int idx = 0;,0
for (int i = 0; i < keys.length; i++),0
if (used[i]) {,0
"keysBuf.put(idx, keys[i]);",0
"valuesBuf.put(idx, values[i]);",0
idx++;,0
},0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
"int ov, k, v;",0
for (int i = 0; i < keys.length; i++) {,0
if (used[i]) {,0
k = keys[i];,0
ov = denseRep.get(k);,0
v = ov + values[i];,0
"denseRep.put(k, v);",0
if (ov != 0 && v == 0),0
nnz--;,0
},0
},0
"add the PSAgentContext,need fix",0
set MatrixPartitionLocation,0
set attribute,0
return this;,0
return this;,0
return this;,0
TODO:,1
write the max abs,0
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
get configuration from config file,0
set localDir with enviroment set by nm.,0
get master location,0
init task manager and start tasks,0
start heartbeat thread,0
taskManager.assignTaskIds(response.getTaskidsList());,0
todo,1
"if worker timeout, it may be knocked off.",0
"SUCCESS, do nothing",0
heartbeatFailedTime = 0;,0
private KEY currentKey;,0
will be created,0
TODO Auto-generated method stub,1
Bitmap bitmap = new Bitmap();,0
int max = indexArray[size - 1];,0
byte [] bitIndexArray = new byte[max / 8 + 1];,0
for(int i = 0; i < size; i++){,0
int bitIndex = indexArray[i] >> 3;,0
int bitOffset = indexArray[i] - (bitIndex << 3);,0
switch(bitOffset){,0
case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;,0
case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;,0
case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;,0
case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;,0
case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;,0
case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;,0
case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;,0
case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;,0
},0
},0
//////////////////////////////,0
Application Configs,0
//////////////////////////////,0
//////////////////////////////,0
Master Configs,0
//////////////////////////////,0
//////////////////////////////,0
Worker Configs,0
//////////////////////////////,0
//////////////////////////////,0
Task Configs,0
//////////////////////////////,0
//////////////////////////////,0
ParameterServer Configs,0
//////////////////////////////,0
////////////////// IPC //////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
Configs used to ANGEL_PS_PSAGENT running mode future.,0
model parse,0
private Configuration conf;,0
TODO Auto-generated constructor stub,1
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
"set input, output path",0
"set angel resource parameters #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Submit GBDT Train Task,0
Load Model from HDFS.,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"Set trainning data, save model, log path",0
Set actionType train,0
Set MF algorithm parameters,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
LOG.info(sigmoid(data[i]));,0
LOG.info(Math.exp(-data[i]));,0
when b is a negative number,0
LOG.info(sigmoid(data[i]));,0
LOG.info(Math.exp(-data[i]));,0
when b is a negative number,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set MLR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Rank,0
Regularization parameters,0
Learn rage,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set learnType,0
Set feature number,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log sava path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation Ratio,0
Data format,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set basic configuration keys,0
Set data format,0
Use local deploy mode,0
"set angel resource parameters #worker, #task, #PS",0
set sgd SVM algorithm parameters,0
"set input, output path",0
Set save model path,0
Set actionType train,0
Set log path,0
Submit LR Train Task,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log sava path,0
Set actionType prediction,0
double z=pre*y;,0
if(z<=0) return 0.5-z;,0
"else if(z>0 && z<1) return 0.5*Math.pow(1-z,2);",0
return 0.0;,0
logistic loss for binary classification task.,0
"logistic loss, but predict un-transformed margin",0
check if label in range,0
return the default evaluation metric for the objective,0
TODO Auto-generated method stub,1
start row index for words,0
doc ids,0
topic assignments,0
count word,0
build word start index,0
build dks,0
"model.wtMat().increment(w, update);",0
"update.plusBy(t, 1);",0
"model.wtMat().increment(w, update);",0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
Write #rows,0
Write each row,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
loss function,0
gradient and hessian,0
tree node,0
initialize the phase,0
current tree and depth,0
create loss function,0
calculate grad info of each instance,0
"create data sketch, push candidate split value to PS",0
1. calculate candidate split value,0
2. push local sketch to PS,0
3. set phase to GET_SKETCH,0
"pull the global sketch from PS, only called once by each worker",0
sample feature,0
push sampled feature set to the current tree,0
create new tree,0
"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,",0
calculate gradient,0
"1. create new tree, initialize tree nodes and node stats",0
"2. initialize feature set, if sampled, get from PS, otherwise use all the features",0
2.1. pull the sampled features of the current tree,0
"2.2. if use all the features, only called one",0
"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle",0
4. set root node to active,0
"5. reset instance position, set the root node's span",0
6. calculate gradient,0
7. set phase to run active,0
1. start threads of active tree nodes,0
1.1. start threads for active nodes to generate histogram,0
1.2. set thread status to batch num,0
1.3. set the oplog to active,0
"2. check thread stats, if all threads finish, return",0
clock,0
find split,0
"1. find responsible tree node, using RR scheme",0
2. pull gradient histogram,0
2.1. get the name of this node's gradient histogram on PS,0
2.2. pull the histogram,0
histogram = (TDoubleVector) ((GetRowResult) histMat.get(func)).getRow();,0
2.3. find best split result of this tree node,0
2.3.1 using server split,0
"update the grad stats of the root node on PS, only called once by leader worker",0
update the grad stats of children node,0
update the left child,0
update the right child,0
"2.3.2 the updated split result (tree node/feature/value/gain) on PS,",0
"2.3.3 otherwise, the returned histogram contains the gradient info",0
"2.3.4 the updated split result (tree node/feature/value/gain) on PS,",0
2.3.5 reset this tree node's gradient histogram to 0,0
3. push split feature to PS,0
4. push split value to PS,0
5. push split gain to PS,0
6. set phase to AFTER_SPLIT,0
clock,0
1. get split feature,0
2. get split value,0
3. get split gain,0
4. get node weight,0
5. split node,0
"2. check thread stats, if all threads finish, return",0
6. clock,0
"split the span of one node, reset the instance position",0
in case this worker has no instance on this node,0
set the span of left child,0
set the span of right child,0
"1. left to right, find the first instance that should be in the right child",0
"2. right to left, find the first instance that should be in the left child",0
3. swap two instances,0
4. find the cut pos,0
than the split value,0
5. set the span of left child,0
6. set the span of right child,0
set tree node to active,0
set node to leaf,0
set node to inactive,0
finish current tree,0
finish current depth,0
set the tree phase,0
check if there is active node,0
check if finish all the tree,0
update node's grad stats on PS,0
"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split",0
the root node's stats is updated by leader worker,0
1. create the update,0
2. push the update to PS,0
the leader task adds node prediction to flush list,0
1. name of this node's grad histogram on PS,0
2. build the grad histogram of this node,0
3. push the histograms to PS,0
4. reset thread stats to finished,0
5.1. set the children nodes of this node,0
5.2. set split info and grad stats to this node,0
5.2. create children nodes,0
"5.3. create node stats for children nodes, and add them to the tree",0
5.4. reset instance position,0
"5.5. add new active nodes if possible, inc depth, otherwise finish this tree",0
5.6. set children nodes to leaf nodes,0
5.7. set nid to leaf node,0
5.8. deactivate active node,0
"get feature type, 0:empty 1:all equal 2:real",0
feature index used to split,0
feature value used to split,0
loss change after split this node,0
grad stats of the left child,0
grad stats of the right child,0
"LOG.info(""Constructor with fid = -1"");",0
fid = -1: no split currently,0
the minimal split value is the minimal value of feature,0
the splits do not include the maximal value of feature,0
"1. the average distance, (maxValue - minValue) / splitNum",0
2. calculate the candidate split value,0
1. new feature's histogram (grad + hess),0
size: sampled_featureNum * (2 * splitNum),0
"in other words, concatenate each feature's histogram",0
2. get the span of this node,0
int nodeStart = this.controller.nodePosStart[nid];,0
int nodeEnd = this.controller.nodePosEnd[nid];,0
------ 3. using sparse-aware method to build histogram ---,0
"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances",0
3.1. get the instance index,0
3.2. get the grad and hess of the instance,0
3.3. add to the sum,0
3.4. loop the non-zero entries,0
3.4.1. get feature value,0
3.4.2. current feature's position in the sampled feature set,0
3.4.3. find the position of feature value in a histogram,0
"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]",0
3.4.4. add the grad and hess to the corresponding bin,0
3.4.5. add the reverse to the bin that contains 0.0f,0
4. add the grad and hess sum to the zero bin of all features,0
int startIdx = fid * 2 * splitNum;,0
find the best split result of the histogram of a tree node,0
1. calculate the gradStats of the root node,0
"1.1. update the grad stats of the root node on PS, only called once by leader worker",0
2. loop over features,0
2.1. get the ture feature id in the sampled feature set,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"update the grad stats of the root node on PS, only called once by leader worker",0
3. update the grad stats of children node,0
3.1. update the left child,0
3.2. update the right child,0
find the best split result of one feature,0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
split value = sketches[splitIdx+1],0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
partition number,0
cols of each partition,0
"// update the grad stats of the root node on PS, only called once by leader worker",0
if (this.nid == 0) {,0
GradStats rootStats = new GradStats(splitEntry.leftGradStat);,0
rootStats.add(splitEntry.rightGradStat);,0
"this.controller.updateNodeGradStats(this.nid, rootStats);",0
},0
,0
// 3. update the grad stats of children node,0
if (splitEntry.fid != -1) {,0
// 3.1. update the left child,0
"this.controller.updateNodeGradStats(2 * this.nid + 1, splitEntry.leftGradStat);",0
// 3.2. update the right child,0
"this.controller.updateNodeGradStats(2 * this.nid + 2, splitEntry.rightGradStat);",0
},0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
"loop all the possible split value, start from split[1], since the first item is the minimal",0
feature value,0
find the best split result of the histogram of a tree node,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
find the best split result of one feature,0
1. set the feature id,0
splitEntry.setFid(fid);,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]",0
the task use index to find fvalue,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
clear all the information,0
calculate the sum of gradient and hess,0
"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,",0
ridx),0
check if necessary information is ready,0
"same as add, reduce is used in All Reduce",0
"features used in this tree, if equals null, means use all the features without sampling",0
node in the tree,0
the gradient info of each instances,0
initialize nodes,0
gradient,0
second order gradient,0
int sendStartCol = startFid * 7; // each split contains 7 doubles,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"tips: here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]",0
the task use index to find fvalue,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
"System.out.println(""Indices: "" + Arrays.toString(indices));",0
t[i][code]++;,0
else if (Math.random() > 0.5) {,0
t[i][code] = freq;,0
},0
"System.out.println(""Change from "" + t[i][code] + "" to "" + freq);",0
"ret = Math.min(ret, t[i][h[i].encode(key)]);",0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
"quantile sketch, size = featureNum * splitNum",0
"gradient histograms, size = treeNodeNum * featureNum * splitNum",0
"active tree nodes, size = pow(2, treeDepth) -1",0
sampled features. size = treeNum * sampleRatio * featureNum,0
"split features, size = treeNum * treeNodeNum",0
"split values, size = treeNum * treeNodeNum",0
"split gains, size = treeNum * treeNodeNum",0
"node weights, size = treeNum * treeNodeNum",0
"node preds, size = treeNum * treeNodeNum",0
if using PS to perform split,0
step size for a tree,0
number of class,0
minimum loss change required for a split,0
maximum depth of a tree,0
number of features,0
number of nonzero,0
number of candidates split value,0
----- the rest parameters are less important ----,0
base instance weight,0
minimum amount of hessian(weight) allowed in a child,0
L2 regularization factor,0
L1 regularization factor,0
default direction choice,0
maximum delta update we can add in weight estimation,0
this parameter can be used to stabilize update,0
default=0 means no constraint on weight delta,0
whether we want to do subsample for row,0
whether to subsample columns for each tree,0
accuracy of sketch,0
accuracy of sketch,0
leaf vector size,0
option for parallelization,0
option to open cacheline optimization,0
whether to not print info during training.,0
"get feature type, 0:empty 1:all equal 2:real",0
maximum depth of the tree,0
number of features used for tree construction,0
"minimum loss change required for a split, otherwise stop split",0
----- the rest parameters are less important ----,0
default direction choice,0
whether we want to do sample data,0
whether to sample columns during tree construction,0
whether to use histogram for split,0
number of histogram units,0
whether to print info during training.,0
----- the rest parameters are obtained after training ----,0
total number of nodes,0
number of deleted nodes */,0
set basic configuration keys,0
use local deploy mode and dummy data spliter,0
get a angel client,0
add matrix,0
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
in different part,0
TODO Auto-generated constructor stub,1
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
find the max abs,0
compress data,0
import jdk.nashorn.internal.runtime.regexp.joni.Config;,0
"paras[1] = ""abc"";",0
"paras[2] = ""123"";",0
Add standard Hadoop classes,0
Feature number of train data,0
Total iteration number,0
Learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set input data path,0
Set save model path,0
Set actionType train,0
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
"set input, output path",0
"set angel resource parameters #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Load Model from HDFS.,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"Set trainning data, and save model path",0
Set actionType train,0
Set MF algorithm parameters,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
Load model meta,0
Convert model,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Convert model,0
load hadoop configuration,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model,0
load hadoop configuration,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
worker register,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
attempt 0,0
attempt1,0
attempt1,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
set basic configuration keys,0
"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());",0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
Thread.sleep(5000);,0
"response = master.getJobReport(null, request);",0
"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);",0
"assertEquals(response.getJobReport().getCurIteration(), jobIteration);",0
"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
"DenseIntVector deltaVec = new DenseIntVector(100000, delta);",0
deltaVec.setMatrixId(matrixW1Id);,0
deltaVec.setRowId(0);,0
TODO Auto-generated constructor stub,1
import com.tencent.angel.psagent.consistency.SSPConsistencyController;,0
@RunWith(MockitoJUnitRunner.class),0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);",0
get a angel client,0
add matrix,0
psAgent.initAndStart();,0
test conf,0
test master location,0
test app id,0
test user,0
test ps agent attempt id,0
test ps agent id,0
test connection,0
test master client,0
test ip,0
test loc,0
test master location,0
test ps location,0
"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));",0
test all ps ids,0
test all matrix ids,0
test all matrix names,0
test matrix attribute,0
test matrix meta,0
test ps location,0
test partitions,0
"Note:[startRow,endRow)",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
group0Id = new WorkerGroupId(0);,0
"worker0Id = new WorkerId(group0Id, 0);",0
"worker0Attempt0Id = new WorkerAttemptId(worker0Id, 0);",0
task0Id = new TaskId(0);,0
task1Id = new TaskId(1);,0
LOG.info(serverArbitraryIntRow1.getSparseRep());,0
LOG.info(serverArbitraryIntRow1.getSparseRep());,0
test this func in testWriteTo,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
"LOG.info(index[0] + "" "" + value[0]);",0
"LOG.info(index[1] + "" "" + value[1]);",0
"LOG.info(index[2] + "" "" + value[2]);",0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
dot,0
plusBy,0
@Test,0
public void dotDenseFloatVector() throws Exception {,0
int dim = 1000;,0
Random random = new Random(System.currentTimeMillis());,0
,0
double[] values = new double[dim];,0
float[] values_1 = new float[dim];,0
for (int i = 0; i < dim; i++) {,0
values[i] = random.nextDouble();,0
values_1[i] = random.nextFloat();,0
},0
,0
"DenseDoubleVector vec = new DenseDoubleVector(dim, values);",0
"TDoubleVector vec_1 = new DenseFloatVector(dim, values_1);",0
,0
double sum = 0.0;,0
for (int i = 0; i < dim; i++) {,0
sum += values[i] * values_1[i];,0
},0
,0
"assertEquals(sum, vec.dot(vec_1));",0
,0
},0
@Test,0
public void plusDenseFlaotVector() throws Exception {,0
"double[] value_1 = new double[]{0.1, 0.2, 0.3, 0.4, 0.5};",0
"double[] value_2 = new double[]{0.1f, 0.2f, 0.3f, 0.4f, 0.5f};",0
"DenseDoubleVector vec = new DenseDoubleVector(5, value_1);",0
"TDoubleVector vec_1 = new DenseFloatVector(5, value_2);",0
,0
TDoubleVector vec_2 = vec.plus(vec_1);,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(value_1[i] + value_2[i], vec_2.get(i));",0
,0
,0
"TDoubleVector vec_3 = vec.plus(vec_1, 2.0);",0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec_3.get(i), value_1[i] + 2 * value_2[i]);",0
,0
double[] oldValues = vec.getValues().clone();,0
,0
vec.plusBy(vec_1);,0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec.get(i), oldValues[i] + vec_1.get(i));",0
,0
oldValues = vec.getValues().clone();,0
,0
"vec.plusBy(vec_1, 3);",0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec.get(i), oldValues[i] + 3 * vec_1.get(i));",0
},0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
@Test,0
public void plusBy3() throws Exception {,0
"float[][] value = {{1.0f, 2.0f}, {3.0f, 4.0f}};",0
"DenseFloatMatrix mat = new DenseFloatMatrix(2, 2,value);",0
"TFloatVector vec = new DenseFloatVector(2, new float[]{1.0f, 1.0f});",0
vec.setRowId(0);,0
"TDoubleVector vec_1 = new DenseDoubleVector(2, new double[]{1.0f, 1.0f});",0
vec_1.setRowId(1);,0
TDoubleVector vec_2 = new SparseDoubleVector(2);,0
"vec_2.set(1, 1.0);",0
vec_2.setRowId(0);,0
,0
mat.plusBy(vec);,0
mat.plusBy(vec_1);,0
mat.plusBy(vec_2);,0
,0
"assertEquals(2.0f, mat.get(0, 0));",0
"assertEquals(4.0f, mat.get(0, 1));",0
"assertEquals(4.0f, mat.get(1, 0));",0
"assertEquals(5.0f, mat.get(1, 1));",0
},0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
test worker getActiveTaskNum,0
test worker getTaskNum,0
test worker getTaskManager,0
test workerId,0
test workerAttemptId,0
tet worker initFinished,0
test worker getInitMinclock,0
test worker loacation,0
test AppId,0
test Conf,0
test UserName,0
master location,0
masterClient,0
test psAgent,0
test worker get dataBlockManager,0
workerGroup.getSplits();,0
application,0
lcation,0
workerGroup info,0
worker info,0
task,0
Matrix parameters,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
Create an Angel client,0
Add different types of matrix,0
using mock object,0
verification,0
Stubbing,0
Default does nothing.,0
The app injection is optional,0
"renderText(""hello world"");",0
"user choose a workerGroupID from the workergroups page,",0
now we should change the AngelApp params and render the workergroup page;,0
"static final String WORKER_ID = ""worker.id"";",0
"div(""#logo"").",0
"img(""/static/hadoop-st.png"")._().",0
import org.apache.hadoop.yarn.webapp.view.FooterBlock;,0
import org.apache.hadoop.yarn.webapp.view.HeaderBlock;,0
JQueryUI.jsnotice(html);,0
import org.apache.hadoop.conf.Configuration;,0
import java.lang.reflect.Field;,0
get block locations from file system,0
create a list of all block and their locations,0
"if the file is not splitable, just create the one block with",0
full file length,0
each split can be a maximum of maxSize,0
if remainder is between max and 2*max - then,0
"instead of creating splits of size max, left-max we",0
create splits of size left/2 and left/2. This is,0
a heuristic to avoid creating really really small,0
splits.,0
add this block to the block --> node locations map,0
"For blocks that do not have host/rack information,",0
assign to default  rack.,0
add this block to the rack --> block map,0
Add this host to rackToNodes map,0
add this block to the node --> block map,0
"if the file system does not have any rack information, then",0
use dummy rack location.,0
The topology paths have the host name included as the last,0
component. Strip it.,0
get tokens for all the required FileSystems..,0
"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,",0
job.getConfiguration());,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
all the files in input set,0
it is allowed for maxSize to be 0. Disable smoothing load for such cases,0
process all nodes and create splits that are local to a node. Generate,0
"one split per node iteration, and walk over nodes multiple times to",0
distribute the splits across nodes.,0
Skip the node if it has previously been marked as completed.,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
Remove all blocks which may already have been assigned to other,0
splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
Remove entries from blocksInNode so that we don't walk these,0
again.,0
Done creating a single split for this node. Move on to the next,0
node so that splits are distributed across nodes.,0
This implies that the last few blocks (or all in case maxSize=0),0
were not part of a split. The node is complete.,0
if there were any blocks left over and their combined size is,0
"larger than minSplitNode, then combine them into one split.",0
Otherwise add them back to the unprocessed pool. It is likely,0
that they will be combined with other blocks from the,0
same rack later on.,0
This condition also kicks in when max split size is not set. All,0
blocks on a node will be grouped together into a single split.,0
haven't created any split on this machine. so its ok to add a,0
smaller one for parallelism. Otherwise group it in the rack for,0
balanced size create an input split and add it to the splits,0
array,0
Remove entries from blocksInNode so that we don't walk this again.,0
The node is done. This was the last set of blocks for this node.,0
Put the unplaced blocks back into the pool for later rack-allocation.,0
Node is done. All blocks were fit into node-local splits.,0
Check if node-local assignments are complete.,0
All nodes have been walked over and marked as completed or all blocks,0
have been assigned. The rest should be handled via rackLock assignment.,0
"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """,0
"+ completedNodes.size() + "", size left: "" + totalLength);",0
"if blocks in a rack are below the specified minimum size, then keep them",0
"in 'overflow'. After the processing of all racks is complete, these",0
overflow blocks will be combined into splits.,0
Process all racks over and over again until there is no more work to do.,0
Create one split for this rack before moving over to the next rack.,0
Come back to this rack after creating a single split for each of the,0
remaining racks.,0
"Process one rack location at a time, Combine all possible blocks that",0
reside on this rack as one split. (constrained by minimum and maximum,0
split size).,0
iterate over all racks,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"if we created a split, then just go to the next rack",0
"if there is a minimum size specified, then create a single split",0
"otherwise, store these blocks into overflow data structure",0
There were a few blocks in this rack that,0
remained to be processed. Keep them in 'overflow' block list.,0
These will be combined later.,0
Process all overflow blocks,0
"This might cause an exiting rack location to be re-added,",0
but it should be ok.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"Process any remaining blocks, if any.",0
create an input split,0
add this split to the list that is returned,0
long num = totLength / maxSize;,0
all blocks for all the files in input set,0
mapping from a rack name to the list of blocks it has,0
mapping from a block to the nodes on which it has replicas,0
mapping from a node to the list of blocks that it contains,0
populate all the blocks for all files,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
"Security framework already loaded the tokens into current UGI, just use",0
them,0
Now remove the AM->RM token so tasks don't have it,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init container allocator,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
init parameter server manager,0
recover task information if needed,0
init psagent manager and register psagent manager event,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
start a web service if use yarn deploy mode,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
parse parameter server counters,0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
refresh last heartbeat timestamp,0
send a state update event to the specific PSAttempt,0
check if parameter server can commit now.,0
check matrix metadata inconsistencies between master and parameter server.,0
"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix",0
"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix",0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
check whether psagent heartbeat timeout,0
check whether parameter server heartbeat timeout,0
check whether worker heartbeat timeout,0
choose a unused port,0
start RPC server,0
find matrix partitions from master matrix meta manager for this parameter server,0
remove this parameter server attempt from monitor set,0
remove this parameter server attempt from monitor set,0
"if worker attempt id is not in monitor set, we should shutdown it",0
find workergroup in worker manager,0
"if this worker group does not initialized, just return WORKERGROUP_NOTREADY",0
"if this worker group run over, just return WORKERGROUP_EXITED",0
"if this worker group is running now, return tasks, workers, data splits for it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update the clock for this matrix,0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update task iteration,0
private boolean matrixInited;,0
"check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
dispatch matrix partitions to parameter servers,0
update matrix id generator,0
"check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
dispatch matrix partitions to parameter servers,0
get matrix ids in the parameter server report,0
get the matrices parameter server need to create and delete,0
"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix",0
"if a matrix exists on master but not exist on parameter server, this parameter server need build it.",0
waitForMatrixReleaseOnPS(matrixId);,0
"Calculate how many splits we need. As each task handles a separate split of data, so we want",0
the number of splits equal to the number of tasks,0
split data,0
dispatch the splits to workergroups,0
split data,0
dispatch the splits to workergroups,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
write meta data to a temporary file,0
rename the temporary file to final file,0
"if the file exists, read from file and deserialize it",0
write task meta,0
write ps meta,0
generate a temporary file,0
write task meta to the temporary file first,0
rename the temporary file to the final file,0
"if last final task file exist, remove it",0
find task meta file which has max timestamp,0
"if the file does not exist, just return null",0
read task meta from file and deserialize it,0
generate a temporary file,0
write ps meta to the temporary file first.,0
rename the temporary file to the final file,0
"if the old final file exist, just remove it",0
find ps meta file,0
"if ps meta file does not exist, just return null",0
read ps meta from file and deserialize it,0
Init matrix files meta,0
Move output files,0
Write the meta file,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
Transitions from the NEW state.,0
PA_FAILMSG,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG",0
event,0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event",0
dispatched before PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will",0
retry another attempt or failed,0
release container,0
TODO,1
set the launch time,0
"set tarckerName,httpPort, which used by webserver",0
added to psManager so psManager can monitor it;,0
psAttempt.getContext().getParameterServerManager.registerPSAttempt(psAttempt.attemptId);,0
set the finish time only if launch time is set,0
"ParameterServerJVM.setVMEnv(myEnv, conf);",0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Add the env variables passed by the user,0
Set logging level in the environment.,0
"This is so that, if the child forks another ""bin/hadoop"" (common in",0
streaming) it will have the correct loglevel.,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
Transitions from the NEW state.,0
Transitions from the RUNNING state.,0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
LocalStorageToken is needed irrespective of whether security is enabled,0
or not.,0
"TokenCache.setJobToken(jobToken, taskCredentials);",0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Construct the actual Container,0
The null fields are per-container and will be constructed for each,0
container separately.,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
"a * in the classpath will only find a .jar, so we need to filter out",0
all .jars and add everything else,0
Propagate the system classpath when using the mini cluster,0
Add standard Hadoop classes,0
Cache archives,0
Cache files,0
Sanity check,0
Add URI fragment or just the filename,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Final commmand,0
"if amTask is not null, we should clone task state from it",0
"if all parameter server complete commit, master can commit now",0
init and start master committer,0
Transitions from the NEW state.,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event",0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event dispatched before",0
PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.COMMITTING state,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
reqeuest resource:send a resource request to the resource allocator,0
"Once the resource is applied, build and send the launch request to the container launcher",0
deallocator the resource of the ps attempt:send a resource deallocator request to the,0
resource allocator,0
set the launch time,0
add the ps attempt to the heartbeat timeout monitoring list,0
parse ps attempt location and put it to location manager,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt",0
or failed,0
remove ps attempt id from heartbeat timeout monitor list,0
release container:send a release request to container launcher,0
set the finish time only if launch time is set,0
private long scheduledTime;,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
notify the event handler of state change,0
"If new state is not RUNNING and COMMITTING, add it to state timeout monitor",0
"if forcedState is set, just return",0
else get state from state machine,0
add this worker group to the success set,0
check if all worker group run over,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
calculate the actual number of worker groups and the total number of tasks based on the number of data split,0
"init all tasks , workers and worker groups and put them to the corresponding maps",0
just return the total task number now,0
TODO,1
"if workerAttempt is not null, we should clone task state from it",0
from NEW state,0
from SCHEDULED state,0
get data splits location for data locality,0
reqeuest resource:send a resource request to the resource allocator,0
"once the resource is applied, build and send the launch request to the container launcher",0
notify failed message to the worker,0
notify killed message to the worker,0
release the allocated container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
release the allocated container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
clean the container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
record the finish time,0
clean the container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listening list,0
record the finish time,0
"if the worker attempt launch successfully, add it to heartbeat timeout listening list",0
set worker attempt location,0
notify the register message to the worker,0
record the launch time,0
update worker attempt metrics,0
update tasks metrics,0
clean the container,0
notify the worker attempt run successfully message to the worker,0
record the finish time,0
init a worker attempt for the worker,0
schedule the worker attempt,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
"If we need Yarn to restart a new application master, we should not unregister from Yarn RM",0
register to Yarn RM,0
send heartbeat to Yarn RM every rmPollInterval milliseconds,0
"catch YarnRuntimeException, we should exit and need not retry",0
build heartbeat request,0
send heartbeat request to rm,0
"This can happen if the RM has been restarted. If it is in that state,",0
this application must clean itself up.,0
Setting NMTokens,0
assgin containers,0
"if some container is not assigned, release them",0
handle finish containers,0
dispatch container exit message to corresponding components,0
killed by framework,0
killed by framework,0
killed by framework,0
get application finish state,0
build application diagnostics,0
TODO:add a job history for angel,1
build unregister request,0
send unregister request to rm,0
Note this down for next interaction with ResourceManager,0
based on blacklisting comments above we can end up decrementing more,0
than requested. so guard for that.,0
send the updated resource request to RM,0
send 0 container count requests also to cancel previous requests,0
Update resource requests,0
try to assign to all nodes first to match node local,0
try to match all rack local,0
assign remaining,0
Update resource requests,0
send the container-assigned event to task attempt,0
build the start container request use launch context,0
send the start request to Yarn nm,0
send the message that the container starts successfully to the corresponding component,0
"after launching, send launched event to task attempt to move",0
it from ASSIGNED to RUNNING state,0
send the message that the container starts failed to the corresponding component,0
kill the remote container if already launched,0
start a thread pool to startup the container,0
See if we need up the pool size only if haven't reached the,0
maximum limit yet.,0
nodes where containers will run at *this* point of time. This is,0
*not* the cluster size and doesn't need to be.,0
"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the",0
later is just a buffer so we are not always increasing the,0
pool-size,0
the events from the queue are handled in parallel,0
using a thread pool,0
return if already stopped,0
shutdown any containers that might be left running,0
Build and initialize rpc client to master,0
Build local location,0
"Initialize matrix info, this method will wait until master accepts the information from",0
client,0
Get ps locations from master and put them to the location cache.,0
Initialize matrix meta information,0
Start heartbeat thread if need,0
Start all services,0
Register to master first,0
Report state to master every specified time,0
Notify run success to master only on ANGEL_PS_PSAGENT running mode,0
Stop all modules,0
Exit the process if on ANGEL_PS_PSAGENT mode,0
Notify run success to master only on ANGEL_PS_PSAGENT running mode,0
Stop all modules,0
Exit the process if on ANGEL_PS_PSAGENT mode,0
get configuration from config file,0
set localDir with enviroment set by nm.,0
Update generic resource counters,0
Updating resources specified in ResourceCalculatorProcessTree,0
Remove the CPU time consumed previously by JVM reuse,0
array stores clock for each row and clock,0
local task num,0
mapping from task index to taskId,0
mapping from taskId to task index,0
TODO Auto-generated method stub,1
Generate a flush request and put it to request queue,0
Generate a clock request and put it to request queue,0
Generate a merge request and put it to request queue,0
"If the matrix op log cache does not exist for the matrix, create a new one for the",0
matrix,0
and add it to cache maps,0
Add the message to the tree map,0
"If there are flush / clock requests blocked, we need to put this merge request into",0
the waiting queue,0
Launch a merge worker to merge the update to matrix op log cache,0
Remove the message from the tree map,0
Wake up blocked flush/clock request,0
Add flush/clock request to listener list to waiting for all the existing,0
updates are merged,0
Wake up blocked flush/clock request,0
"If all updates are merged for this matrix, we need wake up flush/clock requests which are",0
blocked.,0
Get next merge message sequence id,0
Wake up listeners(flush/clock requests) that have little sequence id than current merge,0
position,0
Wake up blocked merge requests,0
Get minimal sequence id from listeners,0
"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we",0
should flush updates to local matrix storage,0
unused now,1
Get partitions for the matrix,0
"Filter it, removing zero values",0
Doing average or not,0
Split this row according the matrix partitions,0
Add the splits to the result container,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
int seqId = ((ByteBuf) msg).readInt();,0
"LOG.info(""receive result of seqId="" + seqId);",0
((ByteBuf) msg).resetReaderIndex();,0
TODO: use Epoll for linux future,1
closeChannelForServer(request.getServerId());,0
closeChannelForServer(request.getServerId());,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
Then submit normal task until reach upper limit of flow control or all tasks are submit,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"LOG.info(""choose put server "" + psIds[index]);",0
allocate the bytebuf,0
"check the location of server is ready, if not, we should wait",0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
channelManager.removeChannelPool(loc);,0
find the partition request context from cache,0
Check if the result of the sub-request is received,0
Update received result number,0
Get row splits received,0
Put the row split to the cache(row index to row splits map),0
"If all splits of the row are received, means this row can be merged",0
TODO Auto-generated method stub,1
"Check futures, if the result of a sub-request is received, put it to the result queue",0
Now we just support pipelined row splits merging for dense type row,0
Wait until the clock value of this row is greater than or equal to the value,0
Get partitions for this row,0
First get this row from matrix storage,0
"If the row exists in the matrix storage and the clock value meets the requirements, just",0
return,0
Get row splits of this row from the matrix cache first,0
"If the row split does not exist in cache, get it from parameter server",0
Wait the final result,0
Put it to the matrix cache,0
Split the matrix oplog according to the matrix partitions,0
"If need update clock, we should send requests to all partitions",0
use update index if exist,0
Filter the rowIds which are fetching now,0
Send the rowIndex to rpc dispatcher and return immediately,0
Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition,0
Generate dispatch items and add them to the corresponding queues,0
Pre-fetching is disable default,0
matrix id to clock map,0
"task index, it must be unique for whole application",0
if (matrixManager.getMatrixMeta(createResponse.getMatrixId()) != null) {,0
return matrixManager.getMatrixMeta(createResponse.getMatrixId());,0
},0
Deserialize data splits meta,0
Get workers,0
Send request to every ps,0
Wait the responses,0
Update clock cache,0
Get row from cache.,0
"if row clock is satisfy ssp staleness limit, just return.",0
Get row from ps.,0
"For ASYNC mode, just get from pss.",0
"For BSP/SSP, get rows from storage/cache first",0
Get from ps.,0
"For ASYNC, just get rows from pss.",0
no more retries.,0
calculate sleep time and return.,0
parse the i-th sleep-time,0
parse the i-th number-of-retries,0
calculateSleepTime may overflow.,0
"A few common retry policies, with no delays.",0
close is a local operation and should finish within milliseconds; timeout just to be safe,0
response will be null for one way messages.,0
maxFrameLength = 2G,0
lengthFieldOffset = 0,0
lengthFieldLength = 8,0
"lengthAdjustment = -8, i.e. exclude the 8 byte length itself",0
"initialBytesToStrip = 8, i.e. strip out the length field itself",0
indicates whether this connection's life cycle is managed,0
See if we already have a connection (common case),0
create a unique lock for this RS + protocol (if necessary),0
get the RS lock,0
do one more lookup in case we were stalled above,0
Only create isa when we need to.,0
definitely a cache miss. establish an RPC for,0
this RS,0
Throw what the RemoteException was carrying.,0
check,0
every,0
minutes,0
TODO,1
failoverHandler,0
"The number of times this invocation handler has ever been failed over,",0
before this method invocation attempt. Used to prevent concurrent,0
failed method invocations from triggering multiple failover attempts.,0
Make sure that concurrent failed method invocations,0
only cause a,0
single actual fail over.,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
"LOG.info(""method "" + method.getName() + ""construct request time = """,0
+ (System.currentTimeMillis() - beforeConstructTs));,0
get an instance of the method arg type,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
Message (hand written code usually has only a single,0
argument),0
log any RPC responses that are slower than the configured,0
warn,0
response time or larger than configured warning size,0
"when tagging, we let TooLarge trump TooSmall to keep",0
output simple,0
note that large responses will often also be slow.,0
provides a count of log-reported slow responses,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
unexpected,0
"in the protobuf methods, args[1] is the only significant argument",0
for JSON encoding,0
base information that is reported regardless of type of call,0
Disable Nagle's Algorithm since we don't want packets to wait,0
Configure the event pipeline factory.,0
Make a new connection.,0
Remove all pending requests (will be canceled after relinquishing,0
write lock).,0
Cancel any pending requests by sending errors to the callbacks:,0
Close the channel:,0
Close the connection:,0
Shut down all thread pools to exit.,0
"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());",0
See NettyServer.prepareResponse for where we write out the response.,0
"It writes the call.id (int), a boolean signifying any error (and if",0
"so the exception name/trace), and the response bytes",0
Read the call id.,0
"When the stream is closed, protobuf doesn't raise an EOFException,",0
"instead, it returns a null message object.",0
"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +",0
System.currentTimeMillis());,0
"It would be good widen this to just Throwable, but IOException is what we",0
allow now,0
not implemented,1
not implemented,1
"track what RpcEngine is used by a proxy class, for stopProxy()",0
cache of RpcEngines by protocol,0
return the RpcEngine configured to handle a protocol,0
We only handle the ConnectException.,0
This is the exception we can't handle.,0
check if timed out,0
wait for retry,0
IGNORE,0
return the RpcEngine that handles a proxy object,0
The default implementation works synchronously,0
punt: allocate a new buffer & copy into it,0
Parse cmd parameters,0
load hadoop configuration,0
load angel system configuration,0
load user configuration:,0
load user config file,0
load command line parameters,0
load user job resource files,0
load user job jar if it exist,0
Expand the environment variable,0
Add default fs(local fs) for lib jars.,0
"LOG.info(System.getProperty(""user.dir""));",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
a simple hdfs copy function assume src path and dest path are in same hdfs,0
and FileSystem object has same schema,0
"LOG.warn(""interrupted while sleeping"", ie);",0
private static UnpooledByteBufAllocator allocator = new UnpooledByteBufAllocator(false);,0
public static String getHostname() {,0
try {,0
"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();",0
} catch (UnknownHostException uhe) {,0
},0
"return new StringBuilder().append("""").append(uhe).toString();",0
},0
"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {",0
String hostname = getHostname();,0
String classname = clazz.getSimpleName();,0
"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new",0
"StringBuilder().append(""Starting "").append(classname).toString(), new",0
"StringBuilder().append(""  host = "").append(hostname).toString(), new",0
"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new",0
"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new",0
"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));",0
,0
"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {",0
public void run() {,0
"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +",0
"this.val$classname + "" at "" + this.val$hostname}));",0
},0
});,0
},0
"We we interrupted because we're meant to stop? If not, just",0
continue ignoring the interruption,0
Recalculate waitTime.,0
// Begin delegation to Thread,0
// End delegation to Thread,0
instance submitter class,0
Obtain filename from path,0
Split filename to prexif and suffix (extension),0
Check if the filename is okay,0
Prepare temporary file,0
Prepare buffer for data copying,0
Open and check input stream,0
Open output stream and copy data between source file in JAR and the temporary file,0
"If read/write fails, close streams safely before throwing an exception",0
"Finally, load the library",0
little endian load order,0
tail,0
fallthrough,0
fallthrough,0
finalization,0
fmix(h1);,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
JobStateProto jobState = report.getJobState();,0
the leaf level file should be readable by others,0
the subdirs in the path should have execute permissions for,0
others,0
2.get job id,0
Credentials credentials = new Credentials();,0
4.copy resource files to hdfs,0
5.write configuration to a xml file,0
6.create am container context,0
7.Submit to ResourceManager,0
8.get app master client,0
Create a number of filenames in the JobTracker's fs namespace,0
add all the command line files/ jars and archive,0
first copy them to jobtrackers filesystem,0
should not throw a uri exception,0
should not throw an uri excpetion,0
set the timestamps of the archives and files,0
set the public/private visibility of the archives and files,0
get DelegationToken for each cached file,0
check if we do not need to copy the files,0
is jt using the same file system.,0
just checking for uri strings... doing no dns lookups,0
to see if the filesystems are the same. This is not optimal.,0
but avoids name resolution.,0
this might have name collisions. copy will throw an exception,0
parse the original path to create new path,0
check for ports,0
Write job file to JobTracker's fs,0
Setup resource requirements,0
Setup LocalResources,0
Setup security tokens,0
Setup the command to run the AM,0
Add AM user command opts,0
Final command,0
Setup the CLASSPATH in environment,0
"i.e. add { Hadoop jars, job jar, CWD } to classpath.",0
Setup the environment variables for Admin first,0
"Setup the environment variables (LD_LIBRARY_PATH, etc)",0
Parse distributed cache,0
Setup ContainerLaunchContext for AM container,0
Set up the ApplicationSubmissionContext,0
resposne.encode(buf);,0
TODO:,1
resposne.encode(buf);,0
TODO:,1
resposne.encode(buf);,0
TODO:,1
resposne.encode(buf);,0
TODO:,1
Add tokens to new user so that it may execute its task correctly.,0
to exit,0
private final ParameterServer psServer;,0
TODO,1
"when we should write snapshot to hdfs? clearly, we have two methods:",0
"1. write snapshot at regular time, if there are updates, just write them.",0
"2. write snapshot every N iterations, this method depends on notification of master",0
"FSDataOutputStream output = fileContext.create(snapshotsTempFilePath,",0
EnumSet.of(CreateFlag.CREATE));,0
@brief get filename of the old snapshot written before,0
"no snapshotFile write before, maybe write snapshots the first time",0
data.rewind();,0
data.rewind();,0
data.rewind();,0
output.writeInt(clock);,0
clock = input.readInt();,0
Read matrix meta from meta file,0
Load partitions from file use fork-join,0
Save partitions to files use fork-join,0
Write the ps matrix meta to the meta file,0
Mapping from taskId to clock value.,0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
nnz = 0;,0
for (int i = 0; i < keys.length; i++),0
if (used[i]) {,0
"denseRep.put(keys[i], values[i]);",0
nnz++;,0
},0
sparseRep = null;,0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
for (int i = 0; i < keys.length; i++),0
if (used[i]) {,0
"denseRep.put(keys[i], values[i]);",0
},0
sparseRep = null;,0
output.writeInt(data.length);,0
@Override,0
public void serialize(ByteBuf buf) {,0
if (sparseRep != null),0
return serializeSparse();,0
else if (denseRep != null),0
return serializeDense();,0
return serializeEmpty();,0
},0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
int idx = 0;,0
for (int i = 0; i < keys.length; i++),0
if (used[i]) {,0
"keysBuf.put(idx, keys[i]);",0
"valuesBuf.put(idx, values[i]);",0
idx++;,0
},0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
"int ov, k, v;",0
for (int i = 0; i < keys.length; i++) {,0
if (used[i]) {,0
k = keys[i];,0
ov = denseRep.get(k);,0
v = ov + values[i];,0
"denseRep.put(k, v);",0
if (ov != 0 && v == 0),0
nnz--;,0
},0
},0
"add the PSAgentContext,need fix",0
set MatrixPartitionLocation,0
set attribute,0
return this;,0
return this;,0
return this;,0
TODO:,1
write the max abs,0
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
get configuration from config file,0
set localDir with enviroment set by nm.,0
get master location,0
init task manager and start tasks,0
start heartbeat thread,0
taskManager.assignTaskIds(response.getTaskidsList());,0
todo,1
"if worker timeout, it may be knocked off.",0
"SUCCESS, do nothing",0
heartbeatFailedTime = 0;,0
private KEY currentKey;,0
will be created,0
TODO Auto-generated method stub,1
Bitmap bitmap = new Bitmap();,0
int max = indexArray[size - 1];,0
byte [] bitIndexArray = new byte[max / 8 + 1];,0
for(int i = 0; i < size; i++){,0
int bitIndex = indexArray[i] >> 3;,0
int bitOffset = indexArray[i] - (bitIndex << 3);,0
switch(bitOffset){,0
case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;,0
case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;,0
case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;,0
case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;,0
case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;,0
case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;,0
case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;,0
case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;,0
},0
},0
//////////////////////////////,0
Application Configs,0
//////////////////////////////,0
//////////////////////////////,0
Master Configs,0
//////////////////////////////,0
//////////////////////////////,0
Worker Configs,0
//////////////////////////////,0
//////////////////////////////,0
Task Configs,0
//////////////////////////////,0
//////////////////////////////,0
ParameterServer Configs,0
//////////////////////////////,0
////////////////// IPC //////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
Configs used to ANGEL_PS_PSAGENT running mode future.,0
model parse,0
private Configuration conf;,0
TODO Auto-generated constructor stub,1
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
"set input, output path",0
"set angel resource parameters #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Submit GBDT Train Task,0
Load Model from HDFS.,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"Set trainning data, save model, log path",0
Set actionType train,0
Set MF algorithm parameters,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
LOG.info(sigmoid(data[i]));,0
LOG.info(Math.exp(-data[i]));,0
when b is a negative number,0
LOG.info(sigmoid(data[i]));,0
LOG.info(Math.exp(-data[i]));,0
when b is a negative number,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set MLR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Rank,0
Regularization parameters,0
Learn rage,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set learnType,0
Set feature number,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log sava path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation Ratio,0
Data format,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set basic configuration keys,0
Set data format,0
Use local deploy mode,0
"set angel resource parameters #worker, #task, #PS",0
set sgd SVM algorithm parameters,0
"set input, output path",0
Set save model path,0
Set actionType train,0
Set log path,0
Submit LR Train Task,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log sava path,0
Set actionType prediction,0
double z=pre*y;,0
if(z<=0) return 0.5-z;,0
"else if(z>0 && z<1) return 0.5*Math.pow(1-z,2);",0
return 0.0;,0
logistic loss for binary classification task.,0
"logistic loss, but predict un-transformed margin",0
check if label in range,0
return the default evaluation metric for the objective,0
TODO Auto-generated method stub,1
start row index for words,0
doc ids,0
topic assignments,0
count word,0
build word start index,0
build dks,0
"model.wtMat().increment(w, update);",0
"update.plusBy(t, 1);",0
"model.wtMat().increment(w, update);",0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
Write #rows,0
Write each row,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
loss function,0
gradient and hessian,0
tree node,0
initialize the phase,0
current tree and depth,0
create loss function,0
calculate grad info of each instance,0
"create data sketch, push candidate split value to PS",0
1. calculate candidate split value,0
2. push local sketch to PS,0
3. set phase to GET_SKETCH,0
"pull the global sketch from PS, only called once by each worker",0
sample feature,0
push sampled feature set to the current tree,0
create new tree,0
"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,",0
calculate gradient,0
"1. create new tree, initialize tree nodes and node stats",0
"2. initialize feature set, if sampled, get from PS, otherwise use all the features",0
2.1. pull the sampled features of the current tree,0
"2.2. if use all the features, only called one",0
"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle",0
4. set root node to active,0
"5. reset instance position, set the root node's span",0
6. calculate gradient,0
7. set phase to run active,0
1. start threads of active tree nodes,0
1.1. start threads for active nodes to generate histogram,0
1.2. set thread status to batch num,0
1.3. set the oplog to active,0
"2. check thread stats, if all threads finish, return",0
clock,0
find split,0
"1. find responsible tree node, using RR scheme",0
2. pull gradient histogram,0
2.1. get the name of this node's gradient histogram on PS,0
2.2. pull the histogram,0
histogram = (TDoubleVector) ((GetRowResult) histMat.get(func)).getRow();,0
2.3. find best split result of this tree node,0
2.3.1 using server split,0
"update the grad stats of the root node on PS, only called once by leader worker",0
update the grad stats of children node,0
update the left child,0
update the right child,0
"2.3.2 the updated split result (tree node/feature/value/gain) on PS,",0
"2.3.3 otherwise, the returned histogram contains the gradient info",0
"2.3.4 the updated split result (tree node/feature/value/gain) on PS,",0
2.3.5 reset this tree node's gradient histogram to 0,0
3. push split feature to PS,0
4. push split value to PS,0
5. push split gain to PS,0
6. set phase to AFTER_SPLIT,0
clock,0
1. get split feature,0
2. get split value,0
3. get split gain,0
4. get node weight,0
5. split node,0
"2. check thread stats, if all threads finish, return",0
6. clock,0
"split the span of one node, reset the instance position",0
in case this worker has no instance on this node,0
set the span of left child,0
set the span of right child,0
"1. left to right, find the first instance that should be in the right child",0
"2. right to left, find the first instance that should be in the left child",0
3. swap two instances,0
4. find the cut pos,0
than the split value,0
5. set the span of left child,0
6. set the span of right child,0
set tree node to active,0
set node to leaf,0
set node to inactive,0
finish current tree,0
finish current depth,0
set the tree phase,0
check if there is active node,0
check if finish all the tree,0
update node's grad stats on PS,0
"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split",0
the root node's stats is updated by leader worker,0
1. create the update,0
2. push the update to PS,0
the leader task adds node prediction to flush list,0
1. name of this node's grad histogram on PS,0
2. build the grad histogram of this node,0
3. push the histograms to PS,0
4. reset thread stats to finished,0
5.1. set the children nodes of this node,0
5.2. set split info and grad stats to this node,0
5.2. create children nodes,0
"5.3. create node stats for children nodes, and add them to the tree",0
5.4. reset instance position,0
"5.5. add new active nodes if possible, inc depth, otherwise finish this tree",0
5.6. set children nodes to leaf nodes,0
5.7. set nid to leaf node,0
5.8. deactivate active node,0
"get feature type, 0:empty 1:all equal 2:real",0
feature index used to split,0
feature value used to split,0
loss change after split this node,0
grad stats of the left child,0
grad stats of the right child,0
"LOG.info(""Constructor with fid = -1"");",0
fid = -1: no split currently,0
the minimal split value is the minimal value of feature,0
the splits do not include the maximal value of feature,0
"1. the average distance, (maxValue - minValue) / splitNum",0
2. calculate the candidate split value,0
1. new feature's histogram (grad + hess),0
size: sampled_featureNum * (2 * splitNum),0
"in other words, concatenate each feature's histogram",0
2. get the span of this node,0
int nodeStart = this.controller.nodePosStart[nid];,0
int nodeEnd = this.controller.nodePosEnd[nid];,0
------ 3. using sparse-aware method to build histogram ---,0
"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances",0
3.1. get the instance index,0
3.2. get the grad and hess of the instance,0
3.3. add to the sum,0
3.4. loop the non-zero entries,0
3.4.1. get feature value,0
3.4.2. current feature's position in the sampled feature set,0
3.4.3. find the position of feature value in a histogram,0
"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]",0
3.4.4. add the grad and hess to the corresponding bin,0
3.4.5. add the reverse to the bin that contains 0.0f,0
4. add the grad and hess sum to the zero bin of all features,0
int startIdx = fid * 2 * splitNum;,0
find the best split result of the histogram of a tree node,0
1. calculate the gradStats of the root node,0
"1.1. update the grad stats of the root node on PS, only called once by leader worker",0
2. loop over features,0
2.1. get the ture feature id in the sampled feature set,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"update the grad stats of the root node on PS, only called once by leader worker",0
3. update the grad stats of children node,0
3.1. update the left child,0
3.2. update the right child,0
find the best split result of one feature,0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
split value = sketches[splitIdx+1],0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
partition number,0
cols of each partition,0
"// update the grad stats of the root node on PS, only called once by leader worker",0
if (this.nid == 0) {,0
GradStats rootStats = new GradStats(splitEntry.leftGradStat);,0
rootStats.add(splitEntry.rightGradStat);,0
"this.controller.updateNodeGradStats(this.nid, rootStats);",0
},0
,0
// 3. update the grad stats of children node,0
if (splitEntry.fid != -1) {,0
// 3.1. update the left child,0
"this.controller.updateNodeGradStats(2 * this.nid + 1, splitEntry.leftGradStat);",0
// 3.2. update the right child,0
"this.controller.updateNodeGradStats(2 * this.nid + 2, splitEntry.rightGradStat);",0
},0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
"loop all the possible split value, start from split[1], since the first item is the minimal",0
feature value,0
find the best split result of the histogram of a tree node,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
find the best split result of one feature,0
1. set the feature id,0
splitEntry.setFid(fid);,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]",0
the task use index to find fvalue,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
clear all the information,0
calculate the sum of gradient and hess,0
"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,",0
ridx),0
check if necessary information is ready,0
"same as add, reduce is used in All Reduce",0
"features used in this tree, if equals null, means use all the features without sampling",0
node in the tree,0
the gradient info of each instances,0
initialize nodes,0
gradient,0
second order gradient,0
int sendStartCol = startFid * 7; // each split contains 7 doubles,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"tips: here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]",0
the task use index to find fvalue,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
"System.out.println(""Indices: "" + Arrays.toString(indices));",0
t[i][code]++;,0
else if (Math.random() > 0.5) {,0
t[i][code] = freq;,0
},0
"System.out.println(""Change from "" + t[i][code] + "" to "" + freq);",0
"ret = Math.min(ret, t[i][h[i].encode(key)]);",0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
"quantile sketch, size = featureNum * splitNum",0
"gradient histograms, size = treeNodeNum * featureNum * splitNum",0
"active tree nodes, size = pow(2, treeDepth) -1",0
sampled features. size = treeNum * sampleRatio * featureNum,0
"split features, size = treeNum * treeNodeNum",0
"split values, size = treeNum * treeNodeNum",0
"split gains, size = treeNum * treeNodeNum",0
"node weights, size = treeNum * treeNodeNum",0
"node preds, size = treeNum * treeNodeNum",0
if using PS to perform split,0
step size for a tree,0
number of class,0
minimum loss change required for a split,0
maximum depth of a tree,0
number of features,0
number of nonzero,0
number of candidates split value,0
----- the rest parameters are less important ----,0
base instance weight,0
minimum amount of hessian(weight) allowed in a child,0
L2 regularization factor,0
L1 regularization factor,0
default direction choice,0
maximum delta update we can add in weight estimation,0
this parameter can be used to stabilize update,0
default=0 means no constraint on weight delta,0
whether we want to do subsample for row,0
whether to subsample columns for each tree,0
accuracy of sketch,0
accuracy of sketch,0
leaf vector size,0
option for parallelization,0
option to open cacheline optimization,0
whether to not print info during training.,0
"get feature type, 0:empty 1:all equal 2:real",0
maximum depth of the tree,0
number of features used for tree construction,0
"minimum loss change required for a split, otherwise stop split",0
----- the rest parameters are less important ----,0
default direction choice,0
whether we want to do sample data,0
whether to sample columns during tree construction,0
whether to use histogram for split,0
number of histogram units,0
whether to print info during training.,0
----- the rest parameters are obtained after training ----,0
total number of nodes,0
number of deleted nodes */,0
set basic configuration keys,0
use local deploy mode and dummy data spliter,0
get a angel client,0
add matrix,0
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
in different part,0
TODO Auto-generated constructor stub,1
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
find the max abs,0
compress data,0
TODO: a better way is needed to deal with defaultValue,1
TODO: a better way is needed to deal with defaultValue,1
TODO: a better way is needed to deal with defaultValue,1
import jdk.nashorn.internal.runtime.regexp.joni.Config;,0
"paras[1] = ""abc"";",0
"paras[2] = ""123"";",0
Add standard Hadoop classes,0
Feature number of train data,0
Total iteration number,0
Learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set input data path,0
Set save model path,0
Set actionType train,0
Set PS Model values,0
Wait for all tasks finish this clock,0
Get values of index array,0
Set PS Model values,0
Wait for all tasks finish this clock,0
Get values of index array,0
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Set basic configuration keys,0
Use local deploy mode and data format,0
"set input, output path",0
"set angel resource parameters #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Load Model from HDFS.,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"Set trainning data, and save model path",0
Set actionType train,0
Set MF algorithm parameters,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Default location for user home directories #,0
Default value for FS_HOME_DIR_KEY #,0
Default umask for files created in HDFS #,0
Default value for FS_PERMISSIONS_UMASK_KEY #,0
How often does RPC client send pings to RPC server #,0
Default value for IPC_PING_INTERVAL_KEY #,0
Enables pings from RPC client to the server #,0
Default value of IPC_CLIENT_PING_KEY #,0
Responses larger than this will be logged #,0
Default value for IPC_SERVER_RPC_MAX_RESPONSE_SIZE_KEY #,0
Number of threads in RPC server reading from the socket #,0
Default value for IPC_SERVER_RPC_READ_THREADS_KEY #,0
How many calls per handler are allowed in the queue. #,0
Default value for IPC_SERVER_HANDLER_QUEUE_SIZE_KEY #,0
Internal buffer size for Lzo compressordecompressors #/,0
Default value for IO_COMPRESSION_CODEC_LZO_BUFFERSIZE_KEY #,0
This is for specifying the implementation for the mappings from,0
hostnames to the racks they belong to,0
Internal buffer size for Snappy compressordecompressors #/,0
Default value for IO_COMPRESSION_CODEC_SNAPPY_BUFFERSIZE_KEY #,0
Internal buffer size for Snappy compressordecompressors #/,0
Default value for IO_COMPRESSION_CODEC_SNAPPY_BUFFERSIZE_KEY #,0
Service Authorization,0
HA health monitor and failover controller.,0
How often to retry connecting to the service.,0
How often to check the service.,0
How long to sleep after an unexpected RPC error.,0
Timeout for the actual monitorHealth() calls. *,0
Timeout that the FC waits for the new active to become active,0
Timeout that the FC waits for the old active to go to standby,0
FC connection retries for graceful fencing,0
"Timeout that the CLI (manual) FC waits for monitorHealth, getServiceState",0
Static user web-filter properties.,0
See StaticUserWebFilter.,0
EnableDisable aliases serving from jetty,0
Path to the Kerberos ticket cache.  Setting this will force,0
UserGroupInformation to use only this ticket cache file when creating a,0
FileSystem instance.,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
"Input Path, please modify ${YOUR_ANGEL_HOME} as your local angel installation path,",0
"e.g. if your path is /home/angel/angel_1.3.0, your input_path should be:",0
"""file:///home/angel/angel_1.3.0/data/exampledata/GBDTLocalExampleData/agaricus.txt.train"",",0
"and your out_path could be: ""file:///home/angel/angel_1.3.0/data/output""",0
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Set GBDT category feature,0
"set input, output path",0
Set GBDT algorithm parameters,0
Load Model from HDFS.,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Feature number of train data,0
Total iteration number,0
Validation sample ratio,0
"Data format,libsvm or dummy",0
Train batch number per epoch,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
"Set job queue, if you use YARN deploy mode, you can set job queue by",0
"self.conf.set('mapreduce.job.queue.name', 'default')",0
Set local deploy mode,0
Set basic self.configuration keys,0
Set data format,0
"set angel resource parameters #worker, #tast, #ps",0
set sgd LR algorithim parameters # feature # epoch,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Set local deploy mode,0
Set basic self.configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"Set trainning data, and save model path",0
Set actionType train,0
Set MF algorithm parameters,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.,0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
Unless required by applicable law or agreed to in writing] = software distributed under the License is,0
"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Set log path,0
Set actionType train,0
Load Model from HDFS.,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic self.configuration key,0
"Set angel resource parameters #worker, #task, #PS",0
Set Kmeans algorithm parameters #cluster #feature #epoch,0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log sava path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Feature number of train data,0
Total iteration number,0
Rank,0
Regularization parameters,0
Learn rage,0
Set local deploy mode,0
Set basic self.configuration keys,0
"Set angel resource parameters #worker, #task, #PS",0
Set FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set learnType,0
Set feature number,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
If the enviroment has ANGEL_HOME set trust it.,0
Add the path of the PyAngel module if it exists,0
If we are installed in edit mode also look two dirs up,0
Not pip installed no worries,0
If we are installed in edit mode also look two dirs up,0
Not pip installed no worries,0
Normalize the paths,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Get Java HashMap instance which converted from a python dict,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Run ParameterServer  & ParameterServerAgent,0
Only Run ParameterServer,0
Run ParameterServer & Worker(embedded ParameterServerAgent),0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
To Do,0
Modify the way to get current Angel version,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Launch the Py4j gateway,0
Start a socket that will be used by PythonGatewayServer to communicate its port to python sub-proc,0
Don't send ctrl-c / SIGINT to the Java gateway:,0
We use select() here in order to avoid blocking indefinitely if the subprocess dies,0
before connecting,0
Determine which ephemeral port the server started on:,0
Connect to the gateway,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.,0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
Unless required by applicable law or agreed to in writing] = software distributed under the License is,0
"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.,0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
Unless required by applicable law or agreed to in writing] = software distributed under the License is,0
"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
To do: need python edition of TVector,0
To do: need python edition of GetFunc,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Create an angel job client,0
Submit this application,0
Create a KMeans model,0
Load model meta to client,0
Start,0
"Run user task and wait for completion,",0
User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS,0
Save the trained model to HDFS,0
Stop,0
Create an angel job client,0
Submit this application,0
Create KMeans model,0
Add the model meta to client,0
Start,0
"Run user task and wait for completion,",0
User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS,0
Stop,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Training job to obtain a model,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Create an angel job client,0
Submit this application,0
Create a model,0
Load model meta to client,0
Run user task,0
"Wait for completion,",0
User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS,0
Save the incremental trained model to HDFS,0
Stop,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
If the enviroment has ANGEL_HOME set trust it.,0
Add the path of the PyAngel module if it exists,0
If we are installed in edit mode also look two dirs up,0
Not pip installed no worries,0
If we are installed in edit mode also look two dirs up,0
Not pip installed no worries,0
Normalize the paths,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Get Java HashMap instance which converted from a python dict,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Run ParameterServer  & ParameterServerAgent,0
Only Run ParameterServer,0
Run ParameterServer & Worker(embedded ParameterServerAgent),0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
To Do,0
Modify the way to get current Angel version,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Launch the Py4j gateway,0
Start a socket that will be used by PythonGatewayServer to communicate its port to python sub-proc,0
Don't send ctrl-c / SIGINT to the Java gateway:,0
We use select() here in order to avoid blocking indefinitely if the subprocess dies,0
before connecting,0
Determine which ephemeral port the server started on:,0
Connect to the gateway,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.,0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
Unless required by applicable law or agreed to in writing] = software distributed under the License is,0
"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
To do: need python edition of TVector,0
To do: need python edition of GetFunc,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Training job to obtain a model,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Create an angel job client,0
Submit this application,0
Create a model,0
Load model meta to client,0
Run user task,0
"Wait for completion,",0
User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS,0
Save the incremental trained model to HDFS,0
Stop,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Load model meta,0
Convert model,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Convert model,0
load hadoop configuration,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model,0
load hadoop configuration,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setBoolean(AngelConf.ANGEL_PS_HA_USE_EVENT_PUSH, true);",0
"conf.setBoolean(AngelConf.ANGEL_PS_HA_PUSH_SYNC, true);",0
get a angel client,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
worker register,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setBoolean(AngelConf.ANGEL_PS_HA_USE_EVENT_PUSH, true);",0
"conf.setBoolean(AngelConf.ANGEL_PS_HA_PUSH_SYNC, true);",0
get a angel client,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
attempt 0,0
attempt1,0
attempt1,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
set basic configuration keys,0
"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());",0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
Thread.sleep(5000);,0
"response = master.getJobReport(null, request);",0
"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);",0
"assertEquals(response.getJobReport().getCurIteration(), jobIteration);",0
"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
"DenseIntVector deltaVec = new DenseIntVector(100000, delta);",0
deltaVec.setMatrixId(matrixW1Id);,0
deltaVec.setRowId(0);,0
TODO Auto-generated constructor stub,1
import com.tencent.angel.psagent.consistency.SSPConsistencyController;,0
@RunWith(MockitoJUnitRunner.class),0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);",0
get a angel client,0
add matrix,0
psAgent.initAndStart();,0
test conf,0
test master location,0
test app id,0
test user,0
test ps agent attempt id,0
test ps agent id,0
test connection,0
test master client,0
test ip,0
test loc,0
test master location,0
test ps location,0
"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));",0
test all ps ids,0
test all matrix ids,0
test all matrix names,0
test matrix attribute,0
test matrix meta,0
test ps location,0
test partitions,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
group0Id = new WorkerGroupId(0);,0
"worker0Id = new WorkerId(group0Id, 0);",0
"worker0Attempt0Id = new WorkerAttemptId(worker0Id, 0);",0
task0Id = new TaskId(0);,0
task1Id = new TaskId(1);,0
LOG.info(serverArbitraryIntRow1.getSparseRep());,0
LOG.info(serverArbitraryIntRow1.getSparseRep());,0
test this func in testWriteTo,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
"LOG.info(index[0] + "" "" + value[0]);",0
"LOG.info(index[1] + "" "" + value[1]);",0
"LOG.info(index[2] + "" "" + value[2]);",0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
dot,0
plusBy,0
@Test,0
public void dotDenseFloatVector() throws Exception {,0
int dim = 1000;,0
Random random = new Random(System.currentTimeMillis());,0
,0
double[] values = new double[dim];,0
float[] values_1 = new float[dim];,0
for (int i = 0; i < dim; i++) {,0
values[i] = random.nextDouble();,0
values_1[i] = random.nextFloat();,0
},0
,0
"DenseDoubleVector vec = new DenseDoubleVector(dim, values);",0
"TDoubleVector vec_1 = new DenseFloatVector(dim, values_1);",0
,0
double sum = 0.0;,0
for (int i = 0; i < dim; i++) {,0
sum += values[i] * values_1[i];,0
},0
,0
"assertEquals(sum, vec.dot(vec_1));",0
,0
},0
@Test,0
public void plusDenseFlaotVector() throws Exception {,0
"double[] value_1 = new double[]{0.1, 0.2, 0.3, 0.4, 0.5};",0
"double[] value_2 = new double[]{0.1f, 0.2f, 0.3f, 0.4f, 0.5f};",0
"DenseDoubleVector vec = new DenseDoubleVector(5, value_1);",0
"TDoubleVector vec_1 = new DenseFloatVector(5, value_2);",0
,0
TDoubleVector vec_2 = vec.plus(vec_1);,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(value_1[i] + value_2[i], vec_2.get(i));",0
,0
,0
"TDoubleVector vec_3 = vec.plus(vec_1, 2.0);",0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec_3.get(i), value_1[i] + 2 * value_2[i]);",0
,0
double[] oldValues = vec.getValues().clone();,0
,0
vec.plusBy(vec_1);,0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec.get(i), oldValues[i] + vec_1.get(i));",0
,0
oldValues = vec.getValues().clone();,0
,0
"vec.plusBy(vec_1, 3);",0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec.get(i), oldValues[i] + 3 * vec_1.get(i));",0
},0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
@Test,0
public void plusBy3() throws Exception {,0
"float[][] value = {{1.0f, 2.0f}, {3.0f, 4.0f}};",0
"DenseFloatMatrix mat = new DenseFloatMatrix(2, 2,value);",0
"TFloatVector vec = new DenseFloatVector(2, new float[]{1.0f, 1.0f});",0
vec.setRowId(0);,0
"TDoubleVector vec_1 = new DenseDoubleVector(2, new double[]{1.0f, 1.0f});",0
vec_1.setRowId(1);,0
TDoubleVector vec_2 = new SparseDoubleVector(2);,0
"vec_2.set(1, 1.0);",0
vec_2.setRowId(0);,0
,0
mat.plusBy(vec);,0
mat.plusBy(vec_1);,0
mat.plusBy(vec_2);,0
,0
"assertEquals(2.0f, mat.get(0, 0));",0
"assertEquals(4.0f, mat.get(0, 1));",0
"assertEquals(4.0f, mat.get(1, 0));",0
"assertEquals(5.0f, mat.get(1, 1));",0
},0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
Start PS,0
Start to run application,0
Assert.assertTrue(index.length == row.size());,0
Assert.assertTrue(index.length == row.size());,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
test worker getActiveTaskNum,0
test worker getTaskNum,0
test worker getTaskManager,0
test workerId,0
test workerAttemptId,0
tet worker initFinished,0
test worker getInitMinclock,0
test worker loacation,0
test AppId,0
test Conf,0
test UserName,0
master location,0
masterClient,0
test psAgent,0
test worker get dataBlockManager,0
workerGroup.getSplits();,0
application,0
lcation,0
workerGroup info,0
worker info,0
task,0
Matrix parameters,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
Create an Angel client,0
Add different types of matrix,0
using mock object,0
verification,0
Stubbing,0
Default does nothing.,0
The app injection is optional,0
"renderText(""hello world"");",0
"user choose a workerGroupID from the workergroups page,",0
now we should change the AngelApp params and render the workergroup page;,0
"static final String WORKER_ID = ""worker.id"";",0
"div(""#logo"").",0
"img(""/static/hadoop-st.png"")._().",0
import org.apache.hadoop.yarn.webapp.view.FooterBlock;,0
import org.apache.hadoop.yarn.webapp.view.HeaderBlock;,0
JQueryUI.jsnotice(html);,0
import org.apache.hadoop.conf.Configuration;,0
import java.lang.reflect.Field;,0
get block locations from file system,0
create a list of all block and their locations,0
"if the file is not splitable, just create the one block with",0
full file length,0
each split can be a maximum of maxSize,0
if remainder is between max and 2*max - then,0
"instead of creating splits of size max, left-max we",0
create splits of size left/2 and left/2. This is,0
a heuristic to avoid creating really really small,0
splits.,0
add this block to the block --> node locations map,0
"For blocks that do not have host/rack information,",0
assign to default  rack.,0
add this block to the rack --> block map,0
Add this host to rackToNodes map,0
add this block to the node --> block map,0
"if the file system does not have any rack information, then",0
use dummy rack location.,0
The topology paths have the host name included as the last,0
component. Strip it.,0
get tokens for all the required FileSystems..,0
"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,",0
job.getConfiguration());,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
all the files in input set,0
it is allowed for maxSize to be 0. Disable smoothing load for such cases,0
process all nodes and create splits that are local to a node. Generate,0
"one split per node iteration, and walk over nodes multiple times to",0
distribute the splits across nodes.,0
Skip the node if it has previously been marked as completed.,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
Remove all blocks which may already have been assigned to other,0
splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
Remove entries from blocksInNode so that we don't walk these,0
again.,0
Done creating a single split for this node. Move on to the next,0
node so that splits are distributed across nodes.,0
This implies that the last few blocks (or all in case maxSize=0),0
were not part of a split. The node is complete.,0
if there were any blocks left over and their combined size is,0
"larger than minSplitNode, then combine them into one split.",0
Otherwise add them back to the unprocessed pool. It is likely,0
that they will be combined with other blocks from the,0
same rack later on.,0
This condition also kicks in when max split size is not set. All,0
blocks on a node will be grouped together into a single split.,0
haven't created any split on this machine. so its ok to add a,0
smaller one for parallelism. Otherwise group it in the rack for,0
balanced size create an input split and add it to the splits,0
array,0
Remove entries from blocksInNode so that we don't walk this again.,0
The node is done. This was the last set of blocks for this node.,0
Put the unplaced blocks back into the pool for later rack-allocation.,0
Node is done. All blocks were fit into node-local splits.,0
Check if node-local assignments are complete.,0
All nodes have been walked over and marked as completed or all blocks,0
have been assigned. The rest should be handled via rackLock assignment.,0
"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """,0
"+ completedNodes.size() + "", size left: "" + totalLength);",0
"if blocks in a rack are below the specified minimum size, then keep them",0
"in 'overflow'. After the processing of all racks is complete, these",0
overflow blocks will be combined into splits.,0
Process all racks over and over again until there is no more work to do.,0
Create one split for this rack before moving over to the next rack.,0
Come back to this rack after creating a single split for each of the,0
remaining racks.,0
"Process one rack location at a time, Combine all possible blocks that",0
reside on this rack as one split. (constrained by minimum and maximum,0
split size).,0
iterate over all racks,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"if we created a split, then just go to the next rack",0
"if there is a minimum size specified, then create a single split",0
"otherwise, store these blocks into overflow data structure",0
There were a few blocks in this rack that,0
remained to be processed. Keep them in 'overflow' block list.,0
These will be combined later.,0
Process all overflow blocks,0
"This might cause an exiting rack location to be re-added,",0
but it should be ok.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"Process any remaining blocks, if any.",0
create an input split,0
add this split to the list that is returned,0
long num = totLength / maxSize;,0
all blocks for all the files in input set,0
mapping from a rack name to the list of blocks it has,0
mapping from a block to the nodes on which it has replicas,0
mapping from a node to the list of blocks that it contains,0
populate all the blocks for all files,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
"Security framework already loaded the tokens into current UGI, just use",0
them,0
Now remove the AM->RM token so tasks don't have it,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init container allocator,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
init parameter server manager,0
recover task information if needed,0
init psagent manager and register psagent manager event,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
start a web service if use yarn deploy mode,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
parse parameter server counters,0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
refresh last heartbeat timestamp,0
send a state update event to the specific PSAttempt,0
check if parameter server can commit now.,0
Update PS failed counters,0
check matrix metadata inconsistencies between master and parameter server.,0
"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix",0
"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix",0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
check whether psagent heartbeat timeout,0
check whether parameter server heartbeat timeout,0
check whether worker heartbeat timeout,0
choose a unused port,0
start RPC server,0
remove this parameter server attempt from monitor set,0
remove this parameter server attempt from monitor set,0
"if worker attempt id is not in monitor set, we should shutdown it",0
find workergroup in worker manager,0
"if this worker group does not initialized, just return WORKERGROUP_NOTREADY",0
"if this worker group run over, just return WORKERGROUP_EXITED",0
"if this worker group is running now, return tasks, workers, data splits for it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update the clock for this matrix,0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update task iteration,0
"Calculate how many splits we need. As each task handles a separate split of data, so we want",0
the number of splits equal to the number of tasks,0
split data,0
dispatch the splits to workergroups,0
split data,0
dispatch the splits to workergroups,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
write meta data to a temporary file,0
rename the temporary file to final file,0
"if the file exists, read from file and deserialize it",0
write task meta,0
write ps meta,0
generate a temporary file,0
write task meta to the temporary file first,0
rename the temporary file to the final file,0
"if last final task file exist, remove it",0
find task meta file which has max timestamp,0
"if the file does not exist, just return null",0
read task meta from file and deserialize it,0
generate a temporary file,0
write ps meta to the temporary file first.,0
rename the temporary file to the final file,0
"if the old final file exist, just remove it",0
find ps meta file,0
"if ps meta file does not exist, just return null",0
read ps meta from file and deserialize it,0
Init matrix files meta,0
Move output files,0
Write the meta file,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
Transitions from the NEW state.,0
PA_FAILMSG,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG",0
event,0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event",0
dispatched before PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will",0
retry another attempt or failed,0
release container,0
TODO,1
set the launch time,0
"set tarckerName,httpPort, which used by webserver",0
added to psManager so psManager can monitor it;,0
psAttempt.getContext().getParameterServerManager.registerPSAttempt(psAttempt.attemptId);,0
set the finish time only if launch time is set,0
"ParameterServerJVM.setVMEnv(myEnv, conf);",0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Add the env variables passed by the user,0
Set logging level in the environment.,0
"This is so that, if the child forks another ""bin/hadoop"" (common in",0
streaming) it will have the correct loglevel.,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
Transitions from the NEW state.,0
Transitions from the RUNNING state.,0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
LocalStorageToken is needed irrespective of whether security is enabled,0
or not.,0
"TokenCache.setJobToken(jobToken, taskCredentials);",0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Construct the actual Container,0
The null fields are per-container and will be constructed for each,0
container separately.,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
"a * in the classpath will only find a .jar, so we need to filter out",0
all .jars and add everything else,0
Propagate the system classpath when using the mini cluster,0
Add standard Hadoop classes,0
Cache archives,0
Cache files,0
Sanity check,0
Add URI fragment or just the filename,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Final commmand,0
"if amTask is not null, we should clone task state from it",0
"if all parameter server complete commit, master can commit now",0
init and start master committer,0
Transitions from the NEW state.,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event",0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event dispatched before",0
PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.COMMITTING state,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
reqeuest resource:send a resource request to the resource allocator,0
"Once the resource is applied, build and send the launch request to the container launcher",0
deallocator the resource of the ps attempt:send a resource deallocator request to the,0
resource allocator,0
set the launch time,0
add the ps attempt to the heartbeat timeout monitoring list,0
parse ps attempt location and put it to location manager,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt",0
or failed,0
remove ps attempt id from heartbeat timeout monitor list,0
release container:send a release request to container launcher,0
set the finish time only if launch time is set,0
private long scheduledTime;,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
Refresh ps location & matrix meta,0
start a new attempt for this ps,0
notify ps manager,0
"getContext().getLocationManager().setPsLocation(id, null);",0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
notify the event handler of state change,0
"If new state is not RUNNING and COMMITTING, add it to state timeout monitor",0
"if forcedState is set, just return",0
else get state from state machine,0
add this worker group to the success set,0
check if all worker group run over,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
"sb.append(""killed and failed workergroup is over tolerate "").append(tolerateFailedGroup);",0
calculate the actual number of worker groups and the total number of tasks based on the number of data split,0
"init all tasks , workers and worker groups and put them to the corresponding maps",0
just return the total task number now,0
TODO,1
"if workerAttempt is not null, we should clone task state from it",0
from NEW state,0
from SCHEDULED state,0
get data splits location for data locality,0
reqeuest resource:send a resource request to the resource allocator,0
"once the resource is applied, build and send the launch request to the container launcher",0
notify failed message to the worker,0
notify killed message to the worker,0
release the allocated container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
release the allocated container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
clean the container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
record the finish time,0
clean the container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listening list,0
record the finish time,0
"if the worker attempt launch successfully, add it to heartbeat timeout listening list",0
set worker attempt location,0
notify the register message to the worker,0
record the launch time,0
update worker attempt metrics,0
update tasks metrics,0
clean the container,0
notify the worker attempt run successfully message to the worker,0
record the finish time,0
init a worker attempt for the worker,0
schedule the worker attempt,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
"If we need Yarn to restart a new application master, we should not unregister from Yarn RM",0
register to Yarn RM,0
send heartbeat to Yarn RM every rmPollInterval milliseconds,0
"catch YarnRuntimeException, we should exit and need not retry",0
build heartbeat request,0
send heartbeat request to rm,0
"This can happen if the RM has been restarted. If it is in that state,",0
this application must clean itself up.,0
Setting NMTokens,0
assgin containers,0
"if some container is not assigned, release them",0
handle finish containers,0
dispatch container exit message to corresponding components,0
killed by framework,0
killed by framework,0
killed by framework,0
get application finish state,0
build application diagnostics,0
TODO:add a job history for angel,1
build unregister request,0
send unregister request to rm,0
Note this down for next interaction with ResourceManager,0
based on blacklisting comments above we can end up decrementing more,0
than requested. so guard for that.,0
send the updated resource request to RM,0
send 0 container count requests also to cancel previous requests,0
Update resource requests,0
try to assign to all nodes first to match node local,0
try to match all rack local,0
assign remaining,0
Update resource requests,0
send the container-assigned event to task attempt,0
build the start container request use launch context,0
send the start request to Yarn nm,0
send the message that the container starts successfully to the corresponding component,0
"after launching, send launched event to task attempt to move",0
it from ASSIGNED to RUNNING state,0
send the message that the container starts failed to the corresponding component,0
kill the remote container if already launched,0
start a thread pool to startup the container,0
See if we need up the pool size only if haven't reached the,0
maximum limit yet.,0
nodes where containers will run at *this* point of time. This is,0
*not* the cluster size and doesn't need to be.,0
"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the",0
later is just a buffer so we are not always increasing the,0
pool-size,0
the events from the queue are handled in parallel,0
using a thread pool,0
return if already stopped,0
shutdown any containers that might be left running,0
"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
get matrix ids in the parameter server report,0
get the matrices parameter server need to create and delete,0
"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix",0
"if a matrix exists on master but not exist on parameter server, this parameter server need build it.",0
Get ps locations from master and put them to the location cache.,0
Build and initialize rpc client to master,0
Build local location,0
Initialize matrix meta information,0
Start heartbeat thread if need,0
Start all services,0
Register to master first,0
Report state to master every specified time,0
Notify run success to master only on ANGEL_PS_PSAGENT running mode,0
Stop all modules,0
Exit the process if on ANGEL_PS_PSAGENT mode,0
Notify run success to master only on ANGEL_PS_PSAGENT running mode,0
Stop all modules,0
Exit the process if on ANGEL_PS_PSAGENT mode,0
get configuration from config file,0
set localDir with enviroment set by nm.,0
Update generic resource counters,0
Updating resources specified in ResourceCalculatorProcessTree,0
Remove the CPU time consumed previously by JVM reuse,0
Generate a flush request and put it to request queue,0
Generate a clock request and put it to request queue,0
Generate a merge request and put it to request queue,0
"If the matrix op log cache does not exist for the matrix, create a new one for the",0
matrix,0
and add it to cache maps,0
Add the message to the tree map,0
"If there are flush / clock requests blocked, we need to put this merge request into",0
the waiting queue,0
Launch a merge worker to merge the update to matrix op log cache,0
Remove the message from the tree map,0
Wake up blocked flush/clock request,0
Add flush/clock request to listener list to waiting for all the existing,0
updates are merged,0
Wake up blocked flush/clock request,0
"If all updates are merged for this matrix, we need wake up flush/clock requests which are",0
blocked.,0
Get next merge message sequence id,0
Wake up listeners(flush/clock requests) that have little sequence id than current merge,0
position,0
Wake up blocked merge requests,0
Get minimal sequence id from listeners,0
"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we",0
should flush updates to local matrix storage,0
unused now,1
"Filter it, removing zero values",0
Doing average or not,0
Split this row according the matrix partitions,0
Add the splits to the result container,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
int seqId = ((ByteBuf) msg).readInt();,0
"LOG.info(""receive result of seqId="" + seqId);",0
((ByteBuf) msg).resetReaderIndex();,0
TODO: use Epoll for linux future,1
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
Then submit normal task until reach upper limit of flow control or all tasks are submit,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"LOG.info(""choose put server "" + psIds[index]);",0
"check the location of server is ready, if not, we should wait",0
allocate the bytebuf,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
channelManager.removeChannelPool(loc);,0
find the partition request context from cache,0
Check if the result of the sub-request is received,0
Update received result number,0
Get row splits received,0
Put the row split to the cache(row index to row splits map),0
"If all splits of the row are received, means this row can be merged",0
TODO Auto-generated method stub,1
"Check futures, if the result of a sub-request is received, put it to the result queue",0
Now we just support pipelined row splits merging for dense type row,0
Wait until the clock value of this row is greater than or equal to the value,0
Get partitions for this row,0
First get this row from matrix storage,0
"If the row exists in the matrix storage and the clock value meets the requirements, just",0
return,0
Get row splits of this row from the matrix cache first,0
"If the row split does not exist in cache, get it from parameter server",0
Wait the final result,0
Put it to the matrix cache,0
Split the matrix oplog according to the matrix partitions,0
"If need update clock, we should send requests to all partitions",0
Filter the rowIds which are fetching now,0
Send the rowIndex to rpc dispatcher and return immediately,0
Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition,0
Generate dispatch items and add them to the corresponding queues,0
Filter the rowIds which are fetching now,0
Pre-fetching is disable default,0
matrix id to clock map,0
"task index, it must be unique for whole application",0
Deserialize data splits meta,0
Get workers,0
Send request to every ps,0
Wait the responses,0
Update clock cache,0
if(syncNum % 1024 == 0) {,0
},0
Get row from cache.,0
"if row clock is satisfy ssp staleness limit, just return.",0
Get row from ps.,0
"For ASYNC mode, just get from pss.",0
"For BSP/SSP, get rows from storage/cache first",0
Get from ps.,0
"For ASYNC, just get rows from pss.",0
no more retries.,0
calculate sleep time and return.,0
parse the i-th sleep-time,0
parse the i-th number-of-retries,0
calculateSleepTime may overflow.,0
"A few common retry policies, with no delays.",0
close is a local operation and should finish within milliseconds; timeout just to be safe,0
response will be null for one way messages.,0
maxFrameLength = 2G,0
lengthFieldOffset = 0,0
lengthFieldLength = 8,0
"lengthAdjustment = -8, i.e. exclude the 8 byte length itself",0
"initialBytesToStrip = 8, i.e. strip out the length field itself",0
indicates whether this connection's life cycle is managed,0
See if we already have a connection (common case),0
create a unique lock for this RS + protocol (if necessary),0
get the RS lock,0
do one more lookup in case we were stalled above,0
Only create isa when we need to.,0
definitely a cache miss. establish an RPC for,0
this RS,0
Throw what the RemoteException was carrying.,0
check,0
every,0
minutes,0
TODO,1
failoverHandler,0
"The number of times this invocation handler has ever been failed over,",0
before this method invocation attempt. Used to prevent concurrent,0
failed method invocations from triggering multiple failover attempts.,0
Make sure that concurrent failed method invocations,0
only cause a,0
single actual fail over.,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
"LOG.info(""method "" + method.getName() + ""construct request time = """,0
+ (System.currentTimeMillis() - beforeConstructTs));,0
get an instance of the method arg type,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
Message (hand written code usually has only a single,0
argument),0
log any RPC responses that are slower than the configured,0
warn,0
response time or larger than configured warning size,0
"when tagging, we let TooLarge trump TooSmall to keep",0
output simple,0
note that large responses will often also be slow.,0
provides a count of log-reported slow responses,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
unexpected,0
"in the protobuf methods, args[1] is the only significant argument",0
for JSON encoding,0
base information that is reported regardless of type of call,0
Disable Nagle's Algorithm since we don't want packets to wait,0
Configure the event pipeline factory.,0
Make a new connection.,0
Remove all pending requests (will be canceled after relinquishing,0
write lock).,0
Cancel any pending requests by sending errors to the callbacks:,0
Close the channel:,0
Close the connection:,0
Shut down all thread pools to exit.,0
"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());",0
See NettyServer.prepareResponse for where we write out the response.,0
"It writes the call.id (int), a boolean signifying any error (and if",0
"so the exception name/trace), and the response bytes",0
Read the call id.,0
"When the stream is closed, protobuf doesn't raise an EOFException,",0
"instead, it returns a null message object.",0
"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +",0
System.currentTimeMillis());,0
"It would be good widen this to just Throwable, but IOException is what we",0
allow now,0
not implemented,1
not implemented,1
"track what RpcEngine is used by a proxy class, for stopProxy()",0
cache of RpcEngines by protocol,0
return the RpcEngine configured to handle a protocol,0
We only handle the ConnectException.,0
This is the exception we can't handle.,0
check if timed out,0
wait for retry,0
IGNORE,0
return the RpcEngine that handles a proxy object,0
The default implementation works synchronously,0
punt: allocate a new buffer & copy into it,0
Parse cmd parameters,0
load hadoop configuration,0
load angel system configuration,0
load user configuration:,0
load user config file,0
load command line parameters,0
load user job resource files,0
load user job jar if it exist,0
Expand the environment variable,0
Add default fs(local fs) for lib jars.,0
"LOG.info(System.getProperty(""user.dir""));",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
a simple hdfs copy function assume src path and dest path are in same hdfs,0
and FileSystem object has same schema,0
"LOG.warn(""interrupted while sleeping"", ie);",0
private static UnpooledByteBufAllocator allocator = new UnpooledByteBufAllocator(false);,0
public static String getHostname() {,0
try {,0
"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();",0
} catch (UnknownHostException uhe) {,0
},0
"return new StringBuilder().append("""").append(uhe).toString();",0
},0
"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {",0
String hostname = getHostname();,0
String classname = clazz.getSimpleName();,0
"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new",0
"StringBuilder().append(""Starting "").append(classname).toString(), new",0
"StringBuilder().append(""  host = "").append(hostname).toString(), new",0
"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new",0
"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new",0
"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));",0
,0
"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {",0
public void run() {,0
"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +",0
"this.val$classname + "" at "" + this.val$hostname}));",0
},0
});,0
},0
"We we interrupted because we're meant to stop? If not, just",0
continue ignoring the interruption,0
Recalculate waitTime.,0
// Begin delegation to Thread,0
// End delegation to Thread,0
instance submitter class,0
Obtain filename from path,0
Split filename to prexif and suffix (extension),0
Check if the filename is okay,0
Prepare temporary file,0
Prepare buffer for data copying,0
Open and check input stream,0
Open output stream and copy data between source file in JAR and the temporary file,0
"If read/write fails, close streams safely before throwing an exception",0
"Finally, load the library",0
little endian load order,0
tail,0
fallthrough,0
fallthrough,0
finalization,0
fmix(h1);,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
JobStateProto jobState = report.getJobState();,0
Used for java code to get a AngelClient instance,0
Used for python code to get a AngelClient instance,0
the leaf level file should be readable by others,0
the subdirs in the path should have execute permissions for,0
others,0
2.get job id,0
Credentials credentials = new Credentials();,0
4.copy resource files to hdfs,0
5.write configuration to a xml file,0
6.create am container context,0
7.Submit to ResourceManager,0
8.get app master client,0
Create a number of filenames in the JobTracker's fs namespace,0
add all the command line files/ jars and archive,0
first copy them to jobtrackers filesystem,0
should not throw a uri exception,0
should not throw an uri excpetion,0
set the timestamps of the archives and files,0
set the public/private visibility of the archives and files,0
get DelegationToken for each cached file,0
check if we do not need to copy the files,0
is jt using the same file system.,0
just checking for uri strings... doing no dns lookups,0
to see if the filesystems are the same. This is not optimal.,0
but avoids name resolution.,0
this might have name collisions. copy will throw an exception,0
parse the original path to create new path,0
check for ports,0
Write job file to JobTracker's fs,0
Setup resource requirements,0
Setup LocalResources,0
Setup security tokens,0
Setup the command to run the AM,0
Add AM user command opts,0
Final command,0
Setup the CLASSPATH in environment,0
"i.e. add { Hadoop jars, job jar, CWD } to classpath.",0
Setup the environment variables for Admin first,0
"Setup the environment variables (LD_LIBRARY_PATH, etc)",0
Parse distributed cache,0
Setup ContainerLaunchContext for AM container,0
Set up the ApplicationSubmissionContext,0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this location, only master ps can accept the update",0
Check the partition state again,0
Start to put the update to the slave pss,0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this partition, if not, just return failed",0
Start to put the update to the slave pss,0
resposne.encode(buf);,0
TODO:,1
resposne.encode(buf);,0
TODO:,1
"context.getMatrixStorageManager().update(partKey, in);",0
resposne.encode(buf);,0
TODO:,1
resposne.encode(buf);,0
TODO:,1
Add tokens to new user so that it may execute its task correctly.,0
to exit,0
context.getSnapshotManager().processRecovery();,0
private final ParameterServer psServer;,0
data.rewind();,0
data.rewind();,0
data.rewind();,0
output.writeInt(clock);,0
clock = input.readInt();,0
private final List<PartitionKey> partitionKeys;,0
Read matrix meta from meta file,0
Load partitions from file use fork-join,0
Save partitions to files use fork-join,0
Write the ps matrix meta to the meta file,0
Mapping from taskId to clock value.,0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
nnz = 0;,0
for (int i = 0; i < keys.length; i++),0
if (used[i]) {,0
"denseRep.put(keys[i], values[i]);",0
nnz++;,0
},0
sparseRep = null;,0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
for (int i = 0; i < keys.length; i++),0
if (used[i]) {,0
"denseRep.put(keys[i], values[i]);",0
},0
sparseRep = null;,0
output.writeInt(data.length);,0
@Override,0
public void serialize(ByteBuf buf) {,0
if (sparseRep != null),0
return serializeSparse();,0
else if (denseRep != null),0
return serializeDense();,0
return serializeEmpty();,0
},0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
int idx = 0;,0
for (int i = 0; i < keys.length; i++),0
if (used[i]) {,0
"keysBuf.put(idx, keys[i]);",0
"valuesBuf.put(idx, values[i]);",0
idx++;,0
},0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
"int ov, k, v;",0
for (int i = 0; i < keys.length; i++) {,0
if (used[i]) {,0
k = keys[i];,0
ov = denseRep.get(k);,0
v = ov + values[i];,0
"denseRep.put(k, v);",0
if (ov != 0 && v == 0),0
nnz--;,0
},0
},0
TODO: use Epoll for linux future,1
find the partition request context from cache,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
channelManager.removeChannelPool(loc);,0
Generate seq id,0
Create a RecoverPartRequest,0
Serialize the request,0
Change the seqId for the request,0
Serialize the request,0
"add the PSAgentContext,need fix",0
return this;,0
return this;,0
return this;,0
TODO Should be implemented,1
TODO Should be implemented,1
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
TODO:,1
protected ParameterServerId psId;,0
protected Location location;,0
if(comeFromPs) {,0
buf.writeInt(psId.getIndex());,0
byte[] data = location.getIp().getBytes();,0
buf.writeInt(data.length);,0
buf.writeBytes(data);,0
buf.writeInt(location.getPort());,0
},0
if(comeFromPs) {,0
psId = new ParameterServerId(buf.readInt());,0
int size = buf.readInt();,0
byte[] data = new byte[size];,0
buf.readBytes(data);,0
"location = new Location(new String(data), buf.readInt());",0
},0
write the max abs,0
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
get configuration from config file,0
set localDir with enviroment set by nm.,0
get master location,0
init task manager and start tasks,0
start heartbeat thread,0
taskManager.assignTaskIds(response.getTaskidsList());,0
todo,1
"if worker timeout, it may be knocked off.",0
"SUCCESS, do nothing",0
heartbeatFailedTime = 0;,0
private KEY currentKey;,0
will be created,0
TODO Auto-generated method stub,1
Bitmap bitmap = new Bitmap();,0
int max = indexArray[size - 1];,0
byte [] bitIndexArray = new byte[max / 8 + 1];,0
for(int i = 0; i < size; i++){,0
int bitIndex = indexArray[i] >> 3;,0
int bitOffset = indexArray[i] - (bitIndex << 3);,0
switch(bitOffset){,0
case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;,0
case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;,0
case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;,0
case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;,0
case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;,0
case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;,0
case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;,0
case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;,0
},0
},0
//////////////////////////////,0
Application Configs,0
//////////////////////////////,0
//////////////////////////////,0
Master Configs,0
//////////////////////////////,0
//////////////////////////////,0
Worker Configs,0
//////////////////////////////,0
//////////////////////////////,0
Task Configs,0
//////////////////////////////,0
//////////////////////////////,0
ParameterServer Configs,0
//////////////////////////////,0
////////////////// IPC //////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
Configs used to ANGEL_PS_PSAGENT running mode future.,0
model parse,0
Mark whether use pyangel or not.,0
private Configuration conf;,0
"Configuration that should be used in python environment, there should only be one",0
configuration instance in each Angel context.,0
Use private access means jconf should not be changed or modified in this way.,0
Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:,0
Do nothing,0
To-DO: add other ways to justify different value types,0
"This is so ugly, must re-implement by more elegance way",1
"Create python path which include angel's jars, the python directory in ANGEL_HOME,",0
and other files submitted by user.,0
Launch python process,0
TODO Auto-generated constructor stub,1
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
"set input, output path",0
"set angel resource parameters #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Submit GBDT Train Task,0
Load Model from HDFS.,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"Set trainning data, save model, log path",0
Set actionType train,0
Set MF algorithm parameters,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
LOG.info(sigmoid(data[i]));,0
LOG.info(Math.exp(-data[i]));,0
when b is a negative number,0
LOG.info(sigmoid(data[i]));,0
LOG.info(Math.exp(-data[i]));,0
when b is a negative number,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set MLR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Rank,0
Regularization parameters,0
Learn rage,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set learnType,0
Set feature number,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log sava path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation Ratio,0
Data format,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set basic configuration keys,0
Set data format,0
Use local deploy mode,0
"set angel resource parameters #worker, #task, #PS",0
set sgd SVM algorithm parameters,0
"set input, output path",0
Set save model path,0
Set actionType train,0
Set log path,0
Submit LR Train Task,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log sava path,0
Set actionType prediction,0
double z=pre*y;,0
if(z<=0) return 0.5-z;,0
"else if(z>0 && z<1) return 0.5*Math.pow(1-z,2);",0
return 0.0;,0
logistic loss for binary classification task.,0
"logistic loss, but predict un-transformed margin",0
check if label in range,0
return the default evaluation metric for the objective,0
TODO Auto-generated method stub,1
start row index for words,0
doc ids,0
topic assignments,0
count word,0
build word start index,0
build dks,0
"model.wtMat().increment(w, update);",0
"update.plusBy(t, 1);",0
"model.wtMat().increment(w, update);",0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
Write #rows,0
Write each row,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
loss function,0
gradient and hessian,0
tree node,0
initialize the phase,0
current tree and depth,0
create loss function,0
calculate grad info of each instance,0
"create data sketch, push candidate split value to PS",0
1. calculate candidate split value,0
categorical features,0
2. push local sketch to PS,0
3. set phase to GET_SKETCH,0
the leader worker,0
merge categorical features,0
create updates,0
"pull the global sketch from PS, only called once by each worker",0
number of categorical feature,0
sample feature,0
push sampled feature set to the current tree,0
create new tree,0
"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,",0
calculate gradient,0
"1. create new tree, initialize tree nodes and node stats",0
"2. initialize feature set, if sampled, get from PS, otherwise use all the features",0
2.1. pull the sampled features of the current tree,0
"2.2. if use all the features, only called one",0
"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle",0
4. set root node to active,0
"5. reset instance position, set the root node's span",0
6. calculate gradient,0
7. set phase to run active,0
1. start threads of active tree nodes,0
1.1. start threads for active nodes to generate histogram,0
1.2. set thread status to batch num,0
1.3. set the oplog to active,0
"2. check thread stats, if all threads finish, return",0
clock,0
find split,0
"1. find responsible tree node, using RR scheme",0
2. pull gradient histogram,0
2.1. get the name of this node's gradient histogram on PS,0
2.2. pull the histogram,0
histogram = (TDoubleVector) ((GetRowResult) histMat.get(func)).getRow();,0
2.3. find best split result of this tree node,0
2.3.1 using server split,0
"update the grad stats of the root node on PS, only called once by leader worker",0
update the grad stats of children node,0
update the left child,0
update the right child,0
"2.3.2 the updated split result (tree node/feature/value/gain) on PS,",0
"2.3.3 otherwise, the returned histogram contains the gradient info",0
"2.3.4 the updated split result (tree node/feature/value/gain) on PS,",0
2.3.5 reset this tree node's gradient histogram to 0,0
3. push split feature to PS,0
4. push split value to PS,0
5. push split gain to PS,0
6. set phase to AFTER_SPLIT,0
clock,0
1. get split feature,0
2. get split value,0
3. get split gain,0
4. get node weight,0
5. split node,0
"2. check thread stats, if all threads finish, return",0
6. clock,0
"split the span of one node, reset the instance position",0
in case this worker has no instance on this node,0
set the span of left child,0
set the span of right child,0
"1. left to right, find the first instance that should be in the right child",0
"2. right to left, find the first instance that should be in the left child",0
3. swap two instances,0
4. find the cut pos,0
than the split value,0
5. set the span of left child,0
6. set the span of right child,0
set tree node to active,0
set node to leaf,0
set node to inactive,0
finish current tree,0
finish current depth,0
set the tree phase,0
check if there is active node,0
check if finish all the tree,0
update node's grad stats on PS,0
"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split",0
the root node's stats is updated by leader worker,0
1. create the update,0
2. push the update to PS,0
the leader task adds node prediction to flush list,0
1. name of this node's grad histogram on PS,0
2. build the grad histogram of this node,0
3. push the histograms to PS,0
4. reset thread stats to finished,0
5.1. set the children nodes of this node,0
5.2. set split info and grad stats to this node,0
5.2. create children nodes,0
"5.3. create node stats for children nodes, and add them to the tree",0
5.4. reset instance position,0
"5.5. add new active nodes if possible, inc depth, otherwise finish this tree",0
5.6. set children nodes to leaf nodes,0
5.7. set nid to leaf node,0
5.8. deactivate active node,0
"get feature type, 0:empty 1:all equal 2:real",0
left child <= split value; right child > split value,0
"the first: minimal, the last: maximal",0
categorical features,0
continuous features,0
left child <= split value; right child > split value,0
feature index used to split,0
feature value used to split,0
loss change after split this node,0
grad stats of the left child,0
grad stats of the right child,0
"LOG.info(""Constructor with fid = -1"");",0
fid = -1: no split currently,0
the minimal split value is the minimal value of feature,0
the splits do not include the maximal value of feature,0
"1. the average distance, (maxValue - minValue) / splitNum",0
2. calculate the candidate split value,0
1. new feature's histogram (grad + hess),0
size: sampled_featureNum * (2 * splitNum),0
"in other words, concatenate each feature's histogram",0
2. get the span of this node,0
------ 3. using sparse-aware method to build histogram ---,0
"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances",0
3.1. get the instance index,0
3.2. get the grad and hess of the instance,0
3.3. add to the sum,0
3.4. loop the non-zero entries,0
3.4.1. get feature value,0
3.4.2. current feature's position in the sampled feature set,0
3.4.3. find the position of feature value in a histogram,0
"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]",0
3.4.4. add the grad and hess to the corresponding bin,0
3.4.5. add the reverse to the bin that contains 0.0f,0
4. add the grad and hess sum to the zero bin of all features,0
find the best split result of the histogram of a tree node,0
1. calculate the gradStats of the root node,0
"1.1. update the grad stats of the root node on PS, only called once by leader worker",0
2. loop over features,0
2.1. get the ture feature id in the sampled feature set,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"update the grad stats of the root node on PS, only called once by leader worker",0
3. update the grad stats of children node,0
3.1. update the left child,0
3.2. update the right child,0
find the best split result of one feature,0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
split value = sketches[splitIdx],0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
partition number,0
cols of each partition,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
"loop all the possible split value, start from split[0], the first item is the minimal feature value",0
find the best split result of the histogram of a tree node,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
find the best split result of one feature,0
1. set the feature id,0
splitEntry.setFid(fid);,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]",0
the task use index to find fvalue,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting",0
"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking.",0
clear all the information,0
calculate the sum of gradient and hess,0
"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,",0
ridx),0
check if necessary information is ready,0
"same as add, reduce is used in All Reduce",0
"features used in this tree, if equals null, means use all the features without sampling",0
node in the tree,0
the gradient info of each instances,0
initialize nodes,0
gradient,0
second order gradient,0
int sendStartCol = startFid * 7; // each split contains 7 doubles,0
"System.out.println(""Indices: "" + Arrays.toString(indices));",0
t[i][code]++;,0
else if (Math.random() > 0.5) {,0
t[i][code] = freq;,0
},0
"System.out.println(""Change from "" + t[i][code] + "" to "" + freq);",0
"ret = Math.min(ret, t[i][h[i].encode(key)]);",0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
"task type: classification, regression, or ranking",0
"quantile sketch, size = featureNum * splitNum",0
"gradient histograms, size = treeNodeNum * featureNum * splitNum",0
"active tree nodes, size = pow(2, treeDepth) -1",0
sampled features. size = treeNum * sampleRatio * featureNum,0
categorical feature. size = workerNum * cateFeatNum * splitNum,0
"split features, size = treeNum * treeNodeNum",0
"split values, size = treeNum * treeNodeNum",0
"split gains, size = treeNum * treeNodeNum",0
"node weights, size = treeNum * treeNodeNum",0
"node preds, size = treeNum * treeNodeNum",0
if using PS to perform split,0
step size for a tree,0
number of class,0
minimum loss change required for a split,0
maximum depth of a tree,0
number of features,0
number of nonzero,0
number of candidates split value,0
----- the rest parameters are less important ----,0
base instance weight,0
minimum amount of hessian(weight) allowed in a child,0
L2 regularization factor,0
L1 regularization factor,0
default direction choice,0
maximum delta update we can add in weight estimation,0
this parameter can be used to stabilize update,0
default=0 means no constraint on weight delta,0
whether we want to do subsample for row,0
whether to subsample columns for each tree,0
accuracy of sketch,0
accuracy of sketch,0
leaf vector size,0
option for parallelization,0
option to open cacheline optimization,0
whether to not print info during training.,0
maximum depth of the tree,0
number of features used for tree construction,0
"minimum loss change required for a split, otherwise stop split",0
----- the rest parameters are less important ----,0
default direction choice,0
whether we want to do sample data,0
whether to sample columns during tree construction,0
whether to use histogram for split,0
number of histogram units,0
whether to print info during training.,0
----- the rest parameters are obtained after training ----,0
total number of nodes,0
number of deleted nodes */,0
implement Zip2Map interface,0
implement Zip3Map interface,0
set basic configuration keys,0
use local deploy mode and dummy data spliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
TODO: Have to deal with default values,1
asum += Math.abs(data.defaultReturnValue()) * (entireSize - data.size());,0
TODO: Have to deal with default values,1
sum += (entireSize - keys.size()) * data1.defaultReturnValue() * data2.defaultReturnValue();,0
TODO: Have to deal with default values,1
"qSum += Math.pow(data.defaultReturnValue(), 2) * (entireSize - data.size());",0
TODO: Have to deal with default values,1
asum += data.defaultReturnValue() * (entireSize - data.size());,0
find the max abs,0
compress data,0
TODO: a better way is needed to deal with defaultValue,1
TODO: a better way is needed to deal with defaultValue,1
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
"Input Path, please modify ${YOUR_ANGEL_HOME} as your local angel installation path,",0
"e.g. if your path is /home/angel/angel_1.3.0, your input_path should be:",0
"""file:///home/angel/angel_1.3.0/data/exampledata/GBDTLocalExampleData/agaricus.txt.train"",",0
"and your out_path could be: ""file:///home/angel/angel_1.3.0/data/output""",0
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
"set input, output path",0
Set GBDT algorithm parameters,0
Load Model from HDFS.,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Feature number of train data,0
Total iteration number,0
Validation sample ratio,0
"Data format,libsvm or dummy",0
Train batch number per epoch,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
"Set job queue, if you use YARN deploy mode, you can set job queue by",0
"self.conf.set('mapreduce.job.queue.name', 'default')",0
Set local deploy mode,0
Set basic self.configuration keys,0
Set data format,0
"set angel resource parameters #worker, #tast, #ps",0
set sgd LR algorithim parameters # feature # epoch,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Set local deploy mode,0
Set basic self.configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"Set trainning data, and save model path",0
Set actionType train,0
Set MF algorithm parameters,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.,0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
Unless required by applicable law or agreed to in writing] = software distributed under the License is,0
"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
"Input Path, please modify ${YOUR_ANGEL_HOME} as your local angel installation path,",0
"e.g. if your path is /home/angel/angel_1.3.0, your input_path should be:",0
"""file:///home/angel/angel_1.3.0/data/exampledata/GBDTLocalExampleData/agaricus.txt.train"",",0
"and your out_path could be: ""file:///home/angel/angel_1.3.0/data/output""",0
"if you need, you can delete the annotation mark before Line35,Line36,Line61,Line62, so",0
there is no need for you to pass the configs every time you submit the pyangel job.,0
"input_path = ""file:///${YOUR_ANGEL_HOME}/data/exampledata/GBDTLocalExampleData/agaricus.txt.train""",0
"output_path = ""file:///${YOUR_ANGEL_HOME}/data/output""",0
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Use local deploy mode and dummy data spliter,0
set input] = output path,0
self.conf[AngelConf.ANGEL_TRAIN_DATA_PATH] = input_path,0
self.conf[AngelConf.ANGEL_SAVE_MODEL_PATH] = output_path,0
Set GBDT algorithm parameters,0
Load Model from HDFS.,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic self.configuration key,0
"Set angel resource parameters #worker, #task, #PS",0
Set Kmeans algorithm parameters #cluster #feature #epoch,0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log sava path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Feature number of train data,0
Total iteration number,0
Rank,0
Regularization parameters,0
Learn rage,0
Set local deploy mode,0
Set basic self.configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set learnType,0
Set feature number,0
import jdk.nashorn.internal.runtime.regexp.joni.Config;,0
"paras[1] = ""abc"";",0
"paras[2] = ""123"";",0
Add standard Hadoop classes,0
Feature number of train data,0
Total iteration number,0
Learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set input data path,0
Set save model path,0
Set actionType train,0
Set PS Model values,0
Wait for all tasks finish this clock,0
Get values of index array,0
Set PS Model values,0
Wait for all tasks finish this clock,0
Get values of index array,0
"Pull func1 = new Pull(client.getMatrixId(), 1);",0
taskContext.globalSync(client.getMatrixId());,0
TVector row1 = ((GetRowResult) client.get(func1)).getRow();,0
double [] delta1 = new double[col];,0
for(int i = 0; i < col; i++) {,0
delta1[i] = 2.0;,0
},0
"DenseDoubleVector deltaV1 = new DenseDoubleVector(col, delta1);",0
deltaV1.setMatrixId(client.getMatrixId());,0
deltaV1.setRowId(1);,0
client.increment(deltaV1);,0
"conf.setInt(AngelConf.ANGEL_STALENESS, -1);",0
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Set basic configuration keys,0
Use local deploy mode and data format,0
"set input, output path",0
"set angel resource parameters #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Load Model from HDFS.,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"Set trainning data, and save model path",0
Set actionType train,0
Set MF algorithm parameters,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Default location for user home directories #,0
Default value for FS_HOME_DIR_KEY #,0
Default umask for files created in HDFS #,0
Default value for FS_PERMISSIONS_UMASK_KEY #,0
How often does RPC client send pings to RPC server #,0
Default value for IPC_PING_INTERVAL_KEY #,0
Enables pings from RPC client to the server #,0
Default value of IPC_CLIENT_PING_KEY #,0
Responses larger than this will be logged #,0
Default value for IPC_SERVER_RPC_MAX_RESPONSE_SIZE_KEY #,0
Number of threads in RPC server reading from the socket #,0
Default value for IPC_SERVER_RPC_READ_THREADS_KEY #,0
How many calls per handler are allowed in the queue. #,0
Default value for IPC_SERVER_HANDLER_QUEUE_SIZE_KEY #,0
Internal buffer size for Lzo compressordecompressors #/,0
Default value for IO_COMPRESSION_CODEC_LZO_BUFFERSIZE_KEY #,0
This is for specifying the implementation for the mappings from,0
hostnames to the racks they belong to,0
Internal buffer size for Snappy compressordecompressors #/,0
Default value for IO_COMPRESSION_CODEC_SNAPPY_BUFFERSIZE_KEY #,0
Internal buffer size for Snappy compressordecompressors #/,0
Default value for IO_COMPRESSION_CODEC_SNAPPY_BUFFERSIZE_KEY #,0
Service Authorization,0
HA health monitor and failover controller.,0
How often to retry connecting to the service.,0
How often to check the service.,0
How long to sleep after an unexpected RPC error.,0
Timeout for the actual monitorHealth() calls. *,0
Timeout that the FC waits for the new active to become active,0
Timeout that the FC waits for the old active to go to standby,0
FC connection retries for graceful fencing,0
"Timeout that the CLI (manual) FC waits for monitorHealth, getServiceState",0
Static user web-filter properties.,0
See StaticUserWebFilter.,0
EnableDisable aliases serving from jetty,0
Path to the Kerberos ticket cache.  Setting this will force,0
UserGroupInformation to use only this ticket cache file when creating a,0
FileSystem instance.,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
"Input Path, please modify ${YOUR_ANGEL_HOME} as your local angel installation path,",0
"e.g. if your path is /home/angel/angel_1.3.0, your input_path should be:",0
"""file:///home/angel/angel_1.3.0/data/exampledata/GBDTLocalExampleData/agaricus.txt.train"",",0
"and your out_path could be: ""file:///home/angel/angel_1.3.0/data/output""",0
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Set GBDT category feature,0
"set input, output path",0
Set GBDT algorithm parameters,0
Load Model from HDFS.,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Feature number of train data,0
Total iteration number,0
Validation sample ratio,0
"Data format,libsvm or dummy",0
Train batch number per epoch,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
"Set job queue, if you use YARN deploy mode, you can set job queue by",0
"self.conf.set('mapreduce.job.queue.name', 'default')",0
Set local deploy mode,0
Set basic self.configuration keys,0
Set data format,0
"set angel resource parameters #worker, #tast, #ps",0
set sgd LR algorithim parameters # feature # epoch,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Set local deploy mode,0
Set basic self.configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"Set trainning data, and save model path",0
Set actionType train,0
Set MF algorithm parameters,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.,0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
Unless required by applicable law or agreed to in writing] = software distributed under the License is,0
"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Set log path,0
Set actionType train,0
Load Model from HDFS.,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic self.configuration key,0
"Set angel resource parameters #worker, #task, #PS",0
Set Kmeans algorithm parameters #cluster #feature #epoch,0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log sava path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Feature number of train data,0
Total iteration number,0
Rank,0
Regularization parameters,0
Learn rage,0
Set local deploy mode,0
Set basic self.configuration keys,0
"Set angel resource parameters #worker, #task, #PS",0
Set FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set learnType,0
Set feature number,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
If the enviroment has ANGEL_HOME set trust it.,0
Add the path of the PyAngel module if it exists,0
If we are installed in edit mode also look two dirs up,0
Not pip installed no worries,0
If we are installed in edit mode also look two dirs up,0
Not pip installed no worries,0
Normalize the paths,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Get Java HashMap instance which converted from a python dict,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Run ParameterServer  & ParameterServerAgent,0
Only Run ParameterServer,0
Run ParameterServer & Worker(embedded ParameterServerAgent),0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
To Do,0
Modify the way to get current Angel version,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Launch the Py4j gateway,0
Start a socket that will be used by PythonGatewayServer to communicate its port to python sub-proc,0
Don't send ctrl-c / SIGINT to the Java gateway:,0
We use select() here in order to avoid blocking indefinitely if the subprocess dies,0
before connecting,0
Determine which ephemeral port the server started on:,0
Connect to the gateway,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.,0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
Unless required by applicable law or agreed to in writing] = software distributed under the License is,0
"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.,0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
Unless required by applicable law or agreed to in writing] = software distributed under the License is,0
"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
To do: need python edition of TVector,0
To do: need python edition of GetFunc,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Create an angel job client,0
Submit this application,0
Create a KMeans model,0
Load model meta to client,0
Start,0
"Run user task and wait for completion,",0
User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS,0
Save the trained model to HDFS,0
Stop,0
Create an angel job client,0
Submit this application,0
Create KMeans model,0
Add the model meta to client,0
Start,0
"Run user task and wait for completion,",0
User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS,0
Stop,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Training job to obtain a model,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Create an angel job client,0
Submit this application,0
Create a model,0
Load model meta to client,0
Run user task,0
"Wait for completion,",0
User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS,0
Save the incremental trained model to HDFS,0
Stop,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
If the enviroment has ANGEL_HOME set trust it.,0
Add the path of the PyAngel module if it exists,0
If we are installed in edit mode also look two dirs up,0
Not pip installed no worries,0
If we are installed in edit mode also look two dirs up,0
Not pip installed no worries,0
Normalize the paths,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Get Java HashMap instance which converted from a python dict,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Run ParameterServer  & ParameterServerAgent,0
Only Run ParameterServer,0
Run ParameterServer & Worker(embedded ParameterServerAgent),0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
To Do,0
Modify the way to get current Angel version,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Launch the Py4j gateway,0
Start a socket that will be used by PythonGatewayServer to communicate its port to python sub-proc,0
Don't send ctrl-c / SIGINT to the Java gateway:,0
We use select() here in order to avoid blocking indefinitely if the subprocess dies,0
before connecting,0
Determine which ephemeral port the server started on:,0
Connect to the gateway,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.,0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
Unless required by applicable law or agreed to in writing] = software distributed under the License is,0
"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
To do: need python edition of TVector,0
To do: need python edition of GetFunc,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Training job to obtain a model,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Create an angel job client,0
Submit this application,0
Create a model,0
Load model meta to client,0
Run user task,0
"Wait for completion,",0
User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS,0
Save the incremental trained model to HDFS,0
Stop,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Load model meta,0
Convert model,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Convert model,0
load hadoop configuration,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model,0
load hadoop configuration,0
mMatrix.setNnz(100000000);,0
mMatrix.setNnz(100000000);,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
worker register,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
attempt 0,0
attempt1,0
attempt1,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
set basic configuration keys,0
"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());",0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
Thread.sleep(5000);,0
"response = master.getJobReport(null, request);",0
"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);",0
"assertEquals(response.getJobReport().getCurIteration(), jobIteration);",0
"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
"DenseIntVector deltaVec = new DenseIntVector(100000, delta);",0
deltaVec.setMatrixId(matrixW1Id);,0
deltaVec.setRowId(0);,0
TODO Auto-generated constructor stub,1
import com.tencent.angel.psagent.consistency.SSPConsistencyController;,0
@RunWith(MockitoJUnitRunner.class),0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);",0
get a angel client,0
add matrix,0
psAgent.initAndStart();,0
test conf,0
test master location,0
test app id,0
test user,0
test ps agent attempt id,0
test connection,0
test master client,0
test ip,0
test loc,0
test master location,0
test ps location,0
"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));",0
test all ps ids,0
test all matrix ids,0
test all matrix names,0
test matrix attribute,0
test matrix meta,0
test ps location,0
test partitions,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
group0Id = new WorkerGroupId(0);,0
"worker0Id = new WorkerId(group0Id, 0);",0
"worker0Attempt0Id = new WorkerAttemptId(worker0Id, 0);",0
task0Id = new TaskId(0);,0
task1Id = new TaskId(1);,0
LOG.info(serverArbitraryIntRow1.getSparseRep());,0
LOG.info(serverArbitraryIntRow1.getSparseRep());,0
test this func in testWriteTo,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
"LOG.info(index[0] + "" "" + value[0]);",0
"LOG.info(index[1] + "" "" + value[1]);",0
"LOG.info(index[2] + "" "" + value[2]);",0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
dot,0
plusBy,0
@Test,0
public void dotDenseFloatVector() throws Exception {,0
int dim = 1000;,0
Random random = new Random(System.currentTimeMillis());,0
,0
double[] values = new double[dim];,0
float[] values_1 = new float[dim];,0
for (int i = 0; i < dim; i++) {,0
values[i] = random.nextDouble();,0
values_1[i] = random.nextFloat();,0
},0
,0
"DenseDoubleVector vec = new DenseDoubleVector(dim, values);",0
"TDoubleVector vec_1 = new DenseFloatVector(dim, values_1);",0
,0
double sum = 0.0;,0
for (int i = 0; i < dim; i++) {,0
sum += values[i] * values_1[i];,0
},0
,0
"assertEquals(sum, vec.dot(vec_1));",0
,0
},0
@Test,0
public void plusDenseFlaotVector() throws Exception {,0
"double[] value_1 = new double[]{0.1, 0.2, 0.3, 0.4, 0.5};",0
"double[] value_2 = new double[]{0.1f, 0.2f, 0.3f, 0.4f, 0.5f};",0
"DenseDoubleVector vec = new DenseDoubleVector(5, value_1);",0
"TDoubleVector vec_1 = new DenseFloatVector(5, value_2);",0
,0
TDoubleVector vec_2 = vec.plus(vec_1);,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(value_1[i] + value_2[i], vec_2.get(i));",0
,0
,0
"TDoubleVector vec_3 = vec.plus(vec_1, 2.0);",0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec_3.get(i), value_1[i] + 2 * value_2[i]);",0
,0
double[] oldValues = vec.getValues().clone();,0
,0
vec.plusBy(vec_1);,0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec.get(i), oldValues[i] + vec_1.get(i));",0
,0
oldValues = vec.getValues().clone();,0
,0
"vec.plusBy(vec_1, 3);",0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec.get(i), oldValues[i] + 3 * vec_1.get(i));",0
},0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
@Test,0
public void plusBy3() throws Exception {,0
"float[][] value = {{1.0f, 2.0f}, {3.0f, 4.0f}};",0
"DenseFloatMatrix mat = new DenseFloatMatrix(2, 2,value);",0
"TFloatVector vec = new DenseFloatVector(2, new float[]{1.0f, 1.0f});",0
vec.setRowId(0);,0
"TDoubleVector vec_1 = new DenseDoubleVector(2, new double[]{1.0f, 1.0f});",0
vec_1.setRowId(1);,0
TDoubleVector vec_2 = new SparseDoubleVector(2);,0
"vec_2.set(1, 1.0);",0
vec_2.setRowId(0);,0
,0
mat.plusBy(vec);,0
mat.plusBy(vec_1);,0
mat.plusBy(vec_2);,0
,0
"assertEquals(2.0f, mat.get(0, 0));",0
"assertEquals(4.0f, mat.get(0, 1));",0
"assertEquals(4.0f, mat.get(1, 0));",0
"assertEquals(5.0f, mat.get(1, 1));",0
},0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
Start PS,0
Start to run application,0
Assert.assertTrue(index.length == row.size());,0
Assert.assertTrue(index.length == row.size());,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
test worker getActiveTaskNum,0
test worker getTaskNum,0
test worker getTaskManager,0
test workerId,0
test workerAttemptId,0
tet worker initFinished,0
test worker getInitMinclock,0
test worker loacation,0
test AppId,0
test Conf,0
test UserName,0
master location,0
masterClient,0
test psAgent,0
test worker get dataBlockManager,0
workerGroup.getSplits();,0
application,0
lcation,0
workerGroup info,0
worker info,0
task,0
Matrix parameters,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
Create an Angel client,0
Add different types of matrix,0
using mock object,0
verification,0
Stubbing,0
Default does nothing.,0
The app injection is optional,0
"renderText(""hello world"");",0
"user choose a workerGroupID from the workergroups page,",0
now we should change the AngelApp params and render the workergroup page;,0
"static final String WORKER_ID = ""worker.id"";",0
"div(""#logo"").",0
"img(""/static/hadoop-st.png"")._().",0
import org.apache.hadoop.yarn.webapp.view.FooterBlock;,0
import org.apache.hadoop.yarn.webapp.view.HeaderBlock;,0
JQueryUI.jsnotice(html);,0
import org.apache.hadoop.conf.Configuration;,0
import java.lang.reflect.Field;,0
get block locations from file system,0
create a list of all block and their locations,0
"if the file is not splitable, just create the one block with",0
full file length,0
each split can be a maximum of maxSize,0
if remainder is between max and 2*max - then,0
"instead of creating splits of size max, left-max we",0
create splits of size left/2 and left/2. This is,0
a heuristic to avoid creating really really small,0
splits.,0
add this block to the block --> node locations map,0
"For blocks that do not have host/rack information,",0
assign to default  rack.,0
add this block to the rack --> block map,0
Add this host to rackToNodes map,0
add this block to the node --> block map,0
"if the file system does not have any rack information, then",0
use dummy rack location.,0
The topology paths have the host name included as the last,0
component. Strip it.,0
get tokens for all the required FileSystems..,0
"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,",0
job.getConfiguration());,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
all the files in input set,0
it is allowed for maxSize to be 0. Disable smoothing load for such cases,0
process all nodes and create splits that are local to a node. Generate,0
"one split per node iteration, and walk over nodes multiple times to",0
distribute the splits across nodes.,0
Skip the node if it has previously been marked as completed.,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
Remove all blocks which may already have been assigned to other,0
splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
Remove entries from blocksInNode so that we don't walk these,0
again.,0
Done creating a single split for this node. Move on to the next,0
node so that splits are distributed across nodes.,0
This implies that the last few blocks (or all in case maxSize=0),0
were not part of a split. The node is complete.,0
if there were any blocks left over and their combined size is,0
"larger than minSplitNode, then combine them into one split.",0
Otherwise add them back to the unprocessed pool. It is likely,0
that they will be combined with other blocks from the,0
same rack later on.,0
This condition also kicks in when max split size is not set. All,0
blocks on a node will be grouped together into a single split.,0
haven't created any split on this machine. so its ok to add a,0
smaller one for parallelism. Otherwise group it in the rack for,0
balanced size create an input split and add it to the splits,0
array,0
Remove entries from blocksInNode so that we don't walk this again.,0
The node is done. This was the last set of blocks for this node.,0
Put the unplaced blocks back into the pool for later rack-allocation.,0
Node is done. All blocks were fit into node-local splits.,0
Check if node-local assignments are complete.,0
All nodes have been walked over and marked as completed or all blocks,0
have been assigned. The rest should be handled via rackLock assignment.,0
"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """,0
"+ completedNodes.size() + "", size left: "" + totalLength);",0
"if blocks in a rack are below the specified minimum size, then keep them",0
"in 'overflow'. After the processing of all racks is complete, these",0
overflow blocks will be combined into splits.,0
Process all racks over and over again until there is no more work to do.,0
Create one split for this rack before moving over to the next rack.,0
Come back to this rack after creating a single split for each of the,0
remaining racks.,0
"Process one rack location at a time, Combine all possible blocks that",0
reside on this rack as one split. (constrained by minimum and maximum,0
split size).,0
iterate over all racks,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"if we created a split, then just go to the next rack",0
"if there is a minimum size specified, then create a single split",0
"otherwise, store these blocks into overflow data structure",0
There were a few blocks in this rack that,0
remained to be processed. Keep them in 'overflow' block list.,0
These will be combined later.,0
Process all overflow blocks,0
"This might cause an exiting rack location to be re-added,",0
but it should be ok.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"Process any remaining blocks, if any.",0
create an input split,0
add this split to the list that is returned,0
long num = totLength / maxSize;,0
all blocks for all the files in input set,0
mapping from a rack name to the list of blocks it has,0
mapping from a block to the nodes on which it has replicas,0
mapping from a node to the list of blocks that it contains,0
populate all the blocks for all files,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
"Security framework already loaded the tokens into current UGI, just use",0
them,0
Now remove the AM->RM token so tasks don't have it,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init container allocator,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
Init Client manager,0
Init PS Client manager,0
init parameter server manager,0
recover task information if needed,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
start a web service if use yarn deploy mode,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
Check Workers,0
Check PSS,0
Check Clients,0
Check PS Clients,0
parse parameter server counters,0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
refresh last heartbeat timestamp,0
send a state update event to the specific PSAttempt,0
check if parameter server can commit now.,0
check matrix metadata inconsistencies between master and parameter server.,0
"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix",0
"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix",0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
choose a unused port,0
start RPC server,0
remove this parameter server attempt from monitor set,0
remove this parameter server attempt from monitor set,0
"if worker attempt id is not in monitor set, we should shutdown it",0
find workergroup in worker manager,0
"if this worker group does not initialized, just return WORKERGROUP_NOTREADY",0
"if this worker group run over, just return WORKERGROUP_EXITED",0
"if this worker group is running now, return tasks, workers, data splits for it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update the clock for this matrix,0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update task iteration,0
"Calculate how many splits we need. As each task handles a separate split of data, so we want",0
the number of splits equal to the number of tasks,0
split data,0
dispatch the splits to workergroups,0
split data,0
dispatch the splits to workergroups,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
write meta data to a temporary file,0
rename the temporary file to final file,0
"if the file exists, read from file and deserialize it",0
write task meta,0
write ps meta,0
generate a temporary file,0
write task meta to the temporary file first,0
rename the temporary file to the final file,0
"if last final task file exist, remove it",0
find task meta file which has max timestamp,0
"if the file does not exist, just return null",0
read task meta from file and deserialize it,0
generate a temporary file,0
write ps meta to the temporary file first.,0
rename the temporary file to the final file,0
"if the old final file exist, just remove it",0
find ps meta file,0
"if ps meta file does not exist, just return null",0
read ps meta from file and deserialize it,0
Init matrix files meta,0
Move output files,0
Write the meta file,0
check whether psagent heartbeat timeout,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
LocalStorageToken is needed irrespective of whether security is enabled,0
or not.,0
"TokenCache.setJobToken(jobToken, taskCredentials);",0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Construct the actual Container,0
The null fields are per-container and will be constructed for each,0
container separately.,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
"a * in the classpath will only find a .jar, so we need to filter out",0
all .jars and add everything else,0
Propagate the system classpath when using the mini cluster,0
Add standard Hadoop classes,0
Cache archives,0
Cache files,0
Sanity check,0
Add URI fragment or just the filename,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Final commmand,0
"if amTask is not null, we should clone task state from it",0
"if all parameter server complete commit, master can commit now",0
init and start master committer,0
check whether parameter server heartbeat timeout,0
Transitions from the NEW state.,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event",0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event dispatched before",0
PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.COMMITTING state,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
reqeuest resource:send a resource request to the resource allocator,0
"Once the resource is applied, build and send the launch request to the container launcher",0
deallocator the resource of the ps attempt:send a resource deallocator request to the,0
resource allocator,0
set the launch time,0
add the ps attempt to the heartbeat timeout monitoring list,0
parse ps attempt location and put it to location manager,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt",0
or failed,0
remove ps attempt id from heartbeat timeout monitor list,0
release container:send a release request to container launcher,0
set the finish time only if launch time is set,0
private long scheduledTime;,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
Refresh ps location & matrix meta,0
start a new attempt for this ps,0
notify ps manager,0
"getContext().getLocationManager().setPsLocation(id, null);",0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
notify the event handler of state change,0
"If new state is not RUNNING and COMMITTING, add it to state timeout monitor",0
"if forcedState is set, just return",0
else get state from state machine,0
add this worker group to the success set,0
check if all worker group run over,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
calculate the actual number of worker groups and the total number of tasks based on the number of data split,0
"init all tasks , workers and worker groups and put them to the corresponding maps",0
just return the total task number now,0
TODO,1
check whether worker heartbeat timeout,0
"if workerAttempt is not null, we should clone task state from it",0
from NEW state,0
from SCHEDULED state,0
get data splits location for data locality,0
reqeuest resource:send a resource request to the resource allocator,0
"once the resource is applied, build and send the launch request to the container launcher",0
notify failed message to the worker,0
notify killed message to the worker,0
release the allocated container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
release the allocated container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
clean the container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
record the finish time,0
clean the container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listening list,0
record the finish time,0
"if the worker attempt launch successfully, add it to heartbeat timeout listening list",0
set worker attempt location,0
notify the register message to the worker,0
record the launch time,0
update worker attempt metrics,0
update tasks metrics,0
clean the container,0
notify the worker attempt run successfully message to the worker,0
record the finish time,0
init a worker attempt for the worker,0
schedule the worker attempt,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
"If we need Yarn to restart a new application master, we should not unregister from Yarn RM",0
register to Yarn RM,0
send heartbeat to Yarn RM every rmPollInterval milliseconds,0
"catch YarnRuntimeException, we should exit and need not retry",0
build heartbeat request,0
send heartbeat request to rm,0
"This can happen if the RM has been restarted. If it is in that state,",0
this application must clean itself up.,0
Setting NMTokens,0
assgin containers,0
"if some container is not assigned, release them",0
handle finish containers,0
dispatch container exit message to corresponding components,0
killed by framework,0
killed by framework,0
get application finish state,0
build application diagnostics,0
TODO:add a job history for angel,1
build unregister request,0
send unregister request to rm,0
Note this down for next interaction with ResourceManager,0
based on blacklisting comments above we can end up decrementing more,0
than requested. so guard for that.,0
send the updated resource request to RM,0
send 0 container count requests also to cancel previous requests,0
Update resource requests,0
try to assign to all nodes first to match node local,0
try to match all rack local,0
assign remaining,0
Update resource requests,0
send the container-assigned event to task attempt,0
build the start container request use launch context,0
send the start request to Yarn nm,0
send the message that the container starts successfully to the corresponding component,0
"after launching, send launched event to task attempt to move",0
it from ASSIGNED to RUNNING state,0
send the message that the container starts failed to the corresponding component,0
kill the remote container if already launched,0
start a thread pool to startup the container,0
See if we need up the pool size only if haven't reached the,0
maximum limit yet.,0
nodes where containers will run at *this* point of time. This is,0
*not* the cluster size and doesn't need to be.,0
"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the",0
later is just a buffer so we are not always increasing the,0
pool-size,0
the events from the queue are handled in parallel,0
using a thread pool,0
return if already stopped,0
shutdown any containers that might be left running,0
"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
get matrix ids in the parameter server report,0
get the matrices parameter server need to create and delete,0
"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix",0
"if a matrix exists on master but not exist on parameter server, this parameter server need build it.",0
Init control connection manager,0
Get ps locations from master and put them to the location cache.,0
Build and initialize rpc client to master,0
Get psagent id,0
Build PS control rpc client manager,0
Build local location,0
Initialize matrix meta information,0
Start all services,0
Stop all modules,0
Stop all modules,0
Update generic resource counters,0
Updating resources specified in ResourceCalculatorProcessTree,0
Remove the CPU time consumed previously by JVM reuse,0
Generate a flush request and put it to request queue,0
Generate a clock request and put it to request queue,0
Generate a merge request and put it to request queue,0
"If the matrix op log cache does not exist for the matrix, create a new one for the",0
matrix,0
and add it to cache maps,0
Add the message to the tree map,0
"If there are flush / clock requests blocked, we need to put this merge request into",0
the waiting queue,0
Launch a merge worker to merge the update to matrix op log cache,0
Remove the message from the tree map,0
Wake up blocked flush/clock request,0
Add flush/clock request to listener list to waiting for all the existing,0
updates are merged,0
Wake up blocked flush/clock request,0
"If all updates are merged for this matrix, we need wake up flush/clock requests which are",0
blocked.,0
Get next merge message sequence id,0
Wake up listeners(flush/clock requests) that have little sequence id than current merge,0
position,0
Wake up blocked merge requests,0
Get minimal sequence id from listeners,0
"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we",0
should flush updates to local matrix storage,0
unused now,1
"Filter it, removing zero values",0
Doing average or not,0
Split this row according the matrix partitions,0
Add the splits to the result container,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());",0
TODO: use Epoll for linux future,1
Update location table,0
Remove the server from failed list,0
Notify refresh success message to request dispatcher,0
Check PS exist or not,0
Check heartbeat timeout,0
Check PS restart or not,0
private final HashSet<ParameterServerId> refreshingServerSet;,0
Add it to failed rpc list,0
Add the server to gray server list,0
Add it to failed rpc list,0
Add the server to gray server list,0
Move from gray server list to failed server list,0
Handle the RPCS to this server,0
Submit the schedulable failed get RPCS,0
Submit new get RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"If the queue is empty, just return 0",0
"If request is not over limit, just submit it",0
Submit the schedulable failed get RPCS,0
Submit new put RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"LOG.info(""choose put server "" + psIds[index]);",0
Check all pending RPCS,0
Check get channel context,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {",0
if(ts - entry.getValue() > requestTimeOut * 2)  {,0
"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())",0
"+ "" milliseconds, close all channels to it"");",0
closeChannels(entry.getKey());,0
"psLocToLastChannelTsMap.put(entry.getKey(), ts);",0
},0
},0
"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);",0
Remove all pending RPCS,0
Close all channel to this PS,0
Get server id and location for this request,0
"If location is null, means that the server is not ready",0
Get the channel for the location,0
Check if need get token first,0
Serialize the request,0
Send the request,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
Allocate the bytebuf and serialize the request,0
find the partition request context from cache,0
Check if the result of the sub-request is received,0
Update received result number,0
Get row splits received,0
Put the row split to the cache(row index to row splits map),0
"If all splits of the row are received, means this row can be merged",0
TODO Auto-generated method stub,1
"Check futures, if the result of a sub-request is received, put it to the result queue",0
Now we just support pipelined row splits merging for dense type row,0
Wait until the clock value of this row is greater than or equal to the value,0
Get partitions for this row,0
First get this row from matrix storage,0
"If the row exists in the matrix storage and the clock value meets the requirements, just",0
return,0
Get row splits of this row from the matrix cache first,0
"If the row split does not exist in cache, get it from parameter server",0
Wait the final result,0
Put it to the matrix cache,0
Split the matrix oplog according to the matrix partitions,0
"If need update clock, we should send requests to all partitions",0
Filter the rowIds which are fetching now,0
Send the rowIndex to rpc dispatcher and return immediately,0
Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition,0
Generate dispatch items and add them to the corresponding queues,0
Filter the rowIds which are fetching now,0
Pre-fetching is disable default,0
matrix id to clock map,0
"task index, it must be unique for whole application",0
Deserialize data splits meta,0
Get workers,0
Send request to every ps,0
Wait the responses,0
Update clock cache,0
if(syncNum % 1024 == 0) {,0
},0
"Use simple flow, do not use any cache",0
Get row from cache.,0
"if row clock is satisfy ssp staleness limit, just return.",0
Get row from ps.,0
"For ASYNC mode, just get from pss.",0
"For BSP/SSP, get rows from storage/cache first",0
Get from ps.,0
"For ASYNC, just get rows from pss.",0
no more retries.,0
calculate sleep time and return.,0
parse the i-th sleep-time,0
parse the i-th number-of-retries,0
calculateSleepTime may overflow.,0
"A few common retry policies, with no delays.",0
close is a local operation and should finish within milliseconds; timeout just to be safe,0
response will be null for one way messages.,0
maxFrameLength = 2G,0
lengthFieldOffset = 0,0
lengthFieldLength = 8,0
"lengthAdjustment = -8, i.e. exclude the 8 byte length itself",0
"initialBytesToStrip = 8, i.e. strip out the length field itself",0
indicates whether this connection's life cycle is managed,0
See if we already have a connection (common case),0
create a unique lock for this RS + protocol (if necessary),0
get the RS lock,0
do one more lookup in case we were stalled above,0
Only create isa when we need to.,0
definitely a cache miss. establish an RPC for,0
this RS,0
Throw what the RemoteException was carrying.,0
check,0
every,0
minutes,0
TODO,1
failoverHandler,0
"The number of times this invocation handler has ever been failed over,",0
before this method invocation attempt. Used to prevent concurrent,0
failed method invocations from triggering multiple failover attempts.,0
Make sure that concurrent failed method invocations,0
only cause a,0
single actual fail over.,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
"LOG.info(""method "" + method.getName() + ""construct request time = """,0
+ (System.currentTimeMillis() - beforeConstructTs));,0
get an instance of the method arg type,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
Message (hand written code usually has only a single,0
argument),0
log any RPC responses that are slower than the configured,0
warn,0
response time or larger than configured warning size,0
"when tagging, we let TooLarge trump TooSmall to keep",0
output simple,0
note that large responses will often also be slow.,0
provides a count of log-reported slow responses,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
unexpected,0
"in the protobuf methods, args[1] is the only significant argument",0
for JSON encoding,0
base information that is reported regardless of type of call,0
Disable Nagle's Algorithm since we don't want packets to wait,0
Configure the event pipeline factory.,0
Make a new connection.,0
Remove all pending requests (will be canceled after relinquishing,0
write lock).,0
Cancel any pending requests by sending errors to the callbacks:,0
Close the channel:,0
Close the connection:,0
Shut down all thread pools to exit.,0
"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());",0
See NettyServer.prepareResponse for where we write out the response.,0
"It writes the call.id (int), a boolean signifying any error (and if",0
"so the exception name/trace), and the response bytes",0
Read the call id.,0
"When the stream is closed, protobuf doesn't raise an EOFException,",0
"instead, it returns a null message object.",0
"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +",0
System.currentTimeMillis());,0
"It would be good widen this to just Throwable, but IOException is what we",0
allow now,0
not implemented,1
not implemented,1
"track what RpcEngine is used by a proxy class, for stopProxy()",0
cache of RpcEngines by protocol,0
return the RpcEngine configured to handle a protocol,0
We only handle the ConnectException.,0
This is the exception we can't handle.,0
check if timed out,0
wait for retry,0
IGNORE,0
return the RpcEngine that handles a proxy object,0
The default implementation works synchronously,0
punt: allocate a new buffer & copy into it,0
Parse cmd parameters,0
load hadoop configuration,0
load angel system configuration,0
load user configuration:,0
load user config file,0
load command line parameters,0
load user job resource files,0
load user job jar if it exist,0
Expand the environment variable,0
Add default fs(local fs) for lib jars.,0
"LOG.info(System.getProperty(""user.dir""));",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
a simple hdfs copy function assume src path and dest path are in same hdfs,0
and FileSystem object has same schema,0
"LOG.warn(""interrupted while sleeping"", ie);",0
public static String getHostname() {,0
try {,0
"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();",0
} catch (UnknownHostException uhe) {,0
},0
"return new StringBuilder().append("""").append(uhe).toString();",0
},0
"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {",0
String hostname = getHostname();,0
String classname = clazz.getSimpleName();,0
"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new",0
"StringBuilder().append(""Starting "").append(classname).toString(), new",0
"StringBuilder().append(""  host = "").append(hostname).toString(), new",0
"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new",0
"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new",0
"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));",0
,0
"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {",0
public void run() {,0
"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +",0
"this.val$classname + "" at "" + this.val$hostname}));",0
},0
});,0
},0
"We we interrupted because we're meant to stop? If not, just",0
continue ignoring the interruption,0
Recalculate waitTime.,0
// Begin delegation to Thread,0
// End delegation to Thread,0
instance submitter class,0
Obtain filename from path,0
Split filename to prexif and suffix (extension),0
Check if the filename is okay,0
Prepare temporary file,0
Prepare buffer for data copying,0
Open and check input stream,0
Open output stream and copy data between source file in JAR and the temporary file,0
"If read/write fails, close streams safely before throwing an exception",0
"Finally, load the library",0
little endian load order,0
tail,0
fallthrough,0
fallthrough,0
finalization,0
fmix(h1);,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
JobStateProto jobState = report.getJobState();,0
Used for java code to get a AngelClient instance,0
Used for python code to get a AngelClient instance,0
the leaf level file should be readable by others,0
the subdirs in the path should have execute permissions for,0
others,0
2.get job id,0
Credentials credentials = new Credentials();,0
4.copy resource files to hdfs,0
5.write configuration to a xml file,0
6.create am container context,0
7.Submit to ResourceManager,0
8.get app master client,0
Create a number of filenames in the JobTracker's fs namespace,0
add all the command line files/ jars and archive,0
first copy them to jobtrackers filesystem,0
should not throw a uri exception,0
should not throw an uri excpetion,0
set the timestamps of the archives and files,0
set the public/private visibility of the archives and files,0
get DelegationToken for each cached file,0
check if we do not need to copy the files,0
is jt using the same file system.,0
just checking for uri strings... doing no dns lookups,0
to see if the filesystems are the same. This is not optimal.,0
but avoids name resolution.,0
this might have name collisions. copy will throw an exception,0
parse the original path to create new path,0
check for ports,0
Write job file to JobTracker's fs,0
Setup resource requirements,0
Setup LocalResources,0
Setup security tokens,0
Setup the command to run the AM,0
Add AM user command opts,0
Final command,0
Setup the CLASSPATH in environment,0
"i.e. add { Hadoop jars, job jar, CWD } to classpath.",0
Setup the environment variables for Admin first,0
"Setup the environment variables (LD_LIBRARY_PATH, etc)",0
Parse distributed cache,0
Setup ContainerLaunchContext for AM container,0
Set up the ApplicationSubmissionContext,0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this location, only master ps can accept the update",0
Check the partition state again,0
Start to put the update to the slave pss,0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this partition, if not, just return failed",0
Start to put the update to the slave pss,0
Add tokens to new user so that it may execute its task correctly.,0
to exit,0
context.getSnapshotManager().processRecovery();,0
private final ParameterServer psServer;,0
return ServerState.GENERAL;,0
lock.readLock().lock();,0
lock.readLock().unlock();,0
data.rewind();,0
lock.readLock().lock();,0
lock.readLock().unlock();,0
data.rewind();,0
lock.readLock().lock();,0
lock.readLock().unlock();,0
data.rewind();,0
output.writeInt(clock);,0
clock = input.readInt();,0
private final List<PartitionKey> partitionKeys;,0
Read matrix meta from meta file,0
Load partitions from file use fork-join,0
Save partitions to files use fork-join,0
Write the ps matrix meta to the meta file,0
Mapping from taskId to clock value.,0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
nnz = 0;,0
for (int i = 0; i < keys.length; i++),0
if (used[i]) {,0
"denseRep.put(keys[i], values[i]);",0
nnz++;,0
},0
sparseRep = null;,0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
for (int i = 0; i < keys.length; i++),0
if (used[i]) {,0
"denseRep.put(keys[i], values[i]);",0
},0
sparseRep = null;,0
output.writeInt(data.length);,0
@Override,0
public void serialize(ByteBuf buf) {,0
if (sparseRep != null),0
return serializeSparse();,0
else if (denseRep != null),0
return serializeDense();,0
return serializeEmpty();,0
},0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
int idx = 0;,0
for (int i = 0; i < keys.length; i++),0
if (used[i]) {,0
"keysBuf.put(idx, keys[i]);",0
"valuesBuf.put(idx, values[i]);",0
idx++;,0
},0
int[] keys = sparseRep.getKeys();,0
int[] values = sparseRep.getValues();,0
boolean[] used = sparseRep.getUsed();,0
"int ov, k, v;",0
for (int i = 0; i < keys.length; i++) {,0
if (used[i]) {,0
k = keys[i];,0
ov = denseRep.get(k);,0
v = ov + values[i];,0
"denseRep.put(k, v);",0
if (ov != 0 && v == 0),0
nnz--;,0
},0
},0
TODO: use Epoll for linux future,1
find the partition request context from cache,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
channelManager.removeChannelPool(loc);,0
Generate seq id,0
Create a RecoverPartRequest,0
Serialize the request,0
Change the seqId for the request,0
Serialize the request,0
"add the PSAgentContext,need fix",0
return this;,0
return this;,0
return this;,0
return this;,0
return this;,0
TODO Should be implemented,1
TODO Should be implemented,1
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
TODO:,1
public String uuid;,0
this.uuid = UUID.randomUUID().toString();,0
byte [] data = uuid.getBytes();,0
buf.writeInt(data.length);,0
buf.writeBytes(data);,0
int size = buf.readInt();,0
byte [] data = new byte[size];,0
buf.readBytes(data);,0
uuid = new String(data);,0
"return ""PartitionRequest{"" + ""clock="" + clock + "", partKey="" + partKey + "", uuid="" + uuid + "", comeFromPs=""",0
"+ comeFromPs + ""} "" + super.toString();",0
public String uuid;,0
write the max abs,0
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
get configuration from config file,0
set localDir with enviroment set by nm.,0
get master location,0
init task manager and start tasks,0
start heartbeat thread,0
taskManager.assignTaskIds(response.getTaskidsList());,0
todo,1
"if worker timeout, it may be knocked off.",0
"SUCCESS, do nothing",0
heartbeatFailedTime = 0;,0
private KEY currentKey;,0
will be created,0
TODO Auto-generated method stub,1
Bitmap bitmap = new Bitmap();,0
int max = indexArray[size - 1];,0
byte [] bitIndexArray = new byte[max / 8 + 1];,0
for(int i = 0; i < size; i++){,0
int bitIndex = indexArray[i] >> 3;,0
int bitOffset = indexArray[i] - (bitIndex << 3);,0
switch(bitOffset){,0
case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;,0
case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;,0
case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;,0
case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;,0
case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;,0
case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;,0
case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;,0
case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;,0
},0
},0
//////////////////////////////,0
Application Configs,0
//////////////////////////////,0
//////////////////////////////,0
Master Configs,0
//////////////////////////////,0
//////////////////////////////,0
Worker Configs,0
//////////////////////////////,0
//////////////////////////////,0
Task Configs,0
//////////////////////////////,0
//////////////////////////////,0
ParameterServer Configs,0
//////////////////////////////,0
////////////////// IPC //////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
Configs used to ANGEL_PS_PSAGENT running mode future.,0
model parse,0
Mark whether use pyangel or not.,0
private Configuration conf;,0
"Configuration that should be used in python environment, there should only be one",0
configuration instance in each Angel context.,0
Use private access means jconf should not be changed or modified in this way.,0
Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:,0
Do nothing,0
To-DO: add other ways to justify different value types,0
"This is so ugly, must re-implement by more elegance way",1
"Create python path which include angel's jars, the python directory in ANGEL_HOME,",0
and other files submitted by user.,0
Launch python process,0
TODO Auto-generated constructor stub,1
Feature number of train data,0
Total iteration number,0
Rank,0
Regularization parameters,0
Learn rage,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set feature number,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set FM predict output path,0
Set actionType train,0
Set feature number,0
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
"set input, output path",0
"set angel resource parameters #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Submit GBDT Train Task,0
Load Model from HDFS.,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"Set trainning data, save model, log path",0
Set actionType train,0
Set MF algorithm parameters,0
Feature number of train data,0
Total iteration number,0
number of mini batch within a update periorid,0
"Data format, libsvm or dummy",0
Batch size,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"conf.setBoolean(MLConf.ML_INDEX_GET_ENABLE(), true);",0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Sample ratio,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set FTRL LR algorithm parameters #feature #epoch,0
FtrlLRPredictTest();,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set predict data path,0
Set load model path,0
Set predict result path,0
Set actionType train,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
LOG.info(sigmoid(data[i]));,0
LOG.info(Math.exp(-data[i]));,0
when b is a negative number,0
LOG.info(sigmoid(data[i]));,0
LOG.info(Math.exp(-data[i]));,0
when b is a negative number,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set MLR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Rank,0
Regularization parameters,0
Learn rage,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set feature number,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set learnType,0
Set feature number,0
Set,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set FM predict output path,0
Set actionType train,0
Set learnType,0
Set feature number,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log sava path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation Ratio,0
Data format,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set basic configuration keys,0
Set data format,0
Use local deploy mode,0
"set angel resource parameters #worker, #task, #PS",0
set sgd SVM algorithm parameters,0
"set input, output path",0
Set save model path,0
Set actionType train,0
Set log path,0
Submit LR Train Task,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log sava path,0
Set actionType prediction,0
double z=pre*y;,0
if(z<=0) return 0.5-z;,0
"else if(z>0 && z<1) return 0.5*Math.pow(1-z,2);",0
return 0.0;,0
logistic loss for binary classification task.,0
"logistic loss, but predict un-transformed margin",0
check if label in range,0
return the default evaluation metric for the objective,0
TODO Auto-generated method stub,1
start row index for words,0
doc ids,0
topic assignments,0
count word,0
build word start index,0
build dks,0
"model.wtMat().increment(w, update);",0
"update.plusBy(t, 1);",0
"model.wtMat().increment(w, update);",0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
Write #rows,0
Write each row,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
calculate columns,0
loss function,0
gradient and hessian,0
"categorical feature set, null: none, empty: all, else: partial",0
tree node,0
initialize the phase,0
current tree and depth,0
create loss function,0
calculate grad info of each instance,0
"create data sketch, push candidate split value to PS",0
1. calculate candidate split value,0
categorical features,0
2. push local sketch to PS,0
3. set phase to GET_SKETCH,0
the leader worker,0
merge categorical features,0
create updates,0
"pull the global sketch from PS, only called once by each worker",0
number of categorical feature,0
sample feature,0
push sampled feature set to the current tree,0
create new tree,0
"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,",0
calculate gradient,0
"1. create new tree, initialize tree nodes and node stats",0
"2. initialize feature set, if sampled, get from PS, otherwise use all the features",0
2.1. pull the sampled features of the current tree,0
"2.2. if use all the features, only called one",0
"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle",0
4. set root node to active,0
"5. reset instance position, set the root node's span",0
6. calculate gradient,0
7. set phase to run active,0
1. start threads of active tree nodes,0
1.1. start threads for active nodes to generate histogram,0
1.2. set thread status to batch num,0
1.3. set the oplog to active,0
"2. check thread stats, if all threads finish, return",0
clock,0
find split,0
"1. find responsible tree node, using RR scheme",0
2. pull gradient histogram,0
2.1. get the name of this node's gradient histogram on PS,0
2.2. pull the histogram,0
histogram = (TDoubleVector) ((GetRowResult) histMat.get(func)).getRow();,0
2.3. find best split result of this tree node,0
2.3.1 using server split,0
"update the grad stats of the root node on PS, only called once by leader worker",0
update the grad stats of children node,0
update the left child,0
update the right child,0
"2.3.2 the updated split result (tree node/feature/value/gain) on PS,",0
"2.3.3 otherwise, the returned histogram contains the gradient info",0
"2.3.4 the updated split result (tree node/feature/value/gain) on PS,",0
2.3.5 reset this tree node's gradient histogram to 0,0
3. push split feature to PS,0
4. push split value to PS,0
5. push split gain to PS,0
6. set phase to AFTER_SPLIT,0
clock,0
1. get split feature,0
2. get split value,0
3. get split gain,0
4. get node weight,0
5. split node,0
"2. check thread stats, if all threads finish, return",0
6. clock,0
"split the span of one node, reset the instance position",0
in case this worker has no instance on this node,0
set the span of left child,0
set the span of right child,0
"1. left to right, find the first instance that should be in the right child",0
"2. right to left, find the first instance that should be in the left child",0
3. swap two instances,0
4. find the cut pos,0
than the split value,0
5. set the span of left child,0
6. set the span of right child,0
set tree node to active,0
set node to leaf,0
set node to inactive,0
finish current tree,0
finish current depth,0
set the tree phase,0
check if there is active node,0
check if finish all the tree,0
update node's grad stats on PS,0
"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split",0
the root node's stats is updated by leader worker,0
1. create the update,0
2. push the update to PS,0
the leader task adds node prediction to flush list,0
1. name of this node's grad histogram on PS,0
2. build the grad histogram of this node,0
3. push the histograms to PS,0
4. reset thread stats to finished,0
5.1. set the children nodes of this node,0
5.2. set split info and grad stats to this node,0
5.2. create children nodes,0
"5.3. create node stats for children nodes, and add them to the tree",0
5.4. reset instance position,0
"5.5. add new active nodes if possible, inc depth, otherwise finish this tree",0
5.6. set children nodes to leaf nodes,0
5.7. set nid to leaf node,0
5.8. deactivate active node,0
"get feature type, 0:empty 1:all equal 2:real",0
left child <= split value; right child > split value,0
"the first: minimal, the last: maximal",0
categorical features,0
continuous features,0
left child <= split value; right child > split value,0
feature index used to split,0
feature value used to split,0
loss change after split this node,0
grad stats of the left child,0
grad stats of the right child,0
"LOG.info(""Constructor with fid = -1"");",0
fid = -1: no split currently,0
the minimal split value is the minimal value of feature,0
the splits do not include the maximal value of feature,0
"1. the average distance, (maxValue - minValue) / splitNum",0
2. calculate the candidate split value,0
1. new feature's histogram (grad + hess),0
size: sampled_featureNum * (2 * splitNum),0
"in other words, concatenate each feature's histogram",0
2. get the span of this node,0
------ 3. using sparse-aware method to build histogram ---,0
"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances",0
3.1. get the instance index,0
3.2. get the grad and hess of the instance,0
3.3. add to the sum,0
3.4. loop the non-zero entries,0
3.4.1. get feature value,0
3.4.2. current feature's position in the sampled feature set,0
3.4.3. find the position of feature value in a histogram,0
"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]",0
3.4.4. add the grad and hess to the corresponding bin,0
3.4.5. add the reverse to the bin that contains 0.0f,0
4. add the grad and hess sum to the zero bin of all features,0
find the best split result of the histogram of a tree node,0
1. calculate the gradStats of the root node,0
"1.1. update the grad stats of the root node on PS, only called once by leader worker",0
2. loop over features,0
2.1. get the ture feature id in the sampled feature set,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"update the grad stats of the root node on PS, only called once by leader worker",0
3. update the grad stats of children node,0
3.1. update the left child,0
3.2. update the right child,0
find the best split result of one feature,0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
split value = sketches[splitIdx],0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
partition number,0
cols of each partition,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
"loop all the possible split value, start from split[0], the first item is the minimal feature value",0
find the best split result of the histogram of a tree node,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
find the best split result of one feature,0
1. set the feature id,0
splitEntry.setFid(fid);,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]",0
the task use index to find fvalue,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting",0
"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking.",0
clear all the information,0
calculate the sum of gradient and hess,0
"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,",0
ridx),0
check if necessary information is ready,0
"same as add, reduce is used in All Reduce",0
"features used in this tree, if equals null, means use all the features without sampling",0
node in the tree,0
the gradient info of each instances,0
initialize nodes,0
gradient,0
second order gradient,0
int sendStartCol = startFid * 7; // each split contains 7 doubles,0
"System.out.println(""Indices: "" + Arrays.toString(indices));",0
t[i][code]++;,0
else if (Math.random() > 0.5) {,0
t[i][code] = freq;,0
},0
"System.out.println(""Change from "" + t[i][code] + "" to "" + freq);",0
"ret = Math.min(ret, t[i][h[i].encode(key)]);",0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
"task type: classification, regression, or ranking",0
"quantile sketch, size = featureNum * splitNum",0
"gradient histograms, size = treeNodeNum * featureNum * splitNum",0
"active tree nodes, size = pow(2, treeDepth) -1",0
sampled features. size = treeNum * sampleRatio * featureNum,0
categorical feature. size = workerNum * cateFeatNum * splitNum,0
"split features, size = treeNum * treeNodeNum",0
"split values, size = treeNum * treeNodeNum",0
"split gains, size = treeNum * treeNodeNum",0
"node weights, size = treeNum * treeNodeNum",0
"node preds, size = treeNum * treeNodeNum",0
if using PS to perform split,0
step size for a tree,0
number of class,0
minimum loss change required for a split,0
maximum depth of a tree,0
number of features,0
number of nonzero,0
number of candidates split value,0
----- the rest parameters are less important ----,0
base instance weight,0
minimum amount of hessian(weight) allowed in a child,0
L2 regularization factor,0
L1 regularization factor,0
default direction choice,0
maximum delta update we can add in weight estimation,0
this parameter can be used to stabilize update,0
default=0 means no constraint on weight delta,0
whether we want to do subsample for row,0
whether to subsample columns for each tree,0
accuracy of sketch,0
accuracy of sketch,0
leaf vector size,0
option for parallelization,0
option to open cacheline optimization,0
whether to not print info during training.,0
maximum depth of the tree,0
number of features used for tree construction,0
"minimum loss change required for a split, otherwise stop split",0
----- the rest parameters are less important ----,0
default direction choice,0
whether we want to do sample data,0
whether to sample columns during tree construction,0
whether to use histogram for split,0
number of histogram units,0
whether to print info during training.,0
----- the rest parameters are obtained after training ----,0
total number of nodes,0
number of deleted nodes */,0
implement Zip2Map interface,0
implement Zip3Map interface,0
set basic configuration keys,0
use local deploy mode and dummy data spliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
TODO: Have to deal with default values,1
asum += Math.abs(data.defaultReturnValue()) * (entireSize - data.size());,0
TODO: Have to deal with default values,1
sum += (entireSize - keys.size()) * data1.defaultReturnValue() * data2.defaultReturnValue();,0
TODO: Have to deal with default values,1
"qSum += Math.pow(data.defaultReturnValue(), 2) * (entireSize - data.size());",0
TODO: Have to deal with default values,1
asum += data.defaultReturnValue() * (entireSize - data.size());,0
find the max abs,0
compress data,0
TODO: a better way is needed to deal with defaultValue,1
TODO: a better way is needed to deal with defaultValue,1
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
"Input Path, please modify ${YOUR_ANGEL_HOME} as your local angel installation path,",0
"e.g. if your path is /home/angel/angel_1.3.0, your input_path should be:",0
"""file:///home/angel/angel_1.3.0/data/exampledata/GBDTLocalExampleData/agaricus.txt.train"",",0
"and your out_path could be: ""file:///home/angel/angel_1.3.0/data/output""",0
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
"set input, output path",0
Set GBDT algorithm parameters,0
Load Model from HDFS.,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Feature number of train data,0
Total iteration number,0
Validation sample ratio,0
"Data format,libsvm or dummy",0
Train batch number per epoch,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
"Set job queue, if you use YARN deploy mode, you can set job queue by",0
"self.conf.set('mapreduce.job.queue.name', 'default')",0
Set local deploy mode,0
Set basic self.configuration keys,0
Set data format,0
"set angel resource parameters #worker, #tast, #ps",0
set sgd LR algorithim parameters # feature # epoch,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Set local deploy mode,0
Set basic self.configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"Set trainning data, and save model path",0
Set actionType train,0
Set MF algorithm parameters,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.,0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
Unless required by applicable law or agreed to in writing] = software distributed under the License is,0
"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
"Input Path, please modify ${YOUR_ANGEL_HOME} as your local angel installation path,",0
"e.g. if your path is /home/angel/angel_1.3.0, your input_path should be:",0
"""file:///home/angel/angel_1.3.0/data/exampledata/GBDTLocalExampleData/agaricus.txt.train"",",0
"and your out_path could be: ""file:///home/angel/angel_1.3.0/data/output""",0
"if you need, you can delete the annotation mark before Line35,Line36,Line61,Line62, so",0
there is no need for you to pass the configs every time you submit the pyangel job.,0
"input_path = ""file:///${YOUR_ANGEL_HOME}/data/exampledata/GBDTLocalExampleData/agaricus.txt.train""",0
"output_path = ""file:///${YOUR_ANGEL_HOME}/data/output""",0
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Use local deploy mode and dummy data spliter,0
set input] = output path,0
self.conf[AngelConf.ANGEL_TRAIN_DATA_PATH] = input_path,0
self.conf[AngelConf.ANGEL_SAVE_MODEL_PATH] = output_path,0
Set GBDT algorithm parameters,0
Load Model from HDFS.,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic self.configuration key,0
"Set angel resource parameters #worker, #task, #PS",0
Set Kmeans algorithm parameters #cluster #feature #epoch,0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log sava path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Feature number of train data,0
Total iteration number,0
Rank,0
Regularization parameters,0
Learn rage,0
Set local deploy mode,0
Set basic self.configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set learnType,0
Set feature number,0
import jdk.nashorn.internal.runtime.regexp.joni.Config;,0
"paras[1] = ""abc"";",0
"paras[2] = ""123"";",0
Add standard Hadoop classes,0
Feature number of train data,0
Total iteration number,0
Learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set input data path,0
Set save model path,0
Set actionType train,0
Set PS Model values,0
Wait for all tasks finish this clock,0
Get values of index array,0
Set PS Model values,0
Wait for all tasks finish this clock,0
Get values of index array,0
"conf.setInt(AngelConf.ANGEL_STALENESS, -1);",0
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Set basic configuration keys,0
Use local deploy mode and data format,0
"set input, output path",0
"set angel resource parameters #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Load Model from HDFS.,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"Set trainning data, and save model path",0
Set actionType train,0
Set MF algorithm parameters,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Default location for user home directories #,0
Default value for FS_HOME_DIR_KEY #,0
Default umask for files created in HDFS #,0
Default value for FS_PERMISSIONS_UMASK_KEY #,0
How often does RPC client send pings to RPC server #,0
Default value for IPC_PING_INTERVAL_KEY #,0
Enables pings from RPC client to the server #,0
Default value of IPC_CLIENT_PING_KEY #,0
Responses larger than this will be logged #,0
Default value for IPC_SERVER_RPC_MAX_RESPONSE_SIZE_KEY #,0
Number of threads in RPC server reading from the socket #,0
Default value for IPC_SERVER_RPC_READ_THREADS_KEY #,0
How many calls per handler are allowed in the queue. #,0
Default value for IPC_SERVER_HANDLER_QUEUE_SIZE_KEY #,0
Internal buffer size for Lzo compressordecompressors #/,0
Default value for IO_COMPRESSION_CODEC_LZO_BUFFERSIZE_KEY #,0
This is for specifying the implementation for the mappings from,0
hostnames to the racks they belong to,0
Internal buffer size for Snappy compressordecompressors #/,0
Default value for IO_COMPRESSION_CODEC_SNAPPY_BUFFERSIZE_KEY #,0
Internal buffer size for Snappy compressordecompressors #/,0
Default value for IO_COMPRESSION_CODEC_SNAPPY_BUFFERSIZE_KEY #,0
Service Authorization,0
HA health monitor and failover controller.,0
How often to retry connecting to the service.,0
How often to check the service.,0
How long to sleep after an unexpected RPC error.,0
Timeout for the actual monitorHealth() calls. *,0
Timeout that the FC waits for the new active to become active,0
Timeout that the FC waits for the old active to go to standby,0
FC connection retries for graceful fencing,0
"Timeout that the CLI (manual) FC waits for monitorHealth, getServiceState",0
Static user web-filter properties.,0
See StaticUserWebFilter.,0
EnableDisable aliases serving from jetty,0
Path to the Kerberos ticket cache.  Setting this will force,0
UserGroupInformation to use only this ticket cache file when creating a,0
FileSystem instance.,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
"Input Path, please modify ${YOUR_ANGEL_HOME} as your local angel installation path,",0
"e.g. if your path is /home/angel/angel_1.3.0, your input_path should be:",0
"""file:///home/angel/angel_1.3.0/data/exampledata/GBDTLocalExampleData/agaricus.txt.train"",",0
"and your out_path could be: ""file:///home/angel/angel_1.3.0/data/output""",0
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Set GBDT category feature,0
"set input, output path",0
Set GBDT algorithm parameters,0
Load Model from HDFS.,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Feature number of train data,0
Total iteration number,0
Validation sample ratio,0
"Data format,libsvm or dummy",0
Train batch number per epoch,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
"Set job queue, if you use YARN deploy mode, you can set job queue by",0
"self.conf.set('mapreduce.job.queue.name', 'default')",0
Set local deploy mode,0
Set basic self.configuration keys,0
Set data format,0
"set angel resource parameters #worker, #tast, #ps",0
set sgd LR algorithim parameters # feature # epoch,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Set local deploy mode,0
Set basic self.configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"Set trainning data, and save model path",0
Set actionType train,0
Set MF algorithm parameters,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.,0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
Unless required by applicable law or agreed to in writing] = software distributed under the License is,0
"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Set log path,0
Set actionType train,0
Load Model from HDFS.,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic self.configuration key,0
"Set angel resource parameters #worker, #task, #PS",0
Set Kmeans algorithm parameters #cluster #feature #epoch,0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log sava path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Feature number of train data,0
Total iteration number,0
Rank,0
Regularization parameters,0
Learn rage,0
Set local deploy mode,0
Set basic self.configuration keys,0
"Set angel resource parameters #worker, #task, #PS",0
Set FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set learnType,0
Set feature number,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
If the enviroment has ANGEL_HOME set trust it.,0
Add the path of the PyAngel module if it exists,0
If we are installed in edit mode also look two dirs up,0
Not pip installed no worries,0
If we are installed in edit mode also look two dirs up,0
Not pip installed no worries,0
Normalize the paths,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Get Java HashMap instance which converted from a python dict,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Run ParameterServer  & ParameterServerAgent,0
Only Run ParameterServer,0
Run ParameterServer & Worker(embedded ParameterServerAgent),0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
To Do,0
Modify the way to get current Angel version,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Launch the Py4j gateway,0
Start a socket that will be used by PythonGatewayServer to communicate its port to python sub-proc,0
Don't send ctrl-c / SIGINT to the Java gateway:,0
We use select() here in order to avoid blocking indefinitely if the subprocess dies,0
before connecting,0
Determine which ephemeral port the server started on:,0
Connect to the gateway,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.,0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
Unless required by applicable law or agreed to in writing] = software distributed under the License is,0
"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.,0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
Unless required by applicable law or agreed to in writing] = software distributed under the License is,0
"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
To do: need python edition of TVector,0
To do: need python edition of GetFunc,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Create an angel job client,0
Submit this application,0
Create a KMeans model,0
Load model meta to client,0
Start,0
"Run user task and wait for completion,",0
User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS,0
Save the trained model to HDFS,0
Stop,0
Create an angel job client,0
Submit this application,0
Create KMeans model,0
Add the model meta to client,0
Start,0
"Run user task and wait for completion,",0
User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS,0
Stop,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Training job to obtain a model,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Create an angel job client,0
Submit this application,0
Create a model,0
Load model meta to client,0
Run user task,0
"Wait for completion,",0
User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS,0
Save the incremental trained model to HDFS,0
Stop,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
If the enviroment has ANGEL_HOME set trust it.,0
Add the path of the PyAngel module if it exists,0
If we are installed in edit mode also look two dirs up,0
Not pip installed no worries,0
If we are installed in edit mode also look two dirs up,0
Not pip installed no worries,0
Normalize the paths,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Get Java HashMap instance which converted from a python dict,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Run ParameterServer  & ParameterServerAgent,0
Only Run ParameterServer,0
Run ParameterServer & Worker(embedded ParameterServerAgent),0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
To Do,0
Modify the way to get current Angel version,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Launch the Py4j gateway,0
Start a socket that will be used by PythonGatewayServer to communicate its port to python sub-proc,0
Don't send ctrl-c / SIGINT to the Java gateway:,0
We use select() here in order to avoid blocking indefinitely if the subprocess dies,0
before connecting,0
Determine which ephemeral port the server started on:,0
Connect to the gateway,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.,0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
Unless required by applicable law or agreed to in writing] = software distributed under the License is,0
"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
To do: need python edition of TVector,0
To do: need python edition of GetFunc,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Training job to obtain a model,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https:#opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Create an angel job client,0
Submit this application,0
Create a model,0
Load model meta to client,0
Run user task,0
"Wait for completion,",0
User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS,0
Save the incremental trained model to HDFS,0
Stop,0
,0
Tencent is pleased to support the open source community by making Angel available.,0
,0
"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.",0
,0
"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in",0
compliance with the License. You may obtain a copy of the License at,0
,0
https://opensource.org/licenses/BSD-3-Clause,0
,0
"Unless required by applicable law or agreed to in writing, software distributed under the License is",0
"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,",0
either express or implied. See the License for the specific language governing permissions and,0
,0
Load model meta,0
Convert model,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Convert model,0
load hadoop configuration,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model,0
load hadoop configuration,0
mMatrix.setNnz(100000000);,0
mMatrix.setNnz(100000000);,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
worker register,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
attempt 0,0
attempt1,0
attempt1,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
set basic configuration keys,0
"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());",0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
Thread.sleep(5000);,0
"response = master.getJobReport(null, request);",0
"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);",0
"assertEquals(response.getJobReport().getCurIteration(), jobIteration);",0
"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
"DenseIntVector deltaVec = new DenseIntVector(100000, delta);",0
deltaVec.setMatrixId(matrixW1Id);,0
deltaVec.setRowId(0);,0
TODO Auto-generated constructor stub,1
import com.tencent.angel.psagent.consistency.SSPConsistencyController;,0
@RunWith(MockitoJUnitRunner.class),0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);",0
get a angel client,0
add matrix,0
psAgent.initAndStart();,0
test conf,0
test master location,0
test app id,0
test user,0
test ps agent attempt id,0
test connection,0
test master client,0
test ip,0
test loc,0
test master location,0
test ps location,0
"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));",0
test all ps ids,0
test all matrix ids,0
test all matrix names,0
test matrix attribute,0
test matrix meta,0
test ps location,0
test partitions,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
group0Id = new WorkerGroupId(0);,0
"worker0Id = new WorkerId(group0Id, 0);",0
"worker0Attempt0Id = new WorkerAttemptId(worker0Id, 0);",0
task0Id = new TaskId(0);,0
task1Id = new TaskId(1);,0
test this func in testWriteTo,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
"LOG.info(index[0] + "" "" + value[0]);",0
"LOG.info(index[1] + "" "" + value[1]);",0
"LOG.info(index[2] + "" "" + value[2]);",0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
dot,0
plusBy,0
@Test,0
public void dotDenseFloatVector() throws Exception {,0
int dim = 1000;,0
Random random = new Random(System.currentTimeMillis());,0
,0
double[] values = new double[dim];,0
float[] values_1 = new float[dim];,0
for (int i = 0; i < dim; i++) {,0
values[i] = random.nextDouble();,0
values_1[i] = random.nextFloat();,0
},0
,0
"DenseDoubleVector vec = new DenseDoubleVector(dim, values);",0
"TDoubleVector vec_1 = new DenseFloatVector(dim, values_1);",0
,0
double sum = 0.0;,0
for (int i = 0; i < dim; i++) {,0
sum += values[i] * values_1[i];,0
},0
,0
"assertEquals(sum, vec.dot(vec_1));",0
,0
},0
@Test,0
public void plusDenseFlaotVector() throws Exception {,0
"double[] value_1 = new double[]{0.1, 0.2, 0.3, 0.4, 0.5};",0
"double[] value_2 = new double[]{0.1f, 0.2f, 0.3f, 0.4f, 0.5f};",0
"DenseDoubleVector vec = new DenseDoubleVector(5, value_1);",0
"TDoubleVector vec_1 = new DenseFloatVector(5, value_2);",0
,0
TDoubleVector vec_2 = vec.plus(vec_1);,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(value_1[i] + value_2[i], vec_2.get(i));",0
,0
,0
"TDoubleVector vec_3 = vec.plus(vec_1, 2.0);",0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec_3.get(i), value_1[i] + 2 * value_2[i]);",0
,0
double[] oldValues = vec.getValues().clone();,0
,0
vec.plusBy(vec_1);,0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec.get(i), oldValues[i] + vec_1.get(i));",0
,0
oldValues = vec.getValues().clone();,0
,0
"vec.plusBy(vec_1, 3);",0
,0
for (int i = 0; i < vec.size(); i++),0
"assertEquals(vec.get(i), oldValues[i] + 3 * vec_1.get(i));",0
},0
dot,0
plus,0
plusBy,0
dot,0
plus,0
plusBy,0
@Test,0
public void plusBy3() throws Exception {,0
"float[][] value = {{1.0f, 2.0f}, {3.0f, 4.0f}};",0
"DenseFloatMatrix mat = new DenseFloatMatrix(2, 2,value);",0
"TFloatVector vec = new DenseFloatVector(2, new float[]{1.0f, 1.0f});",0
vec.setRowId(0);,0
"TDoubleVector vec_1 = new DenseDoubleVector(2, new double[]{1.0f, 1.0f});",0
vec_1.setRowId(1);,0
TDoubleVector vec_2 = new SparseDoubleVector(2);,0
"vec_2.set(1, 1.0);",0
vec_2.setRowId(0);,0
,0
mat.plusBy(vec);,0
mat.plusBy(vec_1);,0
mat.plusBy(vec_2);,0
,0
"assertEquals(2.0f, mat.get(0, 0));",0
"assertEquals(4.0f, mat.get(0, 1));",0
"assertEquals(4.0f, mat.get(1, 0));",0
"assertEquals(5.0f, mat.get(1, 1));",0
},0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
Start PS,0
Start to run application,0
Assert.assertTrue(index.length == row.size());,0
Assert.assertTrue(index.length == row.size());,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
test worker getActiveTaskNum,0
test worker getTaskNum,0
test worker getTaskManager,0
test workerId,0
test workerAttemptId,0
tet worker initFinished,0
test worker getInitMinclock,0
test worker loacation,0
test AppId,0
test Conf,0
test UserName,0
master location,0
masterClient,0
test psAgent,0
test worker get dataBlockManager,0
workerGroup.getSplits();,0
application,0
lcation,0
workerGroup info,0
worker info,0
task,0
Matrix parameters,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
Create an Angel client,0
Add different types of matrix,0
using mock object,0
verification,0
Stubbing,0
Default does nothing.,0
The app injection is optional,0
"renderText(""hello world"");",0
"user choose a workerGroupID from the workergroups page,",0
now we should change the AngelApp params and render the workergroup page;,0
"static final String WORKER_ID = ""worker.id"";",0
"div(""#logo"").",0
"img(""/static/hadoop-st.png"")._().",0
import org.apache.hadoop.yarn.webapp.view.FooterBlock;,0
import org.apache.hadoop.yarn.webapp.view.HeaderBlock;,0
JQueryUI.jsnotice(html);,0
import org.apache.hadoop.conf.Configuration;,0
import java.lang.reflect.Field;,0
get block locations from file system,0
create a list of all block and their locations,0
"if the file is not splitable, just create the one block with",0
full file length,0
each split can be a maximum of maxSize,0
if remainder is between max and 2*max - then,0
"instead of creating splits of size max, left-max we",0
create splits of size left/2 and left/2. This is,0
a heuristic to avoid creating really really small,0
splits.,0
add this block to the block --> node locations map,0
"For blocks that do not have host/rack information,",0
assign to default  rack.,0
add this block to the rack --> block map,0
Add this host to rackToNodes map,0
add this block to the node --> block map,0
"if the file system does not have any rack information, then",0
use dummy rack location.,0
The topology paths have the host name included as the last,0
component. Strip it.,0
get tokens for all the required FileSystems..,0
"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,",0
job.getConfiguration());,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
all the files in input set,0
it is allowed for maxSize to be 0. Disable smoothing load for such cases,0
process all nodes and create splits that are local to a node. Generate,0
"one split per node iteration, and walk over nodes multiple times to",0
distribute the splits across nodes.,0
Skip the node if it has previously been marked as completed.,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
Remove all blocks which may already have been assigned to other,0
splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
Remove entries from blocksInNode so that we don't walk these,0
again.,0
Done creating a single split for this node. Move on to the next,0
node so that splits are distributed across nodes.,0
This implies that the last few blocks (or all in case maxSize=0),0
were not part of a split. The node is complete.,0
if there were any blocks left over and their combined size is,0
"larger than minSplitNode, then combine them into one split.",0
Otherwise add them back to the unprocessed pool. It is likely,0
that they will be combined with other blocks from the,0
same rack later on.,0
This condition also kicks in when max split size is not set. All,0
blocks on a node will be grouped together into a single split.,0
haven't created any split on this machine. so its ok to add a,0
smaller one for parallelism. Otherwise group it in the rack for,0
balanced size create an input split and add it to the splits,0
array,0
Remove entries from blocksInNode so that we don't walk this again.,0
The node is done. This was the last set of blocks for this node.,0
Put the unplaced blocks back into the pool for later rack-allocation.,0
Node is done. All blocks were fit into node-local splits.,0
Check if node-local assignments are complete.,0
All nodes have been walked over and marked as completed or all blocks,0
have been assigned. The rest should be handled via rackLock assignment.,0
"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """,0
"+ completedNodes.size() + "", size left: "" + totalLength);",0
"if blocks in a rack are below the specified minimum size, then keep them",0
"in 'overflow'. After the processing of all racks is complete, these",0
overflow blocks will be combined into splits.,0
Process all racks over and over again until there is no more work to do.,0
Create one split for this rack before moving over to the next rack.,0
Come back to this rack after creating a single split for each of the,0
remaining racks.,0
"Process one rack location at a time, Combine all possible blocks that",0
reside on this rack as one split. (constrained by minimum and maximum,0
split size).,0
iterate over all racks,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"if we created a split, then just go to the next rack",0
"if there is a minimum size specified, then create a single split",0
"otherwise, store these blocks into overflow data structure",0
There were a few blocks in this rack that,0
remained to be processed. Keep them in 'overflow' block list.,0
These will be combined later.,0
Process all overflow blocks,0
"This might cause an exiting rack location to be re-added,",0
but it should be ok.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"Process any remaining blocks, if any.",0
create an input split,0
add this split to the list that is returned,0
long num = totLength / maxSize;,0
all blocks for all the files in input set,0
mapping from a rack name to the list of blocks it has,0
mapping from a block to the nodes on which it has replicas,0
mapping from a node to the list of blocks that it contains,0
populate all the blocks for all files,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
"Security framework already loaded the tokens into current UGI, just use",0
them,0
Now remove the AM->RM token so tasks don't have it,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init container allocator,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
Init Client manager,0
Init PS Client manager,0
init parameter server manager,0
recover task information if needed,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
start a web service if use yarn deploy mode,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
Check Workers,0
Check PSS,0
Check Clients,0
Check PS Clients,0
parse parameter server counters,0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
refresh last heartbeat timestamp,0
send a state update event to the specific PSAttempt,0
check if parameter server can commit now.,0
check matrix metadata inconsistencies between master and parameter server.,0
"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix",0
"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix",0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
choose a unused port,0
start RPC server,0
remove this parameter server attempt from monitor set,0
remove this parameter server attempt from monitor set,0
"if worker attempt id is not in monitor set, we should shutdown it",0
find workergroup in worker manager,0
"if this worker group does not initialized, just return WORKERGROUP_NOTREADY",0
"if this worker group run over, just return WORKERGROUP_EXITED",0
"if this worker group is running now, return tasks, workers, data splits for it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update the clock for this matrix,0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update task iteration,0
"Calculate how many splits we need. As each task handles a separate split of data, so we want",0
the number of splits equal to the number of tasks,0
split data,0
dispatch the splits to workergroups,0
split data,0
dispatch the splits to workergroups,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
write meta data to a temporary file,0
rename the temporary file to final file,0
"if the file exists, read from file and deserialize it",0
write task meta,0
write ps meta,0
generate a temporary file,0
write task meta to the temporary file first,0
rename the temporary file to the final file,0
"if last final task file exist, remove it",0
find task meta file which has max timestamp,0
"if the file does not exist, just return null",0
read task meta from file and deserialize it,0
generate a temporary file,0
write ps meta to the temporary file first.,0
rename the temporary file to the final file,0
"if the old final file exist, just remove it",0
find ps meta file,0
"if ps meta file does not exist, just return null",0
read ps meta from file and deserialize it,0
Init matrix files meta,0
Move output files,0
Write the meta file,0
check whether psagent heartbeat timeout,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
LocalStorageToken is needed irrespective of whether security is enabled,0
or not.,0
"TokenCache.setJobToken(jobToken, taskCredentials);",0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Construct the actual Container,0
The null fields are per-container and will be constructed for each,0
container separately.,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
"a * in the classpath will only find a .jar, so we need to filter out",0
all .jars and add everything else,0
Propagate the system classpath when using the mini cluster,0
Add standard Hadoop classes,0
Add mr,0
Cache archives,0
Cache files,0
Sanity check,0
Add URI fragment or just the filename,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Final commmand,0
"if amTask is not null, we should clone task state from it",0
"if all parameter server complete commit, master can commit now",0
init and start master committer,0
check whether parameter server heartbeat timeout,0
Transitions from the NEW state.,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event",0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event dispatched before",0
PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.COMMITTING state,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
reqeuest resource:send a resource request to the resource allocator,0
"Once the resource is applied, build and send the launch request to the container launcher",0
deallocator the resource of the ps attempt:send a resource deallocator request to the,0
resource allocator,0
set the launch time,0
add the ps attempt to the heartbeat timeout monitoring list,0
parse ps attempt location and put it to location manager,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt",0
or failed,0
remove ps attempt id from heartbeat timeout monitor list,0
release container:send a release request to container launcher,0
set the finish time only if launch time is set,0
private long scheduledTime;,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
Refresh ps location & matrix meta,0
start a new attempt for this ps,0
notify ps manager,0
"getContext().getLocationManager().setPsLocation(id, null);",0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
notify the event handler of state change,0
"If new state is not RUNNING and COMMITTING, add it to state timeout monitor",0
"if forcedState is set, just return",0
else get state from state machine,0
add this worker group to the success set,0
check if all worker group run over,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
calculate the actual number of worker groups and the total number of tasks based on the number of data split,0
"init all tasks , workers and worker groups and put them to the corresponding maps",0
just return the total task number now,0
TODO,1
check whether worker heartbeat timeout,0
"if workerAttempt is not null, we should clone task state from it",0
from NEW state,0
from SCHEDULED state,0
get data splits location for data locality,0
reqeuest resource:send a resource request to the resource allocator,0
"once the resource is applied, build and send the launch request to the container launcher",0
notify failed message to the worker,0
notify killed message to the worker,0
release the allocated container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
release the allocated container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
clean the container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
record the finish time,0
clean the container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listening list,0
record the finish time,0
"if the worker attempt launch successfully, add it to heartbeat timeout listening list",0
set worker attempt location,0
notify the register message to the worker,0
record the launch time,0
update worker attempt metrics,0
update tasks metrics,0
clean the container,0
notify the worker attempt run successfully message to the worker,0
record the finish time,0
init a worker attempt for the worker,0
schedule the worker attempt,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
"If we need Yarn to restart a new application master, we should not unregister from Yarn RM",0
register to Yarn RM,0
send heartbeat to Yarn RM every rmPollInterval milliseconds,0
"catch YarnRuntimeException, we should exit and need not retry",0
build heartbeat request,0
send heartbeat request to rm,0
"This can happen if the RM has been restarted. If it is in that state,",0
this application must clean itself up.,0
Setting NMTokens,0
assgin containers,0
"if some container is not assigned, release them",0
handle finish containers,0
dispatch container exit message to corresponding components,0
killed by framework,0
killed by framework,0
get application finish state,0
build application diagnostics,0
TODO:add a job history for angel,1
build unregister request,0
send unregister request to rm,0
Note this down for next interaction with ResourceManager,0
based on blacklisting comments above we can end up decrementing more,0
than requested. so guard for that.,0
send the updated resource request to RM,0
send 0 container count requests also to cancel previous requests,0
Update resource requests,0
try to assign to all nodes first to match node local,0
try to match all rack local,0
assign remaining,0
Update resource requests,0
send the container-assigned event to task attempt,0
build the start container request use launch context,0
send the start request to Yarn nm,0
send the message that the container starts successfully to the corresponding component,0
"after launching, send launched event to task attempt to move",0
it from ASSIGNED to RUNNING state,0
send the message that the container starts failed to the corresponding component,0
kill the remote container if already launched,0
start a thread pool to startup the container,0
See if we need up the pool size only if haven't reached the,0
maximum limit yet.,0
nodes where containers will run at *this* point of time. This is,0
*not* the cluster size and doesn't need to be.,0
"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the",0
later is just a buffer so we are not always increasing the,0
pool-size,0
the events from the queue are handled in parallel,0
using a thread pool,0
return if already stopped,0
shutdown any containers that might be left running,0
"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
get matrix ids in the parameter server report,0
get the matrices parameter server need to create and delete,0
"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix",0
"if a matrix exists on master but not exist on parameter server, this parameter server need build it.",0
Init control connection manager,0
Get ps locations from master and put them to the location cache.,0
Build and initialize rpc client to master,0
Get psagent id,0
Build PS control rpc client manager,0
Build local location,0
Initialize matrix meta information,0
Start all services,0
Stop all modules,0
Stop all modules,0
Update generic resource counters,0
Updating resources specified in ResourceCalculatorProcessTree,0
Remove the CPU time consumed previously by JVM reuse,0
Generate a flush request and put it to request queue,0
Generate a clock request and put it to request queue,0
Generate a merge request and put it to request queue,0
"If the matrix op log cache does not exist for the matrix, create a new one for the",0
matrix,0
and add it to cache maps,0
Add the message to the tree map,0
"If there are flush / clock requests blocked, we need to put this merge request into",0
the waiting queue,0
Launch a merge worker to merge the update to matrix op log cache,0
Remove the message from the tree map,0
Wake up blocked flush/clock request,0
Add flush/clock request to listener list to waiting for all the existing,0
updates are merged,0
Wake up blocked flush/clock request,0
"If all updates are merged for this matrix, we need wake up flush/clock requests which are",0
blocked.,0
Get next merge message sequence id,0
Wake up listeners(flush/clock requests) that have little sequence id than current merge,0
position,0
Wake up blocked merge requests,0
Get minimal sequence id from listeners,0
"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we",0
should flush updates to local matrix storage,0
unused now,1
"Filter it, removing zero values",0
Doing average or not,0
Split this row according the matrix partitions,0
Add the splits to the result container,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());",0
TODO: use Epoll for linux future,1
Update location table,0
Remove the server from failed list,0
Notify refresh success message to request dispatcher,0
Check PS exist or not,0
Check heartbeat timeout,0
Check PS restart or not,0
private final HashSet<ParameterServerId> refreshingServerSet;,0
Add it to failed rpc list,0
Add the server to gray server list,0
Add it to failed rpc list,0
Add the server to gray server list,0
Move from gray server list to failed server list,0
Handle the RPCS to this server,0
Submit the schedulable failed get RPCS,0
Submit new get RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"If the queue is empty, just return 0",0
"If request is not over limit, just submit it",0
Submit the schedulable failed get RPCS,0
Submit new put RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"LOG.info(""choose put server "" + psIds[index]);",0
Check all pending RPCS,0
Check get channel context,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
channelManager.printPools();,0
"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {",0
if(ts - entry.getValue() > requestTimeOut * 2)  {,0
"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())",0
"+ "" milliseconds, close all channels to it"");",0
closeChannels(entry.getKey());,0
"psLocToLastChannelTsMap.put(entry.getKey(), ts);",0
},0
},0
"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);",0
Remove all pending RPCS,0
Close all channel to this PS,0
Get server id and location for this request,0
"If location is null, means that the server is not ready",0
Get the channel for the location,0
Check if need get token first,0
Serialize the request,0
Send the request,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
Allocate the bytebuf and serialize the request,0
find the partition request context from cache,0
Check if the result of the sub-request is received,0
Update received result number,0
Get row splits received,0
Put the row split to the cache(row index to row splits map),0
"If all splits of the row are received, means this row can be merged",0
TODO Auto-generated method stub,1
"Check futures, if the result of a sub-request is received, put it to the result queue",0
Now we just support pipelined row splits merging for dense type row,0
Get partitions for this row,0
First get this row from matrix storage,0
"If the row exists in the matrix storage and the clock value meets the requirements, just",0
return,0
Get row splits of this row from the matrix cache first,0
"If the row split does not exist in cache, get it from parameter server",0
Wait the final result,0
Put it to the matrix cache,0
Split the matrix oplog according to the matrix partitions,0
"If need update clock, we should send requests to all partitions",0
Filter the rowIds which are fetching now,0
Send the rowIndex to rpc dispatcher and return immediately,0
Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition,0
Generate dispatch items and add them to the corresponding queues,0
Filter the rowIds which are fetching now,0
Pre-fetching is disable default,0
matrix id to clock map,0
"task index, it must be unique for whole application",0
Deserialize data splits meta,0
Get workers,0
Send request to every ps,0
Wait the responses,0
Update clock cache,0
if(syncNum % 1024 == 0) {,0
},0
"Use simple flow, do not use any cache",0
Get row from cache.,0
"if row clock is satisfy ssp staleness limit, just return.",0
Get row from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC mode, just get from pss.",0
"For BSP/SSP, get rows from storage/cache first",0
Get from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC, just get rows from pss.",0
no more retries.,0
calculate sleep time and return.,0
parse the i-th sleep-time,0
parse the i-th number-of-retries,0
calculateSleepTime may overflow.,0
"A few common retry policies, with no delays.",0
close is a local operation and should finish within milliseconds; timeout just to be safe,0
response will be null for one way messages.,0
maxFrameLength = 2G,0
lengthFieldOffset = 0,0
lengthFieldLength = 8,0
"lengthAdjustment = -8, i.e. exclude the 8 byte length itself",0
"initialBytesToStrip = 8, i.e. strip out the length field itself",0
indicates whether this connection's life cycle is managed,0
See if we already have a connection (common case),0
create a unique lock for this RS + protocol (if necessary),0
get the RS lock,0
do one more lookup in case we were stalled above,0
Only create isa when we need to.,0
definitely a cache miss. establish an RPC for,0
this RS,0
Throw what the RemoteException was carrying.,0
check,0
every,0
minutes,0
TODO,1
failoverHandler,0
"The number of times this invocation handler has ever been failed over,",0
before this method invocation attempt. Used to prevent concurrent,0
failed method invocations from triggering multiple failover attempts.,0
Make sure that concurrent failed method invocations,0
only cause a,0
single actual fail over.,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
"LOG.info(""method "" + method.getName() + ""construct request time = """,0
+ (System.currentTimeMillis() - beforeConstructTs));,0
get an instance of the method arg type,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
Message (hand written code usually has only a single,0
argument),0
log any RPC responses that are slower than the configured,0
warn,0
response time or larger than configured warning size,0
"when tagging, we let TooLarge trump TooSmall to keep",0
output simple,0
note that large responses will often also be slow.,0
provides a count of log-reported slow responses,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
unexpected,0
"in the protobuf methods, args[1] is the only significant argument",0
for JSON encoding,0
base information that is reported regardless of type of call,0
Disable Nagle's Algorithm since we don't want packets to wait,0
Configure the event pipeline factory.,0
Make a new connection.,0
Remove all pending requests (will be canceled after relinquishing,0
write lock).,0
Cancel any pending requests by sending errors to the callbacks:,0
Close the channel:,0
Close the connection:,0
Shut down all thread pools to exit.,0
"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());",0
See NettyServer.prepareResponse for where we write out the response.,0
"It writes the call.id (int), a boolean signifying any error (and if",0
"so the exception name/trace), and the response bytes",0
Read the call id.,0
"When the stream is closed, protobuf doesn't raise an EOFException,",0
"instead, it returns a null message object.",0
"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +",0
System.currentTimeMillis());,0
"It would be good widen this to just Throwable, but IOException is what we",0
allow now,0
not implemented,1
not implemented,1
"track what RpcEngine is used by a proxy class, for stopProxy()",0
cache of RpcEngines by protocol,0
return the RpcEngine configured to handle a protocol,0
We only handle the ConnectException.,0
This is the exception we can't handle.,0
check if timed out,0
wait for retry,0
IGNORE,0
return the RpcEngine that handles a proxy object,0
The default implementation works synchronously,0
punt: allocate a new buffer & copy into it,0
Parse cmd parameters,0
load hadoop configuration,0
load angel system configuration,0
load user configuration:,0
load user config file,0
load command line parameters,0
load user job resource files,0
load user job jar if it exist,0
Expand the environment variable,0
Add default fs(local fs) for lib jars.,0
"LOG.info(System.getProperty(""user.dir""));",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
a simple hdfs copy function assume src path and dest path are in same hdfs,0
and FileSystem object has same schema,0
"LOG.warn(""interrupted while sleeping"", ie);",0
public static String getHostname() {,0
try {,0
"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();",0
} catch (UnknownHostException uhe) {,0
},0
"return new StringBuilder().append("""").append(uhe).toString();",0
},0
"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {",0
String hostname = getHostname();,0
String classname = clazz.getSimpleName();,0
"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new",0
"StringBuilder().append(""Starting "").append(classname).toString(), new",0
"StringBuilder().append(""  host = "").append(hostname).toString(), new",0
"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new",0
"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new",0
"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));",0
,0
"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {",0
public void run() {,0
"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +",0
"this.val$classname + "" at "" + this.val$hostname}));",0
},0
});,0
},0
"We we interrupted because we're meant to stop? If not, just",0
continue ignoring the interruption,0
Recalculate waitTime.,0
// Begin delegation to Thread,0
// End delegation to Thread,0
instance submitter class,0
Obtain filename from path,0
Split filename to prexif and suffix (extension),0
Check if the filename is okay,0
Prepare temporary file,0
Prepare buffer for data copying,0
Open and check input stream,0
Open output stream and copy data between source file in JAR and the temporary file,0
"If read/write fails, close streams safely before throwing an exception",0
"Finally, load the library",0
little endian load order,0
tail,0
fallthrough,0
fallthrough,0
finalization,0
fmix(h1);,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
JobStateProto jobState = report.getJobState();,0
Used for java code to get a AngelClient instance,0
Used for python code to get a AngelClient instance,0
the leaf level file should be readable by others,0
the subdirs in the path should have execute permissions for,0
others,0
2.get job id,0
Credentials credentials = new Credentials();,0
4.copy resource files to hdfs,0
5.write configuration to a xml file,0
6.create am container context,0
7.Submit to ResourceManager,0
8.get app master client,0
Create a number of filenames in the JobTracker's fs namespace,0
add all the command line files/ jars and archive,0
first copy them to jobtrackers filesystem,0
should not throw a uri exception,0
should not throw an uri excpetion,0
set the timestamps of the archives and files,0
set the public/private visibility of the archives and files,0
get DelegationToken for each cached file,0
check if we do not need to copy the files,0
is jt using the same file system.,0
just checking for uri strings... doing no dns lookups,0
to see if the filesystems are the same. This is not optimal.,0
but avoids name resolution.,0
this might have name collisions. copy will throw an exception,0
parse the original path to create new path,0
check for ports,0
Write job file to JobTracker's fs,0
Setup resource requirements,0
Setup LocalResources,0
Setup security tokens,0
Setup the command to run the AM,0
Add AM user command opts,0
Final command,0
Setup the CLASSPATH in environment,0
"i.e. add { Hadoop jars, job jar, CWD } to classpath.",0
Setup the environment variables for Admin first,0
"Setup the environment variables (LD_LIBRARY_PATH, etc)",0
Parse distributed cache,0
Setup ContainerLaunchContext for AM container,0
Set up the ApplicationSubmissionContext,0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this location, only master ps can accept the update",0
Check the partition state again,0
Start to put the update to the slave pss,0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this partition, if not, just return failed",0
Start to put the update to the slave pss,0
Add tokens to new user so that it may execute its task correctly.,0
to exit,0
context.getSnapshotManager().processRecovery();,0
private final ParameterServer psServer;,0
return ServerState.GENERAL;,0
lock.readLock().lock();,0
lock.readLock().unlock();,0
data.rewind();,0
lock.readLock().lock();,0
lock.readLock().unlock();,0
data.rewind();,0
lock.readLock().lock();,0
lock.readLock().unlock();,0
data.rewind();,0
output.writeInt(clock);,0
clock = input.readInt();,0
output.writeDouble(getDefaultValue());,0
private final List<PartitionKey> partitionKeys;,0
Read matrix meta from meta file,0
Load partitions from file use fork-join,0
Save partitions to files use fork-join,0
Write the ps matrix meta to the meta file,0
Mapping from taskId to clock value.,0
TODO: use Epoll for linux future,1
find the partition request context from cache,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
channelManager.removeChannelPool(loc);,0
Generate seq id,0
Create a RecoverPartRequest,0
Serialize the request,0
Change the seqId for the request,0
Serialize the request,0
"add the PSAgentContext,need fix",0
return this;,0
return this;,0
return this;,0
return this;,0
return this;,0
TODO Should be implemented,1
TODO Should be implemented,1
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
TODO:,1
public String uuid;,0
this.uuid = UUID.randomUUID().toString();,0
byte [] data = uuid.getBytes();,0
buf.writeInt(data.length);,0
buf.writeBytes(data);,0
int size = buf.readInt();,0
byte [] data = new byte[size];,0
buf.readBytes(data);,0
uuid = new String(data);,0
"return ""PartitionRequest{"" + ""clock="" + clock + "", partKey="" + partKey + "", uuid="" + uuid + "", comeFromPs=""",0
"+ comeFromPs + ""} "" + super.toString();",0
public String uuid;,0
write the max abs,0
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
get configuration from config file,0
set localDir with enviroment set by nm.,0
get master location,0
init task manager and start tasks,0
start heartbeat thread,0
taskManager.assignTaskIds(response.getTaskidsList());,0
todo,1
"if worker timeout, it may be knocked off.",0
"SUCCESS, do nothing",0
heartbeatFailedTime = 0;,0
private KEY currentKey;,0
will be created,0
TODO Auto-generated method stub,1
Bitmap bitmap = new Bitmap();,0
int max = indexArray[size - 1];,0
byte [] bitIndexArray = new byte[max / 8 + 1];,0
for(int i = 0; i < size; i++){,0
int bitIndex = indexArray[i] >> 3;,0
int bitOffset = indexArray[i] - (bitIndex << 3);,0
switch(bitOffset){,0
case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;,0
case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;,0
case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;,0
case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;,0
case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;,0
case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;,0
case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;,0
case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;,0
},0
},0
//////////////////////////////,0
Application Configs,0
//////////////////////////////,0
//////////////////////////////,0
Master Configs,0
//////////////////////////////,0
//////////////////////////////,0
Worker Configs,0
//////////////////////////////,0
//////////////////////////////,0
Task Configs,0
//////////////////////////////,0
//////////////////////////////,0
ParameterServer Configs,0
//////////////////////////////,0
////////////////// IPC //////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
Configs used to ANGEL_PS_PSAGENT running mode future.,0
model parse,0
Mark whether use pyangel or not.,0
private Configuration conf;,0
"Configuration that should be used in python environment, there should only be one",0
configuration instance in each Angel context.,0
Use private access means jconf should not be changed or modified in this way.,0
Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:,0
Do nothing,0
To-DO: add other ways to justify different value types,0
"This is so ugly, must re-implement by more elegance way",1
"Create python path which include angel's jars, the python directory in ANGEL_HOME,",0
and other files submitted by user.,0
Launch python process,0
TODO Auto-generated constructor stub,1
Feature number of train data,0
Total iteration number,0
Rank,0
Regularization parameters,0
Learn rage,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set feature number,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set FM predict output path,0
Set actionType train,0
Set feature number,0
Feature number of train data,0
Number of nonzero features,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Data format,0
Learning rate,0
Set basic configuration keys,0
Use local deploy mode and dummy data spliter,0
"set input, output path",0
"set angel resource parameters #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Submit GBDT Train Task,0
Load Model from HDFS.,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
"Set trainning data, save model, log path",0
Set actionType train,0
Set MF algorithm parameters,0
Feature number of train data,0
Total iteration number,0
number of mini batch within a update periorid,0
"Data format, libsvm or dummy",0
Batch size,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"conf.setBoolean(MLConf.ML_INDEX_GET_ENABLE(), true);",0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Sample ratio,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set FTRL LR algorithm parameters #feature #epoch,0
FtrlLRPredictTest();,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set predict data path,0
Set load model path,0
Set predict result path,0
Set actionType train,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
LOG.info(sigmoid(data[i]));,0
LOG.info(Math.exp(-data[i]));,0
when b is a negative number,0
LOG.info(sigmoid(data[i]));,0
LOG.info(Math.exp(-data[i]));,0
when b is a negative number,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set MLR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set log path,0
Set actionType incremental train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Rank,0
Regularization parameters,0
Learn rage,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set feature number,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set learnType,0
Set feature number,0
Set,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set FM predict output path,0
Set actionType train,0
Set learnType,0
Set feature number,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log sava path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation Ratio,0
Data format,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set basic configuration keys,0
Set data format,0
Use local deploy mode,0
"set angel resource parameters #worker, #task, #PS",0
set sgd SVM algorithm parameters,0
"set input, output path",0
Set save model path,0
Set actionType train,0
Set log path,0
Submit LR Train Task,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set log sava path,0
Set actionType prediction,0
double z=pre*y;,0
if(z<=0) return 0.5-z;,0
"else if(z>0 && z<1) return 0.5*Math.pow(1-z,2);",0
return 0.0;,0
logistic loss for binary classification task.,0
"logistic loss, but predict un-transformed margin",0
check if label in range,0
return the default evaluation metric for the objective,0
TODO Auto-generated method stub,1
start row index for words,0
doc ids,0
topic assignments,0
count word,0
build word start index,0
build dks,0
"model.wtMat().increment(w, update);",0
"update.plusBy(t, 1);",0
"model.wtMat().increment(w, update);",0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
Write #rows,0
Write each row,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
calculate columns,0
loss function,0
gradient and hessian,0
"categorical feature set, null: none, empty: all, else: partial",0
tree node,0
initialize the phase,0
current tree and depth,0
create loss function,0
calculate grad info of each instance,0
"create data sketch, push candidate split value to PS",0
1. calculate candidate split value,0
categorical features,0
2. push local sketch to PS,0
3. set phase to GET_SKETCH,0
the leader worker,0
merge categorical features,0
create updates,0
"pull the global sketch from PS, only called once by each worker",0
number of categorical feature,0
sample feature,0
push sampled feature set to the current tree,0
create new tree,0
"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,",0
calculate gradient,0
"1. create new tree, initialize tree nodes and node stats",0
"2. initialize feature set, if sampled, get from PS, otherwise use all the features",0
2.1. pull the sampled features of the current tree,0
"2.2. if use all the features, only called one",0
"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle",0
4. set root node to active,0
"5. reset instance position, set the root node's span",0
6. calculate gradient,0
7. set phase to run active,0
1. start threads of active tree nodes,0
1.1. start threads for active nodes to generate histogram,0
1.2. set thread status to batch num,0
1.3. set the oplog to active,0
"2. check thread stats, if all threads finish, return",0
clock,0
find split,0
"1. find responsible tree node, using RR scheme",0
2. pull gradient histogram,0
2.1. get the name of this node's gradient histogram on PS,0
2.2. pull the histogram,0
histogram = (TDoubleVector) ((GetRowResult) histMat.get(func)).getRow();,0
2.3. find best split result of this tree node,0
2.3.1 using server split,0
"update the grad stats of the root node on PS, only called once by leader worker",0
update the grad stats of children node,0
update the left child,0
update the right child,0
"2.3.2 the updated split result (tree node/feature/value/gain) on PS,",0
"2.3.3 otherwise, the returned histogram contains the gradient info",0
"2.3.4 the updated split result (tree node/feature/value/gain) on PS,",0
2.3.5 reset this tree node's gradient histogram to 0,0
3. push split feature to PS,0
4. push split value to PS,0
5. push split gain to PS,0
6. set phase to AFTER_SPLIT,0
clock,0
1. get split feature,0
2. get split value,0
3. get split gain,0
4. get node weight,0
5. split node,0
"2. check thread stats, if all threads finish, return",0
6. clock,0
"split the span of one node, reset the instance position",0
in case this worker has no instance on this node,0
set the span of left child,0
set the span of right child,0
"1. left to right, find the first instance that should be in the right child",0
"2. right to left, find the first instance that should be in the left child",0
3. swap two instances,0
4. find the cut pos,0
than the split value,0
5. set the span of left child,0
6. set the span of right child,0
set tree node to active,0
set node to leaf,0
set node to inactive,0
finish current tree,0
finish current depth,0
set the tree phase,0
check if there is active node,0
check if finish all the tree,0
update node's grad stats on PS,0
"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split",0
the root node's stats is updated by leader worker,0
1. create the update,0
2. push the update to PS,0
the leader task adds node prediction to flush list,0
1. name of this node's grad histogram on PS,0
2. build the grad histogram of this node,0
3. push the histograms to PS,0
4. reset thread stats to finished,0
5.1. set the children nodes of this node,0
5.2. set split info and grad stats to this node,0
5.2. create children nodes,0
"5.3. create node stats for children nodes, and add them to the tree",0
5.4. reset instance position,0
"5.5. add new active nodes if possible, inc depth, otherwise finish this tree",0
5.6. set children nodes to leaf nodes,0
5.7. set nid to leaf node,0
5.8. deactivate active node,0
"get feature type, 0:empty 1:all equal 2:real",0
left child <= split value; right child > split value,0
"the first: minimal, the last: maximal",0
categorical features,0
continuous features,0
left child <= split value; right child > split value,0
feature index used to split,0
feature value used to split,0
loss change after split this node,0
grad stats of the left child,0
grad stats of the right child,0
"LOG.info(""Constructor with fid = -1"");",0
fid = -1: no split currently,0
the minimal split value is the minimal value of feature,0
the splits do not include the maximal value of feature,0
"1. the average distance, (maxValue - minValue) / splitNum",0
2. calculate the candidate split value,0
1. new feature's histogram (grad + hess),0
size: sampled_featureNum * (2 * splitNum),0
"in other words, concatenate each feature's histogram",0
2. get the span of this node,0
------ 3. using sparse-aware method to build histogram ---,0
"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances",0
3.1. get the instance index,0
3.2. get the grad and hess of the instance,0
3.3. add to the sum,0
3.4. loop the non-zero entries,0
3.4.1. get feature value,0
3.4.2. current feature's position in the sampled feature set,0
3.4.3. find the position of feature value in a histogram,0
"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]",0
3.4.4. add the grad and hess to the corresponding bin,0
3.4.5. add the reverse to the bin that contains 0.0f,0
4. add the grad and hess sum to the zero bin of all features,0
find the best split result of the histogram of a tree node,0
1. calculate the gradStats of the root node,0
"1.1. update the grad stats of the root node on PS, only called once by leader worker",0
2. loop over features,0
2.1. get the ture feature id in the sampled feature set,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"update the grad stats of the root node on PS, only called once by leader worker",0
3. update the grad stats of children node,0
3.1. update the left child,0
3.2. update the right child,0
find the best split result of one feature,0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
split value = sketches[splitIdx],0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
partition number,0
cols of each partition,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
"loop all the possible split value, start from split[0], the first item is the minimal feature value",0
find the best split result of the histogram of a tree node,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
find the best split result of one feature,0
1. set the feature id,0
splitEntry.setFid(fid);,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]",0
the task use index to find fvalue,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting",0
"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking.",0
clear all the information,0
calculate the sum of gradient and hess,0
"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,",0
ridx),0
check if necessary information is ready,0
"same as add, reduce is used in All Reduce",0
"features used in this tree, if equals null, means use all the features without sampling",0
node in the tree,0
the gradient info of each instances,0
initialize nodes,0
gradient,0
second order gradient,0
int sendStartCol = startFid * 7; // each split contains 7 doubles,0
"System.out.println(""Indices: "" + Arrays.toString(indices));",0
t[i][code]++;,0
else if (Math.random() > 0.5) {,0
t[i][code] = freq;,0
},0
"System.out.println(""Change from "" + t[i][code] + "" to "" + freq);",0
"ret = Math.min(ret, t[i][h[i].encode(key)]);",0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
"task type: classification, regression, or ranking",0
"quantile sketch, size = featureNum * splitNum",0
"gradient histograms, size = treeNodeNum * featureNum * splitNum",0
"active tree nodes, size = pow(2, treeDepth) -1",0
sampled features. size = treeNum * sampleRatio * featureNum,0
categorical feature. size = workerNum * cateFeatNum * splitNum,0
"split features, size = treeNum * treeNodeNum",0
"split values, size = treeNum * treeNodeNum",0
"split gains, size = treeNum * treeNodeNum",0
"node weights, size = treeNum * treeNodeNum",0
"node preds, size = treeNum * treeNodeNum",0
if using PS to perform split,0
step size for a tree,0
number of class,0
minimum loss change required for a split,0
maximum depth of a tree,0
number of features,0
number of nonzero,0
number of candidates split value,0
----- the rest parameters are less important ----,0
base instance weight,0
minimum amount of hessian(weight) allowed in a child,0
L2 regularization factor,0
L1 regularization factor,0
default direction choice,0
maximum delta update we can add in weight estimation,0
this parameter can be used to stabilize update,0
default=0 means no constraint on weight delta,0
whether we want to do subsample for row,0
whether to subsample columns for each tree,0
accuracy of sketch,0
accuracy of sketch,0
leaf vector size,0
option for parallelization,0
option to open cacheline optimization,0
whether to not print info during training.,0
maximum depth of the tree,0
number of features used for tree construction,0
"minimum loss change required for a split, otherwise stop split",0
----- the rest parameters are less important ----,0
default direction choice,0
whether we want to do sample data,0
whether to sample columns during tree construction,0
whether to use histogram for split,0
number of histogram units,0
whether to print info during training.,0
----- the rest parameters are obtained after training ----,0
total number of nodes,0
number of deleted nodes */,0
@maxIndex: this variable contains the max index of node/word,0
some params,0
max index for node/word,0
compute number of nodes for one row,0
check the length of dot values,0
merge dot values from all partitions,0
Skip-Gram model,0
Negative sampling,0
used to accumulate the updates for input vectors,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
Skip-Gram model,0
Negative sampling,0
used to accumulate the updates for input vectors,0
Negative sampling,0
accumulate for the hidden layer,0
update output layer,0
update the hidden layer,0
update input,0
update output,0
Some params,0
compute number of nodes for one row,0
compress the neighbor IDs,0
write out edges,0
write out tags,0
Get node neighbors,0
Use by line with weight,0
evict entry with the smallest degree,0
// calculate bias,0
if (param.getPartKey().getStartCol() <= 0 && param.getPartKey().getEndCol() > 0) {,0
"double zVal = VectorUtils.getDouble(z, 0);",0
"double nVal = VectorUtils.getDouble(n, 0);",0
"VectorUtils.setFloat(w, 0, (float) (-1.0 * alpha * zVal / (beta + Math.sqrt(nVal))));",0
},0
Do nothing.,0
split updates,0
shuffle update splits,0
generate part update splits,0
"set split context: partition key, use int key for long key vector or net",0
how to do intersection for two dense vector with a given indices ??,0
copy the highest levels,0
copy baseBuffer,0
merge two non-empty quantile sketches,0
"if not -1, sufficient space will be allocated at once",0
InstanceRow ins = instanceRows[insId];,0
int[] indices = ins.indices();,0
int[] bins = ins.bins();,0
int nnz = indices.length;,0
for (int j = 0; j < nnz; j++) {,0
int fid = indices[j];,0
if (isFeatUsed[fid - featLo]) {,0
"histograms[fid - featLo].accumulate(bins[j], gradPairs[insId]);",0
},0
},0
1. allocate histogram,0
"2. loop non-zero instances, accumulate to histogram",0
if (nnz <= nodeEnd - nodeStart + 1) { // loop all nnz of current feature,0
3. add remaining grad and hess to default bin,0
"return param.calcWeights(grad, hess);",0
"numClass is usually small, so we do not use arraycopy here",0
"numClass is usually small, so we do not use arraycopy here",0
TODO: use more schema on default bin,1
1. set default bin to left child,0
"2. for other bins, find its location",0
3. create split set,0
this.nodeStats = new GBTNodeStat[numClass == 2 ? 1 : numClass];,0
predict sparse instance with indices and values,0
predict libsvm data,0
"Preconditions.checkArgument(preds.length == labels.length,",0
"""LogLossMetric should be used for binary-label classification"");",0
double loss = 0.0;,0
for (int i = 0; i < preds.length; i++) {,0
"loss += evalOne(preds[i], labels[i]);",0
},0
return loss / labels.length;,0
double error = 0.0;,0
if (preds.length == labels.length) {,0
for (int i = 0; i < preds.length; i++) {,0
"error += evalOne(preds[i], labels[i]);",0
},0
} else {,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"error += evalOne(pred, labels[i]);",0
},0
},0
return error / labels.length;,0
Preconditions.checkArgument(preds.length != labels.length,0
"&& preds.length % labels.length == 0,",0
"""CrossEntropyMetric should be used for multi-label classification"");",0
double loss = 0.0;,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"loss += evalOne(pred, labels[i]);",0
},0
return loss / labels.length;,0
double correct = 0.0;,0
if (preds.length == labels.length) {,0
for (int i = 0; i < preds.length; i++) {,0
"correct += evalOne(preds[i], labels[i]);",0
},0
} else {,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"correct += evalOne(pred, labels[i]);",0
},0
},0
return (float) (correct / labels.length);,0
double errSum = 0.0f;,0
if (preds.length == labels.length) {,0
for (int i = 0; i < preds.length; i++) {,0
"errSum += evalOne(preds[i], labels[i]);",0
},0
} else {,0
int numLabel = preds.length / labels.length;,0
float[] pred = new float[numLabel];,0
for (int i = 0; i < labels.length; i++) {,0
"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);",0
"errSum += evalOne(pred, labels[i]);",0
},0
},0
return Math.sqrt(errSum / labels.length);,0
"System.out.println(""----------"");",0
"System.out.println(""read buffer = "" + Integer.toBinaryString(readBufferT & 0b11111111)",0
"+ "", mask = "" + Integer.toBinaryString(readMaskT));",0
readMaskT <<= 1;,0
"boolean[] bits = new boolean[]{true, true, false, false, true, false, false, false, true, true, false, true};",0
int n = bits.length;,0
BufferedBitSet writeBitSet = new BufferedBitSet(n);,0
"BufferedBitSet readBitSet = new BufferedBitSet(writeBitSet.getBytes(), n);",0
if (bitSet.get(i) != bits[i]) {,0
"throw new RuntimeException("""" + i);",0
},0
private final ByteBuffer bytes;,0
"public BufferedBitSetReader(ByteBuffer bytes, int numBits) {",0
int capacity = bytes.capacity() * 8;,0
readIndexT = bytes.capacity() - 1;,0
return bytes.get(index);,0
TODO: use arraycopy to make it faster,1
assert from >= this.from && to <= this.to;,0
"LOG.debug(String.format(""Create subset: [%d-%d]"", newFrom, newTo));",0
"LOG.debug(String.format(""Get overlap: [%d-%d]"", newFrom, newTo));",0
return bits.clone();,0
private final SerializableBuffer bytes;,0
private final ByteBuffer bytes;,0
this.bytes = ByteBuffer.allocate(numBytes);,0
public BufferedBitSetWriter(ByteBuffer bytes) {,0
this.bytes = bytes;,0
},0
"bytes.put(writeIndex++, (byte) writeBuffer);",0
public ByteBuffer getBytes() {,0
return bytes;,0
},0
ML TreeConf,0
GBDT TreeConf,0
"edges=[x,...] firstFlow=1 => go to right if < x and go to left if > x",0
"edges=[x,...] firstFlow=0 => go to left if < x and go to right if > x",0
"different types of tree node splits, enumerated by their complexity",0
"in order to reduce model size, we give priority to split point",0
"comparison between two split points, we give priority to lower feature index",0
TODO: comparison between two split sets,0
"public boolean leafwise;  // true if leaf-wise training, false if level-wise training",0
TODO: regularization,1
TODO: regularization,1
public float insSampleRatio;  // subsample ratio for instances,0
set basic configuration keys,0
use local deploy mode and dummy data spliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighbor;,0
import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighborParam;,0
import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighborResult;,0
import com.tencent.angel.ml.matrix.psf.graph.adjacency.initneighbor.InitNeighbor;,0
import com.tencent.angel.ml.matrix.psf.graph.adjacency.initneighbor.InitNeighborParam;,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
@Test,0
"public void testInitAndGet() throws ExecutionException, InterruptedException {",0
Worker worker = LocalClusterContext.get().getWorker(worker0Attempt0Id).getWorker();,0
"MatrixClient client1 = worker.getPSAgent().getMatrixClient(""w2"", 0);",0
int matrixW1Id = client1.getMatrixId();,0
// Generate graph data,0
"Map<Integer, int []> adjMap = generateAdjTable(nodeNum, maxNeighborNum);",0
,0
// Init graph adj table,0
"InitNeighbor func = new InitNeighbor(new InitNeighborParam(matrixW1Id, adjMap));",0
client1.update(func);,0
,0
int [] nodeIds = new int[adjMap.size()];,0
int i = 0;,0
for(int nodeId : adjMap.keySet()) {,0
nodeIds[i++] = nodeId;,0
},0
,0
// Get graph adj table from PS,0
"GetNeighbor getFunc = new GetNeighbor(new GetNeighborParam(matrixW1Id, nodeIds, maxNeighborNum));",0
"Map<Integer, int[]> getResults = ((GetNeighborResult) (client1.get(getFunc)))",0
.getNodeIdToNeighborIndices();,0
,0
// Check the result,0
"for(Entry<Integer, int[]> entry : getResults.entrySet()) {",0
"Assert.assertArrayEquals(entry.getValue(), adjMap.get(entry.getKey()));",0
},0
},0
row 0 is a random uniform,0
row 1 is a random normal,0
row 2 is filled with 1.0,0
import jdk.nashorn.internal.runtime.regexp.joni.Config;,0
"paras[1] = ""abc"";",0
"paras[2] = ""123"";",0
Add standard Hadoop classes,0
Feature number of train data,0
Total iteration number,0
Learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set input data path,0
Set save model path,0
Set actionType train,0
QSLRRunner runner = new QSLRRunner();,0
runner.train(conf);,0
"conf.setInt(AngelConf.ANGEL_STALENESS, -1);",0
Dataset,0
"Data format, libsvm or dummy",0
class number,0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set Softmax algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
Feature number of train data,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Ratio of validation,0
Learning rate,0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set DeepFM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
Model type,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set FM algorithm parameters,0
Set model class,0
Dataset,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set WideAndDeep algorithm parameters,0
Set model class,0
Dataset,0
Data format,0
"Set LDA parameters #V, #K",0
Set file system,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource, #worker, #task, #PS",0
Set LDA algorithm parameters,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set SVM algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Dataset,0
"Data format, libsvm or dummy",0
Model type,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model is classification,0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set file system,0
Set basic configuration keys,0
Use local deploy mode and data format,0
Set data path,0
"Set angel resource parameters #worker, #task, #PS",0
Set LR algorithm parameters,0
Set model class,0
Load model meta,0
Convert model,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
Load model meta,0
Convert model,0
load hadoop configuration,0
"Get input path, output path",0
Init serde,0
"Parse need convert model names, if not set, we will convert all models in input directory",0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
RowOffset rowOffset = partMeta.getRowMetas().get(rowId);,0
input.seek(rowOffset.getOffset());,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model meta,0
Check row type,0
Load model,0
Load model,0
load hadoop configuration,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
worker register,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
add matrix,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
attempt 0,0
attempt1,0
attempt1,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
set basic configuration keys,0
"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());",0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add matrix,0
Thread.sleep(5000);,0
"response = master.getJobReport(null, request);",0
"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);",0
"assertEquals(response.getJobReport().getCurIteration(), jobIteration);",0
"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
testDenseDoubleUDF();,0
testSparseDoubleUDF();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
testDenseDoubleUDF();,0
testSparseDoubleUDF();,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add sparse double matrix,0
add comp dense double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
testDenseDoubleUDF();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
Assert.assertTrue(index.length == row.size());,0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();,0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"LOG.info(""id="" + id + "", value="" + row.get(id));",0
"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntFloatVector row = (IntFloatVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
for (int i = 0; i < feaNum; i++) {,0
"deltaVec.set(i, i);",0
},0
"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));",0
IntLongVector row = (IntLongVector) ((GetRowResult) client1.get(func)).getRow();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add sparse float matrix,0
siMat.setPartitionClass(CSRPartition.class);,0
siMat.setPartitionStorageClass(IntCSRStorage.class);,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
get a angel client,0
add dense double matrix,0
add comp dense double matrix,0
add sparse double matrix,0
add component sparse double matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense float matrix,0
add comp dense float matrix,0
add sparse float matrix,0
add component sparse float matrix,0
add dense long matrix,0
add comp dense long matrix,0
add sparse long matrix,0
add component sparse long matrix,0
add comp dense long double matrix,0
add sparse long-key double matrix,0
add component long-key sparse double matrix,0
add component long-key sparse float matrix,0
add sparse long-key float matrix,0
add component long-key sparse float matrix,0
add component long-key sparse int matrix,0
add sparse long-key int matrix,0
add component long-key sparse int matrix,0
add component long-key sparse long matrix,0
add sparse long-key long matrix,0
add component long-key sparse long matrix,0
Start PS,0
Start to run application,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);",0
get a angel client,0
add sparse float matrix,0
MatrixContext siMat = new MatrixContext();,0
siMat.setName(SPARSE_INT_MAT);,0
siMat.setRowType(RowType.T_ANY_INTKEY_SPARSE);,0
siMat.setRowNum(1);,0
siMat.setValidIndexNum(100);,0
siMat.setColNum(10000000000L);,0
siMat.setValueType(Node.class);,0
siMat.setPartitionStorageClass(LongElementMapStorage.class);,0
siMat.setPartitionClass(CSRPartition.class);,0
angelClient.addMatrix(siMat);,0
add sparse long-key double matrix,0
Start PS,0
Start to run application,0
client1.clock().get();,0
"System.out.println(""id="" + id + "", value="" + row.get(id));",0
import com.tencent.angel.psagent.consistency.SSPConsistencyController;,0
@RunWith(MockitoJUnitRunner.class),0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);",0
get a angel client,0
add matrix,0
psAgent.initAndStart();,0
test conf,0
test master location,0
test app id,0
test user,0
test ps agent attempt id,0
test connection,0
test master client,0
test ip,0
test loc,0
test master location,0
test ps location,0
"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));",0
test all ps ids,0
test all matrix ids,0
test all matrix names,0
test matrix attribute,0
test matrix meta,0
test ps location,0
test partitions,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
System.out.println(content);,0
https://blog.csdn.net/cocoonyang/article/details/63068108,0
v1[i] = v1[i] + da * v2[i];,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
v1[i] = v1[i] + da * v2[i];,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
y := alpha*A*x + beta*y,0
"dgemm(String transa, String transb,",0
"int m, int n, int k,",0
"double alpha,",0
"double[] a, int lda,",0
"double[] b, int ldb,",0
"double beta,",0
"double[] c, int ldc);",0
C := alpha*op( A )*op( B ) + beta*C,0
Default does nothing.,0
The app injection is optional,0
"renderText(""hello world"");",0
"user choose a workerGroupID from the workergroups page,",0
now we should change the AngelApp params and render the workergroup page;,0
"static final String WORKER_ID = ""worker.id"";",0
"div(""#logo"").",0
"img(""/static/hadoop-st.png"")._().",0
import org.apache.hadoop.yarn.webapp.view.FooterBlock;,0
import org.apache.hadoop.yarn.webapp.view.HeaderBlock;,0
JQueryUI.jsnotice(html);,0
import org.apache.hadoop.conf.Configuration;,0
import java.lang.reflect.Field;,0
all the files in input set,0
Shuffle the file,0
Get the blocks for all files,0
Adjust the maxSize to make the split more balanced,0
Handle the splittable files,0
Handle the unsplittable files,0
Split the blocks,0
"If the remaining size of the current block is smaller than the required size,",0
the remaining blocks are divided into the current split,0
Update current split length and move to next block,0
Clear the current block offset,0
"Current split length is > maxSize, split the block and generate a new split",0
Clear blocks list for next split,0
Clear the current split length,0
"If splitBlocks is not empty, just genetate a split for it",0
get block locations from file system,0
create an input split,0
get block locations from file system,0
create a list of all block and their locations,0
"if the file is not splitable, just create the one block with",0
full file length,0
each split can be a maximum of maxSize,0
if remainder is between max and 2*max - then,0
"instead of creating splits of size max, left-max we",0
create splits of size left/2 and left/2. This is,0
a heuristic to avoid creating really really small,0
splits.,0
add this block to the block --> node locations map,0
"For blocks that do not have host/rack information,",0
assign to default  rack.,0
add this block to the rack --> block map,0
Add this host to rackToNodes map,0
add this block to the node --> block map,0
"if the file system does not have any rack information, then",0
use dummy rack location.,0
The topology paths have the host name included as the last,0
component. Strip it.,0
get tokens for all the required FileSystems..,0
"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,",0
job.getConfiguration());,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
all the files in input set,0
it is allowed for maxSize to be 0. Disable smoothing load for such cases,0
process all nodes and create splits that are local to a node. Generate,0
"one split per node iteration, and walk over nodes multiple times to",0
distribute the splits across nodes.,0
Skip the node if it has previously been marked as completed.,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
Remove all blocks which may already have been assigned to other,0
splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
Remove entries from blocksInNode so that we don't walk these,0
again.,0
Done creating a single split for this node. Move on to the next,0
node so that splits are distributed across nodes.,0
This implies that the last few blocks (or all in case maxSize=0),0
were not part of a split. The node is complete.,0
if there were any blocks left over and their combined size is,0
"larger than minSplitNode, then combine them into one split.",0
Otherwise add them back to the unprocessed pool. It is likely,0
that they will be combined with other blocks from the,0
same rack later on.,0
This condition also kicks in when max split size is not set. All,0
blocks on a node will be grouped together into a single split.,0
haven't created any split on this machine. so its ok to add a,0
smaller one for parallelism. Otherwise group it in the rack for,0
balanced size create an input split and add it to the splits,0
array,0
Remove entries from blocksInNode so that we don't walk this again.,0
The node is done. This was the last set of blocks for this node.,0
Put the unplaced blocks back into the pool for later rack-allocation.,0
Node is done. All blocks were fit into node-local splits.,0
Check if node-local assignments are complete.,0
All nodes have been walked over and marked as completed or all blocks,0
have been assigned. The rest should be handled via rackLock assignment.,0
"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """,0
"+ completedNodes.size() + "", size left: "" + totalLength);",0
"if blocks in a rack are below the specified minimum size, then keep them",0
"in 'overflow'. After the processing of all racks is complete, these",0
overflow blocks will be combined into splits.,0
Process all racks over and over again until there is no more work to do.,0
Create one split for this rack before moving over to the next rack.,0
Come back to this rack after creating a single split for each of the,0
remaining racks.,0
"Process one rack location at a time, Combine all possible blocks that",0
reside on this rack as one split. (constrained by minimum and maximum,0
split size).,0
iterate over all racks,0
"for each block, copy it into validBlocks. Delete it from",0
blockToNodes so that the same block does not appear in,0
two different splits.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"if we created a split, then just go to the next rack",0
"if there is a minimum size specified, then create a single split",0
"otherwise, store these blocks into overflow data structure",0
There were a few blocks in this rack that,0
remained to be processed. Keep them in 'overflow' block list.,0
These will be combined later.,0
Process all overflow blocks,0
"This might cause an exiting rack location to be re-added,",0
but it should be ok.,0
"if the accumulated split size exceeds the maximum, then",0
create this split.,0
create an input split and add it to the splits array,0
"Process any remaining blocks, if any.",0
create an input split,0
add this split to the list that is returned,0
long num = totLength / maxSize;,0
all blocks for all the files in input set,0
mapping from a rack name to the list of blocks it has,0
mapping from a block to the nodes on which it has replicas,0
mapping from a node to the list of blocks that it contains,0
populate all the blocks for all files,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
"Security framework already loaded the tokens into current UGI, just use",0
them,0
Now remove the AM->RM token so tasks don't have it,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init container allocator,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
Init Client manager,0
Init PS Client manager,0
init parameter server manager,0
recover task information if needed,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
Init model saver & loader,0
start a web service if use yarn deploy mode,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
Check Workers,0
Check PSS,0
Check Clients,0
Check PS Clients,0
stop all services,0
1.write application state to file so that the client can get the state of the application,0
if master exit,0
2.clear tmp and staging directory,0
waiting for client to get application state,0
stop the RPC server,0
add a shutdown hook,0
init app state storage,0
init event dispacher,0
init location manager,0
init a rpc service,0
recover matrix meta if needed,0
recover ps attempt information if need,0
Init Client manager,0
Init PS Client manager,0
init parameter server manager,0
recover task information if needed,0
a dummy data spliter is just for test now,0
recover data splits information if needed,0
init worker manager and register worker manager event,0
register slow worker/ps checker,0
register app manager event and finish event,0
Init model saver & loader,0
k8sClusterManager = new KubernetesClusterManager(appContext);,0
load from app state storage first if attempt index great than 1(the master is not the first,0
retry),0
"if load failed, just build a new MatrixMetaManager",0
load ps attempt index from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
load task information from app state storage first if attempt index great than 1(the master,0
is not the first retry),0
"if load failed, just build a new AMTaskManager",0
load data splits information from app state storage first if attempt index great than 1(the,0
master is not the first retry),0
"if load failed, we need to recalculate the data splits",0
parse parameter server counters,0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
refresh last heartbeat timestamp,0
send a state update event to the specific PSAttempt,0
Check is there save request,0
"LOG.info(""PS "" + psAttemptId + "" need save "" + subSaveContext);",0
Check is there load request,0
"LOG.info(""PS "" + psAttemptId + "" need load "" + subLoadContext);",0
check matrix metadata inconsistencies between master and parameter server.,0
"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix",0
"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix",0
"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command.",0
choose a unused port,0
start RPC server,0
remove this parameter server attempt from monitor set,0
remove this parameter server attempt from monitor set,0
"if worker attempt id is not in monitor set, we should shutdown it",0
find workergroup in worker manager,0
"if this worker group does not initialized, just return WORKERGROUP_NOTREADY",0
"if this worker group run over, just return WORKERGROUP_EXITED",0
"if this worker group is running now, return tasks, workers, data splits for it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"if worker attempt id is not in monitor set, we should shutdown it",0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update the clock for this matrix,0
"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager",0
"in ANGEL_PS mode, task id may can not know advance",0
update task iteration,0
"LOG.info(""error happened in psAttempt "" + psAttemptId + "" error msg="" + request.getMsg());",0
remove this parameter server attempt from monitor set,0
"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);",0
"Calculate how many splits we need. As each task handles a separate split of data, so we want",0
the number of splits equal to the number of tasks,0
split data,0
dispatch the splits to workergroups,0
split data,0
dispatch the splits to workergroups,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
Set split minsize and maxsize to expected split size. We need to get the total size of data,0
"first, then divided by expected split number",0
get input format class from configuration and then instantiation a input format object,0
split data,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
"Since the actual split size is sometimes not exactly equal to the expected split size, we",0
need to fine tune the number of workergroup and task based on the actual split number,0
"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup.",0
Record the location information for the splits in order to data localized schedule,0
write meta data to a temporary file,0
rename the temporary file to final file,0
"if the file exists, read from file and deserialize it",0
write task meta,0
write ps meta,0
generate a temporary file,0
write task meta to the temporary file first,0
rename the temporary file to the final file,0
"if last final task file exist, remove it",0
find task meta file which has max timestamp,0
"if the file does not exist, just return null",0
read task meta from file and deserialize it,0
generate a temporary file,0
write ps meta to the temporary file first.,0
rename the temporary file to the final file,0
"if the old final file exist, just remove it",0
find ps meta file,0
"if ps meta file does not exist, just return null",0
read ps meta from file and deserialize it,0
"Path tmpPath = new Path(new Path(context.getConf().get(AngelConf.ANGEL_JOB_TMP_OUTPUT_PATH)),",0
String.valueOf(requestId));,0
Path tmpPath = HdfsUtil.toTmpPath(new Path(saveContext.getSavePath()));,0
saveContext.setTmpSavePath(tmpPath.toString());,0
Filter old epoch trigger first,0
Split the user request to sub-requests to pss,0
Init matrix files meta,0
Move output files,0
Write the meta file,0
Split the user request to sub-requests to pss,0
check whether psagent heartbeat timeout,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
Application resources,0
Application environment,0
Service data,0
Tokens,0
Set up JobConf to be localized properly on the remote NM.,0
Setup DistributedCache,0
Setup up task credentials buffer,0
LocalStorageToken is needed irrespective of whether security is enabled,0
or not.,0
"TokenCache.setJobToken(jobToken, taskCredentials);",0
"Add pwd to LD_LIBRARY_PATH, add this before adding anything else",0
Construct the actual Container,0
The null fields are per-container and will be constructed for each,0
container separately.,0
Set up the launch command,0
Duplicate the ByteBuffers for access by multiple containers.,0
Construct the actual Container,0
"a * in the classpath will only find a .jar, so we need to filter out",0
all .jars and add everything else,0
Propagate the system classpath when using the mini cluster,0
Add standard Hadoop classes,0
Add mr,0
Cache archives,0
Cache files,0
Sanity check,0
Add URI fragment or just the filename,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Old parameter name,0
Parallel GC parameters,0
G1 params,0
Parallel Scavenge + Parallel Old,0
G1,0
".append("" -XX:G1NewSizePercent="").append(minNewRatio)",0
".append("" -XX:G1MaxNewSizePercent="").append(maxNewRatio)",0
CMS,0
Setup the log4j prop,0
Add main class and its arguments,0
Finally add the jvmID,0
vargs.add(String.valueOf(jvmID.getId()));,0
Final commmand,0
G1 params,0
Add the env variables passed by the user,0
Set logging level in the environment.,0
Setup the log4j prop,0
Add main class and its arguments,0
Final commmand,0
"if amTask is not null, we should clone task state from it",0
"if all parameter server complete commit, master can commit now",0
restartPS(psLoc);,0
check whether parameter server heartbeat timeout,0
Transitions from the NEW state.,0
Transitions from the UNASSIGNED state.,0
"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event",0
Transitions from the ASSIGNED state.,0
"this happened when launch thread run slowly, and PA_REGISTER event dispatched before",0
PA_CONTAINER_LAUNCHED event,0
Transitions from the PSAttemptStateInternal.RUNNING state.,0
Transitions from the PSAttemptStateInternal.COMMITTING state,0
Transitions from the PSAttemptStateInternal.KILLED state,0
Transitions from the PSAttemptStateInternal.FAILED state,0
create the topology tables,0
reqeuest resource:send a resource request to the resource allocator,0
"Once the resource is applied, build and send the launch request to the container launcher",0
deallocator the resource of the ps attempt:send a resource deallocator request to the,0
resource allocator,0
set the launch time,0
add the ps attempt to the heartbeat timeout monitoring list,0
parse ps attempt location and put it to location manager,0
"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt",0
or failed,0
remove ps attempt id from heartbeat timeout monitor list,0
release container:send a release request to container launcher,0
set the finish time only if launch time is set,0
private long scheduledTime;,0
Transitions from the NEW state.,0
Transitions from the SCHEDULED state.,0
Transitions from the RUNNING state.,0
"another attempt launched,",0
Transitions from the SUCCEEDED state,0
Transitions from the KILLED state,0
Transitions from the FAILED state,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
Refresh ps location & matrix meta,0
start a new attempt for this ps,0
notify ps manager,0
"getContext().getLocationManager().setPsLocation(id, null);",0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
start a new attempt for this ps,0
notify ps manager,0
notify the event handler of state change,0
"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor",0
"if forcedState is set, just return",0
else get state from state machine,0
add this worker group to the success set,0
check if all worker group run or run over,0
add this worker group to the success set,0
check if all worker group run over,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
add this worker group to the failed set,0
check if too many worker groups are failed or killed,0
notify a run failed event,0
calculate the actual number of worker groups and the total number of tasks based on the number of data split,0
"init all tasks , workers and worker groups and put them to the corresponding maps",0
just return the total task number now,0
TODO,1
check whether worker heartbeat timeout,0
"if workerAttempt is not null, we should clone task state from it",0
from NEW state,0
from SCHEDULED state,0
get data splits location for data locality,0
reqeuest resource:send a resource request to the resource allocator,0
"once the resource is applied, build and send the launch request to the container launcher",0
notify failed message to the worker,0
notify killed message to the worker,0
release the allocated container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
release the allocated container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
clean the container,0
notify failed message to the worker,0
remove the worker attempt from heartbeat timeout listen list,0
record the finish time,0
clean the container,0
notify killed message to the worker,0
remove the worker attempt from heartbeat timeout listening list,0
record the finish time,0
"if the worker attempt launch successfully, add it to heartbeat timeout listening list",0
set worker attempt location,0
notify the register message to the worker,0
record the launch time,0
update worker attempt metrics,0
update tasks metrics,0
clean the container,0
notify the worker attempt run successfully message to the worker,0
record the finish time,0
todo,1
init a worker attempt for the worker,0
schedule the worker attempt,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
add diagnostic,0
check whether the number of failed attempts is less than the maximum number of allowed,0
init and start a new attempt for this ps,0
notify worker manager,0
"If we need Yarn to restart a new application master, we should not unregister from Yarn RM",0
register to Yarn RM,0
send heartbeat to Yarn RM every rmPollInterval milliseconds,0
"catch YarnException or YarnRuntimeException, we should exit and need not retry",0
build heartbeat request,0
send heartbeat request to rm,0
"This can happen if the RM has been restarted. If it is in that state,",0
this application must clean itself up.,0
Setting NMTokens,0
assgin containers,0
"if some container is not assigned, release them",0
handle finish containers,0
dispatch container exit message to corresponding components,0
killed by framework,0
killed by framework,0
get application finish state,0
build application diagnostics,0
TODO:add a job history for angel,1
build unregister request,0
send unregister request to rm,0
Note this down for next interaction with ResourceManager,0
based on blacklisting comments above we can end up decrementing more,0
than requested. so guard for that.,0
send the updated resource request to RM,0
send 0 container count requests also to cancel previous requests,0
Update resource requests,0
try to assign to all nodes first to match node local,0
try to match all rack local,0
assign remaining,0
Update resource requests,0
send the container-assigned event to task attempt,0
build the start container request use launch context,0
send the start request to Yarn nm,0
send the message that the container starts successfully to the corresponding component,0
"after launching, send launched event to task attempt to move",0
it from ASSIGNED to RUNNING state,0
send the message that the container starts failed to the corresponding component,0
kill the remote container if already launched,0
start a thread pool to startup the container,0
See if we need up the pool size only if haven't reached the,0
maximum limit yet.,0
nodes where containers will run at *this* point of time. This is,0
*not* the cluster size and doesn't need to be.,0
"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the",0
later is just a buffer so we are not always increasing the,0
pool-size,0
the events from the queue are handled in parallel,0
using a thread pool,0
return if already stopped,0
shutdown any containers that might be left running,0
Add one sync matrix,0
addSyncMatrix();,0
"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only",0
"matrixContext.set(MatrixConf.MATRIX_LOAD_PATH, """");",0
"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {",0
"LOG.info(""ps id = "" + psEntry.getKey());",0
"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();",0
"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {",0
"LOG.info(""matrix id = "" + metaEntry.getKey());",0
"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());",0
},0
},0
get matrix ids in the parameter server report,0
get the matrices parameter server need to create and delete,0
"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix",0
"if a matrix exists on master but not exist on parameter server, this parameter server need build it.",0
Init control connection manager,0
Get ps locations from master and put them to the location cache.,0
Build and initialize rpc client to master,0
Get psagent id,0
Build PS control rpc client manager,0
Build local location,0
Initialize matrix meta information,0
Start all services,0
Stop all modules,0
Stop all modules,0
clock first,0
wait,0
Update generic resource counters,0
Updating resources specified in ResourceCalculatorProcessTree,0
Remove the CPU time consumed previously by JVM reuse,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Plus a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Update a vector/matrix to the matrix stored in pss,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get values from pss use row/column indices,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"/ PSF get/update, use can implement their own psf",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
/ Get a row or a batch of rows,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
Just return,0
Just return,0
Just return,0
Just return,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
"checkNotNull(func, ""func"");",0
Return a empty vector,0
Sort the partitions by start column index,0
Generate a flush request and put it to request queue,0
Generate a clock request and put it to request queue,0
Generate a merge request and put it to request queue,0
Generate a merge request and put it to request queue,0
"If the matrix op log cache does not exist for the matrix, create a new one for the",0
matrix,0
and add it to cache maps,0
Add the message to the tree map,0
"If there are flush / clock requests blocked, we need to put this merge request into",0
the waiting queue,0
Launch a merge worker to merge the update to matrix op log cache,0
Remove the message from the tree map,0
Wake up blocked flush/clock request,0
Add flush/clock request to listener list to waiting for all the existing,0
updates are merged,0
Wake up blocked flush/clock request,0
"If all updates are merged for this matrix, we need wake up flush/clock requests which are",0
blocked.,0
Get next merge message sequence id,0
Wake up listeners(flush/clock requests) that have little sequence id than current merge,0
position,0
Wake up blocked merge requests,0
Get minimal sequence id from listeners,0
"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we",0
should flush updates to local matrix storage,0
Doing average or not,0
Filter un-important update,0
Split this row according the matrix partitions,0
Set split context,0
Remove the row from matrix,0
buf.writeDouble(0.0);,0
TODO:,1
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());",0
"LOG.error(""put response message queue failed "", e);",0
Use Epoll for linux,0
Update location table,0
Remove the server from failed list,0
Notify refresh success message to request dispatcher,0
Check PS exist or not,0
Check heartbeat timeout,0
getPSState(entry.getKey());,0
Check PS restart or not,0
private final HashSet<ParameterServerId> refreshingServerSet;,0
Add it to failed rpc list,0
Add the server to gray server list,0
Add it to failed rpc list,0
Add the server to gray server list,0
Move from gray server list to failed server list,0
Handle the RPCS to this server,0
Submit the schedulable failed get RPCS,0
Submit new get RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"If the queue is empty, just return 0",0
"If request is not over limit, just submit it",0
Submit the schedulable failed get RPCS,0
Submit new put RPCS,0
"if submit task in getQueue failed, we should make up the last chosen get queue index",0
"LOG.info(""choose put server "" + psIds[index]);",0
Check all pending RPCS,0
Check get channel context,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
Check all failed PUT RPCS and put it to schedulable list for re-schedule,0
&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {,0
channelManager.printPools();,0
"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {",0
if(ts - entry.getValue() > requestTimeOut * 2)  {,0
"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())",0
"+ "" milliseconds, close all channels to it"");",0
closeChannels(entry.getKey());,0
"psLocToLastChannelTsMap.put(entry.getKey(), ts);",0
},0
},0
"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);",0
Remove all pending RPCS,0
Close all channel to this PS,0
private Channel getChannel(Location loc) throws Exception {,0
"return channelManager.getOrCreateChannel(new Location(loc.getIp(), loc.getPort() + 1));",0
},0
private GenericObjectPool<Channel> getChannelPool(Location loc) throws InterruptedException {,0
"return channelManager.getOrCreateChannelPool(new Location(loc.getIp(), loc.getPort() + 1), PSAgentContext",0
.get(),0
.getConf(),0
".getInt(AngelConf.ANGEL_WORKER_TASK_NUMBER,",0
AngelConf.DEFAULT_ANGEL_WORKER_TASK_NUMBER));,0
},0
Get server id and location for this request,0
"If location is null, means that the server is not ready",0
Get the channel for the location,0
Check if need get token first,0
Serialize the request,0
Send the request,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
request.getContext().setChannelPool(pool);,0
Allocate the bytebuf and serialize the request,0
find the partition request context from cache,0
"updateMatrixCache(request.getPartKey(), response.getPartition());",0
"updateMatrixCache(request.getPartKey(), response.getRowsSplit());",0
TODO,1
ServerRow rowSplit = PSAgentContext.get().getMatricesCache(),0
".getRowSplit(request.getPartKey().getMatrixId(), request.getPartKey(),",0
request.getRowIndex());,0
response.setRowSplit(rowSplit);,0
"updateMatrixCache(request.getPartKey(), response.getRowSplit());",0
"LOG.info(""user request id "" + request.getUserRequestId());",0
"LOG.info(""user request id "" + request.getUserRequestId());",0
TODO,1
"private void updateMatrixCache(PartitionKey partKey, ServerPartition partition) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, partition);",0
},0
"private void updateMatrixCache(PartitionKey partKey, ServerRow rowSplit) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowSplit);",0
},0
"private void updateMatrixCache(PartitionKey partKey, List<ServerRow> rowsSplit) {",0
"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowsSplit);",0
},0
Get partitions for this row,0
Distinct get row requests,0
Get row splits of this row from the matrix cache first,0
responseCache.addSubResponse(rowSplit);,0
"If the row split does not exist in cache, get it from parameter server",0
Split the param use matrix partitions,0
Send request to PSS,0
Split the matrix oplog according to the matrix partitions,0
"If need update clock, we should send requests to all partitions",0
Send request to PSS,0
Filter the rowIds which are fetching now,0
Send the rowIndex to rpc dispatcher and return immediately,0
"LOG.info(""get row split use time="" + (System.currentTimeMillis() - startTs));",0
"LOG.info(""start to request "" + requestId);",0
"LOG.info(""start to request "" + requestId);",0
Split param use matrix partitons,0
"If all sub-results are received, just remove request and result cache",0
Split this row according the matrix partitions,0
Set split context,0
Split this row according the matrix partitions,0
Set split context,0
long startTs = System.currentTimeMillis();,0
"LOG.error(""combine use time = "" + (System.currentTimeMillis() - startTs));",0
Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition,0
Generate dispatch items and add them to the corresponding queues,0
Filter the rowIds which are fetching now,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {",0
return key1.getStartCol() < key2.getStartCol() ? -1 : 1;,0
});,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {",0
return key1.getStartCol() < key2.getStartCol() ? -1 : 1;,0
});,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Put the row split to the cache(row index to row splits map),0
"If all splits of the row are received, means this row can be merged",0
TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage,1
TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage,1
TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage,1
TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage,1
TODO,1
TODO,1
/////////////////////////////////////////////////////////////////////////////////////////////////,0
TODO,1
buf.writeDouble(0);,0
TODO,1
///////////////////////////////////////////////////////////////////////////////////////////////,0
Now we just support pipelined row splits merging for dense type row,0
Pre-fetching is disable default,0
matrix id to clock map,0
"task index, it must be unique for whole application",0
Deserialize data splits meta,0
Get workers,0
Send request to every ps,0
Wait the responses,0
Update clock cache,0
if(syncNum % 1024 == 0) {,0
},0
"Use simple flow, do not use any cache",0
Get row from cache.,0
"if row clock is satisfy ssp staleness limit, just return.",0
Get row from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC mode, just get from pss.",0
"For BSP/SSP, get rows from storage/cache first",0
Get from ps.,0
Wait until the clock value of this row is greater than or equal to the value,0
"For ASYNC, just get rows from pss.",0
no more retries.,0
calculate sleep time and return.,0
parse the i-th sleep-time,0
parse the i-th number-of-retries,0
calculateSleepTime may overflow.,0
"A few common retry policies, with no delays.",0
Read matrix meta from meta file,0
Save partitions to files use fork-join,0
Write the ps matrix meta to the meta file,0
matrix.startServering();,0
return;,0
Read matrix meta from meta file,0
Load partitions from file use fork-join,0
Read matrix meta from meta file,0
Sort partitions,0
TODO:,1
int size = rows.length;,0
int size = rows.length;,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
int size = rows.size();,0
close is a local operation and should finish within milliseconds; timeout just to be safe,0
response will be null for one way messages.,0
maxFrameLength = 2G,0
lengthFieldOffset = 0,0
lengthFieldLength = 8,0
"lengthAdjustment = -8, i.e. exclude the 8 byte length itself",0
"initialBytesToStrip = 8, i.e. strip out the length field itself",0
"return Objects.toStringHelper(this).add(""serial"", serial).add(""listSize"", datas.size())",0
.toString();,0
indicates whether this connection's life cycle is managed,0
See if we already have a connection (common case),0
create a unique lock for this RS + protocol (if necessary),0
get the RS lock,0
do one more lookup in case we were stalled above,0
Only create isa when we need to.,0
definitely a cache miss. establish an RPC for,0
this RS,0
Throw what the RemoteException was carrying.,0
check,0
every,0
minutes,0
TODO,1
failoverHandler,0
"The number of times this invocation handler has ever been failed over,",0
before this method invocation attempt. Used to prevent concurrent,0
failed method invocations from triggering multiple failover attempts.,0
Make sure that concurrent failed method invocations,0
only cause a,0
single actual fail over.,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
"LOG.info(""method "" + method.getName() + ""construct request time = """,0
+ (System.currentTimeMillis() - beforeConstructTs));,0
get an instance of the method arg type,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
Message (hand written code usually has only a single,0
argument),0
log any RPC responses that are slower than the configured,0
warn,0
response time or larger than configured warning size,0
"when tagging, we let TooLarge trump TooSmall to keep",0
output simple,0
note that large responses will often also be slow.,0
provides a count of log-reported slow responses,0
RpcController + Message in the method args,0
(generated code from RPC bits in .proto files have,0
RpcController),0
unexpected,0
"in the protobuf methods, args[1] is the only significant argument",0
for JSON encoding,0
base information that is reported regardless of type of call,0
Disable Nagle's Algorithm since we don't want packets to wait,0
Configure the event pipeline factory.,0
Make a new connection.,0
Remove all pending requests (will be canceled after relinquishing,0
write lock).,0
Cancel any pending requests by sending errors to the callbacks:,0
Close the channel:,0
Close the connection:,0
Shut down all thread pools to exit.,0
"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());",0
See NettyServer.prepareResponse for where we write out the response.,0
"It writes the call.id (int), a boolean signifying any error (and if",0
"so the exception name/trace), and the response bytes",0
Read the call id.,0
"When the stream is closed, protobuf doesn't raise an EOFException,",0
"instead, it returns a null message object.",0
"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +",0
System.currentTimeMillis());,0
"It would be good widen this to just Throwable, but IOException is what we",0
allow now,0
not implemented,1
not implemented,1
"track what RpcEngine is used by a proxy class, for stopProxy()",0
cache of RpcEngines by protocol,0
return the RpcEngine configured to handle a protocol,0
We only handle the ConnectException.,0
This is the exception we can't handle.,0
check if timed out,0
wait for retry,0
IGNORE,0
return the RpcEngine that handles a proxy object,0
The default implementation works synchronously,0
punt: allocate a new buffer & copy into it,0
Parse cmd parameters,0
load hadoop configuration,0
load angel system configuration,0
load user configuration:,0
load user config file,0
load command line parameters,0
load user job resource files,0
load ml conf file for graph based algorithm,0
load user job jar if it exist,0
Expand the environment variable,0
Add default fs(local fs) for lib jars.,0
"LOG.info(System.getProperty(""user.dir""));",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
get tokens for all the required FileSystems..,0
Whether we need to recursive look into the directory structure,0
creates a MultiPathFilter with the hiddenFileFilter and the,0
user provided one (if any).,0
"LOG.info(""Total input paths to process : "" + result.size());",0
a simple hdfs copy function assume src path and dest path are in same hdfs,0
and FileSystem object has same schema,0
"If out path exist , just remove it first",0
Create parent directory if not exist,0
Rename,0
"LOG.warn(""interrupted while sleeping"", ie);",0
public static String getHostname() {,0
try {,0
"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();",0
} catch (UnknownHostException uhe) {,0
},0
"return new StringBuilder().append("""").append(uhe).toString();",0
},0
"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {",0
String hostname = getHostname();,0
String classname = clazz.getSimpleName();,0
"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new",0
"StringBuilder().append(""Starting "").append(classname).toString(), new",0
"StringBuilder().append(""  host = "").append(hostname).toString(), new",0
"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new",0
"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new",0
"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));",0
,0
"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {",0
public void run() {,0
"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +",0
"this.val$classname + "" at "" + this.val$hostname}));",0
},0
});,0
},0
"We we interrupted because we're meant to stop? If not, just",0
continue ignoring the interruption,0
Recalculate waitTime.,0
// Begin delegation to Thread,0
// End delegation to Thread,0
instance submitter class,0
Obtain filename from path,0
Split filename to prexif and suffix (extension),0
Check if the filename is okay,0
Prepare temporary file,0
Prepare buffer for data copying,0
Open and check input stream,0
Open output stream and copy data between source file in JAR and the temporary file,0
"If read/write fails, close streams safely before throwing an exception",0
"Finally, load the library",0
little endian load order,0
tail,0
fallthrough,0
fallthrough,0
finalization,0
fmix(h1);,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
----------,0
body,0
----------,0
tail,0
----------,0
finalization,0
throw new AngelException(e);,0
JobStateProto jobState = report.getJobState();,0
Check need load matrices,0
Used for java code to get a AngelClient instance,0
Used for python code to get a AngelClient instance,0
load user job resource files,0
setLocalAddr();,0
2.get job id,0
5.write configuration to a xml file,0
8.get app master client,0
Write job file to JobTracker's fs,0
the leaf level file should be readable by others,0
the subdirs in the path should have execute permissions for,0
others,0
2.get job id,0
Credentials credentials = new Credentials();,0
4.copy resource files to hdfs,0
5.write configuration to a xml file,0
6.create am container context,0
7.Submit to ResourceManager,0
8.get app master client,0
Create a number of filenames in the JobTracker's fs namespace,0
add all the command line files/ jars and archive,0
first copy them to jobtrackers filesystem,0
should not throw a uri exception,0
should not throw an uri excpetion,0
set the timestamps of the archives and files,0
set the public/private visibility of the archives and files,0
get DelegationToken for each cached file,0
check if we do not need to copy the files,0
is jt using the same file system.,0
just checking for uri strings... doing no dns lookups,0
to see if the filesystems are the same. This is not optimal.,0
but avoids name resolution.,0
this might have name collisions. copy will throw an exception,0
parse the original path to create new path,0
check for ports,0
Write job file to JobTracker's fs,0
Setup resource requirements,0
Setup LocalResources,0
Setup security tokens,0
Setup the command to run the AM,0
Add AM user command opts,0
Final command,0
Setup the CLASSPATH in environment,0
"i.e. add { Hadoop jars, job jar, CWD } to classpath.",0
Setup the environment variables for Admin first,0
"Setup the environment variables (LD_LIBRARY_PATH, etc)",0
Parse distributed cache,0
Setup ContainerLaunchContext for AM container,0
Set up the ApplicationSubmissionContext,0
private volatile PS2PSPusherImpl ps2PSPusher;,0
TODO,1
Add tokens to new user so that it may execute its task correctly.,0
TODO,1
to exit,0
TODO,1
TODO,1
ps2PSPusher.recover(ProtobufUtil.convert(needRecoverParts.get(i)));,0
context.getSnapshotManager().processRecovery();,0
Recover PS from snapshot or load path,0
1. First check old snapshot,0
2. Check new checkpoints,0
3. Check load path setting and old save result,0
Just init it again,0
TODO,1
if(ps2PSPusher != null) {,0
ps2PSPusher.start();,0
},0
public PS2PSPusherImpl getPs2PSPusher() {,0
return ps2PSPusher;,0
},0
Filter the head,0
Get the RPC destination,0
Get and init the queue,0
"If the queue is empty, activate the processor",0
Just put it to the rpc queue,0
if(useInDepWorkers) {,0
Use independent rpc workers,0
if (method == TransportMethod.GET_CLOCKS || method == TransportMethod.UPDATE_CLOCK) {,0
"Small rpc request, use sync to avoid thread switch",0
return false;,0
},0
return true;,0
} else {,0
return false;,0
},0
if (!useSync && useAyncHandler) {,0
"senderPool.execute(new Sender(clientId, seqId, method, ctx, result));",0
} else {,0
"send(clientId, seqId, method, ctx, result);",0
},0
Release the input buffer,0
Release the input buffer,0
"1. handle the rpc, get the response",0
Release the input buffer,0
2. Serialize the response,0
Send the serialized response,0
Exception happened,0
write seq id,0
Just serialize the head,0
Exception happened,0
Allocate result buffer,0
Exception happened,0
Just serialize the head,0
Exception happened,0
runningContext.printToken();,0
Reset the response and allocate buffer again,0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this location, only master ps can accept the update",0
Check the partition state again,0
Start to put the update to the slave pss,0
TODO,1
"context.getPS2PSPusher().put(request, in, partLoc);",0
Get partition and check the partition state,0
Get the stored pss for this partition,0
"Check this ps is the master ps for this partition, if not, just return failed",0
Start to put the update to the slave pss,0
TODO,1
"int maxRPCCounter = Math.max(estSize, (int) (workerNum * factor));",0
"for (Map.Entry<Integer, ClientRunningContext> clientEntry : clientRPCCounters.entrySet()) {",0
"LOG.info(""client "" + clientEntry.getKey() + "" running context:"");",0
clientEntry.getValue().printToken();,0
},0
return ServerState.GENERAL;,0
Use Epoll for linux,0
public String uuid;,0
TODO:,1
part = new ServerPartition();,0
TODO:,1
public void setChannelPool(GenericObjectPool<Channel> channelPool) {,0
this.channelPool = channelPool;,0
},0
private final ParameterServer psServer;,0
Create and start workers,0
Set workers,0
Create and start workers,0
Set workers,0
"If matrix checkpoint path not exist, just return null",0
Return the path with maximum checkpoint id,0
Rename temp to item path,0
Checkpoint base path = Base dir/matrix name,0
Path for this checkpoint,0
Generate tmp path,0
Delete old checkpoints,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"////// network io method, for model transform",0
///////////////////////////////////////////////////////////////////////////////////////////////,0
Serailize the head,0
Serialize the storage,0
Deserailze the head,0
Deseralize the storage,0
Serailize the head,0
Serialize the storage,0
Deserailze the head,0
Deseralize the storage,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
"Methods with out lock operation, you must call startWrite/startRead before using these methods",0
and call endWrite/endRead after,0
////////////////////////////////////////////////////////////////////////////////////////////////,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
"Notice: Basic type ServerRow only support Vector as inner storage now, so you can use methods",0
to get inner vector for basic type ServerRow.,0
///////////////////////////////////////////////////////////////////////////////////////////////,0
Just update the exist element now!!,0
Just update the exist element now!!,0
TODO: just check the value is 0 or not now,1
TODO: just check the value is zero or not now,1
"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low.",0
Attention: Only update the exist values for sorted storage method,0
Attention: Only update exist element,0
Attention: Only update the exist values for sorted storage method,0
Attention: Only update exist element,0
TODO: just check the value is zero or not now,1
"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low.",0
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
Element data,0
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
Attention: Only update the exist values for sorted storage method,0
Attention: Only update exist element,0
TODO: just check the value is zero or not now,1
"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low.",0
TODO: just check the value is 0 or not now,1
Just update the exist element now!!,0
Just update the exist element now!!,0
"Use sparse storage method, as some elements in the array maybe null",0
Array length,0
Valid element number,0
Element data,0
Array len,0
Valid element number,0
"Use sparse storage method, as some elements in the array maybe null",0
Array length,0
Valid element number,0
Element data,0
Element data,0
Array len,0
Valid element number,0
Attention: Only update the exist values for sorted storage method,0
Attention: Only update exist element,0
TODO: just check the value is zero or not now,1
"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low.",0
Row type,0
Storage method,0
Key type,0
Value type,0
Vector dim,0
Vector length,0
Vector data,0
Row type,0
Storage method,0
Key type,0
Value type,0
Vector dim,0
Vector length,0
Init the vector,0
Vector data,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
Impossible now,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
Impossible now,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
Impossible now,0
"Sparse storage, use the iterator to avoid array copy",0
Get the array pair,0
Impossible now,0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
"If use sorted storage, we should get the array pair first",0
Just update the exist element now!!,0
Just update the exist element now!!,0
TODO: just check the value is 0 or not now,1
Just update the exist element now!!,0
Just update the exist element now!!,0
TODO: just check the value is 0 or not now,1
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
Element data,0
Valid element number,0
Element data,0
Valid element number,0
Deserialize the data,0
private final List<PartitionKey> partitionKeys;,0
Get server partition class,0
"If partition class is not set, just use the default partition class",0
Get server partition storage class type,0
Get value class,0
"if col == -1, we use the start/end index to calculate range,",0
we use double to store the range value since two long minus might exceed the,0
range of long.,0
Serialize the head,0
Serialize the storage,0
Deserialize the head,0
Deseralize the storage,0
Serialize the head,0
Serialize the storage,0
Deserialize the head,0
Deseralize the storage,0
Row base partition,0
"If storage class is not set, use default DenseServerRowsStorage",0
Serialize values,0
Deserialize values,0
Array size,0
Actual write size,0
Rows data,0
Row id,0
Row type,0
Row data,0
Array size,0
Actual write row number,0
Rows data,0
Row id,0
Create empty server row,0
Row data,0
Rows data,0
TODO,1
Serialize row offsets,0
Serialize column offsets,0
Deserialize row offset,0
Deserialize row offset,0
"If storage is set, just get a instance",0
"If storage is not set, use default",0
"If storage is set, just get a instance",0
"If storage is not set, use default",0
Map size,0
Actual write size,0
Rows data,0
Row id,0
Row type,0
Row data,0
Array size,0
Actual write row number,0
Rows data,0
Row id,0
Create empty server row,0
Row data,0
Rows data,0
Use Epoll for linux,0
find the partition request context from cache,0
get a channel to server from pool,0
"if channel is not valid, it means maybe the connections to the server are closed",0
channelManager.removeChannelPool(loc);,0
Generate seq id,0
Create a RecoverPartRequest,0
Serialize the request,0
Change the seqId for the request,0
Serialize the request,0
"First check the state of the channels in the pool, if a channel is unused, just return",0
"If all channels are in use, create a new channel or wait",0
Create a new channel,0
"add the PSAgentContext,need fix",0
If col == -1 and start/end not set,0
start/end set,0
"for dense type, we need to set the colNum to set dim for vectors",0
"colNum set, start/end not set",0
Row number must > 0,0
"both set, check its valid",0
public static final int T_INT_ARBITRARY_VALUE = 28;,0
public static final int T_INVALID_VALUE = 29;,0
TODO:add more vector type,1
TODO : subDim set,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Sort the parts by partitionId,0
Sort partition keys use start column index,0
"For each partition, we generate a update split.",0
"Although the split is empty for partitions those without any update data,",0
we still need to generate a update split to update the clock info on ps.,0
Split updates,0
Shuffle update splits,0
Generate part update parameters,0
"Set split context: partition key, use int key for long key vector or not ect",0
write the max abs,0
---------------------------------------------------,0
---------------------------------------------------,0
---------------------------------------------------------------,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
axis = 0: on rows,0
axis = 1: on cols,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
1. find the insert point,0
2. check the capacity and insert,0
3. increase size,0
-----------------,0
-----------------,0
-----------------,0
-----------------,0
-----------------,0
KeepStorage is guaranteed,0
"ignore the isInplace option, since v2 is dense",0
"the value in old storage can be changed safe, so switch a storage",0
"but user required keep storage, we can prevent rehash",0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
KeepStorage is guaranteed,0
we gauss dense storage is more efficient,0
v1Size < v2Size * Constant.sparseThreshold,0
KeepStorage is guaranteed,0
"ignore the isInplace option, since v2 is dense",0
"the value in old storage can be changed safe, so switch a storage",0
"but user required keep storage, we can prevent rehash",0
we gauss dense storage is more efficient,0
prevent rehash,0
KeepStorage is guaranteed,0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
dense preferred,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
sorted preferred,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
multi-rehash,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"dense preferred, KeepStorage is guaranteed",0
"dense preferred, KeepStorage is guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"sparse preferred, keep storage guaranteed",0
"sparse preferred, keep storage guaranteed",0
preferred dense,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
we gauss dense storage is more efficient,0
to avoid multi-rehash,0
"no rehashor one onle rehash is required, nothing to optimization",0
multi-rehash,0
"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large",0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
to avoid multi-rehash,0
"Transform mat1, generate a new matrix",0
Split the row indices of mat1Trans,0
Parallel execute use fork-join,0
"Get the sub-matrix of left matrix, split by row",0
"Transform mat1, generate a new matrix",0
Split the row indices of mat1Trans,0
Parallel execute use fork-join,0
"Get the sub-matrix of left matrix, split by row",0
"mat1 trans true, mat trans true",0
"mat1 trans true, mat trans false",0
"mat1 trans false, mat trans true, important",0
"mat1 trans false, mat trans false",0
"mat1 trans true, mat trans true",0
"mat1 trans true, mat trans false",0
"mat1 trans false, mat trans true, important",0
"mat1 trans false, mat trans false",0
"dger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)",0
"sger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)",0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
not the first time,0
first time and do the sample,0
set to zero,0
get configuration from envs,0
get master location,0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();,0
add dense double matrix,0
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
TODO Auto-generated method stub,1
get configuration from config file,0
set localDir with enviroment set by nm.,0
get master location,0
init task manager and start tasks,0
start heartbeat thread,0
taskManager.assignTaskIds(response.getTaskidsList());,0
todo,1
"if worker timeout, it may be knocked off.",0
"SUCCESS, do nothing",0
heartbeatFailedTime = 0;,0
private KEY currentKey;,0
will be created,0
TODO Auto-generated method stub,1
Bitmap bitmap = new Bitmap();,0
int max = indexArray[size - 1];,0
byte [] bitIndexArray = new byte[max / 8 + 1];,0
for(int i = 0; i < size; i++){,0
int bitIndex = indexArray[i] >> 3;,0
int bitOffset = indexArray[i] - (bitIndex << 3);,0
switch(bitOffset){,0
case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;,0
case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;,0
case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;,0
case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;,0
case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;,0
case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;,0
case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;,0
case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;,0
},0
},0
"true, false",0
//////////////////////////////,0
Application Configs,0
//////////////////////////////,0
//////////////////////////////,0
Master Configs,0
//////////////////////////////,0
//////////////////////////////,0
Worker Configs,0
//////////////////////////////,0
//////////////////////////////,0
Task Configs,0
//////////////////////////////,0
//////////////////////////////,0
ParameterServer Configs,0
//////////////////////////////,0
//////////////////////////////,0
Kubernetes Configs.,0
//////////////////////////////,0
////////////////// IPC //////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
//////////////////////////////,0
Matrix transfer Configs.,0
//////////////////////////////,0
Configs used to ANGEL_PS_PSAGENT running mode future.,0
model parse,0
Mark whether use pyangel or not.,0
private Configuration conf;,0
"Configuration that should be used in python environment, there should only be one",0
configuration instance in each Angel context.,0
Use private access means jconf should not be changed or modified in this way.,0
Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:,0
Do nothing,0
To-DO: add other ways to justify different value types,0
"This is so ugly, must re-implement by more elegance way",1
"Create python path which include angel's jars, the python directory in ANGEL_HOME,",0
and other files submitted by user.,0
Launch python process,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);",0
get a angel client,0
add sparse float matrix,0
siMat.setPartitionStorageClass(LongElementMapStorage.class);,0
siMat.setPartitionClass(CSRPartition.class);,0
Start PS,0
Start to run application,0
Init node neighbors,0
client.asyncUpdate(new InitNeighborOver(new InitNeighborOverParam(matrixId))).get();,0
Sample the neighbors,0
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);",0
get a angel client,0
add sparse float matrix,0
siMat.setValidIndexNum(100);,0
siMat.setColNum(10000000000L);,0
siMat.setPartitionStorageClass(LongElementMapStorage.class);,0
siMat.setPartitionClass(CSRPartition.class);,0
Start PS,0
Start to run application,0
Init node neighbors and feats,0
Sample the neighbors,0
TODO Auto-generated constructor stub,1
set basic configuration keys,0
use local deploy mode and dummy dataspliter,0
"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);",0
get a angel client,0
add sparse float matrix,0
Start PS,0
Start to run application,0
Init node neighbors,0
Sample the neighbors,0
sample continuously beginning from a random index,0
Get node neighbor number,0
"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random",0
Get node neighbor number,0
"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random",0
"ServerLongAnyRow row = (ServerLongAnyRow) psContext.getMatrixStorageManager().getRow(pparam.getPartKey(), 0);",0
ObjectIterator<Long2ObjectMap.Entry<IElement>> iter = row.iterator();,0
while (iter.hasNext()) {,0
Long2ObjectMap.Entry<IElement> entry = iter.next();,0
long key = entry.getLongKey() + pparam.getPartKey().getStartCol();,0
WalkPath value = (WalkPath) entry.getValue();,0
,0
if (workerPartitionId == value.getNextPartitionIdx()) {,0
"result.put(key, value.getTail2());",0
},0
},0
"int matrixId, PartitionKey partKey, long[] keyIds, int startIdx, int endIdx",0
"System.out.println(""PathQueue: put data to queue"");",0
"System.out.println(""queue.size: "" + queue.size());",0
"System.out.println(""CurrPathIdx of "" + wPath.getHead() + "" is "" + wPath.getCurrPathIdx());",0
if (numRetry == retry) {,0
"System.out.println(""retried 3 time, got : "" + result.size());",0
},0
"System.out.println(""popBatch: "" + result.size() +"" | ""+ count);",0
"getRow(partKey.getMatrixId(), rowId, partKey.getPartitionId())",0
StringBuilder sb = new StringBuilder();,0
"sb.append(key).append("" -> {"");",0
for (long n: neighbor) {,0
"sb.append(n).append("", "");",0
},0
"sb.append(""} : "").append(neigh);",0
System.out.println(sb.toString());,0
"System.out.println(""pushed size: "" + pathTail.size());",0
List<LinkedBlockingQueue<WalkPath>> queueList = PathQueue.getQueueList(partKey.getPartitionId());,0
int p = 0;,0
for (LinkedBlockingQueue<WalkPath> queue: queueList) {,0
"System.out.println(""partition "" + p + "", size1 = ""+ pathTail.size() +  "" size2 = "" + queue.size());",0
p++;,0
},0
"System.out.println(""pushed batch finished!"");",0
Get node neighbor number,0
"If the neighbor number is 0, just return a int[0]",0
"If count <= 0 or the neighbor number is less or equal then count, just copy all neighbors to the result array",0
"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random",0
Store the total neighbor number of all nodes in rowOffsets,0
"Put the node ids, node neighbor number, node neighbors to the cache",0
No data in this partition,0
Get total neighbor number,0
Final matrix column indices: neighbors node ids,0
Write positions in cloumnIndices for nodes,0
Copy all cached sub column indices to final column indices,0
Read position for a sub column indices,0
Copy column indices for a node to final column indices,0
Update write position for this node in final column indices,0
Update the read position in sub column indices,0
Clear all temp data,0
Get node neighbor number,0
"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random",0
sample happens here to avoid memory copy on servers,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Data format,0
Feature number of train data,0
Tree number,0
Tree depth,0
Split number,0
Feature sample ratio,0
Ratio of validation,0
Learning rate,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"Set angel resource, #worker, #task, #PS",0
Set GBDT algorithm parameters,0
Set training data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set predict data path,0
Set load model path,0
Set predict result path,0
Set log path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Batch number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Train batch number per epoch.,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd FM algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
Model type,0
String modelType = String.valueOf(RowType.T_FLOAT_DENSE);,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
"conf.set(MLConf.ML_MODEL_TYPE(), modelType);",0
"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);",0
predictTest();,0
"Data format, libsvm or dummy",0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set data format,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
class number,0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set log path,0
Set trainning data path,0
Set save model path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set training data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Cluster center number,0
Feature number of train data,0
Total iteration number,0
Sample ratio per mini-batch,0
C,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
set Kmeans algorithm parameters #cluster #feature #epoch,0
Set data format,0
Set trainning data path,0
Set save model path,0
Set log save path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set save model path,0
Set actionType incremental train,0
Set log path,0
Set testing data path,0
Set load model path,0
Set predict result path,0
Set actionType prediction,0
Set local deploy mode,0
Set basic configuration keys,0
"set angel resource parameters #worker, #task, #PS",0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set predict result path,0
Set actionType prediction,0
Feature number of train data,0
Total iteration number,0
Validation sample Ratio,0
"Data format, libsvm or dummy",0
Model type,0
Learning rate,0
Decay of learning rate,0
Regularization coefficient,0
Set local deploy mode,0
Set basic configuration keys,0
Set data format,0
"set angel resource parameters #worker, #task, #PS",0
set sgd LR algorithm parameters #feature #epoch,0
Set trainning data path,0
Set save model path,0
Set log path,0
Set actionType train,0
Set trainning data path,0
Set load model path,0
Set predict result path,0
TODO: optimize int key indices,1
"System.out.println(""deserialize cols.length="" + nCols);",0
"System.out.print(""deserialize "");",0
"System.out.print(cols[c] + "" "");",0
System.out.println();,0
TODO Auto-generated method stub,1
"LOG.info(""original float length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));",0
"LOG.info(""original double length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));",0
"LOG.info(""parsed float length: "" + length + Arrays.toString(arr));",0
"LOG.info(""parsed double length: "" + length + Arrays.toString(arr));",0
"ground truth: positive, precision: positive",0
start row index for words,0
start row index for docs,0
doc ids,0
topic assignments,0
word to docs reverse index,0
count word,0
build word start index,0
build word to doc reverse idx,0
build dks,0
dks = new TraverseHashMap[n_docs];,0
for (int d = 0; d < n_docs; d++) {,0
if (K < Short.MAX_VALUE) {,0
if (docs.get(d).len < Byte.MAX_VALUE),0
dks[d] = new S2BTraverseMap(docs.get(d).len);,0
if (docs.get(d).len < Short.MAX_VALUE),0
"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));",0
else,0
"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));",0
} else {,0
"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));",0
},0
},0
build dks,0
allocate update maps,0
Skip if no token for this word,0
Check whether error when fetching word-topic,0
Build FTree for current word,0
current doc,0
old topic assignment,0
"Check if error happens. if this happen, it's probably that failures happen to servers.",0
We need to adjust the memory settings or network fetching parameters.,0
Update statistics if needed,0
Calculate psum and sample new topic,0
Update statistics if needed,0
Assign new topic,0
Skip if no token for this word,0
if (u >= p[end]) {,0
"System.out.println(""u="" + u + "" p[end]="" + p[end] + "" start="" + start + "" end="" + end);",0
return end;,0
},0
,0
if (u < p[start]) {,0
"System.out.println(""u="" + u + "" p[start]="" + p[start] + "" start="" + start + "" end="" + end);",0
return start;,0
},0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
The starting point,0
There's always an unused entry.,0
print();,0
Write #rows,0
Write each row,0
dense,0
sparse,0
LOG.info(buf.refCnt());,0
dense,0
sparse,0
calculate columns,0
reset(row);,0
loss function,0
gradient and hessian,0
"categorical feature set, null: none, empty: all, else: partial",0
"node's end index in instancePos, instances in [start, end] belong to a tree node",0
initialize the phase,0
current tree and depth,0
create loss function,0
calculate grad info of each instance,0
"create data sketch, push candidate split value to PS",0
1. calculate candidate split value,0
categorical features,0
2. push local sketch to PS,0
the leader worker,0
merge categorical features,0
create updates,0
"pull the global sketch from PS, only called once by each worker",0
number of categorical feature,0
sample feature,0
push sampled feature set to the current tree,0
create new tree,0
"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,",0
calculate gradient,0
"1. create new tree, initialize tree nodes and node stats",0
"2. initialize feature set, if sampled, get from PS, otherwise use all the features",0
2.1. pull the sampled features of the current tree,0
this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();,0
"2.2. if use all the features, only called one",0
"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle",0
4. set root node to active,0
"5. reset instance position, set the root node's span",0
6. calculate gradient,0
1. decide nodes that should be calculated,0
2. decide calculated and subtracted tree nodes,0
3. calculate threads,0
wait until all threads finish,0
4. subtract threads,0
wait until all threads finish,0
5. send histograms to PS,0
6. update histogram cache,0
clock,0
find split,0
"1. find responsible tree node, using RR scheme",0
2. pull gradient histogram,0
2.1. get the name of this node's gradient histogram on PS,0
2.2. pull the histogram,0
2.3. find best split result of this tree node,0
2.3.1 using server split,0
"update the grad stats of the root node on PS, only called once by leader worker",0
update the grad stats of children node,0
update the left child,0
update the right child,0
"2.3.2 the updated split result (tree node/feature/value/gain) on PS,",0
"2.3.3 otherwise, the returned histogram contains the gradient info",0
"2.3.4 the updated split result (tree node/feature/value/gain) on PS,",0
2.3.5 reset this tree node's gradient histogram to 0,0
3. push split feature to PS,0
4. push split value to PS,0
5. push split gain to PS,0
6. set phase to AFTER_SPLIT,0
this.phase = GBDTPhase.AFTER_SPLIT;,0
clock,0
1. get split feature,0
2. get split value,0
3. get split gain,0
4. get node weight,0
5. split node,0
update local replica,0
create AfterSplit task,0
"2. check thread stats, if all threads finish, return",0
6. clock,0
"split the span of one node, reset the instance position",0
in case this worker has no instance on this node,0
set the span of left child,0
set the span of right child,0
"1. left to right, find the first instance that should be in the right child",0
"2. right to left, find the first instance that should be in the left child",0
3. swap two instances,0
4. find the cut pos,0
5. set the span of left child,0
6. set the span of right child,0
set tree node to active,0
set node to leaf,0
set node to inactive,0
finish current depth,0
finish current tree,0
set the tree phase,0
check if there is active node,0
check if finish all the tree,0
update node's grad stats on PS,0
"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split",0
the root node's stats is updated by leader worker,0
1. create the update,0
2. push the update to PS,0
1. update predictions of training data,0
2. update predictions of validation data,0
the leader task adds node prediction to flush list,0
1. name of this node's grad histogram on PS,0
2. build the grad histogram of this node,0
3. push the histograms to PS,0
4. reset thread stats to finished,0
5.1. set the children nodes of this node,0
5.2. set split info and grad stats to this node,0
5.2. create children nodes,0
"5.3. create node stats for children nodes, and add them to the tree",0
5.4. reset instance position,0
"5.5. add new active nodes if possible, inc depth, otherwise finish this tree",0
5.6. set children nodes to leaf nodes,0
5.7. set nid to leaf node,0
5.8. deactivate active node,0
"get feature type, 0:empty 1:all equal 2:real",0
"if not -1, sufficient space will be allocated at once",0
copy the highest levels,0
copy baseBuffer,0
merge two non-empty quantile sketches,0
left child <= split value; right child > split value,0
"the first: minimal, the last: maximal",0
categorical features,0
continuous features,0
left child <= split value; right child > split value,0
feature index used to split,0
feature value used to split,0
loss change after split this node,0
grad stats of the left child,0
grad stats of the right child,0
"LOG.info(""Constructor with fid = -1"");",0
fid = -1: no split currently,0
the minimal split value is the minimal value of feature,0
the splits do not include the maximal value of feature,0
"1. the average distance, (maxValue - minValue) / splitNum",0
2. calculate the candidate split value,0
1. new feature's histogram (grad + hess),0
size: sampled_featureNum * (2 * splitNum),0
"in other words, concatenate each feature's histogram",0
2. get the span of this node,0
------ 3. using sparse-aware method to build histogram ---,0
"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances",0
3.1. get the instance index,0
3.2. get the grad and hess of the instance,0
3.3. add to the sum,0
3.4. loop the non-zero entries,0
3.4.1. get feature value,0
3.4.2. current feature's position in the sampled feature set,0
"int fPos = findFidPlace(this.controller.fSet, fid);",0
3.4.3. find the position of feature value in a histogram,0
"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]",0
3.4.4. add the grad and hess to the corresponding bin,0
3.4.5. add the reverse to the bin that contains 0.0f,0
4. add the grad and hess sum to the zero bin of all features,0
find the best split result of the histogram of a tree node,0
1. calculate the gradStats of the root node,0
"1.1. update the grad stats of the root node on PS, only called once by leader worker",0
2. loop over features,0
2.1. get the ture feature id in the sampled feature set,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"update the grad stats of the root node on PS, only called once by leader worker",0
3. update the grad stats of children node,0
3.1. update the left child,0
3.2. update the right child,0
find the best split result of one feature,0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
split value = sketches[splitIdx],0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
partition number,0
cols of each partition,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
1. calculate the total grad sum and hess sum,0
2. create the grad stats of the node,0
"loop all the possible split value, start from split[0], the first item is the minimal feature value",0
assert fvalue >= sketch[start] && fvalue <= sketch[end];,0
if (left > end) return end - start;,0
find the best split result of the histogram of a tree node,0
2.2. get the indexes of histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
find the best split result of one feature,0
1. set the feature id,0
splitEntry.setFid(fid);,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
find the best split result of a serve row on the PS,0
"2. the fid here is the index in the sampled feature set, rather than the true feature id",0
2.2. get the start index in histogram of this feature,0
2.3. find the best split of current feature,0
2.4. update the best split result if possible,0
"find the best split result of one feature from a server row, used by the PS",0
1. set the feature id,0
2. create the best left stats and right stats,0
3. the gain of the root node,0
4. create the temp left and right grad stats,0
5. loop over all the data in histogram,0
5.1. get the grad and hess of current hist bin,0
5.2. check whether we can split with current left hessian,0
right = root - left,0
5.3. check whether we can split with current right hessian,0
5.4. calculate the current loss gain,0
5.5. check whether we should update the split result with current loss gain,0
"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]",0
the task use index to find fvalue,0
"5.6. if should update, also update the best left and right grad stats",0
6. set the best left and right grad stats,0
"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting",0
"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking.",0
max and min of each feature,0
clear all the information,0
calculate the sum of gradient and hess,0
"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,",0
ridx),0
check if necessary information is ready,0
"same as add, reduce is used in All Reduce",0
"features used in this tree, if equals null, means use all the features without sampling",0
node in the tree,0
the gradient info of each instances,0
initialize nodes,0
gradient,0
second order gradient,0
int sendStartCol = (int) row.getStartCol();,0
logistic loss for binary classification task.,0
"logistic loss, but predict un-transformed margin",0
check if label in range,0
return the default evaluation metric for the objective,0
"task type: classification, regression, or ranking",0
"quantile sketch, size = featureNum * splitNum",0
"gradient histograms, size = treeNodeNum * featureNum * splitNum",0
"active tree nodes, size = pow(2, treeDepth) -1",0
sampled features. size = treeNum * sampleRatio * featureNum,0
categorical feature. size = workerNum * cateFeatNum * splitNum,0
"split features, size = treeNum * treeNodeNum",0
"split values, size = treeNum * treeNodeNum",0
"split gains, size = treeNum * treeNodeNum",0
"node weights, size = treeNum * treeNodeNum",0
"node preds, size = treeNum * treeNodeNum",0
if using PS to perform split,0
step size for a tree,0
number of class,0
minimum loss change required for a split,0
maximum depth of a tree,0
number of features,0
number of nonzero,0
number of candidates split value,0
----- the rest parameters are less important ----,0
base instance weight,0
minimum amount of hessian(weight) allowed in a child,0
L2 regularization factor,0
L1 regularization factor,0
default direction choice,0
maximum delta update we can add in weight estimation,0
this parameter can be used to stabilize update,0
default=0 means no constraint on weight delta,0
whether we want to do subsample for row,0
whether to subsample columns for each tree,0
accuracy of sketch,0
accuracy of sketch,0
leaf vector size,0
option for parallelization,0
option to open cacheline optimization,0
whether to not print info during training.,0
maximum depth of the tree,0
number of features used for tree construction,0
"minimum loss change required for a split, otherwise stop split",0
----- the rest parameters are less important ----,0
default direction choice,0
whether we want to do sample data,0
whether to sample columns during tree construction,0
whether to use histogram for split,0
number of histogram units,0
whether to print info during training.,0
----- the rest parameters are obtained after training ----,0
total number of nodes,0
number of deleted nodes */,0
