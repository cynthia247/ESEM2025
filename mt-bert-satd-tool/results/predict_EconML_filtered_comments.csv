Commit Message,predict
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile",0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/main/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
TODO: enable type aliases,1
napoleon_preprocess_types = True  # needed for type aliases to work,0
napoleon_type_aliases = {,0
"""array_like"": "":term:`array_like`"",",0
"""ndarray"": ""~numpy.ndarray"",",0
"""RandomState"": "":class:`~numpy.random.RandomState`"",",0
"""DataFrame"": "":class:`~pandas.DataFrame`"",",0
"""Series"": "":class:`~pandas.Series`"",",0
},0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of strings:,0
,0
"source_suffix = ['.rst', '.md']",0
The root toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)",0
def mlasso_model(): return MultiTaskLassoCV(,0
"cv=3, alphas=alpha_regs, max_iter=200)",0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
"alpha_regs = [5e-3, 1e-2, 5e-2]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)",0
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)",0
subset of features that are exogenous and create heterogeneity,0
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features),0
subset of features wrt we estimate heterogeneity,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
introspect the constructor arguments to find the model parameters,0
to represent,0
"if the argument is deprecated, ignore it",0
Extract and sort argument names excluding 'self',0
column names,0
transfer input to numpy arrays,0
transfer input to 2d arrays,0
create dataframe,0
currently dowhy only support single outcome and single treatment,0
call dowhy,0
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update",0
cate estimator but not the effect.,0
don't proxy special methods,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
For when checking input values is disabled,0
Type to column extraction function,0
if not all column names are strings,0
coerce feature names to be strings,0
Prefer sklearn 1.0's get_feature_names_out method to deprecated get_feature_names method,0
"Some featurizers will throw, such as a pipeline with a transformer that doesn't itself support names",0
"Get number of arguments, some sklearn featurizer don't accept feature_names",0
Handles cases where the passed feature names create issues,0
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names',0
Get feature names using featurizer,0
All attempts at retrieving transformed feature names have failed,0
Delegate handling to downstream logic,0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Any access should throw,0
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports",0
return plain dictionary so that erroneous accesses don't half work (see e.g. #708),0
for every dimension of the treatment add some epsilon and observe change in featurized treatment,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
return plain dictionary so that erroneous accesses don't half work (see #708),0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
return plain dictionary so that erroneous accesses don't half work (see #708),0
input feature name is already updated by cate_feature_names.,0
define the index of d_x to filter for each given T,0
filter X after broadcast with T for each given T,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
return plain dictionary so that erroneous accesses don't half work (see #708),0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
return plain dictionary so that erroneous accesses don't half work (see #708),0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains some snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_export.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
make any access to matplotlib or plt throw an exception,0
make any access to graphviz or plt throw an exception,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
Initialize saturation & value; calculate chroma & value shift,0
Calculate some intermediate values,0
Initialize RGB with same hue & chroma as our color,0
Shift the initial RGB values to match value and store,0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use mean of targets,0
Write node mean CATE,0
Write node std of CATE,0
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
Fetch appropriate color for node,0
Write node mean CATE,0
Write node mean CATE,0
Write recommended treatment and value - cost,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
This flag is true when names are set in a child class instead,0
"If names are set in a child class, add an attribute reflecting that",0
This works only if X is passed as a kwarg,0
We plan to enforce X as kwarg only in future releases,0
This checks if names have been set in a child class,0
"If names were set in a child class, don't do it again",0
"Wraps-up fit by setting attributes, cleaning up, etc.",0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
apply defaults before calling inference method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"Treatment names is None, default to BaseCateEstimator",0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
Get input names,0
Summary,0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
Get input names,0
Note: we do not transform feature names since that is done within summary_frame,0
Summary,0
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
fully materialize folds so that they can be reused across models,0
and precompute fitted indices so that we fail fast if there's an issue with them,0
NOTE: if any model is missing scores we will just return None even if another model,0
has scores. this is because we don't know how many scores are missing,0
"for the models that are missing them, so we don't know how to pad the array",0
for convenience we allos a single model to be passed in lieu of a singleton list,0
"in that case, we will also unwrap the model output",0
"when there is more than one model, nuisances from previous models",0
come first as positional arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
Adding the kwargs to ray object store to be used by remote functions,0
for each fold to avoid IO overhead,0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
Define Ray remote function (Ray remote wrapper of the _fit_nuisances function),0
Create Ray remote jobs for parallel processing,0
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same,0
alteration even when we only want to fit_final.,0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
upgrade to a GroupKFold or StratiGroupKFold if groups is not None,0
"we won't have generated a KFold or StratifiedKFold ourselves when groups are passed,",0
"but the user might have supplied one, which won't work",0
self._models_nuisance will be a list of lists or a list of list of lists,0
so we use self._ortho_learner_model_nuisance to determine the nesting level,0
for each mc iteration,0
for each model under cross fit setting,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"TODO: This could be extended to also work with our sparse and 2SLS estimators,",1
if we add an aggregate method to them,0
Remember to update the docs if this changes,0
mix in the appropriate inference class,0
assign all of the attributes from the dummy estimator that would normally be assigned during fitting,0
TODO: This seems hacky; is there a better abstraction to maintain these?,1
"This should also include bias_part_of_coef, model_final_, and fitted_models_final above",0
Assign treatment expansion attributes,0
Methods needed to implement the LinearCateEstimator interface,0
Methods needed to implement the LinearFinalModelCateEstimatorMixin,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Policy Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base Policy tree,0
=============================================================================,0
The values below are required and utilitized by methods in the _SingleTreeExporterMixin,0
HACK: sklearn 1.3 enforces that the input to plot_tree is a DecisionTreeClassifier or DecisionTreeRegressor,1
This is a hack to get around that restriction by declaring that PolicyTree inherits from DecisionTreeClassifier,1
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"Unique treatments (ordered, includes control)",0
Number of treatments (excluding control),0
Indicator for whether,0
Get DR outcomes in training sample,0
Get DR outcomes in validation sample,0
Get DR outcomes in validation sample,0
Calculate ATE in the validation sample,0
Fit propensity in treatment,0
Predict propensity scores,0
Possible treatments (need to allow more than 2),0
Predict outcomes,0
T-learner logic,0
"if CATE is given explicitly or has not been fitted at all previously, fit it now",0
Assign units in validation set to groups,0
Proportion of validations set in group,0
Group average treatment effect (GATE) -- average of DR outcomes in group,0
Average of CATE predictions in group,0
Calculate group calibration score,0
Calculate overall calibration score,0
Calculate R-square calibration score,0
"if CATE is given explicitly or has not been fitted at all previously, fit it now",0
treat each treatment as a separate regression,0
"here, prop_preds should be a matrix",0
with rows corresponding to units and columns corresponding to treatment statuses,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"since we only allow single dimensional y, we could flatten the prediction",0
override only so that we can exclude treatment featurization verbiage in docstring,0
override only so that we can exclude treatment featurization verbiage in docstring,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Handles the corner case when X=None but featurizer might be not None,0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
Replacing to remove docstring,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
data is already validated at initial fit time,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE: important to use the rlearner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the rlearner's copy to retain the information from fitting,0
Handles the corner case when X=None but featurizer might be not None,0
override only so that we can update the docstring to indicate support for `LinearModelFinalInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support freq_weight and sample_var in debiased lasso,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add blb to parent's options,0
override only so that we can update the docstring to indicate,0
support for `GenericSingleTreatmentModelFinalInference`,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
NOTE: important to get parent's wrapped copy so that,0
"after training wrapped featurizer is also trained, etc.",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Fit a doubly robust average effect,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T",0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
"If custom param grid, check that only estimator parameters are being altered",0
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test",0
override only so that we can update the docstring to indicate support for `blb`,0
Get input names,0
Summary,0
Determine output settings,0
"Important: This must be the first invocation of the random state at fit time, so that",0
train/test splits are re-generatable from an external object simply by knowing the,0
random_state parameter of the tree. Can be useful in the future if one wants to create local,0
linear predictions. Currently is also useful for testing.,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
Set min_weight_leaf from min_weight_fraction_leaf,0
Build tree,0
We calculate the maximum number of samples from each half-split that any node in the tree can,0
hold. Used by criterion for memory space savings.,0
Initialize the criterion object and the criterion_val object if honest.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code is a fork from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_base.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
Set parameters,0
Don't instantiate estimators now! Parameters of base_estimator might,0
"still change. Eg., when grid-searching with the nested object syntax.",0
self.estimators_ needs to be filled by the derived classes in fit.,0
Compute the number of jobs,0
Partition estimators between jobs,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
covariance matrix,0
get eigen value and eigen vectors,0
simulate eigen vectors,0
keep the top 4 eigen value and corresponding eigen vector,0
replace the negative eigen values,0
generate a new covariance matrix,0
get linear approximation of eigen values,0
coefs,0
get the indices of each group of features,0
print(ind_same_proxy),0
demo,0
same proxy,0
residuals,0
gmm,0
log normal on outliers,0
positive outliers,0
negative outliers,0
demean the new residual again,0
generate data,0
sample residuals,0
get prediction for current investment,0
get prediction for current proxy,0
get first period prediction,0
iterate the step ahead contruction,0
prepare new x,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
get new covariance matrix,0
get coefs,0
get residuals,0
proxy 1 is the outcome,0
make fixed residuals,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=None here since we measure the effect across all Ts,0
"y is a vector, rather than a 2D array",0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
squeeze the first axis,0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T",0
squeeze the first axis,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector,0
d_t=None here since we measure the effect across all Ts,0
d_t=None here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet",0
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
scale preds,0
scale std errs,0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
offset preds,0
"offset the distribution, too",0
scale preds,0
"scale the distribution, too",0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base GRF tree,0
=============================================================================,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
=============================================================================,0
A MultOutputWrapper for GRF classes,0
=============================================================================,0
=============================================================================,0
Instantiations of Generalized Random Forest,0
=============================================================================,0
"Append a constant treatment if `fit_intercept=True`, the coefficient",0
in front of the constant treatment is the intercept in the moment equation.,0
"Append a constant treatment and constant instrument if `fit_intercept=True`,",0
the coefficient in front of the constant treatment is the intercept in the moment equation.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Base Generalized Random Forest,0
=============================================================================,0
TODO: support freq_weight and sample_var,1
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle,0
We calculate the min eigenvalue proxy that each criterion is considering,0
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`",0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Generating indices a priori before parallelism ended up being orders of magnitude,0
faster than how sklearn does it. The reason is that random samplers do not release the,0
gil it seems.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
####################,0
Variance correction,0
####################,0
Subtract the average within bag variance. This ends up being equal to the,0
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).,0
The negative part is just sq_between.,0
Objective bayes debiasing for the diagonals where we know a-prior they are positive,0
"The off diagonals we have no objective prior, so no correction is applied.",0
Finally correcting the pred_cov or pred_var,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
we have to filter the folds because they contain the indices in the original data not,0
the indices in the period-filtered data,0
translate the indices in a fold to the indices in the period-filtered data,0
"if groups was [3,3,4,4,5,5,6,6,1,1,2,2,0,0] (the group ids can be in any order, but the",0
"time periods for each group should be contguous), and we had [10,11,0,1] as the indices in a fold",0
(so the fold is taking the entries corresponding to groups 2 and 3),0
"then group_period_filter(0) is [0,2,4,6,8,10,12] and gpf(1) is [1,3,5,7,9,11,13]",0
"so for period 1, the fold should be [10,0] => [5,0] (the indices that return 10 and 0 in the t=0 data)",0
"and for period 2, the fold should be [11,1] => [5,0] again (the indices that return 11,1 in the t=1 data)",0
filter to the indices for the time period,0
"now find their index in the period-filtered data, which is always sorted",0
sanity check that the folds are the same no matter the time period,0
TODO: update docs,1
"NOTE: sample weight, sample var are not passed in",0
Compose final model,0
Calculate auxiliary quantities,0
X  T_res,0
"sum(model_final.predict(X, T_res))",0
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J",0
override only so that we can exclude treatment featurization verbiage in docstring,0
override only so that we can exclude treatment featurization verbiage in docstring,0
"we need to set the number of periods before calling super()._prefit, since that will generate the",0
"final and nuisance models, which need to have self._n_periods set",0
Set _d_t to effective number of treatments,0
Required for bootstrap inference,0
for each mc iteration,0
for each model under cross fit setting,0
Handles the corner case when X=None but featurizer might be not None,0
Expand treatments for each time period,0
NOTE: important to use the _ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the _ortho_learner's copy to retain the information from fitting,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
test that the subsampling scheme past to the trees is correct,0
The sample size is chosen in particular to test rounding based error when subsampling,0
test that the estimator calcualtes var correctly,0
test api,0
test accuracy,0
test the projection functionality of forests,0
test that the estimator calcualtes var correctly,0
test api,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
test that the subsampling scheme past to the trees is correct,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
filter directories by regex if the NOTEBOOK_DIR_PATTERN environment variable is set,0
omit the lalonde notebook,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
create directory if necessary,0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
for is_discrete in [False]:,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
No heterogeneity,0
Define indices to test,0
Heterogeneous effects,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
pass sample weight to final step of pipeline,0
create data with missing values,0
model that can handle missing values,0
"test X, W only",0
test W only,0
dowhy does not support missing values in X,0
assert that fitting with missing values fails when allow_missing is False,0
and that setting allow_missing after init still works,0
assert that we fail with a value error when we pass missing X to a model that doesn't support it,0
assert that fitting with missing values fails when allow_missing is False,0
and that setting allow_missing after init still works,0
metalearners don't support W,0
metalearners do support missing values in X,0
dowhy never supports missing values in X,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
identity featurization effect functions,0
polynomial featurization effect functions,0
1d polynomial featurization functions,0
2d-to-1d featurization functions,0
2d-to-1d vector featurization functions,0
use LassoCV rather than also selecting over RandomForests to save time,0
test that treatment names are assigned for the featurized treatment,0
expected shapes,0
check effects,0
ate,0
loose inference checks,0
temporarily skip LinearDRIV and SparseLinearDRIV for weird effect shape reasons,0
effect inference,0
marginal effect inference,0
const marginal effect inference,0
fit a dummy estimator first so the featurizer can be fit to the treatment,0
edge case with transformer that only takes a vector treatment,0
so far will always return None for cate_treatment_names,0
assert proper handling of improper feature names passed to certain transformers,0
"depending on sklearn version, bad feature names either throws error or only uses first relevant name",0
ensure alpha is passed,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
initialize parameters,0
initialize config wtih base config and overwite some values,0
predict tree using config parameters and assert,0
shape of trained tree is the same as y_test,0
initialize config wtih base honest config and overwite some values,0
predict tree using config parameters and assert,0
shape of trained tree is the same as y_test,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
compare with weight,0
compare with weight,0
compare with weight,0
compare with weight,0
Multi-dimensional output y,0
1-d y,0
compare when both sample_var and sample_weight exist,0
multi-d y,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
dgp,0
StatsModels2SLS,0
IV2SLS,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
fixed functions as first stage models,0
they can be anything as long as fitting doesn't modify the predictions,0
"that way, it doesn't matter if they are trained on different subsets of the data",0
all estimators must have opted in to federation,0
all estimators must have the same covariance type,0
test coefficients,0
test effects,0
fixed functions as first stage models,0
they can be anything as long as fitting doesn't modify the predictions,0
"that way, it doesn't matter if they are trained on different subsets of the data",0
test coefficients,0
test effects,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
ensure alpha is passed,0
only is not None when T1 is a constant or a list of constant,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Generate synthetic data,0
Run _crossfit with Ray enabled,0
Run _crossfit without Ray,0
Compare the results,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
make any access to matplotlib or plt throw an exception,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
Invert indices to match latest API,0
Invert indices to match latest API,0
The feature for heterogeneity stays constant,0
Auxiliary function for adding xticks and vertical lines when plotting results,0
for dynamic dml vs ground truth parameters.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
tests that we can recover the right degree of polynomial features,0
implicitly also tests ability to handle pipelines,0
since 'poly' uses pipelines containing PolynomialFeatures,0
"generate larger coefficients in a set of high degree features,",0
weighted towards higher degree features,0
"just test a polynomial T model, since for Y the correct degree also depends on",0
the interation of T and X,0
test corner case with just one model in a list,0
test corner case with empty list,0
test selecting between two fixed models,0
"DGP is a linear model, so linear regression should fit better",0
"DGP is now non-linear, so random forest should fit better",0
these models only work on multi-output data,0
SeparatedModel doesn't support scoring; that should be fine when not compared to other models,0
"on the other hand, when we need to compare the score to other models, it should raise an error",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
test at least one estimator from each category,0
test causal graph,0
need to set matplotlib backend before viewing model,0
test refutation estimate,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Generate random Xs,0
Random covariance matrix of Xs,0
Effect of Xs on outcome,0
Effect of treatment on outcomes,0
Effect of treatment on outcome conditional on X1,0
Generate treatments based on X and random noise,0
"Generate Y (based on X, D, and random noise)",0
"Simple classifier and regressor for propensity, outcome, and cate",0
test the DR outcome difference,0
"Simple classifier and regressor for propensity, outcome, and cate",0
test the DR outcome difference,0
"Simple classifier and regressor for propensity, outcome, and cate",0
test the DR outcome difference,0
use evaluate_blp to fit on validation only,0
"Simple classifier and regressor for propensity, outcome, and cate",0
test the DR outcome difference,0
fit nothing,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
accuracy test,0
"accuracy test, DML",0
uncomment when issue #837 is resolved,0
"NonParamDMLIV(discrete_outcome=discrete_outcome, discrete_treatment=discrete_treatment,",0
"discrete_instrument=discrete_instrument, model_final=LinearRegression())",0
make sure the auto outcome model is a classifier,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"since we're running so many combinations, just use LassoCV/LogisticRegressionCV",0
for the models instead of also selecting over random forest models,0
ensure we can serialize unfit estimator,0
ensure we can serialize fit estimator,0
expected effect size,0
test effect,0
test inference,0
only OrthoIV support inference other than bootstrap,0
test summary,0
test can run score,0
test cate_feature_names,0
test can run shap values,0
dgp,0
no heterogeneity,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"if we aren't fitting on the whole dataset, ensure that the limits are respected",0
ensure that the grouping has worked correctly and we get exactly the number of copies,0
of the items in whichever groups we see,0
DML nested CV works via a 'cv' attribute,0
"want to validate the nested grouping, not the outer grouping in the nesting tests",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
parameter combinations to test,0
"we're running a lot of tests, so use fixed models instead of model selection",0
"IntentToTreatDRIV only supports binary treatments and instruments, and doesn't support fit_cov_directly",0
TODO: serializing/deserializing for every combination -- is this necessary?,1
ensure we can serialize unfit estimator,0
ensure we can serialize fit estimator,0
expected effect size,0
assert calculated constant marginal effect shape is expected,0
const_marginal effect is defined in LinearCateEstimator class,0
assert calculated marginal effect shape is expected,0
test inference,0
test can run score,0
test cate_feature_names,0
test can run shap values,0
"dgp (binary T, binary Z)",0
no heterogeneity,0
with heterogeneity,0
fitting the covariance directly should be at least as good as computing the covariance from separate models,0
set the models so that model selection over random forests doesn't take too much time in the repeated trials,0
directly fitting the covariance should be better than indirectly fitting it,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
make sure we can get out post-fit stuff,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
"with this DGP, since T depends linearly on X, Y depends on X quadratically",0
so we should use a quadratic featurizer,0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
"we have quadratic terms in y, so we need to pipeline with a quadratic featurizer",0
Compare results with and without Ray,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
"with 2 folds, we should get exactly 3 groups per split, each with 10 copies of the y or t value",0
test nested grouping,0
"with 2-fold outer and 2-fold inner grouping, and six total groups,",0
should get 1 or 2 groups per split,0
"Try default, integer, and new user-passed treatment name",0
FunctionTransformers are agnostic to passed treatment names,0
Expected treatment names are the sums of user-passed prefixes and transformer-specific postfixes,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
NOTE: StratifiedGroupKFold has a bug when shuffle is True where it doesn't always stratify properly,0
so we explicitly pass a StratifiedGroupKFold with shuffle=False (the default) rather than letting,0
cross-fit generate one,0
"with 2-fold grouping, we should get exactly 3 groups per split",0
test nested grouping,0
"with 2-fold outer and 2-fold inner grouping, we should get 1-2 groups per split",0
helper class,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
continuous treatments have typical treatment values equal to,0
the mean of the absolute value of non-zero entries,0
discrete treatments have typical treatment value 1,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
continuous treatments have typical treatment values equal to,0
the mean of the absolute value of non-zero entries,0
discrete treatments have typical treatment value 1,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
make sure we don't run into problems dropping every index,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
dgp,0
model,0
model,0
"columns 'd', 'e', 'h' have too many values",0
"columns 'd', 'e' have too many values",0
lowering bound shouldn't affect already fit columns when warm starting,0
"column d is now okay, too",0
verify that we can use a scalar treatment cost,0
verify that we can specify per-treatment costs for each sample,0
verify that using the same state returns the same results each time,0
set the categories for column 'd' explicitly so that b is default,0
"first column: 10 ones, this is fine",0
"second column: 6 categories, plenty of random instances of each",0
this is fine only if we increase the cateogry limit,0
"third column: nine ones, lots of twos, not enough unless we disable check",0
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity",0
forest heterogeneity won't work,0
"sixth column: just 1 one, not enough even without check",0
increase bound on cat expansion,0
skip checks (reducing folds accordingly),0
"Add tests that guarantee that the reliance on DML feature order is not broken, such as",0
"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W",0
Pass an example where W is irrelevant and X is confounder,0
"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be",0
zeroed out and the test will fail,0
"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates",0
rescaling X shouldn't affect the first stage models because they normalize the inputs,0
"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid",0
cross terms,0
scale by 1000 to match the input to this model:,0
"the scale of X does matter for the final model, which keeps results in user-denominated units",0
rescaling X still shouldn't affect the first stage models,0
TODO: we don't recover the correct values with enough accuracy to enable this assertion,1
is there a different way to verify that we are learning the correct coefficients?,1
"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Define data features,0
Added `_df`to names to be different from the default cate_estimator names,0
Generate data,0
################################,0
Single treatment and outcome #,0
################################,0
Test LinearDML,0
|--> Test featurizers,0
"ColumnTransformer behaves differently depending on version of sklearn, so we no longer check the names",0
|--> Test re-fit,0
Test SparseLinearDML,0
Test ForestDML,0
###################################,0
Mutiple treatments and outcomes #,0
###################################,0
Test LinearDML,0
Test SparseLinearDML,0
"Single outcome only, ORF does not support multiple outcomes",0
Test DMLOrthoForest,0
Test DROrthoForest,0
Test XLearner,0
Skipping population summary names test because bootstrap inference is too slow,0
Test SLearner,0
Test TLearner,0
Test LinearDRLearner,0
Test SparseLinearDRLearner,0
Test ForestDRLearner,0
Test LinearIntentToTreatDRIV,0
Test DeepIV,0
Test categorical treatments,0
Check refit,0
Check refit after setting categories,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
import here since otherwise test collection would fail if matplotlib is not installed,0
Treatment effect function,0
Outcome support,0
Treatment support,0
"Generate controls, covariates, treatments and outcomes",0
Heterogeneous treatment effects,0
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
through shap package.,0
test shap could generate the plot from the shap_values,0
"waterfall is broken in this version, fixed by https://github.com/slundberg/shap/pull/2444",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we need the controls' encoded column for training",0
"Thus, we append the controls column before the one-hot-encoded T",0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary",0
TODO: prel_model_effect could allow sample_var and freq_weight?,1
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary",0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous",0
reshape the predictions,0
concat W and Z,0
check nuisances outcome shape,0
Y_res could be a vector or 1-dimensional 2d-array,0
"We're projecting, so we're treating E[T|X,Z] as the instrument (ignoring W for simplicity)",0
"Then beta(X) = E[T (E[T|X,Z]-E[E[T|X,Z]|X)|X] and we can apply the tower rule several times to get",0
"= E[(E[T|X,Z]-E[T|X])^2|X]",0
"and also     = E[(E[T|X,Z]-T)^2|X]",0
so we can compute it either from (T_proj-T_pred)^2 or from (T_proj-T)^2,0
The first of these is just Z_res^2,0
"fit on T*T_proj, covariance will be computed by E[T_res * T_proj] = E[T*T_proj] - E[T]^2",0
"return shape (n,)",0
we will fit on the covariance (T_res*Z_res) directly,0
"fit on TZ, covariance will be computed by E[T_res * Z_res] = TZ_pred - T_pred * Z_pred",0
"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)",0
"shape (n,)",0
"shape (n,)",0
"shape(n,)",0
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary",0
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary",0
"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous",0
reshape the predictions,0
"in the projection case, this is a variance and should always be non-negative",0
check nuisances outcome shape,0
"all could be reshaped to vector since Y, T, Z are all single dimensional.",0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
A helper class that access all the internal fitted objects of a DRIV Cate Estimator.,0
Used by both DRIV and IntentToTreatDRIV.,0
Maggie: I think that would be the case?,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE: important to use the ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
Handles the corner case when X=None but featurizer might be not None,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"this is a regression model since the instrument E[T|X,W,Z] is always continuous",0
"we're using E[T|X,W,Z] as the instrument",0
Define the data generation functions,0
Define the data generation functions,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
Define the data generation functions,0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
Define the data generation functions,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
concat W and Z,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
concat W and Z,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
reshape the predictions,0
"T_res, Z_res, beta expect shape to be (n,1)",0
Define the data generation functions,0
maybe shouldn't expose fit_cate_intercept in this class?,0
Define the data generation functions,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: do correct adjustment for sample_var,1
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
concat W and Z,0
concat W and Z,0
concat W and Z,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Define the data generation functions,0
"train E[T|X,W,Z]",0
"train E[Z|X,W]",0
note: discrete_instrument rather than discrete_treatment in call to _make_first_stage_selector,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE: important to use the ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
Handles the corner case when X=None but featurizer might be not None,0
NOTE This is used by the inference methods and is more for internal use to the library,0
concat W and Z,0
note that groups are not passed to score because they are only used for fitting,0
concat W and Z,0
note that sample_weight and groups are not passed to predict because they are only used for fitting,0
concat W and Z,0
A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.,0
Used by both Parametric and Non Parametric DMLIV.,0
override only so that we can enforce Z to be required,0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Handles the corner case when X=None but featurizer might be not None,0
Define the data generation functions,0
Get input names,0
Summary,0
coefficient,0
intercept,0
Define the data generation functions,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: this utility is documented but internal; reimplement?,1
TODO: this utility is even less public...,1
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged",0
use same Cs as would be used by default by LogisticRegressionCV,0
NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification,0
which could affect how many times each distinct Y value needs to be present in the data,0
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns,0
but also supports get_feature_names with expected signature,0
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value,0
NOTE: we rely on the passthrough columns coming first in the concatenated X;W,0
"when we pipeline scaling with our first stage models later, so the order here is important",0
TODO: remove once older sklearn support is no longer needed,1
Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy,0
Convert python objects to (possibly nested) types that can easily be represented as literals,0
Convert SingleTreeInterpreter to a python dictionary,0
named tuple type for storing results inside CausalAnalysis class;,0
must be lifted to module level to enable pickling,0
"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,",1
"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all",0
"when running the first stage models, but don't want to scale the X columns when running the final model,",0
since then our coefficients will have odd units and our trees will also have decisions using those units.,0
,0
"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)",0
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names,0
Controls are all other columns of X,0
"can't use X[:, feat_ind] when X is a DataFrame",0
TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
we might want to modify OrthoLearner (and other discrete treatment classes),0
so that the user can opt-in to allowing unseen treatment values,0
(and return NaN or something in that case),0
HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models,1
and so we can just peel the first columns off of that combined array for rescaling in the pipeline,0
TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are,1
"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this",0
becomes a valid approach to handling this,0
array checking routines don't accept 0-width arrays,0
perform model selection,0
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative,0
convert to NormalInferenceResults for consistency,0
Set the dictionary values shared between local and global summaries,0
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments",0
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category",0
required to fit a discrete DML model,0
"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular",1
sub-cases of models or also integrate with azure autoML. (post-MVP),0
"TODO: Add other heterogeneity model options, such as {'automl'} for performing",1
"model selection for the causal effect, or {'sparse_linear'} for using a debiased lasso. (post-MVP)",0
TODO: Enable multi-class classification (post-MVP),1
Validate inputs,0
TODO: check compatibility of X and Y lengths,1
"no previous fit, cancel warm start",0
"work with numeric feature indices, so that we can easily compare with categorical ones",0
"if heterogeneity_inds is 1D, repeat it",0
heterogeneity inds should be a 2D list of length same as train_inds,0
replace None elements of heterogeneity_inds and ensure indices are numeric,0
"TODO: bail out also if categorical columns, classification, random_state changed?",1
TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
train the Y model,0
"perform model selection for the Y model using all X, not on a per-column basis",0
"now that we've trained the classifier and wrapped it, ensure that y is transformed to",0
work with the regression wrapper,0
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays,0
"note that this needs to happen after wrapping to generalize to the multi-class case,",0
since otherwise we'll have too many columns to be able to train a classifier,0
start with empty results and default shared insights,0
convert categorical indicators to numeric indices,0
check for indices over the categorical expansion bound,0
assume we'll be able to train former failures this time; we'll add them back if not,0
"can't remove in place while iterating over new_inds, so store in separate list",0
"train the model, but warn",0
no model can be trained in this case since we need more folds,0
"don't train a model, but suggest workaround since there are enough instances of least",1
populated class,0
also remove from train_inds so we don't try to access the result later,0
extract subset of names matching new columns,0
"track indices where an exception was thrown, since we can't remove from dictionary while iterating",0
don't want to cache this failed result,0
properties to return from effect InferenceResults,0
properties to return from PopulationSummaryResults,0
Converts strings to property lookups or method calls as a convenience so that the,0
_point_props and _summary_props above can be applied to an inference object,0
Create a summary combining all results into a single output; this is used,0
by the various causal_effect and causal_effect_dict methods to generate either a dataframe,0
"or a dictionary, respectively, based on the summary function passed into this method",0
"ensure array has shape (m,y,t)",0
population summary is missing sample dimension; add it for consistency,0
outcome dimension is missing; add it for consistency,0
add singleton treatment dimension if missing,0
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce,0
"each attr has dimension (m,y) or (m,y,t)",0
concatenate along treatment dimension,0
"for dictionary representation, want to remove unneeded sample dimension",0
in cohort and global results,0
TODO: enrich outcome logic for multi-class classification when that is supported,1
There is no actual sample level in this data,0
can't drop only level,0
should be serialization-ready and contain no numpy arrays,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
TODO: Note that there's no column metadata for the sample number - should there be?,1
"need to replicate the column info for each sample, then remove from the shared data",0
NOTE: the flattened order has the ouptut dimension before the feature dimension,0
which may need to be revisited once we support multiclass,0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
a global inference indicates the effect of that one feature on the outcome,0
need to reshape the output to match the input,0
we want to offset the inference object by the baseline estimate of y,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
"NOTE: this calculation is correct only if treatment costs are marginal costs,",0
because then scaling the difference between treatment value and treatment costs is the,0
same as scaling the treatment value and subtracting the scaled treatment cost.,0
,0
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for",0
"continuous treatments, the policy value should include the benefit of decreasing treatments",0
(rather than just not treating at all),0
,0
"We can get the total by seeing that if we restrict attention to units where we would treat,",0
2 * policy_value - always_treat,0
includes exactly their contribution because policy_value and always_treat both include it,0
"and likewise restricting attention to the units where we want to decrease treatment,",0
2 * policy_value - always-treat,0
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value",0
is zero and the contribution to always_treat is negative,0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
get dataframe with all but selected column,0
apply 10% of a typical treatment for this feature,0
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely",0
set the effect bounds; for positive treatments these agree with,0
"the estimates; for negative treatments, we need to invert the interval",0
the effect is now always positive since we decrease treatment when negative,0
"for discrete treatment, stack a zero result in front for control",0
we need to call effect_inference to get the correct CI between the two treatment options,0
we now need to construct the delta in the cost between the two treatments and translate the effect,0
remove third dimenions potentially added,0
"find cost of current treatment: equality creates a 2d array with True on each row,",0
only if its the location of the current treatment. Then we take the corresponding cost.,0
construct index of current treatment,0
add second dimension if needed for broadcasting during translation of effect,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
only expose predict_proba if best_model has predict_proba,0
used because logic elsewhere uses hasattr predict proba to check if model is a classifier,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a `groups` argument,0
"drop groups from arg list, which were already used at the outer level and may not be supported by the model",0
the score needs to be compared to another model's,0
"so we don't need to fit the model itself on all of the data, just get the out-of-sample score",0
use _fit_with_groups instead of just fit to handle nested grouping,0
we need to train the model on the data,0
copy common parameters,0
copy common fitted variables,0
make sure all classes agree on best c/l1 combo,0
"We need an R^2 score to compare to other models; ElasticNetCV doesn't provide it,",0
but we can calculate it ourselves from the MSE plus the variance of the target y,0
"l1 ratio doesn't apply to Lasso, only ElasticNet",0
max R^2 corresponds to min MSE,0
"constructor takes cv as a positional or kwarg, just pull it out of a new instance",0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
verbose was removed from sklearn's non-public _fit_and_predict method in 1.4,0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
local import to avoid circular imports,0
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Call weighted lasso on reduced design matrix,0
Weighted tau,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to the single-target estimator by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
set default values for None,0
check freq_weight should be integer and should be accompanied by sample_var,0
check array shape,0
weight X and y and sample_var,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
"For aggregation calculations, always treat wy as an array so that einsum expressions don't need to change",0
We'll collapse results back down afterwards if necessary,0
"for federation, we need to store these 5 arrays when using heteroskedasticity-robust inference",0
y dimension is always first in the output when present so that broadcasting works correctly,0
set default values for None,0
check array shape,0
check dimension of instruments is more than dimension of treatments,0
weight X and y,0
learn point estimate,0
solve first stage linear regression E[T|Z],0
"""that"" means T",0
solve second stage linear regression E[Y|that],0
(T.T*T)^{-1},0
learn cov(theta),0
(T.T*T)^{-1},0
sigma^2,0
reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
average the outcome dimension if it exists and ensure 2d y_pred,0
get index of best treatment,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
TODO: ideally the below private attribute logic should be in .fit but is needed in init,1
for nuisance estimator generation for parent class,0
should refactor later,1
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
override only so that we can exclude treatment featurization verbiage in docstring,0
Override to flatten output if T is flat,0
override only so that we can exclude treatment featurization verbiage in docstring,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
d_t=None here since we measure the effect across all Ts,0
conditionally expand jacobian dimensions to align with einsum str,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile",0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/main/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
TODO: enable type aliases,1
napoleon_preprocess_types = True  # needed for type aliases to work,0
napoleon_type_aliases = {,0
"""array_like"": "":term:`array_like`"",",0
"""ndarray"": ""~numpy.ndarray"",",0
"""RandomState"": "":class:`~numpy.random.RandomState`"",",0
"""DataFrame"": "":class:`~pandas.DataFrame`"",",0
"""Series"": "":class:`~pandas.Series`"",",0
},0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of strings:,0
,0
"source_suffix = ['.rst', '.md']",0
The root toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)",0
def mlasso_model(): return MultiTaskLassoCV(,0
"cv=3, alphas=alpha_regs, max_iter=200)",0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
"alpha_regs = [5e-3, 1e-2, 5e-2]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)",0
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)",0
subset of features that are exogenous and create heterogeneity,0
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features),0
subset of features wrt we estimate heterogeneity,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
introspect the constructor arguments to find the model parameters,0
to represent,0
"if the argument is deprecated, ignore it",0
Extract and sort argument names excluding 'self',0
column names,0
transfer input to numpy arrays,0
transfer input to 2d arrays,0
create dataframe,0
currently dowhy only support single outcome and single treatment,0
call dowhy,0
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update",0
cate estimator but not the effect.,0
don't proxy special methods,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
For when checking input values is disabled,0
Type to column extraction function,0
if not all column names are strings,0
coerce feature names to be strings,0
Prefer sklearn 1.0's get_feature_names_out method to deprecated get_feature_names method,0
"Some featurizers will throw, such as a pipeline with a transformer that doesn't itself support names",0
"Get number of arguments, some sklearn featurizer don't accept feature_names",0
Handles cases where the passed feature names create issues,0
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names',0
Get feature names using featurizer,0
All attempts at retrieving transformed feature names have failed,0
Delegate handling to downstream logic,0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Any access should throw,0
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports",0
return plain dictionary so that erroneous accesses don't half work (see e.g. #708),0
for every dimension of the treatment add some epsilon and observe change in featurized treatment,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
return plain dictionary so that erroneous accesses don't half work (see #708),0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
return plain dictionary so that erroneous accesses don't half work (see #708),0
input feature name is already updated by cate_feature_names.,0
define the index of d_x to filter for each given T,0
filter X after broadcast with T for each given T,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
return plain dictionary so that erroneous accesses don't half work (see #708),0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
return plain dictionary so that erroneous accesses don't half work (see #708),0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains some snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_export.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
make any access to matplotlib or plt throw an exception,0
make any access to graphviz or plt throw an exception,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
Initialize saturation & value; calculate chroma & value shift,0
Calculate some intermediate values,0
Initialize RGB with same hue & chroma as our color,0
Shift the initial RGB values to match value and store,0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use mean of targets,0
Write node mean CATE,0
Write node std of CATE,0
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
Fetch appropriate color for node,0
Write node mean CATE,0
Write node mean CATE,0
Write recommended treatment and value - cost,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
This flag is true when names are set in a child class instead,0
"If names are set in a child class, add an attribute reflecting that",0
This works only if X is passed as a kwarg,0
We plan to enforce X as kwarg only in future releases,0
This checks if names have been set in a child class,0
"If names were set in a child class, don't do it again",0
"Wraps-up fit by setting attributes, cleaning up, etc.",0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
apply defaults before calling inference method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"Treatment names is None, default to BaseCateEstimator",0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
Get input names,0
Summary,0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
Get input names,0
Note: we do not transform feature names since that is done within summary_frame,0
Summary,0
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
fully materialize folds so that they can be reused across models,0
and precompute fitted indices so that we fail fast if there's an issue with them,0
NOTE: if any model is missing scores we will just return None even if another model,0
has scores. this is because we don't know how many scores are missing,0
"for the models that are missing them, so we don't know how to pad the array",0
for convenience we allos a single model to be passed in lieu of a singleton list,0
"in that case, we will also unwrap the model output",0
"when there is more than one model, nuisances from previous models",0
come first as positional arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
Adding the kwargs to ray object store to be used by remote functions,0
for each fold to avoid IO overhead,0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
Define Ray remote function (Ray remote wrapper of the _fit_nuisances function),0
Create Ray remote jobs for parallel processing,0
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same,0
alteration even when we only want to fit_final.,0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
upgrade to a GroupKFold or StratiGroupKFold if groups is not None,0
"we won't have generated a KFold or StratifiedKFold ourselves when groups are passed,",0
"but the user might have supplied one, which won't work",0
self._models_nuisance will be a list of lists or a list of list of lists,0
so we use self._ortho_learner_model_nuisance to determine the nesting level,0
for each mc iteration,0
for each model under cross fit setting,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"TODO: This could be extended to also work with our sparse and 2SLS estimators,",1
if we add an aggregate method to them,0
Remember to update the docs if this changes,0
mix in the appropriate inference class,0
assign all of the attributes from the dummy estimator that would normally be assigned during fitting,0
TODO: This seems hacky; is there a better abstraction to maintain these?,1
"This should also include bias_part_of_coef, model_final_, and fitted_models_final above",0
Assign treatment expansion attributes,0
Methods needed to implement the LinearCateEstimator interface,0
Methods needed to implement the LinearFinalModelCateEstimatorMixin,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Policy Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base Policy tree,0
=============================================================================,0
The values below are required and utilitized by methods in the _SingleTreeExporterMixin,0
HACK: sklearn 1.3 enforces that the input to plot_tree is a DecisionTreeClassifier or DecisionTreeRegressor,1
This is a hack to get around that restriction by declaring that PolicyTree inherits from DecisionTreeClassifier,1
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"Unique treatments (ordered, includes control)",0
Number of treatments (excluding control),0
Indicator for whether,0
Get DR outcomes in training sample,0
Get DR outcomes in validation sample,0
Get DR outcomes in validation sample,0
Calculate ATE in the validation sample,0
Fit propensity in treatment,0
Predict propensity scores,0
Possible treatments (need to allow more than 2),0
Predict outcomes,0
T-learner logic,0
"if CATE is given explicitly or has not been fitted at all previously, fit it now",0
Assign units in validation set to groups,0
Proportion of validations set in group,0
Group average treatment effect (GATE) -- average of DR outcomes in group,0
Average of CATE predictions in group,0
Calculate group calibration score,0
Calculate overall calibration score,0
Calculate R-square calibration score,0
"if CATE is given explicitly or has not been fitted at all previously, fit it now",0
treat each treatment as a separate regression,0
"here, prop_preds should be a matrix",0
with rows corresponding to units and columns corresponding to treatment statuses,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"since we only allow single dimensional y, we could flatten the prediction",0
override only so that we can exclude treatment featurization verbiage in docstring,0
override only so that we can exclude treatment featurization verbiage in docstring,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Handles the corner case when X=None but featurizer might be not None,0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
Replacing to remove docstring,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
data is already validated at initial fit time,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE: important to use the rlearner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the rlearner's copy to retain the information from fitting,0
Handles the corner case when X=None but featurizer might be not None,0
override only so that we can update the docstring to indicate support for `LinearModelFinalInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support freq_weight and sample_var in debiased lasso,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add blb to parent's options,0
override only so that we can update the docstring to indicate,0
support for `GenericSingleTreatmentModelFinalInference`,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
NOTE: important to get parent's wrapped copy so that,0
"after training wrapped featurizer is also trained, etc.",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Fit a doubly robust average effect,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T",0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
"If custom param grid, check that only estimator parameters are being altered",0
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test",0
override only so that we can update the docstring to indicate support for `blb`,0
Get input names,0
Summary,0
Determine output settings,0
"Important: This must be the first invocation of the random state at fit time, so that",0
train/test splits are re-generatable from an external object simply by knowing the,0
random_state parameter of the tree. Can be useful in the future if one wants to create local,0
linear predictions. Currently is also useful for testing.,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
Set min_weight_leaf from min_weight_fraction_leaf,0
Build tree,0
We calculate the maximum number of samples from each half-split that any node in the tree can,0
hold. Used by criterion for memory space savings.,0
Initialize the criterion object and the criterion_val object if honest.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code is a fork from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_base.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
Set parameters,0
Don't instantiate estimators now! Parameters of base_estimator might,0
"still change. Eg., when grid-searching with the nested object syntax.",0
self.estimators_ needs to be filled by the derived classes in fit.,0
Compute the number of jobs,0
Partition estimators between jobs,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
covariance matrix,0
get eigen value and eigen vectors,0
simulate eigen vectors,0
keep the top 4 eigen value and corresponding eigen vector,0
replace the negative eigen values,0
generate a new covariance matrix,0
get linear approximation of eigen values,0
coefs,0
get the indices of each group of features,0
print(ind_same_proxy),0
demo,0
same proxy,0
residuals,0
gmm,0
log normal on outliers,0
positive outliers,0
negative outliers,0
demean the new residual again,0
generate data,0
sample residuals,0
get prediction for current investment,0
get prediction for current proxy,0
get first period prediction,0
iterate the step ahead contruction,0
prepare new x,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
get new covariance matrix,0
get coefs,0
get residuals,0
proxy 1 is the outcome,0
make fixed residuals,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=None here since we measure the effect across all Ts,0
"y is a vector, rather than a 2D array",0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
squeeze the first axis,0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T",0
squeeze the first axis,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector,0
d_t=None here since we measure the effect across all Ts,0
d_t=None here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet",0
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
scale preds,0
scale std errs,0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
offset preds,0
"offset the distribution, too",0
scale preds,0
"scale the distribution, too",0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base GRF tree,0
=============================================================================,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
=============================================================================,0
A MultOutputWrapper for GRF classes,0
=============================================================================,0
=============================================================================,0
Instantiations of Generalized Random Forest,0
=============================================================================,0
"Append a constant treatment if `fit_intercept=True`, the coefficient",0
in front of the constant treatment is the intercept in the moment equation.,0
"Append a constant treatment and constant instrument if `fit_intercept=True`,",0
the coefficient in front of the constant treatment is the intercept in the moment equation.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Base Generalized Random Forest,0
=============================================================================,0
TODO: support freq_weight and sample_var,1
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle,0
We calculate the min eigenvalue proxy that each criterion is considering,0
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`",0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Generating indices a priori before parallelism ended up being orders of magnitude,0
faster than how sklearn does it. The reason is that random samplers do not release the,0
gil it seems.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
####################,0
Variance correction,0
####################,0
Subtract the average within bag variance. This ends up being equal to the,0
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).,0
The negative part is just sq_between.,0
Objective bayes debiasing for the diagonals where we know a-prior they are positive,0
"The off diagonals we have no objective prior, so no correction is applied.",0
Finally correcting the pred_cov or pred_var,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
we have to filter the folds because they contain the indices in the original data not,0
the indices in the period-filtered data,0
translate the indices in a fold to the indices in the period-filtered data,0
"if groups was [3,3,4,4,5,5,6,6,1,1,2,2,0,0] (the group ids can be in any order, but the",0
"time periods for each group should be contguous), and we had [10,11,0,1] as the indices in a fold",0
(so the fold is taking the entries corresponding to groups 2 and 3),0
"then group_period_filter(0) is [0,2,4,6,8,10,12] and gpf(1) is [1,3,5,7,9,11,13]",0
"so for period 1, the fold should be [10,0] => [5,0] (the indices that return 10 and 0 in the t=0 data)",0
"and for period 2, the fold should be [11,1] => [5,0] again (the indices that return 11,1 in the t=1 data)",0
filter to the indices for the time period,0
"now find their index in the period-filtered data, which is always sorted",0
sanity check that the folds are the same no matter the time period,0
TODO: update docs,1
"NOTE: sample weight, sample var are not passed in",0
Compose final model,0
Calculate auxiliary quantities,0
X  T_res,0
"sum(model_final.predict(X, T_res))",0
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J",0
override only so that we can exclude treatment featurization verbiage in docstring,0
override only so that we can exclude treatment featurization verbiage in docstring,0
"we need to set the number of periods before calling super()._prefit, since that will generate the",0
"final and nuisance models, which need to have self._n_periods set",0
Set _d_t to effective number of treatments,0
Required for bootstrap inference,0
for each mc iteration,0
for each model under cross fit setting,0
Handles the corner case when X=None but featurizer might be not None,0
Expand treatments for each time period,0
NOTE: important to use the _ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the _ortho_learner's copy to retain the information from fitting,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
test that the subsampling scheme past to the trees is correct,0
The sample size is chosen in particular to test rounding based error when subsampling,0
test that the estimator calcualtes var correctly,0
test api,0
test accuracy,0
test the projection functionality of forests,0
test that the estimator calcualtes var correctly,0
test api,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
test that the subsampling scheme past to the trees is correct,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
filter directories by regex if the NOTEBOOK_DIR_PATTERN environment variable is set,0
omit the lalonde notebook,0
make sure that coverage outputs reflect notebook contents,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
"remove added coverage cell, then decrement execution_count for other cells to account for it",0
create directory if necessary,0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
for is_discrete in [False]:,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
No heterogeneity,0
Define indices to test,0
Heterogeneous effects,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
pass sample weight to final step of pipeline,0
create data with missing values,0
model that can handle missing values,0
"test X, W only",0
test W only,0
dowhy does not support missing values in X,0
assert that fitting with missing values fails when allow_missing is False,0
and that setting allow_missing after init still works,0
assert that we fail with a value error when we pass missing X to a model that doesn't support it,0
assert that fitting with missing values fails when allow_missing is False,0
and that setting allow_missing after init still works,0
metalearners don't support W,0
metalearners do support missing values in X,0
dowhy never supports missing values in X,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
identity featurization effect functions,0
polynomial featurization effect functions,0
1d polynomial featurization functions,0
2d-to-1d featurization functions,0
2d-to-1d vector featurization functions,0
use LassoCV rather than also selecting over RandomForests to save time,0
test that treatment names are assigned for the featurized treatment,0
expected shapes,0
check effects,0
ate,0
loose inference checks,0
temporarily skip LinearDRIV and SparseLinearDRIV for weird effect shape reasons,0
effect inference,0
marginal effect inference,0
const marginal effect inference,0
fit a dummy estimator first so the featurizer can be fit to the treatment,0
edge case with transformer that only takes a vector treatment,0
so far will always return None for cate_treatment_names,0
assert proper handling of improper feature names passed to certain transformers,0
"depending on sklearn version, bad feature names either throws error or only uses first relevant name",0
ensure alpha is passed,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
initialize parameters,0
initialize config wtih base config and overwite some values,0
predict tree using config parameters and assert,0
shape of trained tree is the same as y_test,0
initialize config wtih base honest config and overwite some values,0
predict tree using config parameters and assert,0
shape of trained tree is the same as y_test,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
compare with weight,0
compare with weight,0
compare with weight,0
compare with weight,0
Multi-dimensional output y,0
1-d y,0
compare when both sample_var and sample_weight exist,0
multi-d y,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
dgp,0
StatsModels2SLS,0
IV2SLS,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
fixed functions as first stage models,0
they can be anything as long as fitting doesn't modify the predictions,0
"that way, it doesn't matter if they are trained on different subsets of the data",0
all estimators must have opted in to federation,0
all estimators must have the same covariance type,0
test coefficients,0
test effects,0
fixed functions as first stage models,0
they can be anything as long as fitting doesn't modify the predictions,0
"that way, it doesn't matter if they are trained on different subsets of the data",0
test coefficients,0
test effects,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
ensure alpha is passed,0
only is not None when T1 is a constant or a list of constant,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Generate synthetic data,0
Run _crossfit with Ray enabled,0
Run _crossfit without Ray,0
Compare the results,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
make any access to matplotlib or plt throw an exception,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
Invert indices to match latest API,0
Invert indices to match latest API,0
The feature for heterogeneity stays constant,0
Auxiliary function for adding xticks and vertical lines when plotting results,0
for dynamic dml vs ground truth parameters.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
tests that we can recover the right degree of polynomial features,0
implicitly also tests ability to handle pipelines,0
since 'poly' uses pipelines containing PolynomialFeatures,0
"generate larger coefficients in a set of high degree features,",0
weighted towards higher degree features,0
"just test a polynomial T model, since for Y the correct degree also depends on",0
the interation of T and X,0
test corner case with just one model in a list,0
test corner case with empty list,0
test selecting between two fixed models,0
"DGP is a linear model, so linear regression should fit better",0
"DGP is now non-linear, so random forest should fit better",0
these models only work on multi-output data,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
test at least one estimator from each category,0
test causal graph,0
need to set matplotlib backend before viewing model,0
test refutation estimate,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Generate random Xs,0
Random covariance matrix of Xs,0
Effect of Xs on outcome,0
Effect of treatment on outcomes,0
Effect of treatment on outcome conditional on X1,0
Generate treatments based on X and random noise,0
"Generate Y (based on X, D, and random noise)",0
"Simple classifier and regressor for propensity, outcome, and cate",0
test the DR outcome difference,0
"Simple classifier and regressor for propensity, outcome, and cate",0
test the DR outcome difference,0
"Simple classifier and regressor for propensity, outcome, and cate",0
test the DR outcome difference,0
use evaluate_blp to fit on validation only,0
"Simple classifier and regressor for propensity, outcome, and cate",0
test the DR outcome difference,0
fit nothing,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
accuracy test,0
"accuracy test, DML",0
uncomment when issue #837 is resolved,0
"NonParamDMLIV(discrete_outcome=discrete_outcome, discrete_treatment=discrete_treatment,",0
"discrete_instrument=discrete_instrument, model_final=LinearRegression())",0
make sure the auto outcome model is a classifier,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"since we're running so many combinations, just use LassoCV/LogisticRegressionCV",0
for the models instead of also selecting over random forest models,0
ensure we can serialize unfit estimator,0
ensure we can serialize fit estimator,0
expected effect size,0
test effect,0
test inference,0
only OrthoIV support inference other than bootstrap,0
test summary,0
test can run score,0
test cate_feature_names,0
test can run shap values,0
dgp,0
no heterogeneity,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"if we aren't fitting on the whole dataset, ensure that the limits are respected",0
ensure that the grouping has worked correctly and we get exactly the number of copies,0
of the items in whichever groups we see,0
DML nested CV works via a 'cv' attribute,0
"want to validate the nested grouping, not the outer grouping in the nesting tests",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
parameter combinations to test,0
"we're running a lot of tests, so use fixed models instead of model selection",0
"IntentToTreatDRIV only supports binary treatments and instruments, and doesn't support fit_cov_directly",0
TODO: serializing/deserializing for every combination -- is this necessary?,1
ensure we can serialize unfit estimator,0
ensure we can serialize fit estimator,0
expected effect size,0
assert calculated constant marginal effect shape is expected,0
const_marginal effect is defined in LinearCateEstimator class,0
assert calculated marginal effect shape is expected,0
test inference,0
test can run score,0
test cate_feature_names,0
test can run shap values,0
"dgp (binary T, binary Z)",0
no heterogeneity,0
with heterogeneity,0
fitting the covariance directly should be at least as good as computing the covariance from separate models,0
set the models so that model selection over random forests doesn't take too much time in the repeated trials,0
directly fitting the covariance should be better than indirectly fitting it,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
make sure we can get out post-fit stuff,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
"with this DGP, since T depends linearly on X, Y depends on X quadratically",0
so we should use a quadratic featurizer,0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
"we have quadratic terms in y, so we need to pipeline with a quadratic featurizer",0
Compare results with and without Ray,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
"with 2 folds, we should get exactly 3 groups per split, each with 10 copies of the y or t value",0
test nested grouping,0
"with 2-fold outer and 2-fold inner grouping, and six total groups,",0
should get 1 or 2 groups per split,0
"Try default, integer, and new user-passed treatment name",0
FunctionTransformers are agnostic to passed treatment names,0
Expected treatment names are the sums of user-passed prefixes and transformer-specific postfixes,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
NOTE: StratifiedGroupKFold has a bug when shuffle is True where it doesn't always stratify properly,0
so we explicitly pass a StratifiedGroupKFold with shuffle=False (the default) rather than letting,0
cross-fit generate one,0
"with 2-fold grouping, we should get exactly 3 groups per split",0
test nested grouping,0
"with 2-fold outer and 2-fold inner grouping, we should get 1-2 groups per split",0
helper class,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
continuous treatments have typical treatment values equal to,0
the mean of the absolute value of non-zero entries,0
discrete treatments have typical treatment value 1,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
continuous treatments have typical treatment values equal to,0
the mean of the absolute value of non-zero entries,0
discrete treatments have typical treatment value 1,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
make sure we don't run into problems dropping every index,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
dgp,0
model,0
model,0
"columns 'd', 'e', 'h' have too many values",0
"columns 'd', 'e' have too many values",0
lowering bound shouldn't affect already fit columns when warm starting,0
"column d is now okay, too",0
verify that we can use a scalar treatment cost,0
verify that we can specify per-treatment costs for each sample,0
verify that using the same state returns the same results each time,0
set the categories for column 'd' explicitly so that b is default,0
"first column: 10 ones, this is fine",0
"second column: 6 categories, plenty of random instances of each",0
this is fine only if we increase the cateogry limit,0
"third column: nine ones, lots of twos, not enough unless we disable check",0
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity",0
forest heterogeneity won't work,0
"sixth column: just 1 one, not enough even without check",0
increase bound on cat expansion,0
skip checks (reducing folds accordingly),0
"Add tests that guarantee that the reliance on DML feature order is not broken, such as",0
"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W",0
Pass an example where W is irrelevant and X is confounder,0
"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be",0
zeroed out and the test will fail,0
"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates",0
rescaling X shouldn't affect the first stage models because they normalize the inputs,0
"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid",0
cross terms,0
scale by 1000 to match the input to this model:,0
"the scale of X does matter for the final model, which keeps results in user-denominated units",0
rescaling X still shouldn't affect the first stage models,0
TODO: we don't recover the correct values with enough accuracy to enable this assertion,1
is there a different way to verify that we are learning the correct coefficients?,1
"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Define data features,0
Added `_df`to names to be different from the default cate_estimator names,0
Generate data,0
################################,0
Single treatment and outcome #,0
################################,0
Test LinearDML,0
|--> Test featurizers,0
"ColumnTransformer behaves differently depending on version of sklearn, so we no longer check the names",0
|--> Test re-fit,0
Test SparseLinearDML,0
Test ForestDML,0
###################################,0
Mutiple treatments and outcomes #,0
###################################,0
Test LinearDML,0
Test SparseLinearDML,0
"Single outcome only, ORF does not support multiple outcomes",0
Test DMLOrthoForest,0
Test DROrthoForest,0
Test XLearner,0
Skipping population summary names test because bootstrap inference is too slow,0
Test SLearner,0
Test TLearner,0
Test LinearDRLearner,0
Test SparseLinearDRLearner,0
Test ForestDRLearner,0
Test LinearIntentToTreatDRIV,0
Test DeepIV,0
Test categorical treatments,0
Check refit,0
Check refit after setting categories,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
import here since otherwise test collection would fail if matplotlib is not installed,0
Treatment effect function,0
Outcome support,0
Treatment support,0
"Generate controls, covariates, treatments and outcomes",0
Heterogeneous treatment effects,0
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
through shap package.,0
test shap could generate the plot from the shap_values,0
"waterfall is broken in this version, fixed by https://github.com/slundberg/shap/pull/2444",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we need the controls' encoded column for training",0
"Thus, we append the controls column before the one-hot-encoded T",0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary",0
TODO: prel_model_effect could allow sample_var and freq_weight?,1
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary",0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous",0
reshape the predictions,0
concat W and Z,0
check nuisances outcome shape,0
Y_res could be a vector or 1-dimensional 2d-array,0
"We're projecting, so we're treating E[T|X,Z] as the instrument (ignoring W for simplicity)",0
"Then beta(X) = E[T (E[T|X,Z]-E[E[T|X,Z]|X)|X] and we can apply the tower rule several times to get",0
"= E[(E[T|X,Z]-E[T|X])^2|X]",0
"and also     = E[(E[T|X,Z]-T)^2|X]",0
so we can compute it either from (T_proj-T_pred)^2 or from (T_proj-T)^2,0
The first of these is just Z_res^2,0
"fit on T*T_proj, covariance will be computed by E[T_res * T_proj] = E[T*T_proj] - E[T]^2",0
"return shape (n,)",0
we will fit on the covariance (T_res*Z_res) directly,0
"fit on TZ, covariance will be computed by E[T_res * Z_res] = TZ_pred - T_pred * Z_pred",0
"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)",0
"shape (n,)",0
"shape (n,)",0
"shape(n,)",0
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary",0
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary",0
"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous",0
reshape the predictions,0
"in the projection case, this is a variance and should always be non-negative",0
check nuisances outcome shape,0
"all could be reshaped to vector since Y, T, Z are all single dimensional.",0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
A helper class that access all the internal fitted objects of a DRIV Cate Estimator.,0
Used by both DRIV and IntentToTreatDRIV.,0
Maggie: I think that would be the case?,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE: important to use the ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
Handles the corner case when X=None but featurizer might be not None,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"this is a regression model since the instrument E[T|X,W,Z] is always continuous",0
"we're using E[T|X,W,Z] as the instrument",0
Define the data generation functions,0
Define the data generation functions,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
Define the data generation functions,0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
Define the data generation functions,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
concat W and Z,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
concat W and Z,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
reshape the predictions,0
"T_res, Z_res, beta expect shape to be (n,1)",0
Define the data generation functions,0
maybe shouldn't expose fit_cate_intercept in this class?,0
Define the data generation functions,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: do correct adjustment for sample_var,1
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
concat W and Z,0
concat W and Z,0
concat W and Z,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Define the data generation functions,0
"train E[T|X,W,Z]",0
"train E[Z|X,W]",0
note: discrete_instrument rather than discrete_treatment in call to _make_first_stage_selector,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE: important to use the ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
Handles the corner case when X=None but featurizer might be not None,0
NOTE This is used by the inference methods and is more for internal use to the library,0
concat W and Z,0
note that groups are not passed to score because they are only used for fitting,0
concat W and Z,0
note that sample_weight and groups are not passed to predict because they are only used for fitting,0
concat W and Z,0
A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.,0
Used by both Parametric and Non Parametric DMLIV.,0
override only so that we can enforce Z to be required,0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Handles the corner case when X=None but featurizer might be not None,0
Define the data generation functions,0
Get input names,0
Summary,0
coefficient,0
intercept,0
Define the data generation functions,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: this utility is documented but internal; reimplement?,1
TODO: this utility is even less public...,1
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged",0
use same Cs as would be used by default by LogisticRegressionCV,0
NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification,0
which could affect how many times each distinct Y value needs to be present in the data,0
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns,0
but also supports get_feature_names with expected signature,0
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value,0
NOTE: we rely on the passthrough columns coming first in the concatenated X;W,0
"when we pipeline scaling with our first stage models later, so the order here is important",0
TODO: remove once older sklearn support is no longer needed,1
Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy,0
Convert python objects to (possibly nested) types that can easily be represented as literals,0
Convert SingleTreeInterpreter to a python dictionary,0
named tuple type for storing results inside CausalAnalysis class;,0
must be lifted to module level to enable pickling,0
"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,",1
"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all",0
"when running the first stage models, but don't want to scale the X columns when running the final model,",0
since then our coefficients will have odd units and our trees will also have decisions using those units.,0
,0
"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)",0
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names,0
Controls are all other columns of X,0
"can't use X[:, feat_ind] when X is a DataFrame",0
TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
we might want to modify OrthoLearner (and other discrete treatment classes),0
so that the user can opt-in to allowing unseen treatment values,0
(and return NaN or something in that case),0
HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models,1
and so we can just peel the first columns off of that combined array for rescaling in the pipeline,0
TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are,1
"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this",0
becomes a valid approach to handling this,0
array checking routines don't accept 0-width arrays,0
perform model selection,0
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative,0
convert to NormalInferenceResults for consistency,0
Set the dictionary values shared between local and global summaries,0
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments",0
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category",0
required to fit a discrete DML model,0
"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular",1
sub-cases of models or also integrate with azure autoML. (post-MVP),0
"TODO: Add other heterogeneity model options, such as {'automl'} for performing",1
"model selection for the causal effect, or {'sparse_linear'} for using a debiased lasso. (post-MVP)",0
TODO: Enable multi-class classification (post-MVP),1
Validate inputs,0
TODO: check compatibility of X and Y lengths,1
"no previous fit, cancel warm start",0
"work with numeric feature indices, so that we can easily compare with categorical ones",0
"if heterogeneity_inds is 1D, repeat it",0
heterogeneity inds should be a 2D list of length same as train_inds,0
replace None elements of heterogeneity_inds and ensure indices are numeric,0
"TODO: bail out also if categorical columns, classification, random_state changed?",1
TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
train the Y model,0
"perform model selection for the Y model using all X, not on a per-column basis",0
"now that we've trained the classifier and wrapped it, ensure that y is transformed to",0
work with the regression wrapper,0
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays,0
"note that this needs to happen after wrapping to generalize to the multi-class case,",0
since otherwise we'll have too many columns to be able to train a classifier,0
start with empty results and default shared insights,0
convert categorical indicators to numeric indices,0
check for indices over the categorical expansion bound,0
assume we'll be able to train former failures this time; we'll add them back if not,0
"can't remove in place while iterating over new_inds, so store in separate list",0
"train the model, but warn",0
no model can be trained in this case since we need more folds,0
"don't train a model, but suggest workaround since there are enough instances of least",1
populated class,0
also remove from train_inds so we don't try to access the result later,0
extract subset of names matching new columns,0
"track indices where an exception was thrown, since we can't remove from dictionary while iterating",0
don't want to cache this failed result,0
properties to return from effect InferenceResults,0
properties to return from PopulationSummaryResults,0
Converts strings to property lookups or method calls as a convenience so that the,0
_point_props and _summary_props above can be applied to an inference object,0
Create a summary combining all results into a single output; this is used,0
by the various causal_effect and causal_effect_dict methods to generate either a dataframe,0
"or a dictionary, respectively, based on the summary function passed into this method",0
"ensure array has shape (m,y,t)",0
population summary is missing sample dimension; add it for consistency,0
outcome dimension is missing; add it for consistency,0
add singleton treatment dimension if missing,0
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce,0
"each attr has dimension (m,y) or (m,y,t)",0
concatenate along treatment dimension,0
"for dictionary representation, want to remove unneeded sample dimension",0
in cohort and global results,0
TODO: enrich outcome logic for multi-class classification when that is supported,1
There is no actual sample level in this data,0
can't drop only level,0
should be serialization-ready and contain no numpy arrays,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
TODO: Note that there's no column metadata for the sample number - should there be?,1
"need to replicate the column info for each sample, then remove from the shared data",0
NOTE: the flattened order has the ouptut dimension before the feature dimension,0
which may need to be revisited once we support multiclass,0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
a global inference indicates the effect of that one feature on the outcome,0
need to reshape the output to match the input,0
we want to offset the inference object by the baseline estimate of y,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
"NOTE: this calculation is correct only if treatment costs are marginal costs,",0
because then scaling the difference between treatment value and treatment costs is the,0
same as scaling the treatment value and subtracting the scaled treatment cost.,0
,0
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for",0
"continuous treatments, the policy value should include the benefit of decreasing treatments",0
(rather than just not treating at all),0
,0
"We can get the total by seeing that if we restrict attention to units where we would treat,",0
2 * policy_value - always_treat,0
includes exactly their contribution because policy_value and always_treat both include it,0
"and likewise restricting attention to the units where we want to decrease treatment,",0
2 * policy_value - always-treat,0
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value",0
is zero and the contribution to always_treat is negative,0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
get dataframe with all but selected column,0
apply 10% of a typical treatment for this feature,0
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely",0
set the effect bounds; for positive treatments these agree with,0
"the estimates; for negative treatments, we need to invert the interval",0
the effect is now always positive since we decrease treatment when negative,0
"for discrete treatment, stack a zero result in front for control",0
we need to call effect_inference to get the correct CI between the two treatment options,0
we now need to construct the delta in the cost between the two treatments and translate the effect,0
remove third dimenions potentially added,0
"find cost of current treatment: equality creates a 2d array with True on each row,",0
only if its the location of the current treatment. Then we take the corresponding cost.,0
construct index of current treatment,0
add second dimension if needed for broadcasting during translation of effect,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
only expose predict_proba if best_model has predict_proba,0
used because logic elsewhere uses hasattr predict proba to check if model is a classifier,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a `groups` argument,0
"drop groups from arg list, which were already used at the outer level and may not be supported by the model",0
"since needs_fit is False, is_selecting will only be true if",0
the score needs to be compared to another model's,0
"so we don't need to fit the model itself, just get the out-of-sample score",0
use _fit_with_groups instead of just fit to handle nested grouping,0
we need to train the model on the data,0
copy common parameters,0
copy common fitted variables,0
make sure all classes agree on best c/l1 combo,0
"We need an R^2 score to compare to other models; ElasticNetCV doesn't provide it,",0
but we can calculate it ourselves from the MSE plus the variance of the target y,0
"l1 ratio doesn't apply to Lasso, only ElasticNet",0
max R^2 corresponds to min MSE,0
"constructor takes cv as a positional or kwarg, just pull it out of a new instance",0
but it would be complicated to check that,0
"technically, if there is just one model and it doesn't need to be fit we don't need to fit it,",0
but that complicates the training logic so we don't bother with that case,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
verbose was removed from sklearn's non-public _fit_and_predict method in 1.4,0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
local import to avoid circular imports,0
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Call weighted lasso on reduced design matrix,0
Weighted tau,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to the single-target estimator by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
set default values for None,0
check freq_weight should be integer and should be accompanied by sample_var,0
check array shape,0
weight X and y and sample_var,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
"For aggregation calculations, always treat wy as an array so that einsum expressions don't need to change",0
We'll collapse results back down afterwards if necessary,0
"for federation, we need to store these 5 arrays when using heteroskedasticity-robust inference",0
y dimension is always first in the output when present so that broadcasting works correctly,0
set default values for None,0
check array shape,0
check dimension of instruments is more than dimension of treatments,0
weight X and y,0
learn point estimate,0
solve first stage linear regression E[T|Z],0
"""that"" means T",0
solve second stage linear regression E[Y|that],0
(T.T*T)^{-1},0
learn cov(theta),0
(T.T*T)^{-1},0
sigma^2,0
reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
average the outcome dimension if it exists and ensure 2d y_pred,0
get index of best treatment,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
TODO: ideally the below private attribute logic should be in .fit but is needed in init,1
for nuisance estimator generation for parent class,0
should refactor later,1
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
override only so that we can exclude treatment featurization verbiage in docstring,0
Override to flatten output if T is flat,0
override only so that we can exclude treatment featurization verbiage in docstring,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
d_t=None here since we measure the effect across all Ts,0
conditionally expand jacobian dimensions to align with einsum str,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile",0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/main/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
TODO: enable type aliases,1
napoleon_preprocess_types = True  # needed for type aliases to work,0
napoleon_type_aliases = {,0
"""array_like"": "":term:`array_like`"",",0
"""ndarray"": ""~numpy.ndarray"",",0
"""RandomState"": "":class:`~numpy.random.RandomState`"",",0
"""DataFrame"": "":class:`~pandas.DataFrame`"",",0
"""Series"": "":class:`~pandas.Series`"",",0
},0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of strings:,0
,0
"source_suffix = ['.rst', '.md']",0
The root toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)",0
def mlasso_model(): return MultiTaskLassoCV(,0
"cv=3, alphas=alpha_regs, max_iter=200)",0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
"alpha_regs = [5e-3, 1e-2, 5e-2]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)",0
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)",0
subset of features that are exogenous and create heterogeneity,0
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features),0
subset of features wrt we estimate heterogeneity,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
introspect the constructor arguments to find the model parameters,0
to represent,0
"if the argument is deprecated, ignore it",0
Extract and sort argument names excluding 'self',0
column names,0
transfer input to numpy arrays,0
transfer input to 2d arrays,0
create dataframe,0
currently dowhy only support single outcome and single treatment,0
call dowhy,0
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update",0
cate estimator but not the effect.,0
don't proxy special methods,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
For when checking input values is disabled,0
Type to column extraction function,0
if not all column names are strings,0
coerce feature names to be strings,0
Prefer sklearn 1.0's get_feature_names_out method to deprecated get_feature_names method,0
"Some featurizers will throw, such as a pipeline with a transformer that doesn't itself support names",0
"Get number of arguments, some sklearn featurizer don't accept feature_names",0
Handles cases where the passed feature names create issues,0
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names',0
Get feature names using featurizer,0
All attempts at retrieving transformed feature names have failed,0
Delegate handling to downstream logic,0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Any access should throw,0
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports",0
return plain dictionary so that erroneous accesses don't half work (see e.g. #708),0
for every dimension of the treatment add some epsilon and observe change in featurized treatment,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
return plain dictionary so that erroneous accesses don't half work (see #708),0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
return plain dictionary so that erroneous accesses don't half work (see #708),0
input feature name is already updated by cate_feature_names.,0
define the index of d_x to filter for each given T,0
filter X after broadcast with T for each given T,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
return plain dictionary so that erroneous accesses don't half work (see #708),0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
return plain dictionary so that erroneous accesses don't half work (see #708),0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains some snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_export.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
make any access to matplotlib or plt throw an exception,0
make any access to graphviz or plt throw an exception,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
Initialize saturation & value; calculate chroma & value shift,0
Calculate some intermediate values,0
Initialize RGB with same hue & chroma as our color,0
Shift the initial RGB values to match value and store,0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use mean of targets,0
Write node mean CATE,0
Write node std of CATE,0
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
Fetch appropriate color for node,0
Write node mean CATE,0
Write node mean CATE,0
Write recommended treatment and value - cost,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
This flag is true when names are set in a child class instead,0
"If names are set in a child class, add an attribute reflecting that",0
This works only if X is passed as a kwarg,0
We plan to enforce X as kwarg only in future releases,0
This checks if names have been set in a child class,0
"If names were set in a child class, don't do it again",0
"Wraps-up fit by setting attributes, cleaning up, etc.",0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
apply defaults before calling inference method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"Treatment names is None, default to BaseCateEstimator",0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
Get input names,0
Summary,0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
Get input names,0
Note: we do not transform feature names since that is done within summary_frame,0
Summary,0
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
remove None arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
Adding the kwargs to ray object store to be used by remote functions for each fold to avoid IO overhead,0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
Define Ray remote function (Ray remote wrapper of the _fit_nuisances function),0
Create Ray remote jobs for parallel processing,0
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same,0
alteration even when we only want to fit_final.,0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
upgrade to a GroupKFold or StratiGroupKFold if groups is not None,0
"we won't have generated a KFold or StratifiedKFold ourselves when groups are passed,",0
"but the user might have supplied one, which won't work",0
for each mc iteration,0
for each model under cross fit setting,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"TODO: This could be extended to also work with our sparse and 2SLS estimators,",1
if we add an aggregate method to them,0
Remember to update the docs if this changes,0
mix in the appropriate inference class,0
assign all of the attributes from the dummy estimator that would normally be assigned during fitting,0
TODO: This seems hacky; is there a better abstraction to maintain these?,1
"This should also include bias_part_of_coef, model_final_, and fitted_models_final above",0
Assign treatment expansion attributes,0
Methods needed to implement the LinearCateEstimator interface,0
Methods needed to implement the LinearFinalModelCateEstimatorMixin,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Policy Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base Policy tree,0
=============================================================================,0
The values below are required and utilitized by methods in the _SingleTreeExporterMixin,0
HACK: sklearn 1.3 enforces that the input to plot_tree is a DecisionTreeClassifier or DecisionTreeRegressor,1
This is a hack to get around that restriction by declaring that PolicyTree inherits from DecisionTreeClassifier,1
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"Unique treatments (ordered, includes control)",0
Number of treatments (excluding control),0
Indicator for whether,0
Get DR outcomes in training sample,0
Get DR outcomes in validation sample,0
Get DR outcomes in validation sample,0
Calculate ATE in the validation sample,0
Fit propensity in treatment,0
Predict propensity scores,0
Possible treatments (need to allow more than 2),0
Predict outcomes,0
T-learner logic,0
"if CATE is given explicitly or has not been fitted at all previously, fit it now",0
Assign units in validation set to groups,0
Proportion of validations set in group,0
Group average treatment effect (GATE) -- average of DR outcomes in group,0
Average of CATE predictions in group,0
Calculate group calibration score,0
Calculate overall calibration score,0
Calculate R-square calibration score,0
"if CATE is given explicitly or has not been fitted at all previously, fit it now",0
treat each treatment as a separate regression,0
"here, prop_preds should be a matrix",0
with rows corresponding to units and columns corresponding to treatment statuses,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"since we only allow single dimensional y, we could flatten the prediction",0
override only so that we can exclude treatment featurization verbiage in docstring,0
override only so that we can exclude treatment featurization verbiage in docstring,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Handles the corner case when X=None but featurizer might be not None,0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
Replacing to remove docstring,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
data is already validated at initial fit time,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE: important to use the rlearner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the rlearner's copy to retain the information from fitting,0
Handles the corner case when X=None but featurizer might be not None,0
override only so that we can update the docstring to indicate support for `LinearModelFinalInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support freq_weight and sample_var in debiased lasso,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add blb to parent's options,0
override only so that we can update the docstring to indicate,0
support for `GenericSingleTreatmentModelFinalInference`,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
NOTE: important to get parent's wrapped copy so that,0
"after training wrapped featurizer is also trained, etc.",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Fit a doubly robust average effect,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T",0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
"If custom param grid, check that only estimator parameters are being altered",0
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test",0
override only so that we can update the docstring to indicate support for `blb`,0
Get input names,0
Summary,0
Determine output settings,0
"Important: This must be the first invocation of the random state at fit time, so that",0
train/test splits are re-generatable from an external object simply by knowing the,0
random_state parameter of the tree. Can be useful in the future if one wants to create local,0
linear predictions. Currently is also useful for testing.,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
Set min_weight_leaf from min_weight_fraction_leaf,0
Build tree,0
We calculate the maximum number of samples from each half-split that any node in the tree can,0
hold. Used by criterion for memory space savings.,0
Initialize the criterion object and the criterion_val object if honest.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code is a fork from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_base.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
Set parameters,0
Don't instantiate estimators now! Parameters of base_estimator might,0
"still change. Eg., when grid-searching with the nested object syntax.",0
self.estimators_ needs to be filled by the derived classes in fit.,0
Compute the number of jobs,0
Partition estimators between jobs,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
covariance matrix,0
get eigen value and eigen vectors,0
simulate eigen vectors,0
keep the top 4 eigen value and corresponding eigen vector,0
replace the negative eigen values,0
generate a new covariance matrix,0
get linear approximation of eigen values,0
coefs,0
get the indices of each group of features,0
print(ind_same_proxy),0
demo,0
same proxy,0
residuals,0
gmm,0
log normal on outliers,0
positive outliers,0
negative outliers,0
demean the new residual again,0
generate data,0
sample residuals,0
get prediction for current investment,0
get prediction for current proxy,0
get first period prediction,0
iterate the step ahead contruction,0
prepare new x,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
get new covariance matrix,0
get coefs,0
get residuals,0
proxy 1 is the outcome,0
make fixed residuals,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=None here since we measure the effect across all Ts,0
"y is a vector, rather than a 2D array",0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
squeeze the first axis,0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T",0
squeeze the first axis,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector,0
d_t=None here since we measure the effect across all Ts,0
d_t=None here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet",0
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
scale preds,0
scale std errs,0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
offset preds,0
"offset the distribution, too",0
scale preds,0
"scale the distribution, too",0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base GRF tree,0
=============================================================================,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
=============================================================================,0
A MultOutputWrapper for GRF classes,0
=============================================================================,0
=============================================================================,0
Instantiations of Generalized Random Forest,0
=============================================================================,0
"Append a constant treatment if `fit_intercept=True`, the coefficient",0
in front of the constant treatment is the intercept in the moment equation.,0
"Append a constant treatment and constant instrument if `fit_intercept=True`,",0
the coefficient in front of the constant treatment is the intercept in the moment equation.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Base Generalized Random Forest,0
=============================================================================,0
TODO: support freq_weight and sample_var,1
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle,0
We calculate the min eigenvalue proxy that each criterion is considering,0
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`",0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Generating indices a priori before parallelism ended up being orders of magnitude,0
faster than how sklearn does it. The reason is that random samplers do not release the,0
gil it seems.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
####################,0
Variance correction,0
####################,0
Subtract the average within bag variance. This ends up being equal to the,0
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).,0
The negative part is just sq_between.,0
Objective bayes debiasing for the diagonals where we know a-prior they are positive,0
"The off diagonals we have no objective prior, so no correction is applied.",0
Finally correcting the pred_cov or pred_var,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: update docs,1
"NOTE: sample weight, sample var are not passed in",0
Compose final model,0
Calculate auxiliary quantities,0
X  T_res,0
"sum(model_final.predict(X, T_res))",0
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J",0
override only so that we can exclude treatment featurization verbiage in docstring,0
override only so that we can exclude treatment featurization verbiage in docstring,0
"we need to set the number of periods before calling super()._prefit, since that will generate the",0
"final and nuisance models, which need to have self._n_periods set",0
Set _d_t to effective number of treatments,0
Required for bootstrap inference,0
for each mc iteration,0
for each model under cross fit setting,0
Handles the corner case when X=None but featurizer might be not None,0
Expand treatments for each time period,0
NOTE: important to use the _ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the _ortho_learner's copy to retain the information from fitting,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
test that the subsampling scheme past to the trees is correct,0
The sample size is chosen in particular to test rounding based error when subsampling,0
test that the estimator calcualtes var correctly,0
test api,0
test accuracy,0
test the projection functionality of forests,0
test that the estimator calcualtes var correctly,0
test api,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
test that the subsampling scheme past to the trees is correct,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
filter directories by regex if the NOTEBOOK_DIR_PATTERN environment variable is set,0
omit the lalonde notebook,0
make sure that coverage outputs reflect notebook contents,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
"remove added coverage cell, then decrement execution_count for other cells to account for it",0
create directory if necessary,0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
for is_discrete in [False]:,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
No heterogeneity,0
Define indices to test,0
Heterogeneous effects,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
pass sample weight to final step of pipeline,0
create data with missing values,0
model that can handle missing values,0
"test X, W only",0
test W only,0
dowhy does not support missing values in X,0
assert that fitting with missing values fails when allow_missing is False,0
and that setting allow_missing after init still works,0
assert that we fail with a value error when we pass missing X to a model that doesn't support it,0
assert that fitting with missing values fails when allow_missing is False,0
and that setting allow_missing after init still works,0
metalearners don't support W,0
metalearners do support missing values in X,0
dowhy never supports missing values in X,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
identity featurization effect functions,0
polynomial featurization effect functions,0
1d polynomial featurization functions,0
2d-to-1d featurization functions,0
2d-to-1d vector featurization functions,0
use LassoCV rather than also selecting over RandomForests to save time,0
test that treatment names are assigned for the featurized treatment,0
expected shapes,0
check effects,0
ate,0
loose inference checks,0
temporarily skip LinearDRIV and SparseLinearDRIV for weird effect shape reasons,0
effect inference,0
marginal effect inference,0
const marginal effect inference,0
fit a dummy estimator first so the featurizer can be fit to the treatment,0
edge case with transformer that only takes a vector treatment,0
so far will always return None for cate_treatment_names,0
assert proper handling of improper feature names passed to certain transformers,0
"depending on sklearn version, bad feature names either throws error or only uses first relevant name",0
ensure alpha is passed,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
initialize parameters,0
initialize config wtih base config and overwite some values,0
predict tree using config parameters and assert,0
shape of trained tree is the same as y_test,0
initialize config wtih base honest config and overwite some values,0
predict tree using config parameters and assert,0
shape of trained tree is the same as y_test,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
compare with weight,0
compare with weight,0
compare with weight,0
compare with weight,0
Multi-dimensional output y,0
1-d y,0
compare when both sample_var and sample_weight exist,0
multi-d y,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
dgp,0
StatsModels2SLS,0
IV2SLS,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
fixed functions as first stage models,0
they can be anything as long as fitting doesn't modify the predictions,0
"that way, it doesn't matter if they are trained on different subsets of the data",0
test coefficients,0
test effects,0
fixed functions as first stage models,0
they can be anything as long as fitting doesn't modify the predictions,0
"that way, it doesn't matter if they are trained on different subsets of the data",0
test coefficients,0
test effects,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
ensure alpha is passed,0
only is not None when T1 is a constant or a list of constant,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Generate synthetic data,0
Run _crossfit with Ray enabled,0
Run _crossfit without Ray,0
Compare the results,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
make any access to matplotlib or plt throw an exception,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
Invert indices to match latest API,0
Invert indices to match latest API,0
The feature for heterogeneity stays constant,0
Auxiliary function for adding xticks and vertical lines when plotting results,0
for dynamic dml vs ground truth parameters.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
test at least one estimator from each category,0
test causal graph,0
test refutation estimate,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Generate random Xs,0
Random covariance matrix of Xs,0
Effect of Xs on outcome,0
Effect of treatment on outcomes,0
Effect of treatment on outcome conditional on X1,0
Generate treatments based on X and random noise,0
"Generate Y (based on X, D, and random noise)",0
"Simple classifier and regressor for propensity, outcome, and cate",0
test the DR outcome difference,0
"Simple classifier and regressor for propensity, outcome, and cate",0
test the DR outcome difference,0
"Simple classifier and regressor for propensity, outcome, and cate",0
test the DR outcome difference,0
use evaluate_blp to fit on validation only,0
"Simple classifier and regressor for propensity, outcome, and cate",0
test the DR outcome difference,0
fit nothing,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"since we're running so many combinations, just use LassoCV/LogisticRegressionCV",0
for the models instead of also selecting over random forest models,0
ensure we can serialize unfit estimator,0
ensure we can serialize fit estimator,0
expected effect size,0
test effect,0
test inference,0
only OrthoIV support inference other than bootstrap,0
test summary,0
test can run score,0
test cate_feature_names,0
test can run shap values,0
dgp,0
no heterogeneity,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"if we aren't fitting on the whole dataset, ensure that the limits are respected",0
ensure that the grouping has worked correctly and we get exactly the number of copies,0
of the items in whichever groups we see,0
DML nested CV works via a 'cv' attribute,0
"want to validate the nested grouping, not the outer grouping in the nesting tests",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
parameter combinations to test,0
"we're running a lot of tests, so use fixed models instead of model selection",0
"IntentToTreatDRIV only supports binary treatments and instruments, and doesn't support fit_cov_directly",0
TODO: serializing/deserializing for every combination -- is this necessary?,1
ensure we can serialize unfit estimator,0
ensure we can serialize fit estimator,0
expected effect size,0
assert calculated constant marginal effect shape is expected,0
const_marginal effect is defined in LinearCateEstimator class,0
assert calculated marginal effect shape is expected,0
test inference,0
test can run score,0
test cate_feature_names,0
test can run shap values,0
"dgp (binary T, binary Z)",0
no heterogeneity,0
with heterogeneity,0
fitting the covariance directly should be at least as good as computing the covariance from separate models,0
set the models so that model selection over random forests doesn't take too much time in the repeated trials,0
directly fitting the covariance should be better than indirectly fitting it,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
make sure we can get out post-fit stuff,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
"with this DGP, since T depends linearly on X, Y depends on X quadratically",0
so we should use a quadratic featurizer,0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
"we have quadratic terms in y, so we need to pipeline with a quadratic featurizer",0
Compare results with and without Ray,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
"with 2 folds, we should get exactly 3 groups per split, each with 10 copies of the y or t value",0
test nested grouping,0
"with 2-fold outer and 2-fold inner grouping, and six total groups,",0
should get 1 or 2 groups per split,0
"Try default, integer, and new user-passed treatment name",0
FunctionTransformers are agnostic to passed treatment names,0
Expected treatment names are the sums of user-passed prefixes and transformer-specific postfixes,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
NOTE: StratifiedGroupKFold has a bug when shuffle is True where it doesn't always stratify properly,0
so we explicitly pass a StratifiedGroupKFold with shuffle=False (the default) rather than letting,0
cross-fit generate one,0
"with 2-fold grouping, we should get exactly 3 groups per split",0
test nested grouping,0
"with 2-fold outer and 2-fold inner grouping, we should get 1-2 groups per split",0
helper class,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
continuous treatments have typical treatment values equal to,0
the mean of the absolute value of non-zero entries,0
discrete treatments have typical treatment value 1,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
continuous treatments have typical treatment values equal to,0
the mean of the absolute value of non-zero entries,0
discrete treatments have typical treatment value 1,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
make sure we don't run into problems dropping every index,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
dgp,0
model,0
model,0
"columns 'd', 'e', 'h' have too many values",0
"columns 'd', 'e' have too many values",0
lowering bound shouldn't affect already fit columns when warm starting,0
"column d is now okay, too",0
verify that we can use a scalar treatment cost,0
verify that we can specify per-treatment costs for each sample,0
verify that using the same state returns the same results each time,0
set the categories for column 'd' explicitly so that b is default,0
"first column: 10 ones, this is fine",0
"second column: 6 categories, plenty of random instances of each",0
this is fine only if we increase the cateogry limit,0
"third column: nine ones, lots of twos, not enough unless we disable check",0
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity",0
forest heterogeneity won't work,0
"sixth column: just 1 one, not enough even without check",0
increase bound on cat expansion,0
skip checks (reducing folds accordingly),0
"Add tests that guarantee that the reliance on DML feature order is not broken, such as",0
"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W",0
Pass an example where W is irrelevant and X is confounder,0
"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be",0
zeroed out and the test will fail,0
"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates",0
rescaling X shouldn't affect the first stage models because they normalize the inputs,0
"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid",0
cross terms,0
scale by 1000 to match the input to this model:,0
"the scale of X does matter for the final model, which keeps results in user-denominated units",0
rescaling X still shouldn't affect the first stage models,0
TODO: we don't recover the correct values with enough accuracy to enable this assertion,1
is there a different way to verify that we are learning the correct coefficients?,1
"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Define data features,0
Added `_df`to names to be different from the default cate_estimator names,0
Generate data,0
################################,0
Single treatment and outcome #,0
################################,0
Test LinearDML,0
|--> Test featurizers,0
"ColumnTransformer behaves differently depending on version of sklearn, so we no longer check the names",0
|--> Test re-fit,0
Test SparseLinearDML,0
Test ForestDML,0
###################################,0
Mutiple treatments and outcomes #,0
###################################,0
Test LinearDML,0
Test SparseLinearDML,0
"Single outcome only, ORF does not support multiple outcomes",0
Test DMLOrthoForest,0
Test DROrthoForest,0
Test XLearner,0
Skipping population summary names test because bootstrap inference is too slow,0
Test SLearner,0
Test TLearner,0
Test LinearDRLearner,0
Test SparseLinearDRLearner,0
Test ForestDRLearner,0
Test LinearIntentToTreatDRIV,0
Test DeepIV,0
Test categorical treatments,0
Check refit,0
Check refit after setting categories,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
import here since otherwise test collection would fail if matplotlib is not installed,0
Treatment effect function,0
Outcome support,0
Treatment support,0
"Generate controls, covariates, treatments and outcomes",0
Heterogeneous treatment effects,0
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
through shap package.,0
test shap could generate the plot from the shap_values,0
"waterfall is broken in this version, fixed by https://github.com/slundberg/shap/pull/2444",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we need the controls' encoded column for training",0
"Thus, we append the controls column before the one-hot-encoded T",0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary",0
"need to fit, too, since we call predict later inside this train method",0
"need to fit, too, since we call predict later inside this train method",0
"need to fit, too, since we call predict later inside this train method",0
"We're projecting, so we're treating E[T|X,Z] as the instrument (ignoring W for simplicity)",0
"Then beta(X) = E[T (E[T|X,Z]-E[E[T|X,Z]|X)|X] and we can apply the tower rule several times to get",0
"= E[(E[T|X,Z]-E[T|X])^2|X]",0
"and also     = E[(E[T|X,Z]-T)^2|X]",0
so we can compute it either from (T_proj-T_pred)^2 or from (T_proj-T)^2,0
"return shape (n,)",0
"need to avoid erroneous broadcasting when one of Z_res or T_res is (n,1) and the other is (n,)",0
"TODO: if the T and Z models overfit, then this will be biased towards 0;",1
consider using nested cross-fitting,0
a similar comment applies to the projection case,0
"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)",0
"shape (n,)",0
"shape (n,)",0
"shape(n,)",0
TODO: prel_model_effect could allow sample_var and freq_weight?,1
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary",0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"if discrete, return shape (n,1); if continuous return shape (n,)",0
target will be discrete and will be inversed from FirstStageWrapper,0
"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous",0
reshape the predictions,0
concat W and Z,0
"in the projection case, this is a variance and should always be non-negative",0
check nuisances outcome shape,0
Y_res could be a vector or 1-dimensional 2d-array,0
"all could be reshaped to vector since Y, T, Z are all single dimensional.",0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
A helper class that access all the internal fitted objects of a DRIV Cate Estimator.,0
Used by both DRIV and IntentToTreatDRIV.,0
Maggie: I think that would be the case?,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE: important to use the ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
Handles the corner case when X=None but featurizer might be not None,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"this is a regression model since the instrument E[T|X,W,Z] is always continuous",0
"we're using E[T|X,W,Z] as the instrument",0
Define the data generation functions,0
Define the data generation functions,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
Define the data generation functions,0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
Define the data generation functions,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
concat W and Z,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
concat W and Z,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
reshape the predictions,0
"T_res, Z_res, beta expect shape to be (n,1)",0
Define the data generation functions,0
maybe shouldn't expose fit_cate_intercept in this class?,0
Define the data generation functions,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: do correct adjustment for sample_var,1
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
concat W and Z,0
concat W and Z,0
concat W and Z,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Define the data generation functions,0
"train E[T|X,W,Z]",0
"train E[Z|X,W]",0
note: discrete_instrument rather than discrete_treatment in call to _make_first_stage_selector,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE: important to use the ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
Handles the corner case when X=None but featurizer might be not None,0
NOTE This is used by the inference methods and is more for internal use to the library,0
concat W and Z,0
note that groups are not passed to score because they are only used for fitting,0
concat W and Z,0
note that sample_weight and groups are not passed to predict because they are only used for fitting,0
concat W and Z,0
A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.,0
Used by both Parametric and Non Parametric DMLIV.,0
override only so that we can enforce Z to be required,0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Handles the corner case when X=None but featurizer might be not None,0
Define the data generation functions,0
Get input names,0
Summary,0
coefficient,0
intercept,0
Define the data generation functions,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: this utility is documented but internal; reimplement?,1
TODO: this utility is even less public...,1
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged",0
use same Cs as would be used by default by LogisticRegressionCV,0
NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification,0
which could affect how many times each distinct Y value needs to be present in the data,0
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns,0
but also supports get_feature_names with expected signature,0
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value,0
NOTE: we rely on the passthrough columns coming first in the concatenated X;W,0
"when we pipeline scaling with our first stage models later, so the order here is important",0
TODO: remove once older sklearn support is no longer needed,1
Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy,0
Convert python objects to (possibly nested) types that can easily be represented as literals,0
Convert SingleTreeInterpreter to a python dictionary,0
named tuple type for storing results inside CausalAnalysis class;,0
must be lifted to module level to enable pickling,0
"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,",1
"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all",0
"when running the first stage models, but don't want to scale the X columns when running the final model,",0
since then our coefficients will have odd units and our trees will also have decisions using those units.,0
,0
"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)",0
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names,0
Controls are all other columns of X,0
"can't use X[:, feat_ind] when X is a DataFrame",0
TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
we might want to modify OrthoLearner (and other discrete treatment classes),0
so that the user can opt-in to allowing unseen treatment values,0
(and return NaN or something in that case),0
HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models,1
and so we can just peel the first columns off of that combined array for rescaling in the pipeline,0
TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are,1
"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this",0
becomes a valid approach to handling this,0
array checking routines don't accept 0-width arrays,0
perform model selection,0
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative,0
convert to NormalInferenceResults for consistency,0
Set the dictionary values shared between local and global summaries,0
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments",0
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category",0
required to fit a discrete DML model,0
"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular",1
sub-cases of models or also integrate with azure autoML. (post-MVP),0
"TODO: Add other heterogeneity model options, such as {'automl'} for performing",1
"model selection for the causal effect, or {'sparse_linear'} for using a debiased lasso. (post-MVP)",0
TODO: Enable multi-class classification (post-MVP),1
Validate inputs,0
TODO: check compatibility of X and Y lengths,1
"no previous fit, cancel warm start",0
"work with numeric feature indices, so that we can easily compare with categorical ones",0
"if heterogeneity_inds is 1D, repeat it",0
heterogeneity inds should be a 2D list of length same as train_inds,0
replace None elements of heterogeneity_inds and ensure indices are numeric,0
"TODO: bail out also if categorical columns, classification, random_state changed?",1
TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
train the Y model,0
"perform model selection for the Y model using all X, not on a per-column basis",0
"now that we've trained the classifier and wrapped it, ensure that y is transformed to",0
work with the regression wrapper,0
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays,0
"note that this needs to happen after wrapping to generalize to the multi-class case,",0
since otherwise we'll have too many columns to be able to train a classifier,0
start with empty results and default shared insights,0
convert categorical indicators to numeric indices,0
check for indices over the categorical expansion bound,0
assume we'll be able to train former failures this time; we'll add them back if not,0
"can't remove in place while iterating over new_inds, so store in separate list",0
"train the model, but warn",0
no model can be trained in this case since we need more folds,0
"don't train a model, but suggest workaround since there are enough instances of least",1
populated class,0
also remove from train_inds so we don't try to access the result later,0
extract subset of names matching new columns,0
"track indices where an exception was thrown, since we can't remove from dictionary while iterating",0
don't want to cache this failed result,0
properties to return from effect InferenceResults,0
properties to return from PopulationSummaryResults,0
Converts strings to property lookups or method calls as a convenience so that the,0
_point_props and _summary_props above can be applied to an inference object,0
Create a summary combining all results into a single output; this is used,0
by the various causal_effect and causal_effect_dict methods to generate either a dataframe,0
"or a dictionary, respectively, based on the summary function passed into this method",0
"ensure array has shape (m,y,t)",0
population summary is missing sample dimension; add it for consistency,0
outcome dimension is missing; add it for consistency,0
add singleton treatment dimension if missing,0
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce,0
"each attr has dimension (m,y) or (m,y,t)",0
concatenate along treatment dimension,0
"for dictionary representation, want to remove unneeded sample dimension",0
in cohort and global results,0
TODO: enrich outcome logic for multi-class classification when that is supported,1
There is no actual sample level in this data,0
can't drop only level,0
should be serialization-ready and contain no numpy arrays,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
TODO: Note that there's no column metadata for the sample number - should there be?,1
"need to replicate the column info for each sample, then remove from the shared data",0
NOTE: the flattened order has the ouptut dimension before the feature dimension,0
which may need to be revisited once we support multiclass,0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
a global inference indicates the effect of that one feature on the outcome,0
need to reshape the output to match the input,0
we want to offset the inference object by the baseline estimate of y,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
"NOTE: this calculation is correct only if treatment costs are marginal costs,",0
because then scaling the difference between treatment value and treatment costs is the,0
same as scaling the treatment value and subtracting the scaled treatment cost.,0
,0
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for",0
"continuous treatments, the policy value should include the benefit of decreasing treatments",0
(rather than just not treating at all),0
,0
"We can get the total by seeing that if we restrict attention to units where we would treat,",0
2 * policy_value - always_treat,0
includes exactly their contribution because policy_value and always_treat both include it,0
"and likewise restricting attention to the units where we want to decrease treatment,",0
2 * policy_value - always-treat,0
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value",0
is zero and the contribution to always_treat is negative,0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
get dataframe with all but selected column,0
apply 10% of a typical treatment for this feature,0
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely",0
set the effect bounds; for positive treatments these agree with,0
"the estimates; for negative treatments, we need to invert the interval",0
the effect is now always positive since we decrease treatment when negative,0
"for discrete treatment, stack a zero result in front for control",0
we need to call effect_inference to get the correct CI between the two treatment options,0
we now need to construct the delta in the cost between the two treatments and translate the effect,0
remove third dimenions potentially added,0
"find cost of current treatment: equality creates a 2d array with True on each row,",0
only if its the location of the current treatment. Then we take the corresponding cost.,0
construct index of current treatment,0
add second dimension if needed for broadcasting during translation of effect,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a `groups` argument,0
"drop groups from arg list, which were already used at the outer level and may not be supported by the model",0
"whether selecting or not, need to train the model on the data",0
"TODO: we need to alter this to use out-of-sample score here, which",1
"will require cross-validation, but should respect grouping, stratifying, etc.",0
copy common parameters,0
copy common fitted variables,0
copy attributes unique to this class,0
make sure all classes agree on best c/l1 combo,0
"l1 ratio doesn't apply to Lasso, only ElasticNet",0
don't need to use _fit_with_groups here since none of these models support it,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values",0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
local import to avoid circular imports,0
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Call weighted lasso on reduced design matrix,0
Weighted tau,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to the single-target estimator by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
set default values for None,0
check freq_weight should be integer and should be accompanied by sample_var,0
check array shape,0
weight X and y and sample_var,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
"For aggregation calculations, always treat wy as an array so that einsum expressions don't need to change",0
We'll collapse results back down afterwards if necessary,0
"for federation, we need to store these 5 arrays when using heteroskedasticity-robust inference",0
y dimension is always first in the output when present so that broadcasting works correctly,0
set default values for None,0
check array shape,0
check dimension of instruments is more than dimension of treatments,0
weight X and y,0
learn point estimate,0
solve first stage linear regression E[T|Z],0
"""that"" means T",0
solve second stage linear regression E[Y|that],0
(T.T*T)^{-1},0
learn cov(theta),0
(T.T*T)^{-1},0
sigma^2,0
reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
average the outcome dimension if it exists and ensure 2d y_pred,0
get index of best treatment,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
TODO: ideally the below private attribute logic should be in .fit but is needed in init,1
for nuisance estimator generation for parent class,0
should refactor later,1
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
override only so that we can exclude treatment featurization verbiage in docstring,0
Override to flatten output if T is flat,0
override only so that we can exclude treatment featurization verbiage in docstring,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
d_t=None here since we measure the effect across all Ts,0
conditionally expand jacobian dimensions to align with einsum str,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile",0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/main/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
TODO: enable type aliases,1
napoleon_preprocess_types = True  # needed for type aliases to work,0
napoleon_type_aliases = {,0
"""array_like"": "":term:`array_like`"",",0
"""ndarray"": ""~numpy.ndarray"",",0
"""RandomState"": "":class:`~numpy.random.RandomState`"",",0
"""DataFrame"": "":class:`~pandas.DataFrame`"",",0
"""Series"": "":class:`~pandas.Series`"",",0
},0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of strings:,0
,0
"source_suffix = ['.rst', '.md']",0
The root toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)",0
def mlasso_model(): return MultiTaskLassoCV(,0
"cv=3, alphas=alpha_regs, max_iter=200)",0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
"alpha_regs = [5e-3, 1e-2, 5e-2]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)",0
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)",0
subset of features that are exogenous and create heterogeneity,0
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features),0
subset of features wrt we estimate heterogeneity,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
introspect the constructor arguments to find the model parameters,0
to represent,0
"if the argument is deprecated, ignore it",0
Extract and sort argument names excluding 'self',0
column names,0
transfer input to numpy arrays,0
transfer input to 2d arrays,0
create dataframe,0
currently dowhy only support single outcome and single treatment,0
call dowhy,0
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update",0
cate estimator but not the effect.,0
don't proxy special methods,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
For when checking input values is disabled,0
Type to column extraction function,0
if not all column names are strings,0
coerce feature names to be strings,0
Prefer sklearn 1.0's get_feature_names_out method to deprecated get_feature_names method,0
"Some featurizers will throw, such as a pipeline with a transformer that doesn't itself support names",0
"Get number of arguments, some sklearn featurizer don't accept feature_names",0
Handles cases where the passed feature names create issues,0
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names',0
Get feature names using featurizer,0
All attempts at retrieving transformed feature names have failed,0
Delegate handling to downstream logic,0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
assume that we should perform nested cross-validation if and only if,0
the model has a 'cv' attribute; this is a somewhat brittle assumption...,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a 'groups' argument,0
now we have to compute the folds explicitly because some classifiers (like LassoCV),0
don't use the groups when calling split internally,0
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Any access should throw,0
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports",0
return plain dictionary so that erroneous accesses don't half work (see e.g. #708),0
for every dimension of the treatment add some epsilon and observe change in featurized treatment,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
return plain dictionary so that erroneous accesses don't half work (see #708),0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
return plain dictionary so that erroneous accesses don't half work (see #708),0
input feature name is already updated by cate_feature_names.,0
define the index of d_x to filter for each given T,0
filter X after broadcast with T for each given T,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
return plain dictionary so that erroneous accesses don't half work (see #708),0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
return plain dictionary so that erroneous accesses don't half work (see #708),0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains some snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_export.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
make any access to matplotlib or plt throw an exception,0
make any access to graphviz or plt throw an exception,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
Initialize saturation & value; calculate chroma & value shift,0
Calculate some intermediate values,0
Initialize RGB with same hue & chroma as our color,0
Shift the initial RGB values to match value and store,0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use mean of targets,0
Write node mean CATE,0
Write node std of CATE,0
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
Fetch appropriate color for node,0
Write node mean CATE,0
Write node mean CATE,0
Write recommended treatment and value - cost,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
This flag is true when names are set in a child class instead,0
"If names are set in a child class, add an attribute reflecting that",0
This works only if X is passed as a kwarg,0
We plan to enforce X as kwarg only in future releases,0
This checks if names have been set in a child class,0
"If names were set in a child class, don't do it again",0
"Wraps-up fit by setting attributes, cleaning up, etc.",0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"Treatment names is None, default to BaseCateEstimator",0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
Get input names,0
Summary,0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
Get input names,0
Note: we do not transform feature names since that is done within summary_frame,0
Summary,0
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
remove None arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same,0
alteration even when we only want to fit_final.,0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)",0
"however, sklearn doesn't support both stratifying and grouping (see",0
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply",0
their own object that supports grouping if they want to use groups.,0
for each mc iteration,0
for each model under cross fit setting,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Policy Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base Policy tree,0
=============================================================================,0
The values below are required and utilitized by methods in the _SingleTreeExporterMixin,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"since we only allow single dimensional y, we could flatten the prediction",0
override only so that we can exclude treatment featurization verbiage in docstring,0
override only so that we can exclude treatment featurization verbiage in docstring,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Handles the corner case when X=None but featurizer might be not None,0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
Replacing to remove docstring,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE: important to use the rlearner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the rlearner's copy to retain the information from fitting,0
Handles the corner case when X=None but featurizer might be not None,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `LinearModelFinalInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support freq_weight and sample_var in debiased lasso,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add blb to parent's options,0
override only so that we can update the docstring to indicate,0
support for `GenericSingleTreatmentModelFinalInference`,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
NOTE: important to get parent's wrapped copy so that,0
"after training wrapped featurizer is also trained, etc.",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Fit a doubly robust average effect,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T",0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
"If custom param grid, check that only estimator parameters are being altered",0
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test",0
override only so that we can update the docstring to indicate support for `blb`,0
Get input names,0
Summary,0
Determine output settings,0
"Important: This must be the first invocation of the random state at fit time, so that",0
train/test splits are re-generatable from an external object simply by knowing the,0
random_state parameter of the tree. Can be useful in the future if one wants to create local,0
linear predictions. Currently is also useful for testing.,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
Set min_weight_leaf from min_weight_fraction_leaf,0
Build tree,0
We calculate the maximum number of samples from each half-split that any node in the tree can,0
hold. Used by criterion for memory space savings.,0
Initialize the criterion object and the criterion_val object if honest.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code is a fork from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_base.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
Set parameters,0
Don't instantiate estimators now! Parameters of base_estimator might,0
"still change. Eg., when grid-searching with the nested object syntax.",0
self.estimators_ needs to be filled by the derived classes in fit.,0
Compute the number of jobs,0
Partition estimators between jobs,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
covariance matrix,0
get eigen value and eigen vectors,0
simulate eigen vectors,0
keep the top 4 eigen value and corresponding eigen vector,0
replace the negative eigen values,0
generate a new covariance matrix,0
get linear approximation of eigen values,0
coefs,0
get the indices of each group of features,0
print(ind_same_proxy),0
demo,0
same proxy,0
residuals,0
gmm,0
log normal on outliers,0
positive outliers,0
negative outliers,0
demean the new residual again,0
generate data,0
sample residuals,0
get prediction for current investment,0
get prediction for current proxy,0
get first period prediction,0
iterate the step ahead contruction,0
prepare new x,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
get new covariance matrix,0
get coefs,0
get residuals,0
proxy 1 is the outcome,0
make fixed residuals,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=None here since we measure the effect across all Ts,0
"y is a vector, rather than a 2D array",0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
squeeze the first axis,0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T",0
squeeze the first axis,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector,0
d_t=None here since we measure the effect across all Ts,0
d_t=None here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet",0
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
scale preds,0
scale std errs,0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
offset preds,0
"offset the distribution, too",0
scale preds,0
"scale the distribution, too",0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base GRF tree,0
=============================================================================,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
=============================================================================,0
A MultOutputWrapper for GRF classes,0
=============================================================================,0
=============================================================================,0
Instantiations of Generalized Random Forest,0
=============================================================================,0
"Append a constant treatment if `fit_intercept=True`, the coefficient",0
in front of the constant treatment is the intercept in the moment equation.,0
"Append a constant treatment and constant instrument if `fit_intercept=True`,",0
the coefficient in front of the constant treatment is the intercept in the moment equation.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Base Generalized Random Forest,0
=============================================================================,0
TODO: support freq_weight and sample_var,1
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle,0
We calculate the min eigenvalue proxy that each criterion is considering,0
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`",0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Generating indices a priori before parallelism ended up being orders of magnitude,0
faster than how sklearn does it. The reason is that random samplers do not release the,0
gil it seems.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
####################,0
Variance correction,0
####################,0
Subtract the average within bag variance. This ends up being equal to the,0
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).,0
The negative part is just sq_between.,0
Objective bayes debiasing for the diagonals where we know a-prior they are positive,0
"The off diagonals we have no objective prior, so no correction is applied.",0
Finally correcting the pred_cov or pred_var,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: update docs,1
"NOTE: sample weight, sample var are not passed in",0
Compose final model,0
Calculate auxiliary quantities,0
X  T_res,0
"sum(model_final.predict(X, T_res))",0
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J",0
override only so that we can exclude treatment featurization verbiage in docstring,0
override only so that we can exclude treatment featurization verbiage in docstring,0
"we need to set the number of periods before calling super()._prefit, since that will generate the",0
"final and nuisance models, which need to have self._n_periods set",0
Set _d_t to effective number of treatments,0
Required for bootstrap inference,0
for each mc iteration,0
for each model under cross fit setting,0
Handles the corner case when X=None but featurizer might be not None,0
Expand treatments for each time period,0
NOTE: important to use the _ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the _ortho_learner's copy to retain the information from fitting,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
test that the subsampling scheme past to the trees is correct,0
The sample size is chosen in particular to test rounding based error when subsampling,0
test that the estimator calcualtes var correctly,0
test api,0
test accuracy,0
test the projection functionality of forests,0
test that the estimator calcualtes var correctly,0
test api,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
test that the subsampling scheme past to the trees is correct,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
filter directories by regex if the NOTEBOOK_DIR_PATTERN environment variable is set,0
omit the lalonde notebook,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
for is_discrete in [False]:,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
No heterogeneity,0
Define indices to test,0
Heterogeneous effects,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
identity featurization effect functions,0
polynomial featurization effect functions,0
1d polynomial featurization functions,0
2d-to-1d featurization functions,0
2d-to-1d vector featurization functions,0
test that treatment names are assigned for the featurized treatment,0
expected shapes,0
check effects,0
ate,0
loose inference checks,0
temporarily skip LinearDRIV and SparseLinearDRIV for weird effect shape reasons,0
effect inference,0
marginal effect inference,0
const marginal effect inference,0
fit a dummy estimator first so the featurizer can be fit to the treatment,0
edge case with transformer that only takes a vector treatment,0
so far will always return None for cate_treatment_names,0
assert proper handling of improper feature names passed to certain transformers,0
"depending on sklearn version, bad feature names either throws error or only uses first relevant name",0
ensure alpha is passed,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
initialize parameters,0
initialize config wtih base config and overwite some values,0
predict tree using config parameters and assert,0
shape of trained tree is the same as y_test,0
initialize config wtih base honest config and overwite some values,0
predict tree using config parameters and assert,0
shape of trained tree is the same as y_test,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
compare with weight,0
compare with weight,0
compare with weight,0
compare with weight,0
Multi-dimensional output y,0
1-d y,0
compare when both sample_var and sample_weight exist,0
multi-d y,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
dgp,0
StatsModels2SLS,0
IV2SLS,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
ensure alpha is passed,0
only is not None when T1 is a constant or a list of constant,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
make any access to matplotlib or plt throw an exception,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
Invert indices to match latest API,0
Invert indices to match latest API,0
The feature for heterogeneity stays constant,0
Auxiliary function for adding xticks and vertical lines when plotting results,0
for dynamic dml vs ground truth parameters.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
test at least one estimator from each category,0
test causal graph,0
test refutation estimate,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
ensure we can serialize unfit estimator,0
ensure we can serialize fit estimator,0
expected effect size,0
test effect,0
test inference,0
only OrthoIV support inference other than bootstrap,0
test summary,0
test can run score,0
test cate_feature_names,0
test can run shap values,0
dgp,0
no heterogeneity,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
parameter combinations to test,0
TODO: serializing/deserializing for every combination -- is this necessary?,1
ensure we can serialize unfit estimator,0
ensure we can serialize fit estimator,0
expected effect size,0
assert calculated constant marginal effect shape is expected,0
const_marginal effect is defined in LinearCateEstimator class,0
assert calculated marginal effect shape is expected,0
test inference,0
test can run score,0
test cate_feature_names,0
test can run shap values,0
"dgp (binary T, binary Z)",0
no heterogeneity,0
with heterogeneity,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
make sure we can get out post-fit stuff,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
test nested grouping,0
DML nested CV works via a 'cv' attribute,0
"with 2-fold outer and 2-fold inner grouping, and six total groups,",0
should get 1 or 2 groups per split,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we see,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
"Try default, integer, and new user-passed treatment name",0
FunctionTransformers are agnostic to passed treatment names,0
Expected treatment names are the sums of user-passed prefixes and transformer-specific postfixes,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet",0
test nested grouping,0
DML nested CV works via a 'cv' attribute,0
"with 2-fold outer and 2-fold inner grouping, and six total groups,",0
should get 1 or 2 groups per split,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we see,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
helper class,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
continuous treatments have typical treatment values equal to,0
the mean of the absolute value of non-zero entries,0
discrete treatments have typical treatment value 1,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
continuous treatments have typical treatment values equal to,0
the mean of the absolute value of non-zero entries,0
discrete treatments have typical treatment value 1,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
make sure we don't run into problems dropping every index,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
dgp,0
model,0
model,0
"columns 'd', 'e', 'h' have too many values",0
"columns 'd', 'e' have too many values",0
lowering bound shouldn't affect already fit columns when warm starting,0
"column d is now okay, too",0
verify that we can use a scalar treatment cost,0
verify that we can specify per-treatment costs for each sample,0
verify that using the same state returns the same results each time,0
set the categories for column 'd' explicitly so that b is default,0
"first column: 10 ones, this is fine",0
"second column: 6 categories, plenty of random instances of each",0
this is fine only if we increase the cateogry limit,0
"third column: nine ones, lots of twos, not enough unless we disable check",0
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity",0
forest heterogeneity won't work,0
"sixth column: just 1 one, not enough even without check",0
increase bound on cat expansion,0
skip checks (reducing folds accordingly),0
"Add tests that guarantee that the reliance on DML feature order is not broken, such as",0
"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W",0
Pass an example where W is irrelevant and X is confounder,0
"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be",0
zeroed out and the test will fail,0
"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates",0
rescaling X shouldn't affect the first stage models because they normalize the inputs,0
"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid",0
cross terms,0
scale by 1000 to match the input to this model:,0
"the scale of X does matter for the final model, which keeps results in user-denominated units",0
rescaling X still shouldn't affect the first stage models,0
TODO: we don't recover the correct values with enough accuracy to enable this assertion,1
is there a different way to verify that we are learning the correct coefficients?,1
"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Define data features,0
Added `_df`to names to be different from the default cate_estimator names,0
Generate data,0
################################,0
Single treatment and outcome #,0
################################,0
Test LinearDML,0
|--> Test featurizers,0
"ColumnTransformer behaves differently depending on version of sklearn, so we no longer check the names",0
|--> Test re-fit,0
Test SparseLinearDML,0
Test ForestDML,0
###################################,0
Mutiple treatments and outcomes #,0
###################################,0
Test LinearDML,0
Test SparseLinearDML,0
"Single outcome only, ORF does not support multiple outcomes",0
Test DMLOrthoForest,0
Test DROrthoForest,0
Test XLearner,0
Skipping population summary names test because bootstrap inference is too slow,0
Test SLearner,0
Test TLearner,0
Test LinearDRLearner,0
Test SparseLinearDRLearner,0
Test ForestDRLearner,0
Test LinearIntentToTreatDRIV,0
Test DeepIV,0
Test categorical treatments,0
Check refit,0
Check refit after setting categories,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
Treatment effect function,0
Outcome support,0
Treatment support,0
"Generate controls, covariates, treatments and outcomes",0
Heterogeneous treatment effects,0
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
through shap package.,0
test shap could generate the plot from the shap_values,0
"waterfall is broken in this version, fixed by https://github.com/slundberg/shap/pull/2444",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we need the controls' encoded column for training",0
"Thus, we append the controls column before the one-hot-encoded T",0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary",0
"fit on projected Z: E[T * E[T|X,Z]|X]",0
"if discrete, return shape (n,1); if continuous return shape (n,)",0
"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)",0
"shape (n,)",0
"shape (n,)",0
"shape(n,)",0
TODO: prel_model_effect could allow sample_var and freq_weight?,1
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary",0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"if discrete, return shape (n,1); if continuous return shape (n,)",0
target will be discrete and will be inversed from FirstStageWrapper,0
"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous",0
reshape the predictions,0
concat W and Z,0
check nuisances outcome shape,0
Y_res could be a vector or 1-dimensional 2d-array,0
"all could be reshaped to vector since Y, T, Z are all single dimensional.",0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
A helper class that access all the internal fitted objects of a DRIV Cate Estimator.,0
Used by both DRIV and IntentToTreatDRIV.,0
Maggie: I think that would be the case?,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE: important to use the ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
Handles the corner case when X=None but featurizer might be not None,0
NOTE This is used by the inference methods and is more for internal use to the library,0
this is a regression model since proj_t is probability,0
outcome is continuous since proj_t is probability,0
Define the data generation functions,0
Define the data generation functions,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
Define the data generation functions,0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
Define the data generation functions,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
concat W and Z,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
concat W and Z,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
reshape the predictions,0
"T_res, Z_res, beta expect shape to be (n,1)",0
Define the data generation functions,0
maybe shouldn't expose fit_cate_intercept in this class?,0
Define the data generation functions,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: do correct adjustment for sample_var,1
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
concat W and Z,0
concat W and Z,0
concat W and Z,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Define the data generation functions,0
"train E[T|X,W,Z]",0
"train [Z|X,W]",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE: important to use the ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
Handles the corner case when X=None but featurizer might be not None,0
NOTE This is used by the inference methods and is more for internal use to the library,0
concat W and Z,0
note that groups are not passed to score because they are only used for fitting,0
concat W and Z,0
note that sample_weight and groups are not passed to predict because they are only used for fitting,0
concat W and Z,0
A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.,0
Used by both Parametric and Non Parametric DMLIV.,0
override only so that we can enforce Z to be required,0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Handles the corner case when X=None but featurizer might be not None,0
Define the data generation functions,0
Get input names,0
Summary,0
coefficient,0
intercept,0
Define the data generation functions,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: this utility is documented but internal; reimplement?,1
TODO: this utility is even less public...,1
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged",0
use same Cs as would be used by default by LogisticRegressionCV,0
NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification,0
which could affect how many times each distinct Y value needs to be present in the data,0
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns,0
but also supports get_feature_names with expected signature,0
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value,0
NOTE: we rely on the passthrough columns coming first in the concatenated X;W,0
"when we pipeline scaling with our first stage models later, so the order here is important",0
TODO: remove once older sklearn support is no longer needed,1
Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy,0
Convert python objects to (possibly nested) types that can easily be represented as literals,0
Convert SingleTreeInterpreter to a python dictionary,0
named tuple type for storing results inside CausalAnalysis class;,0
must be lifted to module level to enable pickling,0
"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,",1
"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all",0
"when running the first stage models, but don't want to scale the X columns when running the final model,",0
since then our coefficients will have odd units and our trees will also have decisions using those units.,0
,0
"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)",0
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names,0
Controls are all other columns of X,0
"can't use X[:, feat_ind] when X is a DataFrame",0
TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
we might want to modify OrthoLearner (and other discrete treatment classes),0
so that the user can opt-in to allowing unseen treatment values,0
(and return NaN or something in that case),0
HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models,1
and so we can just peel the first columns off of that combined array for rescaling in the pipeline,0
TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are,1
"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this",0
becomes a valid approach to handling this,0
array checking routines don't accept 0-width arrays,0
perform model selection,0
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative,0
convert to NormalInferenceResults for consistency,0
Set the dictionary values shared between local and global summaries,0
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments",0
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category",0
required to fit a discrete DML model,0
"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular",1
sub-cases of models or also integrate with azure autoML. (post-MVP),0
"TODO: Add other heterogeneity model options, such as {'automl'} for performing",1
"model selection for the causal effect, or {'sparse_linear'} for using a debiased lasso. (post-MVP)",0
TODO: Enable multi-class classification (post-MVP),1
Validate inputs,0
TODO: check compatibility of X and Y lengths,1
"no previous fit, cancel warm start",0
"work with numeric feature indices, so that we can easily compare with categorical ones",0
"if heterogeneity_inds is 1D, repeat it",0
heterogeneity inds should be a 2D list of length same as train_inds,0
replace None elements of heterogeneity_inds and ensure indices are numeric,0
"TODO: bail out also if categorical columns, classification, random_state changed?",1
TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
train the Y model,0
"perform model selection for the Y model using all X, not on a per-column basis",0
"now that we've trained the classifier and wrapped it, ensure that y is transformed to",0
work with the regression wrapper,0
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays,0
"note that this needs to happen after wrapping to generalize to the multi-class case,",0
since otherwise we'll have too many columns to be able to train a classifier,0
start with empty results and default shared insights,0
convert categorical indicators to numeric indices,0
check for indices over the categorical expansion bound,0
assume we'll be able to train former failures this time; we'll add them back if not,0
"can't remove in place while iterating over new_inds, so store in separate list",0
"train the model, but warn",0
no model can be trained in this case since we need more folds,0
"don't train a model, but suggest workaround since there are enough instances of least",1
populated class,0
also remove from train_inds so we don't try to access the result later,0
extract subset of names matching new columns,0
"track indices where an exception was thrown, since we can't remove from dictionary while iterating",0
don't want to cache this failed result,0
properties to return from effect InferenceResults,0
properties to return from PopulationSummaryResults,0
Converts strings to property lookups or method calls as a convenience so that the,0
_point_props and _summary_props above can be applied to an inference object,0
Create a summary combining all results into a single output; this is used,0
by the various causal_effect and causal_effect_dict methods to generate either a dataframe,0
"or a dictionary, respectively, based on the summary function passed into this method",0
"ensure array has shape (m,y,t)",0
population summary is missing sample dimension; add it for consistency,0
outcome dimension is missing; add it for consistency,0
add singleton treatment dimension if missing,0
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce,0
"each attr has dimension (m,y) or (m,y,t)",0
concatenate along treatment dimension,0
"for dictionary representation, want to remove unneeded sample dimension",0
in cohort and global results,0
TODO: enrich outcome logic for multi-class classification when that is supported,1
There is no actual sample level in this data,0
can't drop only level,0
should be serialization-ready and contain no numpy arrays,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
TODO: Note that there's no column metadata for the sample number - should there be?,1
"need to replicate the column info for each sample, then remove from the shared data",0
NOTE: the flattened order has the ouptut dimension before the feature dimension,0
which may need to be revisited once we support multiclass,0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
a global inference indicates the effect of that one feature on the outcome,0
need to reshape the output to match the input,0
we want to offset the inference object by the baseline estimate of y,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
"NOTE: this calculation is correct only if treatment costs are marginal costs,",0
because then scaling the difference between treatment value and treatment costs is the,0
same as scaling the treatment value and subtracting the scaled treatment cost.,0
,0
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for",0
"continuous treatments, the policy value should include the benefit of decreasing treatments",0
(rather than just not treating at all),0
,0
"We can get the total by seeing that if we restrict attention to units where we would treat,",0
2 * policy_value - always_treat,0
includes exactly their contribution because policy_value and always_treat both include it,0
"and likewise restricting attention to the units where we want to decrease treatment,",0
2 * policy_value - always-treat,0
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value",0
is zero and the contribution to always_treat is negative,0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
get dataframe with all but selected column,0
apply 10% of a typical treatment for this feature,0
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely",0
set the effect bounds; for positive treatments these agree with,0
"the estimates; for negative treatments, we need to invert the interval",0
the effect is now always positive since we decrease treatment when negative,0
"for discrete treatment, stack a zero result in front for control",0
we need to call effect_inference to get the correct CI between the two treatment options,0
we now need to construct the delta in the cost between the two treatments and translate the effect,0
remove third dimenions potentially added,0
"find cost of current treatment: equality creates a 2d array with True on each row,",0
only if its the location of the current treatment. Then we take the corresponding cost.,0
construct index of current treatment,0
add second dimension if needed for broadcasting during translation of effect,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values",0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
"TODO: once we drop support for sklearn < 1.0, we can remove this",1
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
normalize was deprecated or removed; don't need to do anything,0
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Call weighted lasso on reduced design matrix,0
Weighted tau,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to WeightedLassoCV by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
set default values for None,0
check freq_weight should be integer and should be accompanied by sample_var,0
check array shape,0
weight X and y and sample_var,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
set default values for None,0
check array shape,0
check dimension of instruments is more than dimension of treatments,0
weight X and y,0
learn point estimate,0
solve first stage linear regression E[T|Z],0
"""that"" means T",0
solve second stage linear regression E[Y|that],0
(T.T*T)^{-1},0
learn cov(theta),0
(T.T*T)^{-1},0
sigma^2,0
reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
average the outcome dimension if it exists and ensure 2d y_pred,0
get index of best treatment,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
override only so that we can exclude treatment featurization verbiage in docstring,0
Override to flatten output if T is flat,0
override only so that we can exclude treatment featurization verbiage in docstring,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
d_t=None here since we measure the effect across all Ts,0
conditionally expand jacobian dimensions to align with einsum str,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T",0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) PyWhy contributors. All rights reserved.,0
Licensed under the MIT License.,0
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile",0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/main/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
TODO: enable type aliases,1
napoleon_preprocess_types = True  # needed for type aliases to work,0
napoleon_type_aliases = {,0
"""array_like"": "":term:`array_like`"",",0
"""ndarray"": ""~numpy.ndarray"",",0
"""RandomState"": "":class:`~numpy.random.RandomState`"",",0
"""DataFrame"": "":class:`~pandas.DataFrame`"",",0
"""Series"": "":class:`~pandas.Series`"",",0
},0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of strings:,0
,0
"source_suffix = ['.rst', '.md']",0
The root toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)",0
def mlasso_model(): return MultiTaskLassoCV(,0
"cv=3, alphas=alpha_regs, max_iter=200)",0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
"alpha_regs = [5e-3, 1e-2, 5e-2]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)",0
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)",0
subset of features that are exogenous and create heterogeneity,0
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features),0
subset of features wrt we estimate heterogeneity,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
introspect the constructor arguments to find the model parameters,0
to represent,0
"if the argument is deprecated, ignore it",0
Extract and sort argument names excluding 'self',0
column names,0
transfer input to numpy arrays,0
transfer input to 2d arrays,0
create dataframe,0
currently dowhy only support single outcome and single treatment,0
call dowhy,0
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update",0
cate estimator but not the effect.,0
don't proxy special methods,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
For when checking input values is disabled,0
Type to column extraction function,0
if not all column names are strings,0
coerce feature names to be strings,0
Prefer sklearn 1.0's get_feature_names_out method to deprecated get_feature_names method,0
"Some featurizers will throw, such as a pipeline with a transformer that doesn't itself support names",0
"Get number of arguments, some sklearn featurizer don't accept feature_names",0
Handles cases where the passed feature names create issues,0
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names',0
Get feature names using featurizer,0
All attempts at retrieving transformed feature names have failed,0
Delegate handling to downstream logic,0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
assume that we should perform nested cross-validation if and only if,0
the model has a 'cv' attribute; this is a somewhat brittle assumption...,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a 'groups' argument,0
now we have to compute the folds explicitly because some classifiers (like LassoCV),0
don't use the groups when calling split internally,0
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Any access should throw,0
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports",0
for every dimension of the treatment add some epsilon and observe change in featurized treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
input feature name is already updated by cate_feature_names.,0
define the index of d_x to filter for each given T,0
filter X after broadcast with T for each given T,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains some snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_export.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
make any access to matplotlib or plt throw an exception,0
make any access to graphviz or plt throw an exception,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
Initialize saturation & value; calculate chroma & value shift,0
Calculate some intermediate values,0
Initialize RGB with same hue & chroma as our color,0
Shift the initial RGB values to match value and store,0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use mean of targets,0
Write node mean CATE,0
Write node std of CATE,0
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
Fetch appropriate color for node,0
Write node mean CATE,0
Write node mean CATE,0
Write recommended treatment and value - cost,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
This flag is true when names are set in a child class instead,0
"If names are set in a child class, add an attribute reflecting that",0
This works only if X is passed as a kwarg,0
We plan to enforce X as kwarg only in future releases,0
This checks if names have been set in a child class,0
"If names were set in a child class, don't do it again",0
"Wraps-up fit by setting attributes, cleaning up, etc.",0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"Treatment names is None, default to BaseCateEstimator",0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
Get input names,0
Summary,0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
Get input names,0
Note: we do not transform feature names since that is done within summary_frame,0
Summary,0
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
remove None arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same,0
alteration even when we only want to fit_final.,0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)",0
"however, sklearn doesn't support both stratifying and grouping (see",0
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply",0
their own object that supports grouping if they want to use groups.,0
for each mc iteration,0
for each model under cross fit setting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Policy Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base Policy tree,0
=============================================================================,0
The values below are required and utilitized by methods in the _SingleTreeExporterMixin,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"since we only allow single dimensional y, we could flatten the prediction",0
override only so that we can exclude treatment featurization verbiage in docstring,0
override only so that we can exclude treatment featurization verbiage in docstring,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Handles the corner case when X=None but featurizer might be not None,0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
Replacing to remove docstring,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE: important to use the rlearner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the rlearner's copy to retain the information from fitting,0
Handles the corner case when X=None but featurizer might be not None,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `LinearModelFinalInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support freq_weight and sample_var in debiased lasso,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add blb to parent's options,0
override only so that we can update the docstring to indicate,0
support for `GenericSingleTreatmentModelFinalInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
NOTE: important to get parent's wrapped copy so that,0
"after training wrapped featurizer is also trained, etc.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Fit a doubly robust average effect,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T",0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
"If custom param grid, check that only estimator parameters are being altered",0
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test",0
override only so that we can update the docstring to indicate support for `blb`,0
Get input names,0
Summary,0
Determine output settings,0
"Important: This must be the first invocation of the random state at fit time, so that",0
train/test splits are re-generatable from an external object simply by knowing the,0
random_state parameter of the tree. Can be useful in the future if one wants to create local,0
linear predictions. Currently is also useful for testing.,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
Set min_weight_leaf from min_weight_fraction_leaf,0
Build tree,0
We calculate the maximum number of samples from each half-split that any node in the tree can,0
hold. Used by criterion for memory space savings.,0
Initialize the criterion object and the criterion_val object if honest.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code is a fork from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_base.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
Set parameters,0
Don't instantiate estimators now! Parameters of base_estimator might,0
"still change. Eg., when grid-searching with the nested object syntax.",0
self.estimators_ needs to be filled by the derived classes in fit.,0
Compute the number of jobs,0
Partition estimators between jobs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
covariance matrix,0
get eigen value and eigen vectors,0
simulate eigen vectors,0
keep the top 4 eigen value and corresponding eigen vector,0
replace the negative eigen values,0
generate a new covariance matrix,0
get linear approximation of eigen values,0
coefs,0
get the indices of each group of features,0
print(ind_same_proxy),0
demo,0
same proxy,0
residuals,0
gmm,0
log normal on outliers,0
positive outliers,0
negative outliers,0
demean the new residual again,0
generate data,0
sample residuals,0
get prediction for current investment,0
get prediction for current proxy,0
get first period prediction,0
iterate the step ahead contruction,0
prepare new x,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
get new covariance matrix,0
get coefs,0
get residuals,0
proxy 1 is the outcome,0
make fixed residuals,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=None here since we measure the effect across all Ts,0
"y is a vector, rather than a 2D array",0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
squeeze the first axis,0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T",0
squeeze the first axis,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector,0
d_t=None here since we measure the effect across all Ts,0
d_t=None here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet",0
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
scale preds,0
scale std errs,0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
offset preds,0
"offset the distribution, too",0
scale preds,0
"scale the distribution, too",0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base GRF tree,0
=============================================================================,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
=============================================================================,0
A MultOutputWrapper for GRF classes,0
=============================================================================,0
=============================================================================,0
Instantiations of Generalized Random Forest,0
=============================================================================,0
"Append a constant treatment if `fit_intercept=True`, the coefficient",0
in front of the constant treatment is the intercept in the moment equation.,0
"Append a constant treatment and constant instrument if `fit_intercept=True`,",0
the coefficient in front of the constant treatment is the intercept in the moment equation.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Base Generalized Random Forest,0
=============================================================================,0
TODO: support freq_weight and sample_var,1
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle,0
We calculate the min eigenvalue proxy that each criterion is considering,0
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`",0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Generating indices a priori before parallelism ended up being orders of magnitude,0
faster than how sklearn does it. The reason is that random samplers do not release the,0
gil it seems.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
####################,0
Variance correction,0
####################,0
Subtract the average within bag variance. This ends up being equal to the,0
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).,0
The negative part is just sq_between.,0
Objective bayes debiasing for the diagonals where we know a-prior they are positive,0
"The off diagonals we have no objective prior, so no correction is applied.",0
Finally correcting the pred_cov or pred_var,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: update docs,1
"NOTE: sample weight, sample var are not passed in",0
Compose final model,0
Calculate auxiliary quantities,0
X  T_res,0
"sum(model_final.predict(X, T_res))",0
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J",0
override only so that we can exclude treatment featurization verbiage in docstring,0
override only so that we can exclude treatment featurization verbiage in docstring,0
generate an instance of the final model,0
generate an instance of the nuisance model,0
Set _d_t to effective number of treatments,0
Required for bootstrap inference,0
for each mc iteration,0
for each model under cross fit setting,0
Handles the corner case when X=None but featurizer might be not None,0
Expand treatments for each time period,0
NOTE: important to use the _ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the _ortho_learner's copy to retain the information from fitting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
test that the subsampling scheme past to the trees is correct,0
The sample size is chosen in particular to test rounding based error when subsampling,0
test that the estimator calcualtes var correctly,0
test api,0
test accuracy,0
test the projection functionality of forests,0
test that the estimator calcualtes var correctly,0
test api,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
test that the subsampling scheme past to the trees is correct,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
filter directories by regex if the NOTEBOOK_DIR_PATTERN environment variable is set,0
omit the lalonde notebook,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
for is_discrete in [False]:,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
No heterogeneity,0
Define indices to test,0
Heterogeneous effects,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
identity featurization effect functions,0
polynomial featurization effect functions,0
1d polynomial featurization functions,0
2d-to-1d featurization functions,0
2d-to-1d vector featurization functions,0
test that treatment names are assigned for the featurized treatment,0
expected shapes,0
check effects,0
ate,0
loose inference checks,0
temporarily skip LinearDRIV and SparseLinearDRIV for weird effect shape reasons,0
effect inference,0
marginal effect inference,0
const marginal effect inference,0
fit a dummy estimator first so the featurizer can be fit to the treatment,0
edge case with transformer that only takes a vector treatment,0
so far will always return None for cate_treatment_names,0
assert proper handling of improper feature names passed to certain transformers,0
"depending on sklearn version, bad feature names either throws error or only uses first relevant name",0
ensure alpha is passed,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
initialize parameters,0
initialize config wtih base config and overwite some values,0
predict tree using config parameters and assert,0
shape of trained tree is the same as y_test,0
initialize config wtih base honest config and overwite some values,0
predict tree using config parameters and assert,0
shape of trained tree is the same as y_test,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
compare with weight,0
compare with weight,0
compare with weight,0
compare with weight,0
Multi-dimensional output y,0
1-d y,0
compare when both sample_var and sample_weight exist,0
multi-d y,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
dgp,0
StatsModels2SLS,0
IV2SLS,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
ensure alpha is passed,0
only is not None when T1 is a constant or a list of constant,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make any access to matplotlib or plt throw an exception,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
Invert indices to match latest API,0
Invert indices to match latest API,0
The feature for heterogeneity stays constant,0
Auxiliary function for adding xticks and vertical lines when plotting results,0
for dynamic dml vs ground truth parameters.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test at least one estimator from each category,0
test causal graph,0
test refutation estimate,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure we can serialize unfit estimator,0
ensure we can serialize fit estimator,0
expected effect size,0
test effect,0
test inference,0
only OrthoIV support inference other than bootstrap,0
test summary,0
test can run score,0
test cate_feature_names,0
test can run shap values,0
dgp,0
no heterogeneity,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
parameter combinations to test,0
TODO: serializing/deserializing for every combination -- is this necessary?,1
ensure we can serialize unfit estimator,0
ensure we can serialize fit estimator,0
expected effect size,0
assert calculated constant marginal effect shape is expected,0
const_marginal effect is defined in LinearCateEstimator class,0
assert calculated marginal effect shape is expected,0
test inference,0
test can run score,0
test cate_feature_names,0
test can run shap values,0
"dgp (binary T, binary Z)",0
no heterogeneity,0
with heterogeneity,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
make sure we can get out post-fit stuff,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
test nested grouping,0
DML nested CV works via a 'cv' attribute,0
"with 2-fold outer and 2-fold inner grouping, and six total groups,",0
should get 1 or 2 groups per split,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we see,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
"Try default, integer, and new user-passed treatment name",0
FunctionTransformers are agnostic to passed treatment names,0
Expected treatment names are the sums of user-passed prefixes and transformer-specific postfixes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet",0
test nested grouping,0
DML nested CV works via a 'cv' attribute,0
"with 2-fold outer and 2-fold inner grouping, and six total groups,",0
should get 1 or 2 groups per split,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we see,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
helper class,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
continuous treatments have typical treatment values equal to,0
the mean of the absolute value of non-zero entries,0
discrete treatments have typical treatment value 1,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
continuous treatments have typical treatment values equal to,0
the mean of the absolute value of non-zero entries,0
discrete treatments have typical treatment value 1,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
make sure we don't run into problems dropping every index,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
dgp,0
model,0
model,0
"columns 'd', 'e', 'h' have too many values",0
"columns 'd', 'e' have too many values",0
lowering bound shouldn't affect already fit columns when warm starting,0
"column d is now okay, too",0
verify that we can use a scalar treatment cost,0
verify that we can specify per-treatment costs for each sample,0
verify that using the same state returns the same results each time,0
set the categories for column 'd' explicitly so that b is default,0
"first column: 10 ones, this is fine",0
"second column: 6 categories, plenty of random instances of each",0
this is fine only if we increase the cateogry limit,0
"third column: nine ones, lots of twos, not enough unless we disable check",0
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity",0
forest heterogeneity won't work,0
"sixth column: just 1 one, not enough even without check",0
increase bound on cat expansion,0
skip checks (reducing folds accordingly),0
"Add tests that guarantee that the reliance on DML feature order is not broken, such as",0
"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W",0
Pass an example where W is irrelevant and X is confounder,0
"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be",0
zeroed out and the test will fail,0
"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates",0
rescaling X shouldn't affect the first stage models because they normalize the inputs,0
"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid",0
cross terms,0
scale by 1000 to match the input to this model:,0
"the scale of X does matter for the final model, which keeps results in user-denominated units",0
rescaling X still shouldn't affect the first stage models,0
TODO: we don't recover the correct values with enough accuracy to enable this assertion,1
is there a different way to verify that we are learning the correct coefficients?,1
"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Define data features,0
Added `_df`to names to be different from the default cate_estimator names,0
Generate data,0
################################,0
Single treatment and outcome #,0
################################,0
Test LinearDML,0
|--> Test featurizers,0
"ColumnTransformer behaves differently depending on version of sklearn, so we no longer check the names",0
|--> Test re-fit,0
Test SparseLinearDML,0
Test ForestDML,0
###################################,0
Mutiple treatments and outcomes #,0
###################################,0
Test LinearDML,0
Test SparseLinearDML,0
"Single outcome only, ORF does not support multiple outcomes",0
Test DMLOrthoForest,0
Test DROrthoForest,0
Test XLearner,0
Skipping population summary names test because bootstrap inference is too slow,0
Test SLearner,0
Test TLearner,0
Test LinearDRLearner,0
Test SparseLinearDRLearner,0
Test ForestDRLearner,0
Test LinearIntentToTreatDRIV,0
Test DeepIV,0
Test categorical treatments,0
Check refit,0
Check refit after setting categories,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
Treatment effect function,0
Outcome support,0
Treatment support,0
"Generate controls, covariates, treatments and outcomes",0
Heterogeneous treatment effects,0
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
through shap package.,0
test shap could generate the plot from the shap_values,0
"waterfall is broken in this version, fixed by https://github.com/slundberg/shap/pull/2444",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we need the controls' encoded column for training",0
"Thus, we append the controls column before the one-hot-encoded T",0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary",0
"fit on projected Z: E[T * E[T|X,Z]|X]",0
"if discrete, return shape (n,1); if continuous return shape (n,)",0
"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)",0
"shape (n,)",0
"shape (n,)",0
"shape(n,)",0
TODO: prel_model_effect could allow sample_var and freq_weight?,1
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary",0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"if discrete, return shape (n,1); if continuous return shape (n,)",0
target will be discrete and will be inversed from FirstStageWrapper,0
"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous",0
reshape the predictions,0
concat W and Z,0
check nuisances outcome shape,0
Y_res could be a vector or 1-dimensional 2d-array,0
"all could be reshaped to vector since Y, T, Z are all single dimensional.",0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
A helper class that access all the internal fitted objects of a DRIV Cate Estimator.,0
Used by both DRIV and IntentToTreatDRIV.,0
Maggie: I think that would be the case?,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE: important to use the ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
Handles the corner case when X=None but featurizer might be not None,0
NOTE This is used by the inference methods and is more for internal use to the library,0
this is a regression model since proj_t is probability,0
outcome is continuous since proj_t is probability,0
Define the data generation functions,0
Define the data generation functions,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
Define the data generation functions,0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
Define the data generation functions,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
concat W and Z,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
concat W and Z,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
reshape the predictions,0
"T_res, Z_res, beta expect shape to be (n,1)",0
Define the data generation functions,0
maybe shouldn't expose fit_cate_intercept in this class?,0
Define the data generation functions,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: do correct adjustment for sample_var,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
concat W and Z,0
concat W and Z,0
concat W and Z,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Define the data generation functions,0
"train E[T|X,W,Z]",0
"train [Z|X,W]",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE: important to use the ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
Handles the corner case when X=None but featurizer might be not None,0
NOTE This is used by the inference methods and is more for internal use to the library,0
concat W and Z,0
note that groups are not passed to score because they are only used for fitting,0
concat W and Z,0
note that sample_weight and groups are not passed to predict because they are only used for fitting,0
concat W and Z,0
A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.,0
Used by both Parametric and Non Parametric DMLIV.,0
override only so that we can enforce Z to be required,0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Handles the corner case when X=None but featurizer might be not None,0
Define the data generation functions,0
Get input names,0
Summary,0
coefficient,0
intercept,0
Define the data generation functions,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: this utility is documented but internal; reimplement?,1
TODO: this utility is even less public...,1
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged",0
use same Cs as would be used by default by LogisticRegressionCV,0
NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification,0
which could affect how many times each distinct Y value needs to be present in the data,0
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns,0
but also supports get_feature_names with expected signature,0
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value,0
NOTE: we rely on the passthrough columns coming first in the concatenated X;W,0
"when we pipeline scaling with our first stage models later, so the order here is important",0
Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy,0
Convert python objects to (possibly nested) types that can easily be represented as literals,0
Convert SingleTreeInterpreter to a python dictionary,0
named tuple type for storing results inside CausalAnalysis class;,0
must be lifted to module level to enable pickling,0
"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,",1
"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all",0
"when running the first stage models, but don't want to scale the X columns when running the final model,",0
since then our coefficients will have odd units and our trees will also have decisions using those units.,0
,0
"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)",0
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names,0
Controls are all other columns of X,0
"can't use X[:, feat_ind] when X is a DataFrame",0
TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
we might want to modify OrthoLearner (and other discrete treatment classes),0
so that the user can opt-in to allowing unseen treatment values,0
(and return NaN or something in that case),0
HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models,1
and so we can just peel the first columns off of that combined array for rescaling in the pipeline,0
TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are,1
"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this",0
becomes a valid approach to handling this,0
array checking routines don't accept 0-width arrays,0
perform model selection,0
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative,0
convert to NormalInferenceResults for consistency,0
Set the dictionary values shared between local and global summaries,0
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments",0
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category",0
required to fit a discrete DML model,0
"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular",1
sub-cases of models or also integrate with azure autoML. (post-MVP),0
"TODO: Add other heterogeneity model options, such as {'automl'} for performing",1
"model selection for the causal effect, or {'sparse_linear'} for using a debiased lasso. (post-MVP)",0
TODO: Enable multi-class classification (post-MVP),1
Validate inputs,0
TODO: check compatibility of X and Y lengths,1
"no previous fit, cancel warm start",0
"work with numeric feature indices, so that we can easily compare with categorical ones",0
"if heterogeneity_inds is 1D, repeat it",0
heterogeneity inds should be a 2D list of length same as train_inds,0
replace None elements of heterogeneity_inds and ensure indices are numeric,0
"TODO: bail out also if categorical columns, classification, random_state changed?",1
TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
train the Y model,0
"perform model selection for the Y model using all X, not on a per-column basis",0
"now that we've trained the classifier and wrapped it, ensure that y is transformed to",0
work with the regression wrapper,0
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays,0
"note that this needs to happen after wrapping to generalize to the multi-class case,",0
since otherwise we'll have too many columns to be able to train a classifier,0
start with empty results and default shared insights,0
convert categorical indicators to numeric indices,0
check for indices over the categorical expansion bound,0
assume we'll be able to train former failures this time; we'll add them back if not,0
"can't remove in place while iterating over new_inds, so store in separate list",0
"train the model, but warn",0
no model can be trained in this case since we need more folds,0
"don't train a model, but suggest workaround since there are enough instances of least",1
populated class,0
also remove from train_inds so we don't try to access the result later,0
extract subset of names matching new columns,0
"track indices where an exception was thrown, since we can't remove from dictionary while iterating",0
don't want to cache this failed result,0
properties to return from effect InferenceResults,0
properties to return from PopulationSummaryResults,0
Converts strings to property lookups or method calls as a convenience so that the,0
_point_props and _summary_props above can be applied to an inference object,0
Create a summary combining all results into a single output; this is used,0
by the various causal_effect and causal_effect_dict methods to generate either a dataframe,0
"or a dictionary, respectively, based on the summary function passed into this method",0
"ensure array has shape (m,y,t)",0
population summary is missing sample dimension; add it for consistency,0
outcome dimension is missing; add it for consistency,0
add singleton treatment dimension if missing,0
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce,0
"each attr has dimension (m,y) or (m,y,t)",0
concatenate along treatment dimension,0
"for dictionary representation, want to remove unneeded sample dimension",0
in cohort and global results,0
TODO: enrich outcome logic for multi-class classification when that is supported,1
There is no actual sample level in this data,0
can't drop only level,0
should be serialization-ready and contain no numpy arrays,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
TODO: Note that there's no column metadata for the sample number - should there be?,1
"need to replicate the column info for each sample, then remove from the shared data",0
NOTE: the flattened order has the ouptut dimension before the feature dimension,0
which may need to be revisited once we support multiclass,0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
a global inference indicates the effect of that one feature on the outcome,0
need to reshape the output to match the input,0
we want to offset the inference object by the baseline estimate of y,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
"NOTE: this calculation is correct only if treatment costs are marginal costs,",0
because then scaling the difference between treatment value and treatment costs is the,0
same as scaling the treatment value and subtracting the scaled treatment cost.,0
,0
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for",0
"continuous treatments, the policy value should include the benefit of decreasing treatments",0
(rather than just not treating at all),0
,0
"We can get the total by seeing that if we restrict attention to units where we would treat,",0
2 * policy_value - always_treat,0
includes exactly their contribution because policy_value and always_treat both include it,0
"and likewise restricting attention to the units where we want to decrease treatment,",0
2 * policy_value - always-treat,0
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value",0
is zero and the contribution to always_treat is negative,0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
get dataframe with all but selected column,0
apply 10% of a typical treatment for this feature,0
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely",0
set the effect bounds; for positive treatments these agree with,0
"the estimates; for negative treatments, we need to invert the interval",0
the effect is now always positive since we decrease treatment when negative,0
"for discrete treatment, stack a zero result in front for control",0
we need to call effect_inference to get the correct CI between the two treatment options,0
we now need to construct the delta in the cost between the two treatments and translate the effect,0
remove third dimenions potentially added,0
"find cost of current treatment: equality creates a 2d array with True on each row,",0
only if its the location of the current treatment. Then we take the corresponding cost.,0
construct index of current treatment,0
add second dimension if needed for broadcasting during translation of effect,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values",0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
"TODO: once we drop support for sklearn < 1.0, we can remove this",1
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
normalize was deprecated or removed; don't need to do anything,0
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Call weighted lasso on reduced design matrix,0
Weighted tau,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to WeightedLassoCV by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
set default values for None,0
check freq_weight should be integer and should be accompanied by sample_var,0
check array shape,0
weight X and y and sample_var,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
set default values for None,0
check array shape,0
check dimension of instruments is more than dimension of treatments,0
weight X and y,0
learn point estimate,0
solve first stage linear regression E[T|Z],0
"""that"" means T",0
solve second stage linear regression E[Y|that],0
(T.T*T)^{-1},0
learn cov(theta),0
(T.T*T)^{-1},0
sigma^2,0
reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
average the outcome dimension if it exists and ensure 2d y_pred,0
get index of best treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
override only so that we can exclude treatment featurization verbiage in docstring,0
Override to flatten output if T is flat,0
override only so that we can exclude treatment featurization verbiage in docstring,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
d_t=None here since we measure the effect across all Ts,0
conditionally expand jacobian dimensions to align with einsum str,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile",0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/main/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The root toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)",0
def mlasso_model(): return MultiTaskLassoCV(,0
"cv=3, alphas=alpha_regs, max_iter=200)",0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
"alpha_regs = [5e-3, 1e-2, 5e-2]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)",0
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)",0
subset of features that are exogenous and create heterogeneity,0
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features),0
subset of features wrt we estimate heterogeneity,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
introspect the constructor arguments to find the model parameters,0
to represent,0
"if the argument is deprecated, ignore it",0
Extract and sort argument names excluding 'self',0
column names,0
transfer input to numpy arrays,0
transfer input to 2d arrays,0
create dataframe,0
currently dowhy only support single outcome and single treatment,0
call dowhy,0
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update",0
cate estimator but not the effect.,0
don't proxy special methods,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
For when checking input values is disabled,0
Type to column extraction function,0
Prefer sklearn 1.0's get_feature_names_out method to deprecated get_feature_names method,0
"Some featurizers will throw, such as a pipeline with a transformer that doesn't itself support names",0
"Get number of arguments, some sklearn featurizer don't accept feature_names",0
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names',0
Get feature names using featurizer,0
All attempts at retrieving transformed feature names have failed,0
Delegate handling to downstream logic,0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
assume that we should perform nested cross-validation if and only if,0
the model has a 'cv' attribute; this is a somewhat brittle assumption...,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a 'groups' argument,0
now we have to compute the folds explicitly because some classifiers (like LassoCV),0
don't use the groups when calling split internally,0
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Any access should throw,0
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
input feature name is already updated by cate_feature_names.,0
define the index of d_x to filter for each given T,0
filter X after broadcast with T for each given T,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains some snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_export.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
make any access to matplotlib or plt throw an exception,0
make any access to graphviz or plt throw an exception,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
Initialize saturation & value; calculate chroma & value shift,0
Calculate some intermediate values,0
Initialize RGB with same hue & chroma as our color,0
Shift the initial RGB values to match value and store,0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use mean of targets,0
Write node mean CATE,0
Write node std of CATE,0
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
Fetch appropriate color for node,0
Write node mean CATE,0
Write node mean CATE,0
Write recommended treatment and value - cost,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
This flag is true when names are set in a child class instead,0
"If names are set in a child class, add an attribute reflecting that",0
This works only if X is passed as a kwarg,0
We plan to enforce X as kwarg only in future releases,0
This checks if names have been set in a child class,0
"If names were set in a child class, don't do it again",0
"Wraps-up fit by setting attributes, cleaning up, etc.",0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"Treatment names is None, default to BaseCateEstimator",0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
Get input names,0
Summary,0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
Get input names,0
Note: we do not transform feature names since that is done within summary_frame,0
Summary,0
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
remove None arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same,0
alteration even when we only want to fit_final.,0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)",0
"however, sklearn doesn't support both stratifying and grouping (see",0
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply",0
their own object that supports grouping if they want to use groups.,0
for each mc iteration,0
for each model under cross fit setting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Policy Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base Policy tree,0
=============================================================================,0
The values below are required and utilitized by methods in the _SingleTreeExporterMixin,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"since we only allow single dimensional y, we could flatten the prediction",0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Handles the corner case when X=None but featurizer might be not None,0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
Replacing to remove docstring,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE: important to use the rlearner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the rlearner's copy to retain the information from fitting,0
Handles the corner case when X=None but featurizer might be not None,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `LinearModelFinalInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support freq_weight and sample_var in debiased lasso,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add blb to parent's options,0
override only so that we can update the docstring to indicate,0
support for `GenericSingleTreatmentModelFinalInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
NOTE: important to get parent's wrapped copy so that,0
"after training wrapped featurizer is also trained, etc.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Fit a doubly robust average effect,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
"If custom param grid, check that only estimator parameters are being altered",0
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test",0
override only so that we can update the docstring to indicate support for `blb`,0
Get input names,0
Summary,0
Determine output settings,0
"Important: This must be the first invocation of the random state at fit time, so that",0
train/test splits are re-generatable from an external object simply by knowing the,0
random_state parameter of the tree. Can be useful in the future if one wants to create local,0
linear predictions. Currently is also useful for testing.,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
Set min_weight_leaf from min_weight_fraction_leaf,0
Build tree,0
We calculate the maximum number of samples from each half-split that any node in the tree can,0
hold. Used by criterion for memory space savings.,0
Initialize the criterion object and the criterion_val object if honest.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code is a fork from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_base.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
Set parameters,0
Don't instantiate estimators now! Parameters of base_estimator might,0
"still change. Eg., when grid-searching with the nested object syntax.",0
self.estimators_ needs to be filled by the derived classes in fit.,0
Compute the number of jobs,0
Partition estimators between jobs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
covariance matrix,0
get eigen value and eigen vectors,0
simulate eigen vectors,0
keep the top 4 eigen value and corresponding eigen vector,0
replace the negative eigen values,0
generate a new covariance matrix,0
get linear approximation of eigen values,0
coefs,0
get the indices of each group of features,0
print(ind_same_proxy),0
demo,0
same proxy,0
residuals,0
gmm,0
log normal on outliers,0
positive outliers,0
negative outliers,0
demean the new residual again,0
generate data,0
sample residuals,0
get prediction for current investment,0
get prediction for current proxy,0
get first period prediction,0
iterate the step ahead contruction,0
prepare new x,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
get new covariance matrix,0
get coefs,0
get residuals,0
proxy 1 is the outcome,0
make fixed residuals,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=None here since we measure the effect across all Ts,0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
squeeze the first axis,0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector,0
d_t=None here since we measure the effect across all Ts,0
d_t=None here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet",0
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
scale preds,0
scale std errs,0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
offset preds,0
"offset the distribution, too",0
scale preds,0
"scale the distribution, too",0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base GRF tree,0
=============================================================================,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
=============================================================================,0
A MultOutputWrapper for GRF classes,0
=============================================================================,0
=============================================================================,0
Instantiations of Generalized Random Forest,0
=============================================================================,0
"Append a constant treatment if `fit_intercept=True`, the coefficient",0
in front of the constant treatment is the intercept in the moment equation.,0
"Append a constant treatment and constant instrument if `fit_intercept=True`,",0
the coefficient in front of the constant treatment is the intercept in the moment equation.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Base Generalized Random Forest,0
=============================================================================,0
TODO: support freq_weight and sample_var,1
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle,0
We calculate the min eigenvalue proxy that each criterion is considering,0
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`",0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Generating indices a priori before parallelism ended up being orders of magnitude,0
faster than how sklearn does it. The reason is that random samplers do not release the,0
gil it seems.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
####################,0
Variance correction,0
####################,0
Subtract the average within bag variance. This ends up being equal to the,0
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).,0
The negative part is just sq_between.,0
Objective bayes debiasing for the diagonals where we know a-prior they are positive,0
"The off diagonals we have no objective prior, so no correction is applied.",0
Finally correcting the pred_cov or pred_var,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
test that the subsampling scheme past to the trees is correct,0
The sample size is chosen in particular to test rounding based error when subsampling,0
test that the estimator calcualtes var correctly,0
test api,0
test accuracy,0
test the projection functionality of forests,0
test that the estimator calcualtes var correctly,0
test api,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
test that the subsampling scheme past to the trees is correct,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
filter directories by regex if the NOTEBOOK_DIR_PATTERN environment variable is set,0
omit the lalonde notebook,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
for is_discrete in [False]:,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
No heterogeneity,0
Define indices to test,0
Heterogeneous effects,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
initialize parameters,0
initialize config wtih base config and overwite some values,0
predict tree using config parameters and assert,0
shape of trained tree is the same as y_test,0
initialize config wtih base honest config and overwite some values,0
predict tree using config parameters and assert,0
shape of trained tree is the same as y_test,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
compare with weight,0
compare with weight,0
compare with weight,0
compare with weight,0
Multi-dimensional output y,0
1-d y,0
compare when both sample_var and sample_weight exist,0
multi-d y,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
dgp,0
StatsModels2SLS,0
IV2SLS,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
ensure alpha is passed,0
only is not None when T1 is a constant or a list of constant,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make any access to matplotlib or plt throw an exception,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
Invert indices to match latest API,0
Invert indices to match latest API,0
The feature for heterogeneity stays constant,0
Auxiliary function for adding xticks and vertical lines when plotting results,0
for dynamic dml vs ground truth parameters.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test at least one estimator from each category,0
test causal graph,0
test refutation estimate,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure we can serialize unfit estimator,0
ensure we can serialize fit estimator,0
expected effect size,0
test effect,0
test inference,0
only OrthoIV support inference other than bootstrap,0
test summary,0
test can run score,0
test cate_feature_names,0
test can run shap values,0
dgp,0
no heterogeneity,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
parameter combinations to test,0
TODO: serializing/deserializing for every combination -- is this necessary?,1
ensure we can serialize unfit estimator,0
ensure we can serialize fit estimator,0
expected effect size,0
assert calculated constant marginal effect shape is expected,0
const_marginal effect is defined in LinearCateEstimator class,0
assert calculated marginal effect shape is expected,0
test inference,0
test can run score,0
test cate_feature_names,0
test can run shap values,0
"dgp (binary T, binary Z)",0
no heterogeneity,0
with heterogeneity,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
make sure we can get out post-fit stuff,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
test nested grouping,0
DML nested CV works via a 'cv' attribute,0
"with 2-fold outer and 2-fold inner grouping, and six total groups,",0
should get 1 or 2 groups per split,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we see,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet",0
test nested grouping,0
DML nested CV works via a 'cv' attribute,0
"with 2-fold outer and 2-fold inner grouping, and six total groups,",0
should get 1 or 2 groups per split,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we see,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
helper class,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
continuous treatments have typical treatment values equal to,0
the mean of the absolute value of non-zero entries,0
discrete treatments have typical treatment value 1,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
continuous treatments have typical treatment values equal to,0
the mean of the absolute value of non-zero entries,0
discrete treatments have typical treatment value 1,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
make sure we don't run into problems dropping every index,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
dgp,0
model,0
model,0
"columns 'd', 'e', 'h' have too many values",0
"columns 'd', 'e' have too many values",0
lowering bound shouldn't affect already fit columns when warm starting,0
"column d is now okay, too",0
verify that we can use a scalar treatment cost,0
verify that we can specify per-treatment costs for each sample,0
verify that using the same state returns the same results each time,0
set the categories for column 'd' explicitly so that b is default,0
"first column: 10 ones, this is fine",0
"second column: 6 categories, plenty of random instances of each",0
this is fine only if we increase the cateogry limit,0
"third column: nine ones, lots of twos, not enough unless we disable check",0
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity",0
forest heterogeneity won't work,0
"sixth column: just 1 one, not enough even without check",0
increase bound on cat expansion,0
skip checks (reducing folds accordingly),0
"Add tests that guarantee that the reliance on DML feature order is not broken, such as",0
"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W",0
Pass an example where W is irrelevant and X is confounder,0
"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be",0
zeroed out and the test will fail,0
"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates",0
rescaling X shouldn't affect the first stage models because they normalize the inputs,0
"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid",0
cross terms,0
scale by 1000 to match the input to this model:,0
"the scale of X does matter for the final model, which keeps results in user-denominated units",0
rescaling X still shouldn't affect the first stage models,0
TODO: we don't recover the correct values with enough accuracy to enable this assertion,1
is there a different way to verify that we are learning the correct coefficients?,1
"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Define data features,0
Added `_df`to names to be different from the default cate_estimator names,0
Generate data,0
################################,0
Single treatment and outcome #,0
################################,0
Test LinearDML,0
|--> Test featurizers,0
"ColumnTransformer behaves differently depending on version of sklearn, so we no longer check the names",0
|--> Test re-fit,0
Test SparseLinearDML,0
Test ForestDML,0
###################################,0
Mutiple treatments and outcomes #,0
###################################,0
Test LinearDML,0
Test SparseLinearDML,0
"Single outcome only, ORF does not support multiple outcomes",0
Test DMLOrthoForest,0
Test DROrthoForest,0
Test XLearner,0
Skipping population summary names test because bootstrap inference is too slow,0
Test SLearner,0
Test TLearner,0
Test LinearDRLearner,0
Test SparseLinearDRLearner,0
Test ForestDRLearner,0
Test LinearIntentToTreatDRIV,0
Test DeepIV,0
Test categorical treatments,0
Check refit,0
Check refit after setting categories,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
Treatment effect function,0
Outcome support,0
Treatment support,0
"Generate controls, covariates, treatments and outcomes",0
Heterogeneous treatment effects,0
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
through shap package.,0
test shap could generate the plot from the shap_values,0
"waterfall is broken in this version, fixed by https://github.com/slundberg/shap/pull/2444",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we need the controls' encoded column for training",0
"Thus, we append the controls column before the one-hot-encoded T",0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary",0
"fit on projected Z: E[T * E[T|X,Z]|X]",0
"if discrete, return shape (n,1); if continuous return shape (n,)",0
"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)",0
"shape (n,)",0
"shape (n,)",0
"shape(n,)",0
TODO: prel_model_effect could allow sample_var and freq_weight?,1
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary",0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"if discrete, return shape (n,1); if continuous return shape (n,)",0
target will be discrete and will be inversed from FirstStageWrapper,0
"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous",0
reshape the predictions,0
concat W and Z,0
check nuisances outcome shape,0
Y_res could be a vector or 1-dimensional 2d-array,0
"all could be reshaped to vector since Y, T, Z are all single dimensional.",0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
A helper class that access all the internal fitted objects of a DRIV Cate Estimator.,0
Used by both DRIV and IntentToTreatDRIV.,0
Maggie: I think that would be the case?,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE: important to use the ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
Handles the corner case when X=None but featurizer might be not None,0
NOTE This is used by the inference methods and is more for internal use to the library,0
this is a regression model since proj_t is probability,0
outcome is continuous since proj_t is probability,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
concat W and Z,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
concat W and Z,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
reshape the predictions,0
"T_res, Z_res, beta expect shape to be (n,1)",0
maybe shouldn't expose fit_cate_intercept in this class?,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: do correct adjustment for sample_var,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
concat W and Z,0
concat W and Z,0
concat W and Z,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
"train E[T|X,W,Z]",0
"train [Z|X,W]",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE: important to use the ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
Handles the corner case when X=None but featurizer might be not None,0
NOTE This is used by the inference methods and is more for internal use to the library,0
concat W and Z,0
note that groups are not passed to score because they are only used for fitting,0
concat W and Z,0
note that sample_weight and groups are not passed to predict because they are only used for fitting,0
concat W and Z,0
A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.,0
Used by both Parametric and Non Parametric DMLIV.,0
override only so that we can enforce Z to be required,0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Handles the corner case when X=None but featurizer might be not None,0
Get input names,0
Summary,0
coefficient,0
intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: this utility is documented but internal; reimplement?,1
TODO: this utility is even less public...,1
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged",0
use same Cs as would be used by default by LogisticRegressionCV,0
NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification,0
which could affect how many times each distinct Y value needs to be present in the data,0
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns,0
but also supports get_feature_names with expected signature,0
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value,0
NOTE: we rely on the passthrough columns coming first in the concatenated X;W,0
"when we pipeline scaling with our first stage models later, so the order here is important",0
Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy,0
Convert python objects to (possibly nested) types that can easily be represented as literals,0
Convert SingleTreeInterpreter to a python dictionary,0
named tuple type for storing results inside CausalAnalysis class;,0
must be lifted to module level to enable pickling,0
"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,",1
"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all",0
"when running the first stage models, but don't want to scale the X columns when running the final model,",0
since then our coefficients will have odd units and our trees will also have decisions using those units.,0
,0
"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)",0
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names,0
Controls are all other columns of X,0
"can't use X[:, feat_ind] when X is a DataFrame",0
TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
we might want to modify OrthoLearner (and other discrete treatment classes),0
so that the user can opt-in to allowing unseen treatment values,0
(and return NaN or something in that case),0
HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models,1
and so we can just peel the first columns off of that combined array for rescaling in the pipeline,0
TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are,1
"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this",0
becomes a valid approach to handling this,0
array checking routines don't accept 0-width arrays,0
perform model selection,0
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative,0
convert to NormalInferenceResults for consistency,0
Set the dictionary values shared between local and global summaries,0
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments",0
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category",0
required to fit a discrete DML model,0
"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular",1
sub-cases of models or also integrate with azure autoML. (post-MVP),0
"TODO: Add other heterogeneity model options, such as {'automl'} for performing",1
"model selection for the causal effect, or {'sparse_linear'} for using a debiased lasso. (post-MVP)",0
TODO: Enable multi-class classification (post-MVP),1
Validate inputs,0
TODO: check compatibility of X and Y lengths,1
"no previous fit, cancel warm start",0
"work with numeric feature indices, so that we can easily compare with categorical ones",0
"if heterogeneity_inds is 1D, repeat it",0
heterogeneity inds should be a 2D list of length same as train_inds,0
replace None elements of heterogeneity_inds and ensure indices are numeric,0
"TODO: bail out also if categorical columns, classification, random_state changed?",1
TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
train the Y model,0
"perform model selection for the Y model using all X, not on a per-column basis",0
"now that we've trained the classifier and wrapped it, ensure that y is transformed to",0
work with the regression wrapper,0
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays,0
"note that this needs to happen after wrapping to generalize to the multi-class case,",0
since otherwise we'll have too many columns to be able to train a classifier,0
start with empty results and default shared insights,0
convert categorical indicators to numeric indices,0
check for indices over the categorical expansion bound,0
assume we'll be able to train former failures this time; we'll add them back if not,0
"can't remove in place while iterating over new_inds, so store in separate list",0
"train the model, but warn",0
no model can be trained in this case since we need more folds,0
"don't train a model, but suggest workaround since there are enough instances of least",1
populated class,0
also remove from train_inds so we don't try to access the result later,0
extract subset of names matching new columns,0
"track indices where an exception was thrown, since we can't remove from dictionary while iterating",0
don't want to cache this failed result,0
properties to return from effect InferenceResults,0
properties to return from PopulationSummaryResults,0
Converts strings to property lookups or method calls as a convenience so that the,0
_point_props and _summary_props above can be applied to an inference object,0
Create a summary combining all results into a single output; this is used,0
by the various causal_effect and causal_effect_dict methods to generate either a dataframe,0
"or a dictionary, respectively, based on the summary function passed into this method",0
"ensure array has shape (m,y,t)",0
population summary is missing sample dimension; add it for consistency,0
outcome dimension is missing; add it for consistency,0
add singleton treatment dimension if missing,0
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce,0
"each attr has dimension (m,y) or (m,y,t)",0
concatenate along treatment dimension,0
"for dictionary representation, want to remove unneeded sample dimension",0
in cohort and global results,0
TODO: enrich outcome logic for multi-class classification when that is supported,1
There is no actual sample level in this data,0
can't drop only level,0
should be serialization-ready and contain no numpy arrays,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
TODO: Note that there's no column metadata for the sample number - should there be?,1
"need to replicate the column info for each sample, then remove from the shared data",0
NOTE: the flattened order has the ouptut dimension before the feature dimension,0
which may need to be revisited once we support multiclass,0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
a global inference indicates the effect of that one feature on the outcome,0
need to reshape the output to match the input,0
we want to offset the inference object by the baseline estimate of y,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
"NOTE: this calculation is correct only if treatment costs are marginal costs,",0
because then scaling the difference between treatment value and treatment costs is the,0
same as scaling the treatment value and subtracting the scaled treatment cost.,0
,0
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for",0
"continuous treatments, the policy value should include the benefit of decreasing treatments",0
(rather than just not treating at all),0
,0
"We can get the total by seeing that if we restrict attention to units where we would treat,",0
2 * policy_value - always_treat,0
includes exactly their contribution because policy_value and always_treat both include it,0
"and likewise restricting attention to the units where we want to decrease treatment,",0
2 * policy_value - always-treat,0
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value",0
is zero and the contribution to always_treat is negative,0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
get dataframe with all but selected column,0
apply 10% of a typical treatment for this feature,0
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely",0
set the effect bounds; for positive treatments these agree with,0
"the estimates; for negative treatments, we need to invert the interval",0
the effect is now always positive since we decrease treatment when negative,0
"for discrete treatment, stack a zero result in front for control",0
we need to call effect_inference to get the correct CI between the two treatment options,0
we now need to construct the delta in the cost between the two treatments and translate the effect,0
remove third dimenions potentially added,0
"find cost of current treatment: equality creates a 2d array with True on each row,",0
only if its the location of the current treatment. Then we take the corresponding cost.,0
construct index of current treatment,0
add second dimension if needed for broadcasting during translation of effect,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values",0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
"TODO: once we drop support for sklearn < 1.0, we can remove this",1
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
normalize was deprecated or removed; don't need to do anything,0
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Call weighted lasso on reduced design matrix,0
Weighted tau,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to WeightedLassoCV by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
set default values for None,0
check freq_weight should be integer and should be accompanied by sample_var,0
check array shape,0
weight X and y and sample_var,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
set default values for None,0
check array shape,0
check dimension of instruments is more than dimension of treatments,0
weight X and y,0
learn point estimate,0
solve first stage linear regression E[T|Z],0
"""that"" means T",0
solve second stage linear regression E[Y|that],0
(T.T*T)^{-1},0
learn cov(theta),0
(T.T*T)^{-1},0
sigma^2,0
reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
average the outcome dimension if it exists and ensure 2d y_pred,0
get index of best treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
d_t=None here since we measure the effect across all Ts,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: update docs,1
"NOTE: sample weight, sample var are not passed in",0
Compose final model,0
Calculate auxiliary quantities,0
X  T_res,0
"sum(model_final.predict(X, T_res))",0
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
Set _d_t to effective number of treatments,0
Required for bootstrap inference,0
for each mc iteration,0
for each model under cross fit setting,0
Handles the corner case when X=None but featurizer might be not None,0
Expand treatments for each time period,0
NOTE: important to use the _ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the _ortho_learner's copy to retain the information from fitting,0
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile",0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/main/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The root toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)",0
def mlasso_model(): return MultiTaskLassoCV(,0
"cv=3, alphas=alpha_regs, max_iter=200)",0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
"alpha_regs = [5e-3, 1e-2, 5e-2]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)",0
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)",0
subset of features that are exogenous and create heterogeneity,0
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features),0
subset of features wrt we estimate heterogeneity,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
introspect the constructor arguments to find the model parameters,0
to represent,0
"if the argument is deprecated, ignore it",0
Extract and sort argument names excluding 'self',0
column names,0
transfer input to numpy arrays,0
transfer input to 2d arrays,0
create dataframe,0
currently dowhy only support single outcome and single treatment,0
call dowhy,0
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update",0
cate estimator but not the effect.,0
don't proxy special methods,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
For when checking input values is disabled,0
Type to column extraction function,0
Prefer sklearn 1.0's get_feature_names_out method to deprecated get_feature_names method,0
"Some featurizers will throw, such as a pipeline with a transformer that doesn't itself support names",0
"Get number of arguments, some sklearn featurizer don't accept feature_names",0
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names',0
Get feature names using featurizer,0
All attempts at retrieving transformed feature names have failed,0
Delegate handling to downstream logic,0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
assume that we should perform nested cross-validation if and only if,0
the model has a 'cv' attribute; this is a somewhat brittle assumption...,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a 'groups' argument,0
now we have to compute the folds explicitly because some classifiers (like LassoCV),0
don't use the groups when calling split internally,0
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Any access should throw,0
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
input feature name is already updated by cate_feature_names.,0
define the index of d_x to filter for each given T,0
filter X after broadcast with T for each given T,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains some snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_export.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
make any access to matplotlib or plt throw an exception,0
make any access to graphviz or plt throw an exception,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
Initialize saturation & value; calculate chroma & value shift,0
Calculate some intermediate values,0
Initialize RGB with same hue & chroma as our color,0
Shift the initial RGB values to match value and store,0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use mean of targets,0
Write node mean CATE,0
Write node std of CATE,0
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
Fetch appropriate color for node,0
Write node mean CATE,0
Write node mean CATE,0
Write recommended treatment and value - cost,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
This flag is true when names are set in a child class instead,0
"If names are set in a child class, add an attribute reflecting that",0
This works only if X is passed as a kwarg,0
We plan to enforce X as kwarg only in future releases,0
This checks if names have been set in a child class,0
"If names were set in a child class, don't do it again",0
"Wraps-up fit by setting attributes, cleaning up, etc.",0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"Treatment names is None, default to BaseCateEstimator",0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
Get input names,0
Summary,0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
Get input names,0
Note: we do not transform feature names since that is done within summary_frame,0
Summary,0
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
remove None arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same,0
alteration even when we only want to fit_final.,0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)",0
"however, sklearn doesn't support both stratifying and grouping (see",0
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply",0
their own object that supports grouping if they want to use groups.,0
for each mc iteration,0
for each model under cross fit setting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Policy Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base Policy tree,0
=============================================================================,0
The values below are required and utilitized by methods in the _SingleTreeExporterMixin,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"since we only allow single dimensional y, we could flatten the prediction",0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Handles the corner case when X=None but featurizer might be not None,0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
Replacing to remove docstring,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE: important to use the rlearner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the rlearner's copy to retain the information from fitting,0
Handles the corner case when X=None but featurizer might be not None,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `LinearModelFinalInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support freq_weight and sample_var in debiased lasso,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add blb to parent's options,0
override only so that we can update the docstring to indicate,0
support for `GenericSingleTreatmentModelFinalInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
NOTE: important to get parent's wrapped copy so that,0
"after training wrapped featurizer is also trained, etc.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Fit a doubly robust average effect,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
"If custom param grid, check that only estimator parameters are being altered",0
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test",0
override only so that we can update the docstring to indicate support for `blb`,0
Get input names,0
Summary,0
Determine output settings,0
"Important: This must be the first invocation of the random state at fit time, so that",0
train/test splits are re-generatable from an external object simply by knowing the,0
random_state parameter of the tree. Can be useful in the future if one wants to create local,0
linear predictions. Currently is also useful for testing.,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
Set min_weight_leaf from min_weight_fraction_leaf,0
Build tree,0
We calculate the maximum number of samples from each half-split that any node in the tree can,0
hold. Used by criterion for memory space savings.,0
Initialize the criterion object and the criterion_val object if honest.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code is a fork from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_base.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
Set parameters,0
Don't instantiate estimators now! Parameters of base_estimator might,0
"still change. Eg., when grid-searching with the nested object syntax.",0
self.estimators_ needs to be filled by the derived classes in fit.,0
Compute the number of jobs,0
Partition estimators between jobs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
covariance matrix,0
get eigen value and eigen vectors,0
simulate eigen vectors,0
keep the top 4 eigen value and corresponding eigen vector,0
replace the negative eigen values,0
generate a new covariance matrix,0
get linear approximation of eigen values,0
coefs,0
get the indices of each group of features,0
print(ind_same_proxy),0
demo,0
same proxy,0
residuals,0
gmm,0
log normal on outliers,0
positive outliers,0
negative outliers,0
demean the new residual again,0
generate data,0
sample residuals,0
get prediction for current investment,0
get prediction for current proxy,0
get first period prediction,0
iterate the step ahead contruction,0
prepare new x,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
get new covariance matrix,0
get coefs,0
get residuals,0
proxy 1 is the outcome,0
make fixed residuals,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=None here since we measure the effect across all Ts,0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
squeeze the first axis,0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector,0
d_t=None here since we measure the effect across all Ts,0
d_t=None here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet",0
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
scale preds,0
scale std errs,0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
offset preds,0
"offset the distribution, too",0
scale preds,0
"scale the distribution, too",0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base GRF tree,0
=============================================================================,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
=============================================================================,0
A MultOutputWrapper for GRF classes,0
=============================================================================,0
=============================================================================,0
Instantiations of Generalized Random Forest,0
=============================================================================,0
"Append a constant treatment if `fit_intercept=True`, the coefficient",0
in front of the constant treatment is the intercept in the moment equation.,0
"Append a constant treatment and constant instrument if `fit_intercept=True`,",0
the coefficient in front of the constant treatment is the intercept in the moment equation.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Base Generalized Random Forest,0
=============================================================================,0
TODO: support freq_weight and sample_var,1
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle,0
We calculate the min eigenvalue proxy that each criterion is considering,0
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`",0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Generating indices a priori before parallelism ended up being orders of magnitude,0
faster than how sklearn does it. The reason is that random samplers do not release the,0
gil it seems.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
####################,0
Variance correction,0
####################,0
Subtract the average within bag variance. This ends up being equal to the,0
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).,0
The negative part is just sq_between.,0
Objective bayes debiasing for the diagonals where we know a-prior they are positive,0
"The off diagonals we have no objective prior, so no correction is applied.",0
Finally correcting the pred_cov or pred_var,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
test that the subsampling scheme past to the trees is correct,0
The sample size is chosen in particular to test rounding based error when subsampling,0
test that the estimator calcualtes var correctly,0
test api,0
test accuracy,0
test the projection functionality of forests,0
test that the estimator calcualtes var correctly,0
test api,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
test that the subsampling scheme past to the trees is correct,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
filter directories by regex if the NOTEBOOK_DIR_PATTERN environment variable is set,0
omit the lalonde notebook,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
for is_discrete in [False]:,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
No heterogeneity,0
Define indices to test,0
Heterogeneous effects,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
compare with weight,0
compare with weight,0
compare with weight,0
compare with weight,0
Multi-dimensional output y,0
1-d y,0
compare when both sample_var and sample_weight exist,0
multi-d y,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
dgp,0
StatsModels2SLS,0
IV2SLS,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
ensure alpha is passed,0
only is not None when T1 is a constant or a list of constant,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make any access to matplotlib or plt throw an exception,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
Invert indices to match latest API,0
Invert indices to match latest API,0
The feature for heterogeneity stays constant,0
Auxiliary function for adding xticks and vertical lines when plotting results,0
for dynamic dml vs ground truth parameters.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test at least one estimator from each category,0
test causal graph,0
test refutation estimate,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure we can serialize unfit estimator,0
ensure we can serialize fit estimator,0
expected effect size,0
test effect,0
test inference,0
only OrthoIV support inference other than bootstrap,0
test summary,0
test can run score,0
test cate_feature_names,0
test can run shap values,0
dgp,0
no heterogeneity,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure we can serialize unfit estimator,0
ensure we can serialize fit estimator,0
expected effect size,0
test effect,0
test inference,0
test can run score,0
test cate_feature_names,0
test can run shap values,0
"dgp (binary T, binary Z)",0
no heterogeneity,0
with heterogeneity,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
make sure we can get out post-fit stuff,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet",0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
helper class,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
continuous treatments have typical treatment values equal to,0
the mean of the absolute value of non-zero entries,0
discrete treatments have typical treatment value 1,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
continuous treatments have typical treatment values equal to,0
the mean of the absolute value of non-zero entries,0
discrete treatments have typical treatment value 1,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
make sure we don't run into problems dropping every index,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
dgp,0
model,0
model,0
"columns 'd', 'e', 'h' have too many values",0
"columns 'd', 'e' have too many values",0
lowering bound shouldn't affect already fit columns when warm starting,0
"column d is now okay, too",0
verify that we can use a scalar treatment cost,0
verify that we can specify per-treatment costs for each sample,0
verify that using the same state returns the same results each time,0
set the categories for column 'd' explicitly so that b is default,0
"first column: 10 ones, this is fine",0
"second column: 6 categories, plenty of random instances of each",0
this is fine only if we increase the cateogry limit,0
"third column: nine ones, lots of twos, not enough unless we disable check",0
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity",0
forest heterogeneity won't work,0
"sixth column: just 1 one, not enough even without check",0
increase bound on cat expansion,0
skip checks (reducing folds accordingly),0
"Add tests that guarantee that the reliance on DML feature order is not broken, such as",0
"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W",0
Pass an example where W is irrelevant and X is confounder,0
"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be",0
zeroed out and the test will fail,0
"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates",0
rescaling X shouldn't affect the first stage models because they normalize the inputs,0
"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid",0
cross terms,0
scale by 1000 to match the input to this model:,0
"the scale of X does matter for the final model, which keeps results in user-denominated units",0
rescaling X still shouldn't affect the first stage models,0
TODO: we don't recover the correct values with enough accuracy to enable this assertion,1
is there a different way to verify that we are learning the correct coefficients?,1
"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Define data features,0
Added `_df`to names to be different from the default cate_estimator names,0
Generate data,0
################################,0
Single treatment and outcome #,0
################################,0
Test LinearDML,0
|--> Test featurizers,0
"ColumnTransformer behaves differently depending on version of sklearn, so we no longer check the names",0
|--> Test re-fit,0
Test SparseLinearDML,0
Test ForestDML,0
###################################,0
Mutiple treatments and outcomes #,0
###################################,0
Test LinearDML,0
Test SparseLinearDML,0
"Single outcome only, ORF does not support multiple outcomes",0
Test DMLOrthoForest,0
Test DROrthoForest,0
Test XLearner,0
Skipping population summary names test because bootstrap inference is too slow,0
Test SLearner,0
Test TLearner,0
Test LinearDRLearner,0
Test SparseLinearDRLearner,0
Test ForestDRLearner,0
Test LinearIntentToTreatDRIV,0
Test DeepIV,0
Test categorical treatments,0
Check refit,0
Check refit after setting categories,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
Treatment effect function,0
Outcome support,0
Treatment support,0
"Generate controls, covariates, treatments and outcomes",0
Heterogeneous treatment effects,0
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
through shap package.,0
test shap could generate the plot from the shap_values,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we need the controls' encoded column for training",0
"Thus, we append the controls column before the one-hot-encoded T",0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary",0
"fit on projected Z: E[T * E[T|X,Z]|X]",0
"if discrete, return shape (n,1); if continuous return shape (n,)",0
"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)",0
"shape (n,)",0
"shape (n,)",0
"shape(n,)",0
TODO: prel_model_effect could allow sample_var and freq_weight?,1
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary",0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"if discrete, return shape (n,1); if continuous return shape (n,)",0
target will be discrete and will be inversed from FirstStageWrapper,0
"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous",0
reshape the predictions,0
concat W and Z,0
check nuisances outcome shape,0
Y_res could be a vector or 1-dimensional 2d-array,0
"all could be reshaped to vector since Y, T, Z are all single dimensional.",0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
A helper class that access all the internal fitted objects of a DRIV Cate Estimator.,0
Used by both DRIV and IntentToTreatDRIV.,0
Maggie: I think that would be the case?,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE: important to use the ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
Handles the corner case when X=None but featurizer might be not None,0
NOTE This is used by the inference methods and is more for internal use to the library,0
this is a regression model since proj_t is probability,0
outcome is continuous since proj_t is probability,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
concat W and Z,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
concat W and Z,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
reshape the predictions,0
"T_res, Z_res, beta expect shape to be (n,1)",0
maybe shouldn't expose fit_cate_intercept in this class?,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: do correct adjustment for sample_var,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
concat W and Z,0
concat W and Z,0
concat W and Z,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
"train E[T|X,W,Z]",0
"train [Z|X,W]",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE: important to use the ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
Handles the corner case when X=None but featurizer might be not None,0
NOTE This is used by the inference methods and is more for internal use to the library,0
concat W and Z,0
note that groups are not passed to score because they are only used for fitting,0
concat W and Z,0
note that sample_weight and groups are not passed to predict because they are only used for fitting,0
concat W and Z,0
A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.,0
Used by both Parametric and Non Parametric DMLIV.,0
override only so that we can enforce Z to be required,0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Handles the corner case when X=None but featurizer might be not None,0
Get input names,0
Summary,0
coefficient,0
intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: this utility is documented but internal; reimplement?,1
TODO: this utility is even less public...,1
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged",0
use same Cs as would be used by default by LogisticRegressionCV,0
NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification,0
which could affect how many times each distinct Y value needs to be present in the data,0
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns,0
but also supports get_feature_names with expected signature,0
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value,0
NOTE: we rely on the passthrough columns coming first in the concatenated X;W,0
"when we pipeline scaling with our first stage models later, so the order here is important",0
Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy,0
Convert python objects to (possibly nested) types that can easily be represented as literals,0
Convert SingleTreeInterpreter to a python dictionary,0
named tuple type for storing results inside CausalAnalysis class;,0
must be lifted to module level to enable pickling,0
"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,",1
"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all",0
"when running the first stage models, but don't want to scale the X columns when running the final model,",0
since then our coefficients will have odd units and our trees will also have decisions using those units.,0
,0
"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)",0
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names,0
Controls are all other columns of X,0
"can't use X[:, feat_ind] when X is a DataFrame",0
TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
we might want to modify OrthoLearner (and other discrete treatment classes),0
so that the user can opt-in to allowing unseen treatment values,0
(and return NaN or something in that case),0
HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models,1
and so we can just peel the first columns off of that combined array for rescaling in the pipeline,0
TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are,1
"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this",0
becomes a valid approach to handling this,0
array checking routines don't accept 0-width arrays,0
perform model selection,0
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative,0
convert to NormalInferenceResults for consistency,0
Set the dictionary values shared between local and global summaries,0
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments",0
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category",0
required to fit a discrete DML model,0
"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular",1
sub-cases of models or also integrate with azure autoML. (post-MVP),0
"TODO: Add other heterogeneity model options, such as {'automl'} for performing",1
"model selection for the causal effect, or {'sparse_linear'} for using a debiased lasso. (post-MVP)",0
TODO: Enable multi-class classification (post-MVP),1
Validate inputs,0
TODO: check compatibility of X and Y lengths,1
"no previous fit, cancel warm start",0
"work with numeric feature indices, so that we can easily compare with categorical ones",0
"if heterogeneity_inds is 1D, repeat it",0
heterogeneity inds should be a 2D list of length same as train_inds,0
replace None elements of heterogeneity_inds and ensure indices are numeric,0
"TODO: bail out also if categorical columns, classification, random_state changed?",1
TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
train the Y model,0
"perform model selection for the Y model using all X, not on a per-column basis",0
"now that we've trained the classifier and wrapped it, ensure that y is transformed to",0
work with the regression wrapper,0
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays,0
"note that this needs to happen after wrapping to generalize to the multi-class case,",0
since otherwise we'll have too many columns to be able to train a classifier,0
start with empty results and default shared insights,0
convert categorical indicators to numeric indices,0
check for indices over the categorical expansion bound,0
assume we'll be able to train former failures this time; we'll add them back if not,0
"can't remove in place while iterating over new_inds, so store in separate list",0
"train the model, but warn",0
no model can be trained in this case since we need more folds,0
"don't train a model, but suggest workaround since there are enough instances of least",1
populated class,0
also remove from train_inds so we don't try to access the result later,0
extract subset of names matching new columns,0
"track indices where an exception was thrown, since we can't remove from dictionary while iterating",0
don't want to cache this failed result,0
properties to return from effect InferenceResults,0
properties to return from PopulationSummaryResults,0
Converts strings to property lookups or method calls as a convenience so that the,0
_point_props and _summary_props above can be applied to an inference object,0
Create a summary combining all results into a single output; this is used,0
by the various causal_effect and causal_effect_dict methods to generate either a dataframe,0
"or a dictionary, respectively, based on the summary function passed into this method",0
"ensure array has shape (m,y,t)",0
population summary is missing sample dimension; add it for consistency,0
outcome dimension is missing; add it for consistency,0
add singleton treatment dimension if missing,0
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce,0
"each attr has dimension (m,y) or (m,y,t)",0
concatenate along treatment dimension,0
"for dictionary representation, want to remove unneeded sample dimension",0
in cohort and global results,0
TODO: enrich outcome logic for multi-class classification when that is supported,1
There is no actual sample level in this data,0
can't drop only level,0
should be serialization-ready and contain no numpy arrays,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
TODO: Note that there's no column metadata for the sample number - should there be?,1
"need to replicate the column info for each sample, then remove from the shared data",0
NOTE: the flattened order has the ouptut dimension before the feature dimension,0
which may need to be revisited once we support multiclass,0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
a global inference indicates the effect of that one feature on the outcome,0
need to reshape the output to match the input,0
we want to offset the inference object by the baseline estimate of y,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
"NOTE: this calculation is correct only if treatment costs are marginal costs,",0
because then scaling the difference between treatment value and treatment costs is the,0
same as scaling the treatment value and subtracting the scaled treatment cost.,0
,0
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for",0
"continuous treatments, the policy value should include the benefit of decreasing treatments",0
(rather than just not treating at all),0
,0
"We can get the total by seeing that if we restrict attention to units where we would treat,",0
2 * policy_value - always_treat,0
includes exactly their contribution because policy_value and always_treat both include it,0
"and likewise restricting attention to the units where we want to decrease treatment,",0
2 * policy_value - always-treat,0
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value",0
is zero and the contribution to always_treat is negative,0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
get dataframe with all but selected column,0
apply 10% of a typical treatment for this feature,0
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely",0
set the effect bounds; for positive treatments these agree with,0
"the estimates; for negative treatments, we need to invert the interval",0
the effect is now always positive since we decrease treatment when negative,0
"for discrete treatment, stack a zero result in front for control",0
we need to call effect_inference to get the correct CI between the two treatment options,0
we now need to construct the delta in the cost between the two treatments and translate the effect,0
remove third dimenions potentially added,0
"find cost of current treatment: equality creates a 2d array with True on each row,",0
only if its the location of the current treatment. Then we take the corresponding cost.,0
construct index of current treatment,0
add second dimension if needed for broadcasting during translation of effect,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values",0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Call weighted lasso on reduced design matrix,0
Weighted tau,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to WeightedLassoCV by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
set default values for None,0
check freq_weight should be integer and should be accompanied by sample_var,0
check array shape,0
weight X and y and sample_var,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
set default values for None,0
check array shape,0
check dimension of instruments is more than dimension of treatments,0
weight X and y,0
learn point estimate,0
solve first stage linear regression E[T|Z],0
"""that"" means T",0
solve second stage linear regression E[Y|that],0
(T.T*T)^{-1},0
learn cov(theta),0
(T.T*T)^{-1},0
sigma^2,0
reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
average the outcome dimension if it exists and ensure 2d y_pred,0
get index of best treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
d_t=None here since we measure the effect across all Ts,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: update docs,1
"NOTE: sample weight, sample var are not passed in",0
Compose final model,0
Calculate auxiliary quantities,0
X  T_res,0
"sum(model_final.predict(X, T_res))",0
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
Set _d_t to effective number of treatments,0
Required for bootstrap inference,0
for each mc iteration,0
for each model under cross fit setting,0
Handles the corner case when X=None but featurizer might be not None,0
Expand treatments for each time period,0
NOTE: important to use the _ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the _ortho_learner's copy to retain the information from fitting,0
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile",0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)",0
def mlasso_model(): return MultiTaskLassoCV(,0
"cv=3, alphas=alpha_regs, max_iter=200)",0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
"alpha_regs = [5e-3, 1e-2, 5e-2]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)",0
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)",0
subset of features that are exogenous and create heterogeneity,0
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features),0
subset of features wrt we estimate heterogeneity,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
introspect the constructor arguments to find the model parameters,0
to represent,0
"if the argument is deprecated, ignore it",0
Extract and sort argument names excluding 'self',0
column names,0
transfer input to numpy arrays,0
transfer input to 2d arrays,0
create dataframe,0
currently dowhy only support single outcome and single treatment,0
call dowhy,0
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update",0
cate estimator but not the effect.,0
don't proxy special methods,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
For when checking input values is disabled,0
Type to column extraction function,0
"Get number of arguments, some sklearn featurizer don't accept feature_names",0
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names',0
Get feature names using featurizer,0
All attempts at retrieving transformed feature names have failed,0
Delegate handling to downstream logic,0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
assume that we should perform nested cross-validation if and only if,0
the model has a 'cv' attribute; this is a somewhat brittle assumption...,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a 'groups' argument,0
now we have to compute the folds explicitly because some classifiers (like LassoCV),0
don't use the groups when calling split internally,0
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Any access should throw,0
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
input feature name is already updated by cate_feature_names.,0
define the index of d_x to filter for each given T,0
filter X after broadcast with T for each given T,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains some snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
make any access to matplotlib or plt throw an exception,0
make any access to graphviz or plt throw an exception,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
Initialize saturation & value; calculate chroma & value shift,0
Calculate some intermediate values,0
Initialize RGB with same hue & chroma as our color,0
Shift the initial RGB values to match value and store,0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use mean of targets,0
Write node mean CATE,0
Write node std of CATE,0
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
Fetch appropriate color for node,0
Write node mean CATE,0
Write node mean CATE,0
Write recommended treatment and value - cost,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
This flag is true when names are set in a child class instead,0
"If names are set in a child class, add an attribute reflecting that",0
This works only if X is passed as a kwarg,0
We plan to enforce X as kwarg only in future releases,0
This checks if names have been set in a child class,0
"If names were set in a child class, don't do it again",0
"Wraps-up fit by setting attributes, cleaning up, etc.",0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"Treatment names is None, default to BaseCateEstimator",0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
Get input names,0
Summary,0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
Get input names,0
Summary,0
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
remove None arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same,0
alteration even when we only want to fit_final.,0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)",0
"however, sklearn doesn't support both stratifying and grouping (see",0
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply",0
their own object that supports grouping if they want to use groups.,0
for each mc iteration,0
for each model under cross fit setting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Policy Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base Policy tree,0
=============================================================================,0
The values below are required and utilitized by methods in the _SingleTreeExporterMixin,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"since we only allow single dimensional y, we could flatten the prediction",0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Handles the corner case when X=None but featurizer might be not None,0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
Replacing to remove docstring,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE: important to use the rlearner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the rlearner's copy to retain the information from fitting,0
Handles the corner case when X=None but featurizer might be not None,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `LinearModelFinalInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support freq_weight and sample_var in debiased lasso,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add blb to parent's options,0
override only so that we can update the docstring to indicate,0
support for `GenericSingleTreatmentModelFinalInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
NOTE: important to get parent's wrapped copy so that,0
"after training wrapped featurizer is also trained, etc.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Fit a doubly robust average effect,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
"If custom param grid, check that only estimator parameters are being altered",0
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test",0
override only so that we can update the docstring to indicate support for `blb`,0
Get input names,0
Summary,0
Determine output settings,0
"Important: This must be the first invocation of the random state at fit time, so that",0
train/test splits are re-generatable from an external object simply by knowing the,0
random_state parameter of the tree. Can be useful in the future if one wants to create local,0
linear predictions. Currently is also useful for testing.,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
Set min_weight_leaf from min_weight_fraction_leaf,0
Build tree,0
We calculate the maximum number of samples from each half-split that any node in the tree can,0
hold. Used by criterion for memory space savings.,0
Initialize the criterion object and the criterion_val object if honest.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code is a fork from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
Set parameters,0
Don't instantiate estimators now! Parameters of base_estimator might,0
"still change. Eg., when grid-searching with the nested object syntax.",0
self.estimators_ needs to be filled by the derived classes in fit.,0
Compute the number of jobs,0
Partition estimators between jobs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
covariance matrix,0
get eigen value and eigen vectors,0
simulate eigen vectors,0
keep the top 4 eigen value and corresponding eigen vector,0
replace the negative eigen values,0
generate a new covariance matrix,0
get linear approximation of eigen values,0
coefs,0
get the indices of each group of features,0
print(ind_same_proxy),0
demo,0
same proxy,0
residuals,0
gmm,0
log normal on outliers,0
positive outliers,0
negative outliers,0
demean the new residual again,0
generate data,0
sample residuals,0
get prediction for current investment,0
get prediction for current proxy,0
get first period prediction,0
iterate the step ahead contruction,0
prepare new x,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
get new covariance matrix,0
get coefs,0
get residuals,0
proxy 1 is the outcome,0
make fixed residuals,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=None here since we measure the effect across all Ts,0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
squeeze the first axis,0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector,0
d_t=None here since we measure the effect across all Ts,0
d_t=None here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet",0
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
scale preds,0
scale std errs,0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
offset preds,0
"offset the distribution, too",0
scale preds,0
"scale the distribution, too",0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base GRF tree,0
=============================================================================,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
=============================================================================,0
A MultOutputWrapper for GRF classes,0
=============================================================================,0
=============================================================================,0
Instantiations of Generalized Random Forest,0
=============================================================================,0
"Append a constant treatment if `fit_intercept=True`, the coefficient",0
in front of the constant treatment is the intercept in the moment equation.,0
"Append a constant treatment and constant instrument if `fit_intercept=True`,",0
the coefficient in front of the constant treatment is the intercept in the moment equation.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Base Generalized Random Forest,0
=============================================================================,0
TODO: support freq_weight and sample_var,1
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle,0
We calculate the min eigenvalue proxy that each criterion is considering,0
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`",0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Generating indices a priori before parallelism ended up being orders of magnitude,0
faster than how sklearn does it. The reason is that random samplers do not release the,0
gil it seems.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
####################,0
Variance correction,0
####################,0
Subtract the average within bag variance. This ends up being equal to the,0
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).,0
The negative part is just sq_between.,0
Objective bayes debiasing for the diagonals where we know a-prior they are positive,0
"The off diagonals we have no objective prior, so no correction is applied.",0
Finally correcting the pred_cov or pred_var,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
test that the subsampling scheme past to the trees is correct,0
The sample size is chosen in particular to test rounding based error when subsampling,0
test that the estimator calcualtes var correctly,0
test api,0
test accuracy,0
test the projection functionality of forests,0
test that the estimator calcualtes var correctly,0
test api,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
test that the subsampling scheme past to the trees is correct,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
omit the lalonde notebook,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
for is_discrete in [False]:,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
No heterogeneity,0
Define indices to test,0
Heterogeneous effects,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
compare with weight,0
compare with weight,0
compare with weight,0
compare with weight,0
Multi-dimensional output y,0
1-d y,0
compare when both sample_var and sample_weight exist,0
multi-d y,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
dgp,0
StatsModels2SLS,0
IV2SLS,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
ensure alpha is passed,0
only is not None when T1 is a constant or a list of constant,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make any access to matplotlib or plt throw an exception,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
Invert indices to match latest API,0
Invert indices to match latest API,0
The feature for heterogeneity stays constant,0
Auxiliary function for adding xticks and vertical lines when plotting results,0
for dynamic dml vs ground truth parameters.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test at least one estimator from each category,0
test causal graph,0
test refutation estimate,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure we can serialize unfit estimator,0
ensure we can serialize fit estimator,0
expected effect size,0
test effect,0
test inference,0
only OrthoIV support inference other than bootstrap,0
test summary,0
test can run score,0
test cate_feature_names,0
test can run shap values,0
dgp,0
no heterogeneity,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure we can serialize unfit estimator,0
ensure we can serialize fit estimator,0
expected effect size,0
test effect,0
test inference,0
test can run score,0
test cate_feature_names,0
test can run shap values,0
"dgp (binary T, binary Z)",0
no heterogeneity,0
with heterogeneity,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
make sure we can get out post-fit stuff,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet",0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
helper class,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
continuous treatments have typical treatment values equal to,0
the mean of the absolute value of non-zero entries,0
discrete treatments have typical treatment value 1,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
continuous treatments have typical treatment values equal to,0
the mean of the absolute value of non-zero entries,0
discrete treatments have typical treatment value 1,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
make sure we don't run into problems dropping every index,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
dgp,0
model,0
model,0
"columns 'd', 'e', 'h' have too many values",0
"columns 'd', 'e' have too many values",0
lowering bound shouldn't affect already fit columns when warm starting,0
"column d is now okay, too",0
verify that we can use a scalar treatment cost,0
verify that we can specify per-treatment costs for each sample,0
verify that using the same state returns the same results each time,0
set the categories for column 'd' explicitly so that b is default,0
"first column: 10 ones, this is fine",0
"second column: 6 categories, plenty of random instances of each",0
this is fine only if we increase the cateogry limit,0
"third column: nine ones, lots of twos, not enough unless we disable check",0
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity",0
forest heterogeneity won't work,0
"sixth column: just 1 one, not enough even without check",0
increase bound on cat expansion,0
skip checks (reducing folds accordingly),0
"Add tests that guarantee that the reliance on DML feature order is not broken, such as",0
"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W",0
Pass an example where W is irrelevant and X is confounder,0
"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be",0
zeroed out and the test will fail,0
"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates",0
rescaling X shouldn't affect the first stage models because they normalize the inputs,0
"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid",0
cross terms,0
scale by 1000 to match the input to this model:,0
"the scale of X does matter for the final model, which keeps results in user-denominated units",0
rescaling X still shouldn't affect the first stage models,0
TODO: we don't recover the correct values with enough accuracy to enable this assertion,1
is there a different way to verify that we are learning the correct coefficients?,1
"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Define data features,0
Added `_df`to names to be different from the default cate_estimator names,0
Generate data,0
################################,0
Single treatment and outcome #,0
################################,0
Test LinearDML,0
|--> Test featurizers,0
ColumnTransformer doesn't propagate column names,0
|--> Test re-fit,0
Test SparseLinearDML,0
Test ForestDML,0
###################################,0
Mutiple treatments and outcomes #,0
###################################,0
Test LinearDML,0
Test SparseLinearDML,0
"Single outcome only, ORF does not support multiple outcomes",0
Test DMLOrthoForest,0
Test DROrthoForest,0
Test XLearner,0
Skipping population summary names test because bootstrap inference is too slow,0
Test SLearner,0
Test TLearner,0
Test LinearDRLearner,0
Test SparseLinearDRLearner,0
Test ForestDRLearner,0
Test LinearIntentToTreatDRIV,0
Test DeepIV,0
Test categorical treatments,0
Check refit,0
Check refit after setting categories,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
Treatment effect function,0
Outcome support,0
Treatment support,0
"Generate controls, covariates, treatments and outcomes",0
Heterogeneous treatment effects,0
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
through shap package.,0
test shap could generate the plot from the shap_values,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we need the controls' encoded column for training",0
"Thus, we append the controls column before the one-hot-encoded T",0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary",0
"fit on projected Z: E[T * E[T|X,Z]|X]",0
"if discrete, return shape (n,1); if continuous return shape (n,)",0
"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)",0
"shape (n,)",0
"shape (n,)",0
"shape(n,)",0
TODO: prel_model_effect could allow sample_var and freq_weight?,1
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary",0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"if discrete, return shape (n,1); if continuous return shape (n,)",0
target will be discrete and will be inversed from FirstStageWrapper,0
"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous",0
reshape the predictions,0
concat W and Z,0
check nuisances outcome shape,0
Y_res could be a vector or 1-dimensional 2d-array,0
"all could be reshaped to vector since Y, T, Z are all single dimensional.",0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
A helper class that access all the internal fitted objects of a DRIV Cate Estimator.,0
Used by both DRIV and IntentToTreatDRIV.,0
Maggie: I think that would be the case?,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE: important to use the ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
Handles the corner case when X=None but featurizer might be not None,0
NOTE This is used by the inference methods and is more for internal use to the library,0
this is a regression model since proj_t is probability,0
outcome is continuous since proj_t is probability,0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
concat W and Z,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
concat W and Z,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
reshape the predictions,0
"T_res, Z_res, beta expect shape to be (n,1)",0
maybe shouldn't expose fit_cate_intercept in this class?,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: do correct adjustment for sample_var,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
concat W and Z,0
concat W and Z,0
concat W and Z,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
"train E[T|X,W,Z]",0
"train [Z|X,W]",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE: important to use the ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
Handles the corner case when X=None but featurizer might be not None,0
NOTE This is used by the inference methods and is more for internal use to the library,0
concat W and Z,0
note that groups are not passed to score because they are only used for fitting,0
concat W and Z,0
note that sample_weight and groups are not passed to predict because they are only used for fitting,0
concat W and Z,0
A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.,0
Used by both Parametric and Non Parametric DMLIV.,0
override only so that we can enforce Z to be required,0
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Handles the corner case when X=None but featurizer might be not None,0
Get input names,0
Summary,0
coefficient,0
intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: this utility is documented but internal; reimplement?,1
TODO: this utility is even less public...,1
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged",0
use same Cs as would be used by default by LogisticRegressionCV,0
NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification,0
which could affect how many times each distinct Y value needs to be present in the data,0
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns,0
but also supports get_feature_names with expected signature,0
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value,0
NOTE: we rely on the passthrough columns coming first in the concatenated X;W,0
"when we pipeline scaling with our first stage models later, so the order here is important",0
Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy,0
Convert python objects to (possibly nested) types that can easily be represented as literals,0
Convert SingleTreeInterpreter to a python dictionary,0
named tuple type for storing results inside CausalAnalysis class;,0
must be lifted to module level to enable pickling,0
"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,",1
"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all",0
"when running the first stage models, but don't want to scale the X columns when running the final model,",0
since then our coefficients will have odd units and our trees will also have decisions using those units.,0
,0
"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)",0
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names,0
Controls are all other columns of X,0
"can't use X[:, feat_ind] when X is a DataFrame",0
TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
we might want to modify OrthoLearner (and other discrete treatment classes),0
so that the user can opt-in to allowing unseen treatment values,0
(and return NaN or something in that case),0
HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models,1
and so we can just peel the first columns off of that combined array for rescaling in the pipeline,0
TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are,1
"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this",0
becomes a valid approach to handling this,0
array checking routines don't accept 0-width arrays,0
perform model selection,0
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative,0
convert to NormalInferenceResults for consistency,0
Set the dictionary values shared between local and global summaries,0
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments",0
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category",0
required to fit a discrete DML model,0
Validate inputs,0
TODO: check compatibility of X and Y lengths,1
"no previous fit, cancel warm start",0
"work with numeric feature indices, so that we can easily compare with categorical ones",0
"if heterogeneity_inds is 1D, repeat it",0
heterogeneity inds should be a 2D list of length same as train_inds,0
replace None elements of heterogeneity_inds and ensure indices are numeric,0
"TODO: bail out also if categorical columns, classification, random_state changed?",1
TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
train the Y model,0
"perform model selection for the Y model using all X, not on a per-column basis",0
"now that we've trained the classifier and wrapped it, ensure that y is transformed to",0
work with the regression wrapper,0
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays,0
"note that this needs to happen after wrapping to generalize to the multi-class case,",0
since otherwise we'll have too many columns to be able to train a classifier,0
start with empty results and default shared insights,0
convert categorical indicators to numeric indices,0
check for indices over the categorical expansion bound,0
assume we'll be able to train former failures this time; we'll add them back if not,0
"can't remove in place while iterating over new_inds, so store in separate list",0
"train the model, but warn",0
no model can be trained in this case since we need more folds,0
"don't train a model, but suggest workaround since there are enough instances of least",1
populated class,0
also remove from train_inds so we don't try to access the result later,0
extract subset of names matching new columns,0
"track indices where an exception was thrown, since we can't remove from dictionary while iterating",0
don't want to cache this failed result,0
properties to return from effect InferenceResults,0
properties to return from PopulationSummaryResults,0
Converts strings to property lookups or method calls as a convenience so that the,0
_point_props and _summary_props above can be applied to an inference object,0
Create a summary combining all results into a single output; this is used,0
by the various causal_effect and causal_effect_dict methods to generate either a dataframe,0
"or a dictionary, respectively, based on the summary function passed into this method",0
"ensure array has shape (m,y,t)",0
population summary is missing sample dimension; add it for consistency,0
outcome dimension is missing; add it for consistency,0
add singleton treatment dimension if missing,0
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce,0
"each attr has dimension (m,y) or (m,y,t)",0
concatenate along treatment dimension,0
"for dictionary representation, want to remove unneeded sample dimension",0
in cohort and global results,0
TODO: enrich outcome logic for multi-class classification when that is supported,1
There is no actual sample level in this data,0
can't drop only level,0
should be serialization-ready and contain no numpy arrays,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
TODO: Note that there's no column metadata for the sample number - should there be?,1
"need to replicate the column info for each sample, then remove from the shared data",0
NOTE: the flattened order has the ouptut dimension before the feature dimension,0
which may need to be revisited once we support multiclass,0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
a global inference indicates the effect of that one feature on the outcome,0
need to reshape the output to match the input,0
we want to offset the inference object by the baseline estimate of y,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
"NOTE: this calculation is correct only if treatment costs are marginal costs,",0
because then scaling the difference between treatment value and treatment costs is the,0
same as scaling the treatment value and subtracting the scaled treatment cost.,0
,0
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for",0
"continuous treatments, the policy value should include the benefit of decreasing treatments",0
(rather than just not treating at all),0
,0
"We can get the total by seeing that if we restrict attention to units where we would treat,",0
2 * policy_value - always_treat,0
includes exactly their contribution because policy_value and always_treat both include it,0
"and likewise restricting attention to the units where we want to decrease treatment,",0
2 * policy_value - always-treat,0
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value",0
is zero and the contribution to always_treat is negative,0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
get dataframe with all but selected column,0
apply 10% of a typical treatment for this feature,0
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely",0
set the effect bounds; for positive treatments these agree with,0
"the estimates; for negative treatments, we need to invert the interval",0
the effect is now always positive since we decrease treatment when negative,0
"for discrete treatment, stack a zero result in front for control",0
we need to call effect_inference to get the correct CI between the two treatment options,0
we now need to construct the delta in the cost between the two treatments and translate the effect,0
remove third dimenions potentially added,0
"find cost of current treatment: equality creates a 2d array with True on each row,",0
only if its the location of the current treatment. Then we take the corresponding cost.,0
construct index of current treatment,0
add second dimension if needed for broadcasting during translation of effect,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values",0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Call weighted lasso on reduced design matrix,0
Weighted tau,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to WeightedLassoCV by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
set default values for None,0
check freq_weight should be integer and should be accompanied by sample_var,0
check array shape,0
weight X and y and sample_var,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
set default values for None,0
check array shape,0
check dimension of instruments is more than dimension of treatments,0
weight X and y,0
learn point estimate,0
solve first stage linear regression E[T|Z],0
"""that"" means T",0
solve second stage linear regression E[Y|that],0
(T.T*T)^{-1},0
learn cov(theta),0
(T.T*T)^{-1},0
sigma^2,0
reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
average the outcome dimension if it exists and ensure 2d y_pred,0
get index of best treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
d_t=None here since we measure the effect across all Ts,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: update docs,1
"NOTE: sample weight, sample var are not passed in",0
Compose final model,0
Calculate auxiliary quantities,0
X  T_res,0
"sum(model_final.predict(X, T_res))",0
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
Set _d_t to effective number of treatments,0
Required for bootstrap inference,0
for each mc iteration,0
for each model under cross fit setting,0
Handles the corner case when X=None but featurizer might be not None,0
Expand treatments for each time period,0
NOTE: important to use the _ortho_learner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the _ortho_learner's copy to retain the information from fitting,0
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile",0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)",0
def mlasso_model(): return MultiTaskLassoCV(,0
"cv=3, alphas=alpha_regs, max_iter=200)",0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
"alpha_regs = [5e-3, 1e-2, 5e-2]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)",0
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)",0
subset of features that are exogenous and create heterogeneity,0
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features),0
subset of features wrt we estimate heterogeneity,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
introspect the constructor arguments to find the model parameters,0
to represent,0
"if the argument is deprecated, ignore it",0
Extract and sort argument names excluding 'self',0
column names,0
transfer input to numpy arrays,0
transfer input to 2d arrays,0
create dataframe,0
currently dowhy only support single outcome and single treatment,0
call dowhy,0
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update",0
cate estimator but not the effect.,0
don't proxy special methods,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
For when checking input values is disabled,0
Type to column extraction function,0
"Get number of arguments, some sklearn featurizer don't accept feature_names",0
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names',0
Get feature names using featurizer,0
All attempts at retrieving transformed feature names have failed,0
Delegate handling to downstream logic,0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
assume that we should perform nested cross-validation if and only if,0
the model has a 'cv' attribute; this is a somewhat brittle assumption...,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a 'groups' argument,0
now we have to compute the folds explicitly because some classifiers (like LassoCV),0
don't use the groups when calling split internally,0
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Any access should throw,0
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
input feature name is already updated by cate_feature_names.,0
define the index of d_x to filter for each given T,0
filter X after broadcast with T for each given T,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains some snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
make any access to matplotlib or plt throw an exception,0
make any access to graphviz or plt throw an exception,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
Initialize saturation & value; calculate chroma & value shift,0
Calculate some intermediate values,0
Initialize RGB with same hue & chroma as our color,0
Shift the initial RGB values to match value and store,0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use mean of targets,0
Write node mean CATE,0
Write node std of CATE,0
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
Fetch appropriate color for node,0
Write node mean CATE,0
Write node mean CATE,0
Write recommended treatment and value - cost,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
This flag is true when names are set in a child class instead,0
"If names are set in a child class, add an attribute reflecting that",0
This works only if X is passed as a kwarg,0
We plan to enforce X as kwarg only in future releases,0
This checks if names have been set in a child class,0
"If names were set in a child class, don't do it again",0
"Wraps-up fit by setting attributes, cleaning up, etc.",0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"Treatment names is None, default to BaseCateEstimator",0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
Get input names,0
Summary,0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
Get input names,0
Summary,0
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
remove None arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same,0
alteration even when we only want to fit_final.,0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)",0
"however, sklearn doesn't support both stratifying and grouping (see",0
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply",0
their own object that supports grouping if they want to use groups.,0
for each mc iteration,0
for each model under cross fit setting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Policy Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base Policy tree,0
=============================================================================,0
The values below are required and utilitized by methods in the _SingleTreeExporterMixin,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"since we only allow single dimensional y, we could flatten the prediction",0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Handles the corner case when X=None but featurizer might be not None,0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
Replacing to remove docstring,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE: important to use the rlearner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the rlearner's copy to retain the information from fitting,0
Handles the corner case when X=None but featurizer might be not None,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support freq_weight and sample_var in debiased lasso,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add blb to parent's options,0
override only so that we can update the docstring to indicate,0
support for `GenericSingleTreatmentModelFinalInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
NOTE: important to get parent's wrapped copy so that,0
"after training wrapped featurizer is also trained, etc.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Fit a doubly robust average effect,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
"If custom param grid, check that only estimator parameters are being altered",0
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test",0
override only so that we can update the docstring to indicate support for `blb`,0
Get input names,0
Summary,0
Determine output settings,0
"Important: This must be the first invocation of the random state at fit time, so that",0
train/test splits are re-generatable from an external object simply by knowing the,0
random_state parameter of the tree. Can be useful in the future if one wants to create local,0
linear predictions. Currently is also useful for testing.,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
Set min_weight_leaf from min_weight_fraction_leaf,0
Build tree,0
We calculate the maximum number of samples from each half-split that any node in the tree can,0
hold. Used by criterion for memory space savings.,0
Initialize the criterion object and the criterion_val object if honest.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code is a fork from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
Set parameters,0
Don't instantiate estimators now! Parameters of base_estimator might,0
"still change. Eg., when grid-searching with the nested object syntax.",0
self.estimators_ needs to be filled by the derived classes in fit.,0
Compute the number of jobs,0
Partition estimators between jobs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=None here since we measure the effect across all Ts,0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
squeeze the first axis,0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector,0
d_t=None here since we measure the effect across all Ts,0
d_t=None here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet",0
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
scale preds,0
scale std errs,0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
offset preds,0
"offset the distribution, too",0
scale preds,0
"scale the distribution, too",0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base GRF tree,0
=============================================================================,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
=============================================================================,0
A MultOutputWrapper for GRF classes,0
=============================================================================,0
=============================================================================,0
Instantiations of Generalized Random Forest,0
=============================================================================,0
"Append a constant treatment if `fit_intercept=True`, the coefficient",0
in front of the constant treatment is the intercept in the moment equation.,0
"Append a constant treatment and constant instrument if `fit_intercept=True`,",0
the coefficient in front of the constant treatment is the intercept in the moment equation.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Base Generalized Random Forest,0
=============================================================================,0
TODO: support freq_weight and sample_var,1
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle,0
We calculate the min eigenvalue proxy that each criterion is considering,0
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`",0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Generating indices a priori before parallelism ended up being orders of magnitude,0
faster than how sklearn does it. The reason is that random samplers do not release the,0
gil it seems.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
####################,0
Variance correction,0
####################,0
Subtract the average within bag variance. This ends up being equal to the,0
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).,0
The negative part is just sq_between.,0
Objective bayes debiasing for the diagonals where we know a-prior they are positive,0
"The off diagonals we have no objective prior, so no correction is applied.",0
Finally correcting the pred_cov or pred_var,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
test that the subsampling scheme past to the trees is correct,0
The sample size is chosen in particular to test rounding based error when subsampling,0
test that the estimator calcualtes var correctly,0
test api,0
test accuracy,0
test the projection functionality of forests,0
test that the estimator calcualtes var correctly,0
test api,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
test that the subsampling scheme past to the trees is correct,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
omit the lalonde notebook,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
compare with weight,0
compare with weight,0
compare with weight,0
compare with weight,0
Multi-dimensional output y,0
1-d y,0
compare when both sample_var and sample_weight exist,0
multi-d y,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
only is not None when T1 is a constant or a list of constant,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test at least one estimator from each category,0
test causal graph,0
test refutation estimate,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure that we've got at least two of every row,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
need to make sure we get all *joint* combinations,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat requires X,0
ensure we can serialize unfit estimator,0
these support only W but not X,0
"these support only binary, not general discrete T and Z",0
ensure we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
TODO: add tests for extra properties like coef_ where they exist,1
TODO: add tests for extra properties like coef_ where they exist,1
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
"however, with custom splits the checking happens in the first stage wrapper",0
where we don't have all of the required information to do this;,0
we'd probably need to add it to _crossfit instead,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
make sure we can get out post-fit stuff,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet",0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
helper class,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
continuous treatments have typical treatment values equal to,0
the mean of the absolute value of non-zero entries,0
discrete treatments have typical treatment value 1,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
continuous treatments have typical treatment values equal to,0
the mean of the absolute value of non-zero entries,0
discrete treatments have typical treatment value 1,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
make sure we don't run into problems dropping every index,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
dgp,0
model,0
model,0
"columns 'd', 'e', 'h' have too many values",0
"columns 'd', 'e' have too many values",0
lowering bound shouldn't affect already fit columns when warm starting,0
"column d is now okay, too",0
verify that we can use a scalar treatment cost,0
verify that we can specify per-treatment costs for each sample,0
verify that using the same state returns the same results each time,0
set the categories for column 'd' explicitly so that b is default,0
"first column: 10 ones, this is fine",0
"second column: 6 categories, plenty of random instances of each",0
this is fine only if we increase the cateogry limit,0
"third column: nine ones, lots of twos, not enough unless we disable check",0
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity",0
forest heterogeneity won't work,0
"sixth column: just 1 one, not enough even without check",0
increase bound on cat expansion,0
skip checks (reducing folds accordingly),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Define data features,0
Added `_df`to names to be different from the default cate_estimator names,0
Generate data,0
################################,0
Single treatment and outcome #,0
################################,0
Test LinearDML,0
|--> Test featurizers,0
ColumnTransformer doesn't propagate column names,0
|--> Test re-fit,0
Test SparseLinearDML,0
Test ForestDML,0
###################################,0
Mutiple treatments and outcomes #,0
###################################,0
Test LinearDML,0
Test SparseLinearDML,0
"Single outcome only, ORF does not support multiple outcomes",0
Test DMLOrthoForest,0
Test DROrthoForest,0
Test XLearner,0
Skipping population summary names test because bootstrap inference is too slow,0
Test SLearner,0
Test TLearner,0
Test LinearDRLearner,0
Test SparseLinearDRLearner,0
Test ForestDRLearner,0
Test LinearIntentToTreatDRIV,0
Test DeepIV,0
Test categorical treatments,0
Check refit,0
Check refit after setting categories,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
Treatment effect function,0
Outcome support,0
Treatment support,0
"Generate controls, covariates, treatments and outcomes",0
Heterogeneous treatment effects,0
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
through shap package.,0
test shap could generate the plot from the shap_values,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we need the controls' encoded column for training",0
"Thus, we append the controls column before the one-hot-encoded T",0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages",0
output is,0
"* a column of ones if X, W, and Z are all None",0
* just X or W or Z if both of the others are None,0
* hstack([arrs]) for whatever subset are not None otherwise,0
ensure Z is 2D,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
"instruments, and outcomes",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"TODO: check that Y, T, Z do not have multiple columns",0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: do correct adjustment for sample_var,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res",0
TODO: allow the final model to actually use X? Then we'd need to rename the class,1
since we would actually be calculating a CATE rather than ATE.,0
TODO: allow the final model to actually use X?,1
TODO: allow the final model to actually use X?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring",0
TODO: would it be useful to extend to handle controls ala vanilla DML?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: this utility is documented but internal; reimplement?,1
TODO: this utility is even less public...,1
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged",0
use same Cs as would be used by default by LogisticRegressionCV,0
NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification,0
which could affect how many times each distinct Y value needs to be present in the data,0
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns,0
but also supports get_feature_names with expected signature,0
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value,0
Convert python objects to (possibly nested) types that can easily be represented as literals,0
Convert SingleTreeInterpreter to a python dictionary,0
named tuple type for storing results inside CausalAnalysis class;,0
must be lifted to module level to enable pickling,0
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names,0
Controls are all other columns of X,0
"can't use X[:, feat_ind] when X is a DataFrame",0
TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
we might want to modify OrthoLearner (and other discrete treatment classes),0
so that the user can opt-in to allowing unseen treatment values,0
(and return NaN or something in that case),0
array checking routines don't accept 0-width arrays,0
perform model selection,0
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative,0
convert to NormalInferenceResults for consistency,0
Set the dictionary values shared between local and global summaries,0
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments",0
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category",0
required to fit a discrete DML model,0
Validate inputs,0
TODO: check compatibility of X and Y lengths,1
"no previous fit, cancel warm start",0
"work with numeric feature indices, so that we can easily compare with categorical ones",0
"if heterogeneity_inds is 1D, repeat it",0
heterogeneity inds should be a 2D list of length same as train_inds,0
replace None elements of heterogeneity_inds and ensure indices are numeric,0
"TODO: bail out also if categorical columns, classification, random_state changed?",1
TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
train the Y model,0
"perform model selection for the Y model using all X, not on a per-column basis",0
"now that we've trained the classifier and wrapped it, ensure that y is transformed to",0
work with the regression wrapper,0
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays,0
"note that this needs to happen after wrapping to generalize to the multi-class case,",0
since otherwise we'll have too many columns to be able to train a classifier,0
start with empty results and default shared insights,0
convert categorical indicators to numeric indices,0
check for indices over the categorical expansion bound,0
assume we'll be able to train former failures this time; we'll add them back if not,0
"can't remove in place while iterating over new_inds, so store in separate list",0
"train the model, but warn",0
no model can be trained in this case since we need more folds,0
"don't train a model, but suggest workaround since there are enough instances of least",1
populated class,0
also remove from train_inds so we don't try to access the result later,0
extract subset of names matching new columns,0
"track indices where an exception was thrown, since we can't remove from dictionary while iterating",0
don't want to cache this failed result,0
properties to return from effect InferenceResults,0
properties to return from PopulationSummaryResults,0
Converts strings to property lookups or method calls as a convenience so that the,0
_point_props and _summary_props above can be applied to an inference object,0
Create a summary combining all results into a single output; this is used,0
by the various causal_effect and causal_effect_dict methods to generate either a dataframe,0
"or a dictionary, respectively, based on the summary function passed into this method",0
"ensure array has shape (m,y,t)",0
population summary is missing sample dimension; add it for consistency,0
outcome dimension is missing; add it for consistency,0
add singleton treatment dimension if missing,0
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce,0
"each attr has dimension (m,y) or (m,y,t)",0
concatenate along treatment dimension,0
"for dictionary representation, want to remove unneeded sample dimension",0
in cohort and global results,0
TODO: enrich outcome logic for multi-class classification when that is supported,1
There is no actual sample level in this data,0
can't drop only level,0
should be serialization-ready and contain no numpy arrays,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
TODO: Note that there's no column metadata for the sample number - should there be?,1
"need to replicate the column info for each sample, then remove from the shared data",0
NOTE: the flattened order has the ouptut dimension before the feature dimension,0
which may need to be revisited once we support multiclass,0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
a global inference indicates the effect of that one feature on the outcome,0
need to reshape the output to match the input,0
we want to offset the inference object by the baseline estimate of y,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
"NOTE: this calculation is correct only if treatment costs are marginal costs,",0
because then scaling the difference between treatment value and treatment costs is the,0
same as scaling the treatment value and subtracting the scaled treatment cost.,0
,0
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for",0
"continuous treatments, the policy value should include the benefit of decreasing treatments",0
(rather than just not treating at all),0
,0
"We can get the total by seeing that if we restrict attention to units where we would treat,",0
2 * policy_value - always_treat,0
includes exactly their contribution because policy_value and always_treat both include it,0
"and likewise restricting attention to the units where we want to decrease treatment,",0
2 * policy_value - always-treat,0
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value",0
is zero and the contribution to always_treat is negative,0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
get dataframe with all but selected column,0
apply 10% of a typical treatment for this feature,0
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely",0
set the effect bounds; for positive treatments these agree with,0
"the estimates; for negative treatments, we need to invert the interval",0
the effect is now always positive since we decrease treatment when negative,0
"for discrete treatment, stack a zero result in front for control",0
we need to call effect_inference to get the correct CI between the two treatment options,0
we now need to construct the delta in the cost between the two treatments and translate the effect,0
remove third dimenions potentially added,0
"find cost of current treatment: equality creates a 2d array with True on each row,",0
only if its the location of the current treatment. Then we take the corresponding cost.,0
construct index of current treatment,0
add second dimension if needed for broadcasting during translation of effect,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values",0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Call weighted lasso on reduced design matrix,0
Weighted tau,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to WeightedLassoCV by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
set default values for None,0
check freq_weight should be integer and should be accompanied by sample_var,0
check array shape,0
weight X and y and sample_var,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
average the outcome dimension if it exists and ensure 2d y_pred,0
get index of best treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
d_t=None here since we measure the effect across all Ts,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile",0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)",0
def mlasso_model(): return MultiTaskLassoCV(,0
"cv=3, alphas=alpha_regs, max_iter=200)",0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
"alpha_regs = [5e-3, 1e-2, 5e-2]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)",0
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)",0
subset of features that are exogenous and create heterogeneity,0
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features),0
subset of features wrt we estimate heterogeneity,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
introspect the constructor arguments to find the model parameters,0
to represent,0
"if the argument is deprecated, ignore it",0
Extract and sort argument names excluding 'self',0
column names,0
transfer input to numpy arrays,0
transfer input to 2d arrays,0
create dataframe,0
currently dowhy only support single outcome and single treatment,0
call dowhy,0
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update",0
cate estimator but not the effect.,0
don't proxy special methods,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
For when checking input values is disabled,0
Type to column extraction function,0
"Get number of arguments, some sklearn featurizer don't accept feature_names",0
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names',0
Get feature names using featurizer,0
All attempts at retrieving transformed feature names have failed,0
Delegate handling to downstream logic,0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
assume that we should perform nested cross-validation if and only if,0
the model has a 'cv' attribute; this is a somewhat brittle assumption...,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a 'groups' argument,0
now we have to compute the folds explicitly because some classifiers (like LassoCV),0
don't use the groups when calling split internally,0
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Any access should throw,0
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
input feature name is already updated by cate_feature_names.,0
define the index of d_x to filter for each given T,0
filter X after broadcast with T for each given T,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains some snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
make any access to matplotlib or plt throw an exception,0
make any access to graphviz or plt throw an exception,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
Initialize saturation & value; calculate chroma & value shift,0
Calculate some intermediate values,0
Initialize RGB with same hue & chroma as our color,0
Shift the initial RGB values to match value and store,0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use mean of targets,0
Write node mean CATE,0
Write node std of CATE,0
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
Fetch appropriate color for node,0
Write node mean CATE,0
Write node mean CATE,0
Write recommended treatment and value - cost,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
This flag is true when names are set in a child class instead,0
"If names are set in a child class, add an attribute reflecting that",0
This works only if X is passed as a kwarg,0
We plan to enforce X as kwarg only in future releases,0
This checks if names have been set in a child class,0
"If names were set in a child class, don't do it again",0
"Wraps-up fit by setting attributes, cleaning up, etc.",0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"Treatment names is None, default to BaseCateEstimator",0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
Get input names,0
Summary,0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
Get input names,0
Summary,0
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
remove None arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same,0
alteration even when we only want to fit_final.,0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)",0
"however, sklearn doesn't support both stratifying and grouping (see",0
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply",0
their own object that supports grouping if they want to use groups.,0
for each mc iteration,0
for each model under cross fit setting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Policy Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base Policy tree,0
=============================================================================,0
The values below are required and utilitized by methods in the _SingleTreeExporterMixin,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"since we only allow single dimensional y, we could flatten the prediction",0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Handles the corner case when X=None but featurizer might be not None,0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
Replacing to remove docstring,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE: important to use the rlearner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the rlearner's copy to retain the information from fitting,0
Handles the corner case when X=None but featurizer might be not None,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support freq_weight and sample_var in debiased lasso,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add blb to parent's options,0
override only so that we can update the docstring to indicate,0
support for `GenericSingleTreatmentModelFinalInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
NOTE: important to get parent's wrapped copy so that,0
"after training wrapped featurizer is also trained, etc.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Fit a doubly robust average effect,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
"If custom param grid, check that only estimator parameters are being altered",0
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test",0
override only so that we can update the docstring to indicate support for `blb`,0
Get input names,0
Summary,0
Determine output settings,0
"Important: This must be the first invocation of the random state at fit time, so that",0
train/test splits are re-generatable from an external object simply by knowing the,0
random_state parameter of the tree. Can be useful in the future if one wants to create local,0
linear predictions. Currently is also useful for testing.,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
Set min_weight_leaf from min_weight_fraction_leaf,0
Build tree,0
We calculate the maximum number of samples from each half-split that any node in the tree can,0
hold. Used by criterion for memory space savings.,0
Initialize the criterion object and the criterion_val object if honest.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code is a fork from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
Set parameters,0
Don't instantiate estimators now! Parameters of base_estimator might,0
"still change. Eg., when grid-searching with the nested object syntax.",0
self.estimators_ needs to be filled by the derived classes in fit.,0
Compute the number of jobs,0
Partition estimators between jobs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=None here since we measure the effect across all Ts,0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
squeeze the first axis,0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector,0
d_t=None here since we measure the effect across all Ts,0
d_t=None here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet",0
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
scale preds,0
scale std errs,0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
offset preds,0
"offset the distribution, too",0
scale preds,0
"scale the distribution, too",0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base GRF tree,0
=============================================================================,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
=============================================================================,0
A MultOutputWrapper for GRF classes,0
=============================================================================,0
=============================================================================,0
Instantiations of Generalized Random Forest,0
=============================================================================,0
"Append a constant treatment if `fit_intercept=True`, the coefficient",0
in front of the constant treatment is the intercept in the moment equation.,0
"Append a constant treatment and constant instrument if `fit_intercept=True`,",0
the coefficient in front of the constant treatment is the intercept in the moment equation.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Base Generalized Random Forest,0
=============================================================================,0
TODO: support freq_weight and sample_var,1
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle,0
We calculate the min eigenvalue proxy that each criterion is considering,0
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`",0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Generating indices a priori before parallelism ended up being orders of magnitude,0
faster than how sklearn does it. The reason is that random samplers do not release the,0
gil it seems.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
####################,0
Variance correction,0
####################,0
Subtract the average within bag variance. This ends up being equal to the,0
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).,0
The negative part is just sq_between.,0
Objective bayes debiasing for the diagonals where we know a-prior they are positive,0
"The off diagonals we have no objective prior, so no correction is applied.",0
Finally correcting the pred_cov or pred_var,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
test that the subsampling scheme past to the trees is correct,0
The sample size is chosen in particular to test rounding based error when subsampling,0
test that the estimator calcualtes var correctly,0
test api,0
test accuracy,0
test the projection functionality of forests,0
test that the estimator calcualtes var correctly,0
test api,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
test that the subsampling scheme past to the trees is correct,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
omit the lalonde notebook,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
compare with weight,0
compare with weight,0
compare with weight,0
compare with weight,0
Multi-dimensional output y,0
1-d y,0
compare when both sample_var and sample_weight exist,0
multi-d y,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
only is not None when T1 is a constant or a list of constant,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test at least one estimator from each category,0
test causal graph,0
test refutation estimate,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure that we've got at least two of every row,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
need to make sure we get all *joint* combinations,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat requires X,0
ensure we can serialize unfit estimator,0
these support only W but not X,0
"these support only binary, not general discrete T and Z",0
ensure we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
TODO: add tests for extra properties like coef_ where they exist,1
TODO: add tests for extra properties like coef_ where they exist,1
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
"however, with custom splits the checking happens in the first stage wrapper",0
where we don't have all of the required information to do this;,0
we'd probably need to add it to _crossfit instead,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
make sure we can get out post-fit stuff,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet",0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
helper class,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
make sure we don't run into problems dropping every index,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
dgp,0
model,0
model,0
"columns 'd', 'e', 'h' have too many values",0
"columns 'd', 'e' have too many values",0
lowering bound shouldn't affect already fit columns when warm starting,0
"column d is now okay, too",0
verify that we can use a scalar treatment cost,0
verify that we can specify per-treatment costs for each sample,0
verify that using the same state returns the same results each time,0
set the categories for column 'd' explicitly so that b is default,0
"first column: 10 ones, this is fine",0
"second column: 6 categories, plenty of random instances of each",0
this is fine only if we increase the cateogry limit,0
"third column: nine ones, lots of twos, not enough unless we disable check",0
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity",0
forest heterogeneity won't work,0
"sixth column: just 1 one, not enough even without check",0
increase bound on cat expansion,0
skip checks (reducing folds accordingly),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Define data features,0
Added `_df`to names to be different from the default cate_estimator names,0
Generate data,0
################################,0
Single treatment and outcome #,0
################################,0
Test LinearDML,0
|--> Test featurizers,0
ColumnTransformer doesn't propagate column names,0
|--> Test re-fit,0
Test SparseLinearDML,0
Test ForestDML,0
###################################,0
Mutiple treatments and outcomes #,0
###################################,0
Test LinearDML,0
Test SparseLinearDML,0
"Single outcome only, ORF does not support multiple outcomes",0
Test DMLOrthoForest,0
Test DROrthoForest,0
Test XLearner,0
Skipping population summary names test because bootstrap inference is too slow,0
Test SLearner,0
Test TLearner,0
Test LinearDRLearner,0
Test SparseLinearDRLearner,0
Test ForestDRLearner,0
Test LinearIntentToTreatDRIV,0
Test DeepIV,0
Test categorical treatments,0
Check refit,0
Check refit after setting categories,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
Treatment effect function,0
Outcome support,0
Treatment support,0
"Generate controls, covariates, treatments and outcomes",0
Heterogeneous treatment effects,0
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
through shap package.,0
test shap could generate the plot from the shap_values,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we need the controls' encoded column for training",0
"Thus, we append the controls column before the one-hot-encoded T",0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages",0
output is,0
"* a column of ones if X, W, and Z are all None",0
* just X or W or Z if both of the others are None,0
* hstack([arrs]) for whatever subset are not None otherwise,0
ensure Z is 2D,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
"instruments, and outcomes",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"TODO: check that Y, T, Z do not have multiple columns",0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: do correct adjustment for sample_var,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res",0
TODO: allow the final model to actually use X? Then we'd need to rename the class,1
since we would actually be calculating a CATE rather than ATE.,0
TODO: allow the final model to actually use X?,1
TODO: allow the final model to actually use X?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring",0
TODO: would it be useful to extend to handle controls ala vanilla DML?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: this utility is documented but internal; reimplement?,1
TODO: this utility is even less public...,1
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged",0
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns,0
but also supports get_feature_names with expected signature,0
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value,0
Convert python objects to (possibly nested) types that can easily be represented as literals,0
Convert SingleTreeInterpreter to a python dictionary,0
named tuple type for storing results inside CausalAnalysis class;,0
must be lifted to module level to enable pickling,0
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names,0
Controls are all other columns of X,0
"can't use X[:, feat_ind] when X is a DataFrame",0
TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
we might want to modify OrthoLearner (and other discrete treatment classes),0
so that the user can opt-in to allowing unseen treatment values,0
(and return NaN or something in that case),0
array checking routines don't accept 0-width arrays,0
perform model selection,0
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative,0
convert to NormalInferenceResults for consistency,0
Set the dictionary values shared between local and global summaries,0
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments",0
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category",0
required to fit a discrete DML model,0
Validate inputs,0
TODO: check compatibility of X and Y lengths,1
"no previous fit, cancel warm start",0
"work with numeric feature indices, so that we can easily compare with categorical ones",0
"if heterogeneity_inds is 1D, repeat it",0
heterogeneity inds should be a 2D list of length same as train_inds,0
replace None elements of heterogeneity_inds and ensure indices are numeric,0
"TODO: bail out also if categorical columns, classification, random_state changed?",1
TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
train the Y model,0
"perform model selection for the Y model using all X, not on a per-column basis",0
"now that we've trained the classifier and wrapped it, ensure that y is transformed to",0
work with the regression wrapper,0
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays,0
"note that this needs to happen after wrapping to generalize to the multi-class case,",0
since otherwise we'll have too many columns to be able to train a classifier,0
start with empty results and default shared insights,0
convert categorical indicators to numeric indices,0
check for indices over the categorical expansion bound,0
assume we'll be able to train former failures this time; we'll add them back if not,0
"can't remove in place while iterating over new_inds, so store in separate list",0
"train the model, but warn",0
no model can be trained in this case since we need more folds,0
"don't train a model, but suggest workaround since there are enough instances of least",1
populated class,0
also remove from train_inds so we don't try to access the result later,0
extract subset of names matching new columns,0
"track indices where an exception was thrown, since we can't remove from dictionary while iterating",0
don't want to cache this failed result,0
properties to return from effect InferenceResults,0
properties to return from PopulationSummaryResults,0
Converts strings to property lookups or method calls as a convenience so that the,0
_point_props and _summary_props above can be applied to an inference object,0
Create a summary combining all results into a single output; this is used,0
by the various causal_effect and causal_effect_dict methods to generate either a dataframe,0
"or a dictionary, respectively, based on the summary function passed into this method",0
"ensure array has shape (m,y,t)",0
population summary is missing sample dimension; add it for consistency,0
outcome dimension is missing; add it for consistency,0
add singleton treatment dimension if missing,0
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce,0
"each attr has dimension (m,y) or (m,y,t)",0
concatenate along treatment dimension,0
"for dictionary representation, want to remove unneeded sample dimension",0
in cohort and global results,0
TODO: enrich outcome logic for multi-class classification when that is supported,1
There is no actual sample level in this data,0
can't drop only level,0
should be serialization-ready and contain no numpy arrays,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
TODO: Note that there's no column metadata for the sample number - should there be?,1
"need to replicate the column info for each sample, then remove from the shared data",0
NOTE: the flattened order has the ouptut dimension before the feature dimension,0
which may need to be revisited once we support multiclass,0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
a global inference indicates the effect of that one feature on the outcome,0
need to reshape the output to match the input,0
we want to offset the inference object by the baseline estimate of y,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
"NOTE: this calculation is correct only if treatment costs are marginal costs,",0
because then scaling the difference between treatment value and treatment costs is the,0
same as scaling the treatment value and subtracting the scaled treatment cost.,0
,0
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for",0
"continuous treatments, the policy value should include the benefit of decreasing treatments",0
(rather than just not treating at all),0
,0
"We can get the total by seeing that if we restrict attention to units where we would treat,",0
2 * policy_value - always_treat,0
includes exactly their contribution because policy_value and always_treat both include it,0
"and likewise restricting attention to the units where we want to decrease treatment,",0
2 * policy_value - always-treat,0
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value",0
is zero and the contribution to always_treat is negative,0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
get dataframe with all but selected column,0
apply 10% of a typical treatment for this feature,0
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely",0
set the effect bounds; for positive treatments these agree with,0
"the estimates; for negative treatments, we need to invert the interval",0
the effect is now always positive since we decrease treatment when negative,0
"for discrete treatment, stack a zero result in front for control",0
we need to call effect_inference to get the correct CI between the two treatment options,0
we now need to construct the delta in the cost between the two treatments and translate the effect,0
remove third dimenions potentially added,0
"find cost of current treatment: equality creates a 2d array with True on each row,",0
only if its the location of the current treatment. Then we take the corresponding cost.,0
construct index of current treatment,0
add second dimension if needed for broadcasting during translation of effect,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values",0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Call weighted lasso on reduced design matrix,0
Weighted tau,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to WeightedLassoCV by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
set default values for None,0
check freq_weight should be integer and should be accompanied by sample_var,0
check array shape,0
weight X and y and sample_var,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
average the outcome dimension if it exists and ensure 2d y_pred,0
get index of best treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
d_t=None here since we measure the effect across all Ts,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile",0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)",0
def mlasso_model(): return MultiTaskLassoCV(,0
"cv=3, alphas=alpha_regs, max_iter=200)",0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
"alpha_regs = [5e-3, 1e-2, 5e-2]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)",0
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)",0
subset of features that are exogenous and create heterogeneity,0
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features),0
subset of features wrt we estimate heterogeneity,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
introspect the constructor arguments to find the model parameters,0
to represent,0
"if the argument is deprecated, ignore it",0
Extract and sort argument names excluding 'self',0
column names,0
transfer input to numpy arrays,0
transfer input to 2d arrays,0
create dataframe,0
currently dowhy only support single outcome and single treatment,0
call dowhy,0
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update",0
cate estimator but not the effect.,0
don't proxy special methods,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
For when checking input values is disabled,0
Type to column extraction function,0
"Get number of arguments, some sklearn featurizer don't accept feature_names",0
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names',0
Get feature names using featurizer,0
All attempts at retrieving transformed feature names have failed,0
Delegate handling to downstream logic,0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
assume that we should perform nested cross-validation if and only if,0
the model has a 'cv' attribute; this is a somewhat brittle assumption...,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a 'groups' argument,0
now we have to compute the folds explicitly because some classifiers (like LassoCV),0
don't use the groups when calling split internally,0
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Any access should throw,0
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
input feature name is already updated by cate_feature_names.,0
define the index of d_x to filter for each given T,0
filter X after broadcast with T for each given T,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains some snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
make any access to matplotlib or plt throw an exception,0
make any access to graphviz or plt throw an exception,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
Initialize saturation & value; calculate chroma & value shift,0
Calculate some intermediate values,0
Initialize RGB with same hue & chroma as our color,0
Shift the initial RGB values to match value and store,0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use mean of targets,0
Write node mean CATE,0
Write node std of CATE,0
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
Fetch appropriate color for node,0
Write node mean CATE,0
Write node mean CATE,0
Write recommended treatment and value - cost,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
This flag is true when names are set in a child class instead,0
"If names are set in a child class, add an attribute reflecting that",0
This works only if X is passed as a kwarg,0
We plan to enforce X as kwarg only in future releases,0
This checks if names have been set in a child class,0
"If names were set in a child class, don't do it again",0
"Wraps-up fit by setting attributes, cleaning up, etc.",0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"Treatment names is None, default to BaseCateEstimator",0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
Get input names,0
Summary,0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
Get input names,0
Summary,0
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
remove None arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same,0
alteration even when we only want to fit_final.,0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)",0
"however, sklearn doesn't support both stratifying and grouping (see",0
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply",0
their own object that supports grouping if they want to use groups.,0
for each mc iteration,0
for each model under cross fit setting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Policy Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base Policy tree,0
=============================================================================,0
The values below are required and utilitized by methods in the _SingleTreeExporterMixin,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"since we only allow single dimensional y, we could flatten the prediction",0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Handles the corner case when X=None but featurizer might be not None,0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
Replacing to remove docstring,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE: important to use the rlearner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the rlearner's copy to retain the information from fitting,0
Handles the corner case when X=None but featurizer might be not None,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support freq_weight and sample_var in debiased lasso,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add blb to parent's options,0
override only so that we can update the docstring to indicate,0
support for `GenericSingleTreatmentModelFinalInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
NOTE: important to get parent's wrapped copy so that,0
"after training wrapped featurizer is also trained, etc.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Fit a doubly robust average effect,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
"If custom param grid, check that only estimator parameters are being altered",0
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test",0
override only so that we can update the docstring to indicate support for `blb`,0
Get input names,0
Summary,0
Determine output settings,0
"Important: This must be the first invocation of the random state at fit time, so that",0
train/test splits are re-generatable from an external object simply by knowing the,0
random_state parameter of the tree. Can be useful in the future if one wants to create local,0
linear predictions. Currently is also useful for testing.,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
Set min_weight_leaf from min_weight_fraction_leaf,0
Build tree,0
We calculate the maximum number of samples from each half-split that any node in the tree can,0
hold. Used by criterion for memory space savings.,0
Initialize the criterion object and the criterion_val object if honest.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code is a fork from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
Set parameters,0
Don't instantiate estimators now! Parameters of base_estimator might,0
"still change. Eg., when grid-searching with the nested object syntax.",0
self.estimators_ needs to be filled by the derived classes in fit.,0
Compute the number of jobs,0
Partition estimators between jobs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=None here since we measure the effect across all Ts,0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
squeeze the first axis,0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector,0
d_t=None here since we measure the effect across all Ts,0
d_t=None here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet",0
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
scale preds,0
scale std errs,0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
offset preds,0
"offset the distribution, too",0
scale preds,0
"scale the distribution, too",0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base GRF tree,0
=============================================================================,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
=============================================================================,0
A MultOutputWrapper for GRF classes,0
=============================================================================,0
=============================================================================,0
Instantiations of Generalized Random Forest,0
=============================================================================,0
"Append a constant treatment if `fit_intercept=True`, the coefficient",0
in front of the constant treatment is the intercept in the moment equation.,0
"Append a constant treatment and constant instrument if `fit_intercept=True`,",0
the coefficient in front of the constant treatment is the intercept in the moment equation.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Base Generalized Random Forest,0
=============================================================================,0
TODO: support freq_weight and sample_var,1
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle,0
We calculate the min eigenvalue proxy that each criterion is considering,0
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`",0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Generating indices a priori before parallelism ended up being orders of magnitude,0
faster than how sklearn does it. The reason is that random samplers do not release the,0
gil it seems.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
####################,0
Variance correction,0
####################,0
Subtract the average within bag variance. This ends up being equal to the,0
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).,0
The negative part is just sq_between.,0
Objective bayes debiasing for the diagonals where we know a-prior they are positive,0
"The off diagonals we have no objective prior, so no correction is applied.",0
Finally correcting the pred_cov or pred_var,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
test that the subsampling scheme past to the trees is correct,0
The sample size is chosen in particular to test rounding based error when subsampling,0
test that the estimator calcualtes var correctly,0
test api,0
test accuracy,0
test the projection functionality of forests,0
test that the estimator calcualtes var correctly,0
test api,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
test that the subsampling scheme past to the trees is correct,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
omit the lalonde notebook,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
compare with weight,0
compare with weight,0
compare with weight,0
compare with weight,0
Multi-dimensional output y,0
1-d y,0
compare when both sample_var and sample_weight exist,0
multi-d y,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
only is not None when T1 is a constant or a list of constant,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test at least one estimator from each category,0
test causal graph,0
test refutation estimate,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure that we've got at least two of every row,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
need to make sure we get all *joint* combinations,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat requires X,0
ensure we can serialize unfit estimator,0
these support only W but not X,0
"these support only binary, not general discrete T and Z",0
ensure we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
TODO: add tests for extra properties like coef_ where they exist,1
TODO: add tests for extra properties like coef_ where they exist,1
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
"however, with custom splits the checking happens in the first stage wrapper",0
where we don't have all of the required information to do this;,0
we'd probably need to add it to _crossfit instead,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
make sure we can get out post-fit stuff,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet",0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
helper class,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
make sure we don't run into problems dropping every index,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
global and cohort row-wise dicts have d_y * d_t entries,0
local dictionary is flattened to n_rows * d_y * d_t,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
dgp,0
model,0
model,0
"columns 'd', 'e', 'h' have too many values",0
"columns 'd', 'e' have too many values",0
lowering bound shouldn't affect already fit columns when warm starting,0
"column d is now okay, too",0
verify that we can use a scalar treatment cost,0
verify that we can specify per-treatment costs for each sample,0
verify that using the same state returns the same results each time,0
set the categories for column 'd' explicitly so that b is default,0
"first column: 10 ones, this is fine",0
"second column: 6 categories, plenty of random instances of each",0
this is fine only if we increase the cateogry limit,0
"third column: nine ones, lots of twos, not enough unless we disable check",0
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity",0
forest heterogeneity won't work,0
"sixth column: just 1 one, not enough even without check",0
increase bound on cat expansion,0
skip checks (reducing folds accordingly),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Define data features,0
Added `_df`to names to be different from the default cate_estimator names,0
Generate data,0
################################,0
Single treatment and outcome #,0
################################,0
Test LinearDML,0
|--> Test featurizers,0
ColumnTransformer doesn't propagate column names,0
|--> Test re-fit,0
Test SparseLinearDML,0
Test ForestDML,0
###################################,0
Mutiple treatments and outcomes #,0
###################################,0
Test LinearDML,0
Test SparseLinearDML,0
"Single outcome only, ORF does not support multiple outcomes",0
Test DMLOrthoForest,0
Test DROrthoForest,0
Test XLearner,0
Skipping population summary names test because bootstrap inference is too slow,0
Test SLearner,0
Test TLearner,0
Test LinearDRLearner,0
Test SparseLinearDRLearner,0
Test ForestDRLearner,0
Test LinearIntentToTreatDRIV,0
Test DeepIV,0
Test categorical treatments,0
Check refit,0
Check refit after setting categories,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
Treatment effect function,0
Outcome support,0
Treatment support,0
"Generate controls, covariates, treatments and outcomes",0
Heterogeneous treatment effects,0
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
through shap package.,0
test shap could generate the plot from the shap_values,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we need the controls' encoded column for training",0
"Thus, we append the controls column before the one-hot-encoded T",0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages",0
output is,0
"* a column of ones if X, W, and Z are all None",0
* just X or W or Z if both of the others are None,0
* hstack([arrs]) for whatever subset are not None otherwise,0
ensure Z is 2D,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
"instruments, and outcomes",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"TODO: check that Y, T, Z do not have multiple columns",0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: do correct adjustment for sample_var,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res",0
TODO: allow the final model to actually use X? Then we'd need to rename the class,1
since we would actually be calculating a CATE rather than ATE.,0
TODO: allow the final model to actually use X?,1
TODO: allow the final model to actually use X?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring",0
TODO: would it be useful to extend to handle controls ala vanilla DML?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: this utility is documented but internal; reimplement?,1
TODO: this utility is even less public...,1
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged",0
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns,0
but also supports get_feature_names with expected signature,0
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value,0
Convert python objects to (possibly nested) types that can easily be represented as literals,0
Convert SingleTreeInterpreter to a python dictionary,0
named tuple type for storing results inside CausalAnalysis class;,0
must be lifted to module level to enable pickling,0
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names,0
Controls are all other columns of X,0
"can't use X[:, feat_ind] when X is a DataFrame",0
TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
we might want to modify OrthoLearner (and other discrete treatment classes),0
so that the user can opt-in to allowing unseen treatment values,0
(and return NaN or something in that case),0
array checking routines don't accept 0-width arrays,0
perform model selection,0
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative,0
convert to NormalInferenceResults for consistency,0
Set the dictionary values shared between local and global summaries,0
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments",0
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category",0
required to fit a discrete DML model,0
Validate inputs,0
TODO: check compatibility of X and Y lengths,1
"no previous fit, cancel warm start",0
"work with numeric feature indices, so that we can easily compare with categorical ones",0
"if heterogeneity_inds is 1D, repeat it",0
heterogeneity inds should be a 2D list of length same as train_inds,0
replace None elements of heterogeneity_inds and ensure indices are numeric,0
"TODO: bail out also if categorical columns, classification, random_state changed?",1
TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
train the Y model,0
"perform model selection for the Y model using all X, not on a per-column basis",0
"now that we've trained the classifier and wrapped it, ensure that y is transformed to",0
work with the regression wrapper,0
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays,0
"note that this needs to happen after wrapping to generalize to the multi-class case,",0
since otherwise we'll have too many columns to be able to train a classifier,0
start with empty results and default shared insights,0
convert categorical indicators to numeric indices,0
check for indices over the categorical expansion bound,0
assume we'll be able to train former failures this time; we'll add them back if not,0
"can't remove in place while iterating over new_inds, so store in separate list",0
"train the model, but warn",0
no model can be trained in this case since we need more folds,0
"don't train a model, but suggest workaround since there are enough instances of least",1
populated class,0
also remove from train_inds so we don't try to access the result later,0
extract subset of names matching new columns,0
"track indices where an exception was thrown, since we can't remove from dictionary while iterating",0
don't want to cache this failed result,0
properties to return from effect InferenceResults,0
properties to return from PopulationSummaryResults,0
Converts strings to property lookups or method calls as a convenience so that the,0
_point_props and _summary_props above can be applied to an inference object,0
Create a summary combining all results into a single output; this is used,0
by the various causal_effect and causal_effect_dict methods to generate either a dataframe,0
"or a dictionary, respectively, based on the summary function passed into this method",0
"ensure array has shape (m,y,t)",0
population summary is missing sample dimension; add it for consistency,0
outcome dimension is missing; add it for consistency,0
add singleton treatment dimension if missing,0
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce,0
"each attr has dimension (m,y) or (m,y,t)",0
concatenate along treatment dimension,0
"for dictionary representation, want to remove unneeded sample dimension",0
in cohort and global results,0
TODO: enrich outcome logic for multi-class classification when that is supported,1
There is no actual sample level in this data,0
can't drop only level,0
should be serialization-ready and contain no numpy arrays,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
TODO: Note that there's no column metadata for the sample number - should there be?,1
"need to replicate the column info for each sample, then remove from the shared data",0
NOTE: the flattened order has the ouptut dimension before the feature dimension,0
which may need to be revisited once we support multiclass,0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
a global inference indicates the effect of that one feature on the outcome,0
need to reshape the output to match the input,0
we want to offset the inference object by the baseline estimate of y,0
"remove entries belonging to row data, since we're including them in the list of nested dictionaries",0
get the length of the list corresponding to the first dictionary key,0
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into",0
"NOTE: this calculation is correct only if treatment costs are marginal costs,",0
because then scaling the difference between treatment value and treatment costs is the,0
same as scaling the treatment value and subtracting the scaled treatment cost.,0
,0
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for",0
"continuous treatments, the policy value should include the benefit of decreasing treatments",0
(rather than just not treating at all),0
,0
"We can get the total by seeing that if we restrict attention to units where we would treat,",0
2 * policy_value - always_treat,0
includes exactly their contribution because policy_value and always_treat both include it,0
"and likewise restricting attention to the units where we want to decrease treatment,",0
2 * policy_value - always-treat,0
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value",0
is zero and the contribution to always_treat is negative,0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
get dataframe with all but selected column,0
apply 10% of a typical treatment for this feature,0
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely",0
set the effect bounds; for positive treatments these agree with,0
"the estimates; for negative treatments, we need to invert the interval",0
the effect is now always positive since we decrease treatment when negative,0
"for discrete treatment, stack a zero result in front for control",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values",0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Call weighted lasso on reduced design matrix,0
Weighted tau,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to WeightedLassoCV by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
set default values for None,0
check freq_weight should be integer and should be accompanied by sample_var,0
check array shape,0
weight X and y and sample_var,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
average the outcome dimension if it exists and ensure 2d y_pred,0
get index of best treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
d_t=None here since we measure the effect across all Ts,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile",0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)",0
def mlasso_model(): return MultiTaskLassoCV(,0
"cv=3, alphas=alpha_regs, max_iter=200)",0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
"alpha_regs = [5e-3, 1e-2, 5e-2]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)",0
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)",0
subset of features that are exogenous and create heterogeneity,0
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features),0
subset of features wrt we estimate heterogeneity,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
introspect the constructor arguments to find the model parameters,0
to represent,0
"if the argument is deprecated, ignore it",0
Extract and sort argument names excluding 'self',0
column names,0
transfer input to numpy arrays,0
transfer input to 2d arrays,0
create dataframe,0
currently dowhy only support single outcome and single treatment,0
call dowhy,0
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update",0
cate estimator but not the effect.,0
don't proxy special methods,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
For when checking input values is disabled,0
Type to column extraction function,0
"Get number of arguments, some sklearn featurizer don't accept feature_names",0
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names',0
Get feature names using featurizer,0
All attempts at retrieving transformed feature names have failed,0
Delegate handling to downstream logic,0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
assume that we should perform nested cross-validation if and only if,0
the model has a 'cv' attribute; this is a somewhat brittle assumption...,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a 'groups' argument,0
now we have to compute the folds explicitly because some classifiers (like LassoCV),0
don't use the groups when calling split internally,0
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Any access should throw,0
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
input feature name is already updated by cate_feature_names.,0
define the index of d_x to filter for each given T,0
filter X after broadcast with T for each given T,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains some snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
make any access to matplotlib or plt throw an exception,0
make any access to graphviz or plt throw an exception,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
Initialize saturation & value; calculate chroma & value shift,0
Calculate some intermediate values,0
Initialize RGB with same hue & chroma as our color,0
Shift the initial RGB values to match value and store,0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use mean of targets,0
Write node mean CATE,0
Write node std of CATE,0
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
Fetch appropriate color for node,0
Write node mean CATE,0
Write node mean CATE,0
Write recommended treatment and value - cost,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
This flag is true when names are set in a child class instead,0
"If names are set in a child class, add an attribute reflecting that",0
This works only if X is passed as a kwarg,0
We plan to enforce X as kwarg only in future releases,0
This checks if names have been set in a child class,0
"If names were set in a child class, don't do it again",0
"Wraps-up fit by setting attributes, cleaning up, etc.",0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"Treatment names is None, default to BaseCateEstimator",0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
Get input names,0
Summary,0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
Get input names,0
Summary,0
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
remove None arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same,0
alteration even when we only want to fit_final.,0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)",0
"however, sklearn doesn't support both stratifying and grouping (see",0
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply",0
their own object that supports grouping if they want to use groups.,0
for each mc iteration,0
for each model under cross fit setting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Policy Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base Policy tree,0
=============================================================================,0
The values below are required and utilitized by methods in the _SingleTreeExporterMixin,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"since we only allow single dimensional y, we could flatten the prediction",0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Handles the corner case when X=None but featurizer might be not None,0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
Replacing to remove docstring,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE: important to use the rlearner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the rlearner's copy to retain the information from fitting,0
Handles the corner case when X=None but featurizer might be not None,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support freq_weight and sample_var in debiased lasso,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add blb to parent's options,0
override only so that we can update the docstring to indicate,0
support for `GenericSingleTreatmentModelFinalInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
NOTE: important to get parent's wrapped copy so that,0
"after training wrapped featurizer is also trained, etc.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Fit a doubly robust average effect,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
"If custom param grid, check that only estimator parameters are being altered",0
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test",0
override only so that we can update the docstring to indicate support for `blb`,0
Get input names,0
Summary,0
Determine output settings,0
"Important: This must be the first invocation of the random state at fit time, so that",0
train/test splits are re-generatable from an external object simply by knowing the,0
random_state parameter of the tree. Can be useful in the future if one wants to create local,0
linear predictions. Currently is also useful for testing.,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
Set min_weight_leaf from min_weight_fraction_leaf,0
Build tree,0
We calculate the maximum number of samples from each half-split that any node in the tree can,0
hold. Used by criterion for memory space savings.,0
Initialize the criterion object and the criterion_val object if honest.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code is a fork from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
Set parameters,0
Don't instantiate estimators now! Parameters of base_estimator might,0
"still change. Eg., when grid-searching with the nested object syntax.",0
self.estimators_ needs to be filled by the derived classes in fit.,0
Compute the number of jobs,0
Partition estimators between jobs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=None here since we measure the effect across all Ts,0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
squeeze the first axis,0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector,0
d_t=None here since we measure the effect across all Ts,0
d_t=None here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet",0
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction,0
scale preds,0
scale std errs,0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
offset preds,0
"offset the distribution, too",0
scale preds,0
"scale the distribution, too",0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base GRF tree,0
=============================================================================,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
=============================================================================,0
A MultOutputWrapper for GRF classes,0
=============================================================================,0
=============================================================================,0
Instantiations of Generalized Random Forest,0
=============================================================================,0
"Append a constant treatment if `fit_intercept=True`, the coefficient",0
in front of the constant treatment is the intercept in the moment equation.,0
"Append a constant treatment and constant instrument if `fit_intercept=True`,",0
the coefficient in front of the constant treatment is the intercept in the moment equation.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Base Generalized Random Forest,0
=============================================================================,0
TODO: support freq_weight and sample_var,1
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle,0
We calculate the min eigenvalue proxy that each criterion is considering,0
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`",0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Generating indices a priori before parallelism ended up being orders of magnitude,0
faster than how sklearn does it. The reason is that random samplers do not release the,0
gil it seems.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
####################,0
Variance correction,0
####################,0
Subtract the average within bag variance. This ends up being equal to the,0
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).,0
The negative part is just sq_between.,0
Objective bayes debiasing for the diagonals where we know a-prior they are positive,0
"The off diagonals we have no objective prior, so no correction is applied.",0
Finally correcting the pred_cov or pred_var,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
test that the subsampling scheme past to the trees is correct,0
The sample size is chosen in particular to test rounding based error when subsampling,0
test that the estimator calcualtes var correctly,0
test api,0
test accuracy,0
test the projection functionality of forests,0
test that the estimator calcualtes var correctly,0
test api,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
test that the subsampling scheme past to the trees is correct,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
omit the lalonde notebook,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
compare with weight,0
compare with weight,0
compare with weight,0
compare with weight,0
Multi-dimensional output y,0
1-d y,0
compare when both sample_var and sample_weight exist,0
multi-d y,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
only is not None when T1 is a constant or a list of constant,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test at least one estimator from each category,0
test causal graph,0
test refutation estimate,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure that we've got at least two of every row,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
need to make sure we get all *joint* combinations,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat requires X,0
ensure we can serialize unfit estimator,0
these support only W but not X,0
"these support only binary, not general discrete T and Z",0
ensure we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
TODO: add tests for extra properties like coef_ where they exist,1
TODO: add tests for extra properties like coef_ where they exist,1
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
"however, with custom splits the checking happens in the first stage wrapper",0
where we don't have all of the required information to do this;,0
we'd probably need to add it to _crossfit instead,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
make sure we can get out post-fit stuff,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet",0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
helper class,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
make sure we don't run into problems dropping every index,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"global shape is (d_y, sum(d_t))",0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
dgp,0
model,0
model,0
"columns 'd', 'e', 'h' have too many values",0
"columns 'd', 'e' have too many values",0
lowering bound shouldn't affect already fit columns when warm starting,0
"column d is now okay, too",0
verify that we can use a scalar treatment cost,0
verify that we can specify per-treatment costs for each sample,0
verify that using the same state returns the same results each time,0
set the categories for column 'd' explicitly so that b is default,0
"first column: 10 ones, this is fine",0
"second column: 6 categories, plenty of random instances of each",0
this is fine only if we increase the cateogry limit,0
"third column: nine ones, lots of twos, not enough unless we disable check",0
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity",0
forest heterogeneity won't work,0
"sixth column: just 1 one, not enough even without check",0
increase bound on cat expansion,0
skip checks (reducing folds accordingly),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Define data features,0
Added `_df`to names to be different from the default cate_estimator names,0
Generate data,0
################################,0
Single treatment and outcome #,0
################################,0
Test LinearDML,0
|--> Test featurizers,0
ColumnTransformer doesn't propagate column names,0
|--> Test re-fit,0
Test SparseLinearDML,0
Test ForestDML,0
###################################,0
Mutiple treatments and outcomes #,0
###################################,0
Test LinearDML,0
Test SparseLinearDML,0
"Single outcome only, ORF does not support multiple outcomes",0
Test DMLOrthoForest,0
Test DROrthoForest,0
Test XLearner,0
Skipping population summary names test because bootstrap inference is too slow,0
Test SLearner,0
Test TLearner,0
Test LinearDRLearner,0
Test SparseLinearDRLearner,0
Test ForestDRLearner,0
Test LinearIntentToTreatDRIV,0
Test DeepIV,0
Test categorical treatments,0
Check refit,0
Check refit after setting categories,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
Treatment effect function,0
Outcome support,0
Treatment support,0
"Generate controls, covariates, treatments and outcomes",0
Heterogeneous treatment effects,0
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
through shap package.,0
test shap could generate the plot from the shap_values,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we need the controls' encoded column for training",0
"Thus, we append the controls column before the one-hot-encoded T",0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages",0
output is,0
"* a column of ones if X, W, and Z are all None",0
* just X or W or Z if both of the others are None,0
* hstack([arrs]) for whatever subset are not None otherwise,0
ensure Z is 2D,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
"instruments, and outcomes",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"TODO: check that Y, T, Z do not have multiple columns",0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: do correct adjustment for sample_var,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res",0
TODO: allow the final model to actually use X? Then we'd need to rename the class,1
since we would actually be calculating a CATE rather than ATE.,0
TODO: allow the final model to actually use X?,1
TODO: allow the final model to actually use X?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring",0
TODO: would it be useful to extend to handle controls ala vanilla DML?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: this utility is documented but internal; reimplement?,1
TODO: this utility is even less public...,1
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged",0
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns,0
but also supports get_feature_names with expected signature,0
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value,0
Convert python objects to (possibly nested) types that can easily be represented as literals,0
Convert SingleTreeInterpreter to a python dictionary,0
named tuple type for storing results inside CausalAnalysis class;,0
must be lifted to module level to enable pickling,0
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names,0
Controls are all other columns of X,0
"can't use X[:, feat_ind] when X is a DataFrame",0
TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
we might want to modify OrthoLearner (and other discrete treatment classes),0
so that the user can opt-in to allowing unseen treatment values,0
(and return NaN or something in that case),0
array checking routines don't accept 0-width arrays,0
perform model selection,0
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative,0
convert to NormalInferenceResults for consistency,0
Set the dictionary values shared between local and global summaries,0
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments",0
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category",0
required to fit a discrete DML model,0
Validate inputs,0
TODO: check compatibility of X and Y lengths,1
"no previous fit, cancel warm start",0
"work with numeric feature indices, so that we can easily compare with categorical ones",0
"if heterogeneity_inds is 1D, repeat it",0
heterogeneity inds should be a 2D list of length same as train_inds,0
replace None elements of heterogeneity_inds and ensure indices are numeric,0
"TODO: bail out also if categorical columns, classification, random_state changed?",1
TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
train the Y model,0
"perform model selection for the Y model using all X, not on a per-column basis",0
"now that we've trained the classifier and wrapped it, ensure that y is transformed to",0
work with the regression wrapper,0
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays,0
"note that this needs to happen after wrapping to generalize to the multi-class case,",0
since otherwise we'll have too many columns to be able to train a classifier,0
start with empty results and default shared insights,0
convert categorical indicators to numeric indices,0
check for indices over the categorical expansion bound,0
assume we'll be able to train former failures this time; we'll add them back if not,0
"can't remove in place while iterating over new_inds, so store in separate list",0
"train the model, but warn",0
no model can be trained in this case since we need more folds,0
"don't train a model, but suggest workaround since there are enough instances of least",1
populated class,0
also remove from train_inds so we don't try to access the result later,0
extract subset of names matching new columns,0
"track indices where an exception was thrown, since we can't remove from dictionary while iterating",0
don't want to cache this failed result,0
properties to return from effect InferenceResults,0
properties to return from PopulationSummaryResults,0
Converts strings to property lookups or method calls as a convenience so that the,0
_point_props and _summary_props above can be applied to an inference object,0
Create a summary combining all results into a single output; this is used,0
by the various causal_effect and causal_effect_dict methods to generate either a dataframe,0
"or a dictionary, respectively, based on the summary function passed into this method",0
"ensure array has shape (m,y,t)",0
population summary is missing sample dimension; add it for consistency,0
outcome dimension is missing; add it for consistency,0
add singleton treatment dimension if missing,0
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce,0
"each attr has dimension (m,y) or (m,y,t)",0
concatenate along treatment dimension,0
"for dictionary representation, want to remove unneeded sample dimension",0
in cohort and global results,0
TODO: enrich outcome logic for multi-class classification when that is supported,1
can't drop only level,0
should be serialization-ready and contain no numpy arrays,0
a global inference indicates the effect of that one feature on the outcome,0
need to reshape the output to match the input,0
we want to offset the inference object by the baseline estimate of y,0
"NOTE: this calculation is correct only if treatment costs are marginal costs,",0
because then scaling the difference between treatment value and treatment costs is the,0
same as scaling the treatment value and subtracting the scaled treatment cost.,0
,0
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for",0
"continuous treatments, the policy value should include the benefit of decreasing treatments",0
(rather than just not treating at all),0
,0
"We can get the total by seeing that if we restrict attention to units where we would treat,",0
2 * policy_value - always_treat,0
includes exactly their contribution because policy_value and always_treat both include it,0
"and likewise restricting attention to the units where we want to decrease treatment,",0
2 * policy_value - always-treat,0
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value",0
is zero and the contribution to always_treat is negative,0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
get dataframe with all but selected column,0
apply 10% of a typical treatment for this feature,0
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely",0
set the effect bounds; for positive treatments these agree with,0
"the estimates; for negative treatments, we need to invert the interval",0
the effect is now always positive since we decrease treatment when negative,0
"for discrete treatment, stack a zero result in front for control",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values",0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Call weighted lasso on reduced design matrix,0
Weighted tau,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to WeightedLassoCV by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
set default values for None,0
check freq_weight should be integer and should be accompanied by sample_var,0
check array shape,0
weight X and y and sample_var,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
average the outcome dimension if it exists and ensure 2d y_pred,0
get index of best treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
d_t=None here since we measure the effect across all Ts,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile",0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)",0
def mlasso_model(): return MultiTaskLassoCV(,0
"cv=3, alphas=alpha_regs, max_iter=200)",0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
"alpha_regs = [5e-3, 1e-2, 5e-2]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)",0
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)",0
subset of features that are exogenous and create heterogeneity,0
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features),0
subset of features wrt we estimate heterogeneity,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
introspect the constructor arguments to find the model parameters,0
to represent,0
"if the argument is deprecated, ignore it",0
Extract and sort argument names excluding 'self',0
column names,0
transfer input to numpy arrays,0
transfer input to 2d arrays,0
create dataframe,0
currently dowhy only support single outcome and single treatment,0
call dowhy,0
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update",0
cate estimator but not the effect.,0
don't proxy special methods,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
For when checking input values is disabled,0
Type to column extraction function,0
"Get number of arguments, some sklearn featurizer don't accept feature_names",0
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names',0
Get feature names using featurizer,0
All attempts at retrieving transformed feature names have failed,0
Delegate handling to downstream logic,0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
assume that we should perform nested cross-validation if and only if,0
the model has a 'cv' attribute; this is a somewhat brittle assumption...,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a 'groups' argument,0
now we have to compute the folds explicitly because some classifiers (like LassoCV),0
don't use the groups when calling split internally,0
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Any access should throw,0
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
input feature name is already updated by cate_feature_names.,0
define the index of d_x to filter for each given T,0
filter X after broadcast with T for each given T,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains some snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
make any access to matplotlib or plt throw an exception,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
Initialize saturation & value; calculate chroma & value shift,0
Calculate some intermediate values,0
Initialize RGB with same hue & chroma as our color,0
Shift the initial RGB values to match value and store,0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use mean of targets,0
Write node mean CATE,0
Write node std of CATE,0
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
Fetch appropriate color for node,0
Write node mean CATE,0
Write node mean CATE,0
Write recommended treatment and value - cost,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
This flag is true when names are set in a child class instead,0
"If names are set in a child class, add an attribute reflecting that",0
This works only if X is passed as a kwarg,0
We plan to enforce X as kwarg only in future releases,0
This checks if names have been set in a child class,0
"If names were set in a child class, don't do it again",0
"Wraps-up fit by setting attributes, cleaning up, etc.",0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"Treatment names is None, default to BaseCateEstimator",0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
Get input names,0
Summary,0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
Get input names,0
Summary,0
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
remove None arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same,0
alteration even when we only want to fit_final.,0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)",0
"however, sklearn doesn't support both stratifying and grouping (see",0
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply",0
their own object that supports grouping if they want to use groups.,0
for each mc iteration,0
for each model under cross fit setting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Policy Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base Policy tree,0
=============================================================================,0
The values below are required and utilitized by methods in the _SingleTreeExporterMixin,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"since we only allow single dimensional y, we could flatten the prediction",0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Handles the corner case when X=None but featurizer might be not None,0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
Replacing to remove docstring,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE: important to use the rlearner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the rlearner's copy to retain the information from fitting,0
Handles the corner case when X=None but featurizer might be not None,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support freq_weight and sample_var in debiased lasso,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add blb to parent's options,0
override only so that we can update the docstring to indicate,0
support for `GenericSingleTreatmentModelFinalInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
NOTE: important to get parent's wrapped copy so that,0
"after training wrapped featurizer is also trained, etc.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Fit a doubly robust average effect,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
"If custom param grid, check that only estimator parameters are being altered",0
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test",0
override only so that we can update the docstring to indicate support for `blb`,0
Get input names,0
Summary,0
Determine output settings,0
"Important: This must be the first invocation of the random state at fit time, so that",0
train/test splits are re-generatable from an external object simply by knowing the,0
random_state parameter of the tree. Can be useful in the future if one wants to create local,0
linear predictions. Currently is also useful for testing.,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
Set min_weight_leaf from min_weight_fraction_leaf,0
Build tree,0
We calculate the maximum number of samples from each half-split that any node in the tree can,0
hold. Used by criterion for memory space savings.,0
Initialize the criterion object and the criterion_val object if honest.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code is a fork from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
Set parameters,0
Don't instantiate estimators now! Parameters of base_estimator might,0
"still change. Eg., when grid-searching with the nested object syntax.",0
self.estimators_ needs to be filled by the derived classes in fit.,0
Compute the number of jobs,0
Partition estimators between jobs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=None here since we measure the effect across all Ts,0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
squeeze the first axis,0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector,0
d_t=None here since we measure the effect across all Ts,0
d_t=None here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet",0
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx,0
NOTE: use np.asarray(offset) because if offset is a pd.Series direct addition would make the sum,0
"a Series as well, which would subsequently break summary_frame because flatten isn't supported",0
NOTE: use np.asarray(factor) because if offset is a pd.Series direct addition would make the product,0
"a Series as well, which would subsequently break summary_frame because flatten isn't supported",0
scale preds,0
scale std errs,0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
offset preds,0
"offset the distribution, too",0
scale preds,0
"scale the distribution, too",0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base GRF tree,0
=============================================================================,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
=============================================================================,0
A MultOutputWrapper for GRF classes,0
=============================================================================,0
=============================================================================,0
Instantiations of Generalized Random Forest,0
=============================================================================,0
"Append a constant treatment if `fit_intercept=True`, the coefficient",0
in front of the constant treatment is the intercept in the moment equation.,0
"Append a constant treatment and constant instrument if `fit_intercept=True`,",0
the coefficient in front of the constant treatment is the intercept in the moment equation.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Base Generalized Random Forest,0
=============================================================================,0
TODO: support freq_weight and sample_var,1
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle,0
We calculate the min eigenvalue proxy that each criterion is considering,0
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`",0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Generating indices a priori before parallelism ended up being orders of magnitude,0
faster than how sklearn does it. The reason is that random samplers do not release the,0
gil it seems.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
####################,0
Variance correction,0
####################,0
Subtract the average within bag variance. This ends up being equal to the,0
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).,0
The negative part is just sq_between.,0
Objective bayes debiasing for the diagonals where we know a-prior they are positive,0
"The off diagonals we have no objective prior, so no correction is applied.",0
Finally correcting the pred_cov or pred_var,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
test that the subsampling scheme past to the trees is correct,0
The sample size is chosen in particular to test rounding based error when subsampling,0
test that the estimator calcualtes var correctly,0
test api,0
test accuracy,0
test the projection functionality of forests,0
test that the estimator calcualtes var correctly,0
test api,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
test that the subsampling scheme past to the trees is correct,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
omit the lalonde notebook,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
compare with weight,0
compare with weight,0
compare with weight,0
compare with weight,0
Multi-dimensional output y,0
1-d y,0
compare when both sample_var and sample_weight exist,0
multi-d y,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
only is not None when T1 is a constant or a list of constant,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test at least one estimator from each category,0
test causal graph,0
test refutation estimate,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure that we've got at least two of every row,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
need to make sure we get all *joint* combinations,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat requires X,0
ensure we can serialize unfit estimator,0
these support only W but not X,0
"these support only binary, not general discrete T and Z",0
ensure we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
TODO: add tests for extra properties like coef_ where they exist,1
TODO: add tests for extra properties like coef_ where they exist,1
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
"however, with custom splits the checking happens in the first stage wrapper",0
where we don't have all of the required information to do this;,0
we'd probably need to add it to _crossfit instead,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
make sure we can get out post-fit stuff,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet",0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
helper class,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
make sure we don't run into problems dropping every index,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"global shape is (d_y, sum(d_t))",0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
dgp,0
model,0
model,0
"columns 'd', 'e', 'h' have too many values",0
"columns 'd', 'e' have too many values",0
lowering bound shouldn't affect already fit columns when warm starting,0
"column d is now okay, too",0
verify that we can use a scalar treatment cost,0
verify that we can specify per-treatment costs for each sample,0
verify that using the same state returns the same results each time,0
set the categories for column 'd' explicitly so that b is default,0
"first column: 10 ones, this is fine",0
"second column: 6 categories, plenty of random instances of each",0
this is fine only if we increase the cateogry limit,0
"third column: nine ones, lots of twos, not enough unless we disable check",0
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity",0
forest heterogeneity won't work,0
"sixth column: just 1 one, not enough even without check",0
increase bound on cat expansion,0
skip checks (reducing folds accordingly),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Define data features,0
Added `_df`to names to be different from the default cate_estimator names,0
Generate data,0
################################,0
Single treatment and outcome #,0
################################,0
Test LinearDML,0
|--> Test featurizers,0
ColumnTransformer doesn't propagate column names,0
|--> Test re-fit,0
Test SparseLinearDML,0
Test ForestDML,0
###################################,0
Mutiple treatments and outcomes #,0
###################################,0
Test LinearDML,0
Test SparseLinearDML,0
"Single outcome only, ORF does not support multiple outcomes",0
Test DMLOrthoForest,0
Test DROrthoForest,0
Test XLearner,0
Skipping population summary names test because bootstrap inference is too slow,0
Test SLearner,0
Test TLearner,0
Test LinearDRLearner,0
Test SparseLinearDRLearner,0
Test ForestDRLearner,0
Test LinearIntentToTreatDRIV,0
Test DeepIV,0
Test categorical treatments,0
Check refit,0
Check refit after setting categories,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
Treatment effect function,0
Outcome support,0
Treatment support,0
"Generate controls, covariates, treatments and outcomes",0
Heterogeneous treatment effects,0
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
through shap package.,0
test shap could generate the plot from the shap_values,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we need the controls' encoded column for training",0
"Thus, we append the controls column before the one-hot-encoded T",0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages",0
output is,0
"* a column of ones if X, W, and Z are all None",0
* just X or W or Z if both of the others are None,0
* hstack([arrs]) for whatever subset are not None otherwise,0
ensure Z is 2D,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
"instruments, and outcomes",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"TODO: check that Y, T, Z do not have multiple columns",0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: do correct adjustment for sample_var,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res",0
TODO: allow the final model to actually use X? Then we'd need to rename the class,1
since we would actually be calculating a CATE rather than ATE.,0
TODO: allow the final model to actually use X?,1
TODO: allow the final model to actually use X?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring",0
TODO: would it be useful to extend to handle controls ala vanilla DML?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: this utility is documented but internal; reimplement?,1
TODO: this utility is even less public...,1
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged",0
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns,0
but also supports get_feature_names with expected signature,0
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value,0
Convert python objects to (possibly nested) types that can easily be represented as literals,0
Convert SingleTreeInterpreter to a python dictionary,0
named tuple type for storing results inside CausalAnalysis class;,0
must be lifted to module level to enable pickling,0
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names,0
Controls are all other columns of X,0
"can't use X[:, feat_ind] when X is a DataFrame",0
TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
we might want to modify OrthoLearner (and other discrete treatment classes),0
so that the user can opt-in to allowing unseen treatment values,0
(and return NaN or something in that case),0
array checking routines don't accept 0-width arrays,0
perform model selection,0
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative,0
convert to NormalInferenceResults for consistency,0
Set the dictionary values shared between local and global summaries,0
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments",0
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category",0
required to fit a discrete DML model,0
Validate inputs,0
TODO: check compatibility of X and Y lengths,1
"no previous fit, cancel warm start",0
"work with numeric feature indices, so that we can easily compare with categorical ones",0
"if heterogeneity_inds is 1D, repeat it",0
heterogeneity inds should be a 2D list of length same as train_inds,0
replace None elements of heterogeneity_inds and ensure indices are numeric,0
"TODO: bail out also if categorical columns, classification, random_state changed?",1
TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
train the Y model,0
"perform model selection for the Y model using all X, not on a per-column basis",0
"now that we've trained the classifier and wrapped it, ensure that y is transformed to",0
work with the regression wrapper,0
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays,0
"note that this needs to happen after wrapping to generalize to the multi-class case,",0
since otherwise we'll have too many columns to be able to train a classifier,0
start with empty results and default shared insights,0
convert categorical indicators to numeric indices,0
check for indices over the categorical expansion bound,0
assume we'll be able to train former failures this time; we'll add them back if not,0
"can't remove in place while iterating over new_inds, so store in separate list",0
"train the model, but warn",0
no model can be trained in this case since we need more folds,0
"don't train a model, but suggest workaround since there are enough instances of least",1
populated class,0
also remove from train_inds so we don't try to access the result later,0
extract subset of names matching new columns,0
"track indices where an exception was thrown, since we can't remove from dictionary while iterating",0
don't want to cache this failed result,0
properties to return from effect InferenceResults,0
properties to return from PopulationSummaryResults,0
Converts strings to property lookups or method calls as a convenience so that the,0
_point_props and _summary_props above can be applied to an inference object,0
Create a summary combining all results into a single output; this is used,0
by the various causal_effect and causal_effect_dict methods to generate either a dataframe,0
"or a dictionary, respectively, based on the summary function passed into this method",0
"ensure array has shape (m,y,t)",0
population summary is missing sample dimension; add it for consistency,0
outcome dimension is missing; add it for consistency,0
add singleton treatment dimension if missing,0
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce,0
"each attr has dimension (m,y) or (m,y,t)",0
concatenate along treatment dimension,0
"for dictionary representation, want to remove unneeded sample dimension",0
in cohort and global results,0
TODO: enrich outcome logic for multi-class classification when that is supported,1
can't drop only level,0
should be serialization-ready and contain no numpy arrays,0
a global inference indicates the effect of that one feature on the outcome,0
need to reshape the output to match the input,0
we want to offset the inference object by the baseline estimate of y,0
"NOTE: this calculation is correct only if treatment costs are marginal costs,",0
because then scaling the difference between treatment value and treatment costs is the,0
same as scaling the treatment value and subtracting the scaled treatment cost.,0
,0
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for",0
"continuous treatments, the policy value should include the benefit of decreasing treatments",0
(rather than just not treating at all),0
,0
"We can get the total by seeing that if we restrict attention to units where we would treat,",0
2 * policy_value - always_treat,0
includes exactly their contribution because policy_value and always_treat both include it,0
"and likewise restricting attention to the units where we want to decrease treatment,",0
2 * policy_value - always-treat,0
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value",0
is zero and the contribution to always_treat is negative,0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
get dataframe with all but selected column,0
apply 10% of a typical treatment for this feature,0
set the effect bounds; for positive treatments these agree with,0
"the estimates; for negative treatments, we need to invert the interval",0
the effect is now always positive since we decrease treatment when negative,0
"for discrete treatment, stack a zero result in front for control",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values",0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Call weighted lasso on reduced design matrix,0
Weighted tau,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to WeightedLassoCV by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
set default values for None,0
check freq_weight should be integer and should be accompanied by sample_var,0
check array shape,0
weight X and y and sample_var,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
average the outcome dimension if it exists and ensure 2d y_pred,0
get index of best treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
d_t=None here since we measure the effect across all Ts,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile",0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)",0
def mlasso_model(): return MultiTaskLassoCV(,0
"cv=3, alphas=alpha_regs, max_iter=200)",0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
"alpha_regs = [5e-3, 1e-2, 5e-2]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)",0
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)",0
subset of features that are exogenous and create heterogeneity,0
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features),0
subset of features wrt we estimate heterogeneity,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
introspect the constructor arguments to find the model parameters,0
to represent,0
"if the argument is deprecated, ignore it",0
Extract and sort argument names excluding 'self',0
column names,0
transfer input to numpy arrays,0
transfer input to 2d arrays,0
create dataframe,0
currently dowhy only support single outcome and single treatment,0
call dowhy,0
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update",0
cate estimator but not the effect.,0
don't proxy special methods,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
For when checking input values is disabled,0
Type to column extraction function,0
"Get number of arguments, some sklearn featurizer don't accept feature_names",0
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names',0
Get feature names using featurizer,0
All attempts at retrieving transformed feature names have failed,0
Delegate handling to downstream logic,0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
assume that we should perform nested cross-validation if and only if,0
the model has a 'cv' attribute; this is a somewhat brittle assumption...,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a 'groups' argument,0
now we have to compute the folds explicitly because some classifiers (like LassoCV),0
don't use the groups when calling split internally,0
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Any access should throw,0
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
input feature name is already updated by cate_feature_names.,0
define the index of d_x to filter for each given T,0
filter X after broadcast with T for each given T,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains some snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
make any access to matplotlib or plt throw an exception,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
Initialize saturation & value; calculate chroma & value shift,0
Calculate some intermediate values,0
Initialize RGB with same hue & chroma as our color,0
Shift the initial RGB values to match value and store,0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use mean of targets,0
Write node mean CATE,0
Write node std of CATE,0
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
Fetch appropriate color for node,0
Write node mean CATE,0
Write node mean CATE,0
Write recommended treatment and value - cost,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
This flag is true when names are set in a child class instead,0
"If names are set in a child class, add an attribute reflecting that",0
This works only if X is passed as a kwarg,0
We plan to enforce X as kwarg only in future releases,0
This checks if names have been set in a child class,0
"If names were set in a child class, don't do it again",0
"Wraps-up fit by setting attributes, cleaning up, etc.",0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"Treatment names is None, default to BaseCateEstimator",0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
Get input names,0
Summary,0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
Get input names,0
Summary,0
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
remove None arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same,0
alteration even when we only want to fit_final.,0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)",0
"however, sklearn doesn't support both stratifying and grouping (see",0
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply",0
their own object that supports grouping if they want to use groups.,0
for each mc iteration,0
for each model under cross fit setting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Policy Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base Policy tree,0
=============================================================================,0
The values below are required and utilitized by methods in the _SingleTreeExporterMixin,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"since we only allow single dimensional y, we could flatten the prediction",0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Handles the corner case when X=None but featurizer might be not None,0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
Replacing to remove docstring,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE: important to use the rlearner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the rlearner's copy to retain the information from fitting,0
Handles the corner case when X=None but featurizer might be not None,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support freq_weight and sample_var in debiased lasso,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add blb to parent's options,0
override only so that we can update the docstring to indicate,0
support for `GenericSingleTreatmentModelFinalInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
NOTE: important to get parent's wrapped copy so that,0
"after training wrapped featurizer is also trained, etc.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Fit a doubly robust average effect,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
"If custom param grid, check that only estimator parameters are being altered",0
override only so that we can update the docstring to indicate support for `blb`,0
Get input names,0
Summary,0
Determine output settings,0
"Important: This must be the first invocation of the random state at fit time, so that",0
train/test splits are re-generatable from an external object simply by knowing the,0
random_state parameter of the tree. Can be useful in the future if one wants to create local,0
linear predictions. Currently is also useful for testing.,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
Set min_weight_leaf from min_weight_fraction_leaf,0
Build tree,0
We calculate the maximum number of samples from each half-split that any node in the tree can,0
hold. Used by criterion for memory space savings.,0
Initialize the criterion object and the criterion_val object if honest.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code is a fork from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
Set parameters,0
Don't instantiate estimators now! Parameters of base_estimator might,0
"still change. Eg., when grid-searching with the nested object syntax.",0
self.estimators_ needs to be filled by the derived classes in fit.,0
Compute the number of jobs,0
Partition estimators between jobs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=None here since we measure the effect across all Ts,0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
squeeze the first axis,0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector,0
d_t=None here since we measure the effect across all Ts,0
d_t=None here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet",0
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx,0
NOTE: use np.asarray(offset) because if offset is a pd.Series direct addition would make the sum,0
"a Series as well, which would subsequently break summary_frame because flatten isn't supported",0
NOTE: use np.asarray(factor) because if offset is a pd.Series direct addition would make the product,0
"a Series as well, which would subsequently break summary_frame because flatten isn't supported",0
scale preds,0
scale std errs,0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
offset preds,0
"offset the distribution, too",0
scale preds,0
"scale the distribution, too",0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base GRF tree,0
=============================================================================,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
=============================================================================,0
A MultOutputWrapper for GRF classes,0
=============================================================================,0
=============================================================================,0
Instantiations of Generalized Random Forest,0
=============================================================================,0
"Append a constant treatment if `fit_intercept=True`, the coefficient",0
in front of the constant treatment is the intercept in the moment equation.,0
"Append a constant treatment and constant instrument if `fit_intercept=True`,",0
the coefficient in front of the constant treatment is the intercept in the moment equation.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Base Generalized Random Forest,0
=============================================================================,0
TODO: support freq_weight and sample_var,1
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle,0
We calculate the min eigenvalue proxy that each criterion is considering,0
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`",0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Generating indices a priori before parallelism ended up being orders of magnitude,0
faster than how sklearn does it. The reason is that random samplers do not release the,0
gil it seems.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
####################,0
Variance correction,0
####################,0
Subtract the average within bag variance. This ends up being equal to the,0
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).,0
The negative part is just sq_between.,0
Objective bayes debiasing for the diagonals where we know a-prior they are positive,0
"The off diagonals we have no objective prior, so no correction is applied.",0
Finally correcting the pred_cov or pred_var,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
test that the subsampling scheme past to the trees is correct,0
The sample size is chosen in particular to test rounding based error when subsampling,0
test that the estimator calcualtes var correctly,0
test api,0
test accuracy,0
test the projection functionality of forests,0
test that the estimator calcualtes var correctly,0
test api,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
test that the subsampling scheme past to the trees is correct,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
omit the lalonde notebook,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
compare with weight,0
compare with weight,0
compare with weight,0
compare with weight,0
Multi-dimensional output y,0
1-d y,0
compare when both sample_var and sample_weight exist,0
multi-d y,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
only is not None when T1 is a constant or a list of constant,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test at least one estimator from each category,0
test causal graph,0
test refutation estimate,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure that we've got at least two of every row,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
need to make sure we get all *joint* combinations,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat requires X,0
ensure we can serialize unfit estimator,0
these support only W but not X,0
"these support only binary, not general discrete T and Z",0
ensure we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
TODO: add tests for extra properties like coef_ where they exist,1
TODO: add tests for extra properties like coef_ where they exist,1
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
"however, with custom splits the checking happens in the first stage wrapper",0
where we don't have all of the required information to do this;,0
we'd probably need to add it to _crossfit instead,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
make sure we can get out post-fit stuff,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet",0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
helper class,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"global shape is (d_y, sum(d_t))",0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
make sure we don't run into problems dropping every index,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"global shape is (d_y, sum(d_t))",0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
"Make sure we handle continuous, binary, and multi-class treatments",0
"For multiple discrete treatments, one ""always treat"" value per non-default treatment",0
policy value should exceed always treating with any treatment,0
dgp,0
model,0
model,0
"columns 'd', 'e', 'h' have too many values",0
"columns 'd', 'e' have too many values",0
lowering bound shouldn't affect already fit columns when warm starting,0
"column d is now okay, too",0
verify that we can use a scalar treatment cost,0
verify that we can specify per-treatment costs for each sample,0
verify that using the same state returns the same results each time,0
set the categories for column 'd' explicitly so that b is default,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Define data features,0
Added `_df`to names to be different from the default cate_estimator names,0
Generate data,0
################################,0
Single treatment and outcome #,0
################################,0
Test LinearDML,0
|--> Test featurizers,0
ColumnTransformer doesn't propagate column names,0
|--> Test re-fit,0
Test SparseLinearDML,0
Test ForestDML,0
###################################,0
Mutiple treatments and outcomes #,0
###################################,0
Test LinearDML,0
Test SparseLinearDML,0
"Single outcome only, ORF does not support multiple outcomes",0
Test DMLOrthoForest,0
Test DROrthoForest,0
Test XLearner,0
Skipping population summary names test because bootstrap inference is too slow,0
Test SLearner,0
Test TLearner,0
Test LinearDRLearner,0
Test SparseLinearDRLearner,0
Test ForestDRLearner,0
Test LinearIntentToTreatDRIV,0
Test DeepIV,0
Test categorical treatments,0
Check refit,0
Check refit after setting categories,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
Treatment effect function,0
Outcome support,0
Treatment support,0
"Generate controls, covariates, treatments and outcomes",0
Heterogeneous treatment effects,0
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
through shap package.,0
test shap could generate the plot from the shap_values,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we need the controls' encoded column for training",0
"Thus, we append the controls column before the one-hot-encoded T",0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages",0
output is,0
"* a column of ones if X, W, and Z are all None",0
* just X or W or Z if both of the others are None,0
* hstack([arrs]) for whatever subset are not None otherwise,0
ensure Z is 2D,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
"instruments, and outcomes",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"TODO: check that Y, T, Z do not have multiple columns",0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: do correct adjustment for sample_var,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res",0
TODO: allow the final model to actually use X? Then we'd need to rename the class,1
since we would actually be calculating a CATE rather than ATE.,0
TODO: allow the final model to actually use X?,1
TODO: allow the final model to actually use X?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring",0
TODO: would it be useful to extend to handle controls ala vanilla DML?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: this utility is documented but internal; reimplement?,1
TODO: this utility is even less public...,1
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged",0
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns,0
but also supports get_feature_names with expected signature,0
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value,0
Convert python objects to (possibly nested) types that can easily be represented as literals,0
Convert SingleTreeInterpreter to a python dictionary,0
named tuple type for storing results inside CausalAnalysis class;,0
must be lifted to module level to enable pickling,0
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names,0
Controls are all other columns of X,0
"can't use X[:, feat_ind] when X is a DataFrame",0
TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
we might want to modify OrthoLearner (and other discrete treatment classes),0
so that the user can opt-in to allowing unseen treatment values,0
(and return NaN or something in that case),0
array checking routines don't accept 0-width arrays,0
perform model selection,0
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative,0
convert to NormalInferenceResults for consistency,0
Set the dictionary values shared between local and global summaries,0
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments",0
Validate inputs,0
TODO: check compatibility of X and Y lengths,1
"no previous fit, cancel warm start",0
"work with numeric feature indices, so that we can easily compare with categorical ones",0
"if heterogeneity_inds is 1D, repeat it",0
heterogeneity inds should be a 2D list of length same as train_inds,0
replace None elements of heterogeneity_inds and ensure indices are numeric,0
"TODO: bail out also if categorical columns, classification, random_state changed?",1
TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
train the Y model,0
"perform model selection for the Y model using all X, not on a per-column basis",0
"now that we've trained the classifier and wrapped it, ensure that y is transformed to",0
work with the regression wrapper,0
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays,0
"note that this needs to happen after wrapping to generalize to the multi-class case,",0
since otherwise we'll have too many columns to be able to train a classifier,0
start with empty results and default shared insights,0
convert categorical indicators to numeric indices,0
check for indices over the categorical expansion bound,0
"can't remove in place while iterating over new_inds, so store in separate list",0
also remove from train_inds so we don't try to access the result later,0
extract subset of names matching new columns,0
properties to return from effect InferenceResults,0
properties to return from PopulationSummaryResults,0
Converts strings to property lookups or method calls as a convenience so that the,0
_point_props and _summary_props above can be applied to an inference object,0
Create a summary combining all results into a single output; this is used,0
by the various causal_effect and causal_effect_dict methods to generate either a dataframe,0
"or a dictionary, respectively, based on the summary function passed into this method",0
"ensure array has shape (m,y,t)",0
population summary is missing sample dimension; add it for consistency,0
outcome dimension is missing; add it for consistency,0
add singleton treatment dimension if missing,0
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce,0
"each attr has dimension (m,y) or (m,y,t)",0
concatenate along treatment dimension,0
"for dictionary representation, want to remove unneeded sample dimension",0
in cohort and global results,0
TODO: enrich outcome logic for multi-class classification when that is supported,1
can't drop only level,0
should be serialization-ready and contain no numpy arrays,0
a global inference indicates the effect of that one feature on the outcome,0
need to reshape the output to match the input,0
we want to offset the inference object by the baseline estimate of y,0
"NOTE: this calculation is correct only if treatment costs are marginal costs,",0
because then scaling the difference between treatment value and treatment costs is the,0
same as scaling the treatment value and subtracting the scaled treatment cost.,0
,0
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for",0
"continuous treatments, the policy value should include the benefit of decreasing treatments",0
(rather than just not treating at all),0
,0
"We can get the total by seeing that if we restrict attention to units where we would treat,",0
2 * policy_value - always_treat,0
includes exactly their contribution because policy_value and always_treat both include it,0
"and likewise restricting attention to the units where we want to decrease treatment,",0
2 * policy_value - always-treat,0
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value",0
is zero and the contribution to always_treat is negative,0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
get dataframe with all but selected column,0
apply 10% of a typical treatment for this feature,0
set the effect bounds; for positive treatments these agree with,0
"the estimates; for negative treatments, we need to invert the interval",0
the effect is now always positive since we decrease treatment when negative,0
"for discrete treatment, stack a zero result in front for control",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values",0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Call weighted lasso on reduced design matrix,0
Weighted tau,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to WeightedLassoCV by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
set default values for None,0
check freq_weight should be integer and should be accompanied by sample_var,0
check array shape,0
weight X and y and sample_var,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
average the outcome dimension if it exists and ensure 2d y_pred,0
get index of best treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
d_t=None here since we measure the effect across all Ts,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile",0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)",0
def mlasso_model(): return MultiTaskLassoCV(,0
"cv=3, alphas=alpha_regs, max_iter=200)",0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
"alpha_regs = [5e-3, 1e-2, 5e-2]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)",0
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)",0
subset of features that are exogenous and create heterogeneity,0
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features),0
subset of features wrt we estimate heterogeneity,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
introspect the constructor arguments to find the model parameters,0
to represent,0
"if the argument is deprecated, ignore it",0
Extract and sort argument names excluding 'self',0
column names,0
transfer input to numpy arrays,0
transfer input to 2d arrays,0
create dataframe,0
currently dowhy only support single outcome and single treatment,0
call dowhy,0
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update",0
cate estimator but not the effect.,0
don't proxy special methods,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
For when checking input values is disabled,0
Type to column extraction function,0
"Get number of arguments, some sklearn featurizer don't accept feature_names",0
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names',0
Get feature names using featurizer,0
All attempts at retrieving transformed feature names have failed,0
Delegate handling to downstream logic,0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
assume that we should perform nested cross-validation if and only if,0
the model has a 'cv' attribute; this is a somewhat brittle assumption...,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a 'groups' argument,0
now we have to compute the folds explicitly because some classifiers (like LassoCV),0
don't use the groups when calling split internally,0
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Any access should throw,0
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
input feature name is already updated by cate_feature_names.,0
define the index of d_x to filter for each given T,0
filter X after broadcast with T for each given T,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains some snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
make any access to matplotlib or plt throw an exception,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
Initialize saturation & value; calculate chroma & value shift,0
Calculate some intermediate values,0
Initialize RGB with same hue & chroma as our color,0
Shift the initial RGB values to match value and store,0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use mean of targets,0
Write node mean CATE,0
Write node std of CATE,0
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
Fetch appropriate color for node,0
Write node mean CATE,0
Write node mean CATE,0
Write recommended treatment and value - cost,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
This flag is true when names are set in a child class instead,0
"If names are set in a child class, add an attribute reflecting that",0
This works only if X is passed as a kwarg,0
We plan to enforce X as kwarg only in future releases,0
This checks if names have been set in a child class,0
"If names were set in a child class, don't do it again",0
"Wraps-up fit by setting attributes, cleaning up, etc.",0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"Treatment names is None, default to BaseCateEstimator",0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
Get input names,0
Summary,0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
Get input names,0
Summary,0
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
remove None arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same,0
alteration even when we only want to fit_final.,0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)",0
"however, sklearn doesn't support both stratifying and grouping (see",0
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply",0
their own object that supports grouping if they want to use groups.,0
for each mc iteration,0
for each model under cross fit setting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Policy Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base Policy tree,0
=============================================================================,0
The values below are required and utilitized by methods in the _SingleTreeExporterMixin,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"since we only allow single dimensional y, we could flatten the prediction",0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Handles the corner case when X=None but featurizer might be not None,0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
Replacing to remove docstring,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE: important to use the rlearner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the rlearner's copy to retain the information from fitting,0
Handles the corner case when X=None but featurizer might be not None,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support freq_weight and sample_var in debiased lasso,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add blb to parent's options,0
override only so that we can update the docstring to indicate,0
support for `GenericSingleTreatmentModelFinalInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
NOTE: important to get parent's wrapped copy so that,0
"after training wrapped featurizer is also trained, etc.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Fit a doubly robust average effect,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
"If custom param grid, check that only estimator parameters are being altered",0
override only so that we can update the docstring to indicate support for `blb`,0
Get input names,0
Summary,0
Determine output settings,0
"Important: This must be the first invocation of the random state at fit time, so that",0
train/test splits are re-generatable from an external object simply by knowing the,0
random_state parameter of the tree. Can be useful in the future if one wants to create local,0
linear predictions. Currently is also useful for testing.,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
Set min_weight_leaf from min_weight_fraction_leaf,0
Build tree,0
We calculate the maximum number of samples from each half-split that any node in the tree can,0
hold. Used by criterion for memory space savings.,0
Initialize the criterion object and the criterion_val object if honest.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code is a fork from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
Set parameters,0
Don't instantiate estimators now! Parameters of base_estimator might,0
"still change. Eg., when grid-searching with the nested object syntax.",0
self.estimators_ needs to be filled by the derived classes in fit.,0
Compute the number of jobs,0
Partition estimators between jobs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=None here since we measure the effect across all Ts,0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
squeeze the first axis,0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector,0
d_t=None here since we measure the effect across all Ts,0
d_t=None here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet",0
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx,0
NOTE: use np.asarray(offset) becuase if offset is a pd.Series direct addition would make the sum,0
"a Series as well, which would subsequently break summary_frame because flatten isn't supported",0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
offset preds,0
"offset the distribution, too",0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base GRF tree,0
=============================================================================,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
=============================================================================,0
A MultOutputWrapper for GRF classes,0
=============================================================================,0
=============================================================================,0
Instantiations of Generalized Random Forest,0
=============================================================================,0
"Append a constant treatment if `fit_intercept=True`, the coefficient",0
in front of the constant treatment is the intercept in the moment equation.,0
"Append a constant treatment and constant instrument if `fit_intercept=True`,",0
the coefficient in front of the constant treatment is the intercept in the moment equation.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Base Generalized Random Forest,0
=============================================================================,0
TODO: support freq_weight and sample_var,1
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle,0
We calculate the min eigenvalue proxy that each criterion is considering,0
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`",0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Generating indices a priori before parallelism ended up being orders of magnitude,0
faster than how sklearn does it. The reason is that random samplers do not release the,0
gil it seems.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
####################,0
Variance correction,0
####################,0
Subtract the average within bag variance. This ends up being equal to the,0
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).,0
The negative part is just sq_between.,0
Objective bayes debiasing for the diagonals where we know a-prior they are positive,0
"The off diagonals we have no objective prior, so no correction is applied.",0
Finally correcting the pred_cov or pred_var,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
test that the subsampling scheme past to the trees is correct,0
The sample size is chosen in particular to test rounding based error when subsampling,0
test that the estimator calcualtes var correctly,0
test api,0
test accuracy,0
test the projection functionality of forests,0
test that the estimator calcualtes var correctly,0
test api,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
test that the subsampling scheme past to the trees is correct,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
omit the lalonde notebook,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
compare with weight,0
compare with weight,0
compare with weight,0
compare with weight,0
Multi-dimensional output y,0
1-d y,0
compare when both sample_var and sample_weight exist,0
multi-d y,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
only is not None when T1 is a constant or a list of constant,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test at least one estimator from each category,0
test causal graph,0
test refutation estimate,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure that we've got at least two of every row,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
need to make sure we get all *joint* combinations,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat requires X,0
ensure we can serialize unfit estimator,0
these support only W but not X,0
"these support only binary, not general discrete T and Z",0
ensure we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
TODO: add tests for extra properties like coef_ where they exist,1
TODO: add tests for extra properties like coef_ where they exist,1
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
"however, with custom splits the checking happens in the first stage wrapper",0
where we don't have all of the required information to do this;,0
we'd probably need to add it to _crossfit instead,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
make sure we can get out post-fit stuff,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet",0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
helper class,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
Can't handle multi-dimensional treatments,0
"global shape is (d_y, sum(d_t))",0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
Can't handle multi-dimensional treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
Can't handle multi-dimensional treatments,0
"global shape is (d_y, sum(d_t))",0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
make sure we don't run into problems dropping every index,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"global shape is (d_y, sum(d_t))",0
Can't handle multi-dimensional treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
Can't handle multi-dimensional treatments,0
dgp,0
model,0
model,0
"columns 'd', 'e', 'h' have too many values",0
"columns 'd', 'e' have too many values",0
lowering bound shouldn't affect already fit columns when warm starting,0
"column d is now okay, too",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Define data features,0
Added `_df`to names to be different from the default cate_estimator names,0
Generate data,0
################################,0
Single treatment and outcome #,0
################################,0
Test LinearDML,0
|--> Test featurizers,0
ColumnTransformer doesn't propagate column names,0
|--> Test re-fit,0
Test SparseLinearDML,0
Test ForestDML,0
###################################,0
Mutiple treatments and outcomes #,0
###################################,0
Test LinearDML,0
Test SparseLinearDML,0
"Single outcome only, ORF does not support multiple outcomes",0
Test DMLOrthoForest,0
Test DROrthoForest,0
Test XLearner,0
Skipping population summary names test because bootstrap inference is too slow,0
Test SLearner,0
Test TLearner,0
Test LinearDRLearner,0
Test SparseLinearDRLearner,0
Test ForestDRLearner,0
Test LinearIntentToTreatDRIV,0
Test DeepIV,0
Test categorical treatments,0
Check refit,0
Check refit after setting categories,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
Treatment effect function,0
Outcome support,0
Treatment support,0
"Generate controls, covariates, treatments and outcomes",0
Heterogeneous treatment effects,0
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
through shap package.,0
test shap could generate the plot from the shap_values,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we need the controls' encoded column for training",0
"Thus, we append the controls column before the one-hot-encoded T",0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages",0
output is,0
"* a column of ones if X, W, and Z are all None",0
* just X or W or Z if both of the others are None,0
* hstack([arrs]) for whatever subset are not None otherwise,0
ensure Z is 2D,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
"instruments, and outcomes",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"TODO: check that Y, T, Z do not have multiple columns",0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: do correct adjustment for sample_var,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res",0
TODO: allow the final model to actually use X? Then we'd need to rename the class,1
since we would actually be calculating a CATE rather than ATE.,0
TODO: allow the final model to actually use X?,1
TODO: allow the final model to actually use X?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring",0
TODO: would it be useful to extend to handle controls ala vanilla DML?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: this utility is documented but internal; reimplement?,1
TODO: this utility is even less public...,1
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged",0
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns,0
but also supports get_feature_names with expected signature,0
Convert python objects to (possibly nested) types that can easily be represented as literals,0
named tuple type for storing results inside CausalAnalysis class;,0
must be lifted to module level to enable pickling,0
Validate inputs,0
TODO: check compatibility of X and Y lengths,1
"no previous fit, cancel warm start",0
"work with numeric feature indices, so that we can easily compare with categorical ones",0
"if heterogeneity_inds is 1D, repeat it",0
heterogeneity inds should be a 2D list of length same as train_inds,0
replace None elements of heterogeneity_inds and ensure indices are numeric,0
"TODO: bail out also if categorical columns, classification changed?",1
TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
train the Y model,0
"perform model selection for the Y model using all X, not on a per-column basis",0
"now that we've trained the classifier and wrapped it, ensure that y is transformed to",0
work with the regression wrapper,0
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays,0
"note that this needs to happen after wrapping to generalize to the multi-class case,",0
since otherwise we'll have too many columns to be able to train a classifier,0
start with empty results and default shared insights,0
convert categorical indicators to numeric indices,0
check for indices over the categorical expansion bound,0
"can't remove in place while iterating over new_inds, so store in separate list",0
also remove from train_inds so we don't try to access the result later,0
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names,0
Controls are all other columns of X,0
"can't use X[:, feat_ind] when X is a DataFrame",0
array checking routines don't accept 0-width arrays,0
perform model selection,0
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative,0
convert to NormalInferenceResults for consistency,0
Set the dictionary values shared between local and global summaries,0
extract subset of names matching new columns,0
properties to return from effect InferenceResults,0
properties to return from PopulationSummaryResults,0
Converts strings to property lookups or method calls as a convenience so that the,0
_point_props and _summary_props above can be applied to an inference object,0
Create a summary combining all results into a single output; this is used,0
by the various causal_effect and causal_effect_dict methods to generate either a dataframe,0
"or a dictionary, respectively, based on the summary function passed into this method",0
"ensure array has shape (m,y,t)",0
population summary is missing sample dimension; add it for consistency,0
outcome dimension is missing; add it for consistency,0
add singleton treatment dimension if missing,0
"each attr has dimension (m,y) or (m,y,t)",0
concatenate along treatment dimension,0
"for dictionary representation, want to remove unneeded sample dimension",0
in cohort and global results,0
TODO: enrich outcome logic for multi-class classification when that is supported,1
can't drop only level,0
should be serialization-ready and contain no numpy arrays,0
a global inference indicates the effect of that one feature on the outcome,0
need to reshape the output to match the input,0
we want to offset the inference object by the baseline estimate of y,0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values",0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Call weighted lasso on reduced design matrix,0
Weighted tau,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to WeightedLassoCV by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
set default values for None,0
check freq_weight should be integer and should be accompanied by sample_var,0
check array shape,0
weight X and y and sample_var,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
average the outcome dimension if it exists and ensure 2d y_pred,0
get index of best treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
d_t=None here since we measure the effect across all Ts,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile",0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)",0
def mlasso_model(): return MultiTaskLassoCV(,0
"cv=3, alphas=alpha_regs, max_iter=200)",0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
"alpha_regs = [5e-3, 1e-2, 5e-2]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)",0
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)",0
subset of features that are exogenous and create heterogeneity,0
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features),0
subset of features wrt we estimate heterogeneity,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
introspect the constructor arguments to find the model parameters,0
to represent,0
"if the argument is deprecated, ignore it",0
Extract and sort argument names excluding 'self',0
column names,0
transfer input to numpy arrays,0
transfer input to 2d arrays,0
create dataframe,0
currently dowhy only support single outcome and single treatment,0
call dowhy,0
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update",0
cate estimator but not the effect.,0
don't proxy special methods,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
For when checking input values is disabled,0
Type to column extraction function,0
"Get number of arguments, some sklearn featurizer don't accept feature_names",0
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names',0
Get feature names using featurizer,0
All attempts at retrieving transformed feature names have failed,0
Delegate handling to downstream logic,0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
assume that we should perform nested cross-validation if and only if,0
the model has a 'cv' attribute; this is a somewhat brittle assumption...,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a 'groups' argument,0
now we have to compute the folds explicitly because some classifiers (like LassoCV),0
don't use the groups when calling split internally,0
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Any access should throw,0
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
input feature name is already updated by cate_feature_names.,0
define the index of d_x to filter for each given T,0
filter X after broadcast with T for each given T,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains some snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
make any access to matplotlib or plt throw an exception,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
Initialize saturation & value; calculate chroma & value shift,0
Calculate some intermediate values,0
Initialize RGB with same hue & chroma as our color,0
Shift the initial RGB values to match value and store,0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use mean of targets,0
Write node mean CATE,0
Write node std of CATE,0
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
Fetch appropriate color for node,0
Write node mean CATE,0
Write node mean CATE,0
Write recommended treatment and value - cost,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
This flag is true when names are set in a child class instead,0
"If names are set in a child class, add an attribute reflecting that",0
This works only if X is passed as a kwarg,0
We plan to enforce X as kwarg only in future releases,0
This checks if names have been set in a child class,0
"If names were set in a child class, don't do it again",0
"Wraps-up fit by setting attributes, cleaning up, etc.",0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"Treatment names is None, default to BaseCateEstimator",0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
Get input names,0
Summary,0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
Get input names,0
Summary,0
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
remove None arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same,0
alteration even when we only want to fit_final.,0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)",0
"however, sklearn doesn't support both stratifying and grouping (see",0
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply",0
their own object that supports grouping if they want to use groups.,0
for each mc iteration,0
for each model under cross fit setting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Policy Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base Policy tree,0
=============================================================================,0
The values below are required and utilitized by methods in the _SingleTreeExporterMixin,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"since we only allow single dimensional y, we could flatten the prediction",0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Handles the corner case when X=None but featurizer might be not None,0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
TODO: support freq_weight and sample_var in debiased lasso,1
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
Replacing to remove docstring,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE: important to use the rlearner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the rlearner's copy to retain the information from fitting,0
Handles the corner case when X=None but featurizer might be not None,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support freq_weight and sample_var in debiased lasso,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add blb to parent's options,0
override only so that we can update the docstring to indicate,0
support for `GenericSingleTreatmentModelFinalInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
NOTE: important to get parent's wrapped copy so that,0
"after training wrapped featurizer is also trained, etc.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Fit a doubly robust average effect,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
"If custom param grid, check that only estimator parameters are being altered",0
override only so that we can update the docstring to indicate support for `blb`,0
Get input names,0
Summary,0
Determine output settings,0
"Important: This must be the first invocation of the random state at fit time, so that",0
train/test splits are re-generatable from an external object simply by knowing the,0
random_state parameter of the tree. Can be useful in the future if one wants to create local,0
linear predictions. Currently is also useful for testing.,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
Set min_weight_leaf from min_weight_fraction_leaf,0
Build tree,0
We calculate the maximum number of samples from each half-split that any node in the tree can,0
hold. Used by criterion for memory space savings.,0
Initialize the criterion object and the criterion_val object if honest.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code is a fork from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
Set parameters,0
Don't instantiate estimators now! Parameters of base_estimator might,0
"still change. Eg., when grid-searching with the nested object syntax.",0
self.estimators_ needs to be filled by the derived classes in fit.,0
Compute the number of jobs,0
Partition estimators between jobs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=None here since we measure the effect across all Ts,0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
squeeze the first axis,0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector,0
d_t=None here since we measure the effect across all Ts,0
d_t=None here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet",0
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx,0
NOTE: use np.asarray(offset) becuase if offset is a pd.Series direct addition would make the sum,0
"a Series as well, which would subsequently break summary_frame because flatten isn't supported",0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
offset preds,0
"offset the distribution, too",0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base GRF tree,0
=============================================================================,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
=============================================================================,0
A MultOutputWrapper for GRF classes,0
=============================================================================,0
=============================================================================,0
Instantiations of Generalized Random Forest,0
=============================================================================,0
"Append a constant treatment if `fit_intercept=True`, the coefficient",0
in front of the constant treatment is the intercept in the moment equation.,0
"Append a constant treatment and constant instrument if `fit_intercept=True`,",0
the coefficient in front of the constant treatment is the intercept in the moment equation.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Base Generalized Random Forest,0
=============================================================================,0
TODO: support freq_weight and sample_var,1
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle,0
We calculate the min eigenvalue proxy that each criterion is considering,0
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`",0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Generating indices a priori before parallelism ended up being orders of magnitude,0
faster than how sklearn does it. The reason is that random samplers do not release the,0
gil it seems.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
####################,0
Variance correction,0
####################,0
Subtract the average within bag variance. This ends up being equal to the,0
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).,0
The negative part is just sq_between.,0
Objective bayes debiasing for the diagonals where we know a-prior they are positive,0
"The off diagonals we have no objective prior, so no correction is applied.",0
Finally correcting the pred_cov or pred_var,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
test api,0
test accuracy,0
test the projection functionality of forests,0
test that the estimator calcualtes var correctly,0
test api,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
test that the subsampling scheme past to the trees is correct,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
omit the lalonde notebook,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
compare with weight,0
compare with weight,0
compare with weight,0
compare with weight,0
Multi-dimensional output y,0
1-d y,0
compare when both sample_var and sample_weight exist,0
multi-d y,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
compare when both sample_var and sample_weight exist,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
only is not None when T1 is a constant or a list of constant,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test at least one estimator from each category,0
test causal graph,0
test refutation estimate,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure that we've got at least two of every row,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
need to make sure we get all *joint* combinations,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat requires X,0
ensure we can serialize unfit estimator,0
these support only W but not X,0
"these support only binary, not general discrete T and Z",0
ensure we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
TODO: add tests for extra properties like coef_ where they exist,1
TODO: add tests for extra properties like coef_ where they exist,1
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
"however, with custom splits the checking happens in the first stage wrapper",0
where we don't have all of the required information to do this;,0
we'd probably need to add it to _crossfit instead,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
make sure we can get out post-fit stuff,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet",0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
helper class,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
Can't handle multi-dimensional treatments,0
"global shape is (d_y, sum(d_t))",0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
Can't handle multi-dimensional treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
Can't handle multi-dimensional treatments,0
"global shape is (d_y, sum(d_t))",0
"ExitStack can be used as a ""do nothing"" ContextManager",0
features; for categoricals they should appear #cats-1 times each,0
make sure we don't run into problems dropping every index,0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
"global shape is (d_y, sum(d_t))",0
Can't handle multi-dimensional treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"global and cohort data should have exactly the same structure, but different values",0
local index should have as many times entries as global as there were rows passed in,0
features; for categoricals they should appear #cats-1 times each,0
"global shape is (d_y, sum(d_t))",0
Can't handle multi-dimensional treatments,0
dgp,0
model,0
model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Define data features,0
Added `_df`to names to be different from the default cate_estimator names,0
Generate data,0
################################,0
Single treatment and outcome #,0
################################,0
Test LinearDML,0
|--> Test featurizers,0
ColumnTransformer doesn't propagate column names,0
|--> Test re-fit,0
Test SparseLinearDML,0
Test ForestDML,0
###################################,0
Mutiple treatments and outcomes #,0
###################################,0
Test LinearDML,0
Test SparseLinearDML,0
"Single outcome only, ORF does not support multiple outcomes",0
Test DMLOrthoForest,0
Test DROrthoForest,0
Test XLearner,0
Skipping population summary names test because bootstrap inference is too slow,0
Test SLearner,0
Test TLearner,0
Test LinearDRLearner,0
Test SparseLinearDRLearner,0
Test ForestDRLearner,0
Test LinearIntentToTreatDRIV,0
Test DeepIV,0
Test categorical treatments,0
Check refit,0
Check refit after setting categories,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
Treatment effect function,0
Outcome support,0
Treatment support,0
"Generate controls, covariates, treatments and outcomes",0
Heterogeneous treatment effects,0
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
through shap package.,0
test shap could generate the plot from the shap_values,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we need the controls' encoded column for training",0
"Thus, we append the controls column before the one-hot-encoded T",0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages",0
output is,0
"* a column of ones if X, W, and Z are all None",0
* just X or W or Z if both of the others are None,0
* hstack([arrs]) for whatever subset are not None otherwise,0
ensure Z is 2D,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
"instruments, and outcomes",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"TODO: check that Y, T, Z do not have multiple columns",0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: do correct adjustment for sample_var,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res",0
TODO: allow the final model to actually use X? Then we'd need to rename the class,1
since we would actually be calculating a CATE rather than ATE.,0
TODO: allow the final model to actually use X?,1
TODO: allow the final model to actually use X?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring",0
TODO: would it be useful to extend to handle controls ala vanilla DML?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: this utility is documented but internal; reimplement?,1
TODO: this utility is even less public...,1
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged",0
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns,0
but also supports get_feature_names with expected signature,0
Validate inputs,0
TODO: check compatibility of X and Y lengths,1
"no previous fit, cancel warm start",0
TODO: implement check for upper bound on categoricals,1
"work with numeric feature indices, so that we can easily compare with categorical ones",0
"if heterogeneity_inds is 1D, repeat it",0
heterogeneity inds should be a 2D list of length same as train_inds,0
replace None elements of heterogeneity_inds and ensure indices are numeric,0
"TODO: bail out also if categorical columns, classification changed?",1
TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
train the Y model,0
"perform model selection for the Y model using all X, not on a per-column basis",0
"now that we've trained the classifier and wrapped it, ensure that y is transformed to",0
work with the regression wrapper,0
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays,0
"note that this needs to happen after wrapping to generalize to the multi-class case,",0
since otherwise we'll have too many columns to be able to train a classifier,0
start with empty results and default shared insights,0
convert categorical indicators to numeric indices,0
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names,0
Controls are all other columns of X,0
"can't use X[:, feat_ind] when X is a DataFrame",0
array checking routines don't accept 0-width arrays,0
perform model selection,0
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative,0
convert to NormalInferenceResults for consistency,0
Set the dictionary values shared between local and global summaries,0
extract subset of names matching new columns,0
properties to return from effect InferenceResults,0
properties to return from PopulationSummaryResults,0
Converts strings to property lookups or method calls as a convenience so that the,0
_point_props and _summary_props above can be applied to an inference object,0
Create a summary combining all results into a single output; this is used,0
by the various causal_effect and causal_effect_dict methods to generate either a dataframe,0
"or a dictionary, respectively, based on the summary function passed into this method",0
"ensure array has shape (m,y,t)",0
population summary is missing sample dimension; add it for consistency,0
outcome dimension is missing; add it for consistency,0
add singleton treatment dimension if missing,0
"each attr has dimension (m,y) or (m,y,t)",0
concatenate along treatment dimension,0
"for dictionary representation, want to remove unneeded sample dimension",0
in cohort and global results,0
TODO: enrich outcome logic for multi-class classification when that is supported,1
can't drop only level,0
should be serialization-ready and contain no numpy arrays,0
a global inference indicates the effect of that one feature on the outcome,0
need to reshape the output to match the input,0
we want to offset the inference object by the baseline estimate of y,0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
"however, the tree can't store the feature and treatment names we compute here...",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values",0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Call weighted lasso on reduced design matrix,0
Weighted tau,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to WeightedLassoCV by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
set default values for None,0
check freq_weight should be integer and should be accompanied by sample_var,0
check array shape,0
weight X and y and sample_var,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
average the outcome dimension if it exists and ensure 2d y_pred,0
get index of best treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
d_t=None here since we measure the effect across all Ts,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile",0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)",0
def mlasso_model(): return MultiTaskLassoCV(,0
"cv=3, alphas=alpha_regs, max_iter=200)",0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
"alpha_regs = [5e-3, 1e-2, 5e-2]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)",0
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)",0
subset of features that are exogenous and create heterogeneity,0
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features),0
subset of features wrt we estimate heterogeneity,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
introspect the constructor arguments to find the model parameters,0
to represent,0
"if the argument is deprecated, ignore it",0
Extract and sort argument names excluding 'self',0
column names,0
transfer input to numpy arrays,0
transfer input to 2d arrays,0
create dataframe,0
currently dowhy only support single outcome and single treatment,0
call dowhy,0
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update",0
cate estimator but not the effect.,0
don't proxy special methods,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
For when checking input values is disabled,0
Type to column extraction function,0
"Get number of arguments, some sklearn featurizer don't accept feature_names",0
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names',0
Get feature names using featurizer,0
All attempts at retrieving transformed feature names have failed,0
Delegate handling to downstream logic,0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
assume that we should perform nested cross-validation if and only if,0
the model has a 'cv' attribute; this is a somewhat brittle assumption...,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a 'groups' argument,0
now we have to compute the folds explicitly because some classifiers (like LassoCV),0
don't use the groups when calling split internally,0
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Any access should throw,0
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
input feature name is already updated by cate_feature_names.,0
define the index of d_x to filter for each given T,0
filter X after broadcast with T for each given T,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains some snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
make any access to matplotlib or plt throw an exception,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
Initialize saturation & value; calculate chroma & value shift,0
Calculate some intermediate values,0
Initialize RGB with same hue & chroma as our color,0
Shift the initial RGB values to match value and store,0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use mean of targets,0
Write node mean CATE,0
Write node std of CATE,0
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
Fetch appropriate color for node,0
Write node mean CATE,0
Write node mean CATE,0
Write recommended treatment and value - cost,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
This flag is true when names are set in a child class instead,0
"If names are set in a child class, add an attribute reflecting that",0
This works only if X is passed as a kwarg,0
We plan to enforce X as kwarg only in future releases,0
This checks if names have been set in a child class,0
"If names were set in a child class, don't do it again",0
"Wraps-up fit by setting attributes, cleaning up, etc.",0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"Treatment names is None, default to BaseCateEstimator",0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
Get input names,0
Summary,0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
Get input names,0
Summary,0
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
remove None arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same,0
alteration even when we only want to fit_final.,0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)",0
"however, sklearn doesn't support both stratifying and grouping (see",0
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply",0
their own object that supports grouping if they want to use groups.,0
for each mc iteration,0
for each model under cross fit setting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Policy Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base Policy tree,0
=============================================================================,0
The values below are required and utilitized by methods in the _SingleTreeExporterMixin,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"since we only allow single dimensional y, we could flatten the prediction",0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Handles the corner case when X=None but featurizer might be not None,0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
TODO: support sample_var,1
Replacing to remove docstring,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE: important to use the rlearner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the rlearner's copy to retain the information from fitting,0
Handles the corner case when X=None but featurizer might be not None,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support sample_var,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add blb to parent's options,0
override only so that we can update the docstring to indicate,0
support for `GenericSingleTreatmentModelFinalInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
NOTE: important to get parent's wrapped copy so that,0
"after training wrapped featurizer is also trained, etc.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Fit a doubly robust average effect,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
"If custom param grid, check that only estimator parameters are being altered",0
override only so that we can update the docstring to indicate support for `blb`,0
Get input names,0
Summary,0
Determine output settings,0
"Important: This must be the first invocation of the random state at fit time, so that",0
train/test splits are re-generatable from an external object simply by knowing the,0
random_state parameter of the tree. Can be useful in the future if one wants to create local,0
linear predictions. Currently is also useful for testing.,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
Set min_weight_leaf from min_weight_fraction_leaf,0
Build tree,0
We calculate the maximum number of samples from each half-split that any node in the tree can,0
hold. Used by criterion for memory space savings.,0
Initialize the criterion object and the criterion_val object if honest.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code is a fork from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
Set parameters,0
Don't instantiate estimators now! Parameters of base_estimator might,0
"still change. Eg., when grid-searching with the nested object syntax.",0
self.estimators_ needs to be filled by the derived classes in fit.,0
Compute the number of jobs,0
Partition estimators between jobs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=None here since we measure the effect across all Ts,0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
squeeze the first axis,0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector,0
d_t=None here since we measure the effect across all Ts,0
d_t=None here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet",0
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx,0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base GRF tree,0
=============================================================================,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
=============================================================================,0
A MultOutputWrapper for GRF classes,0
=============================================================================,0
=============================================================================,0
Instantiations of Generalized Random Forest,0
=============================================================================,0
"Append a constant treatment if `fit_intercept=True`, the coefficient",0
in front of the constant treatment is the intercept in the moment equation.,0
"Append a constant treatment and constant instrument if `fit_intercept=True`,",0
the coefficient in front of the constant treatment is the intercept in the moment equation.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Base Generalized Random Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle,0
We calculate the min eigenvalue proxy that each criterion is considering,0
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`",0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Generating indices a priori before parallelism ended up being orders of magnitude,0
faster than how sklearn does it. The reason is that random samplers do not release the,0
gil it seems.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
####################,0
Variance correction,0
####################,0
Subtract the average within bag variance. This ends up being equal to the,0
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).,0
The negative part is just sq_between.,0
Objective bayes debiasing for the diagonals where we know a-prior they are positive,0
"The off diagonals we have no objective prior, so no correction is applied.",0
Finally correcting the pred_cov or pred_var,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
test api,0
test accuracy,0
test the projection functionality of forests,0
test that the estimator calcualtes var correctly,0
test api,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
test that the subsampling scheme past to the trees is correct,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
Multi-dimensional output y,0
1-d y,0
multi-d y,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
only is not None when T1 is a constant or a list of constant,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test at least one estimator from each category,0
test causal graph,0
test refutation estimate,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure that we've got at least two of every row,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
need to make sure we get all *joint* combinations,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat requires X,0
ensure we can serialize unfit estimator,0
these support only W but not X,0
"these support only binary, not general discrete T and Z",0
ensure we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
TODO: add tests for extra properties like coef_ where they exist,1
TODO: add tests for extra properties like coef_ where they exist,1
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
"however, with custom splits the checking happens in the first stage wrapper",0
where we don't have all of the required information to do this;,0
we'd probably need to add it to _crossfit instead,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet",0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
helper class,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Define data features,0
Added `_df`to names to be different from the default cate_estimator names,0
Generate data,0
################################,0
Single treatment and outcome #,0
################################,0
Test LinearDML,0
|--> Test featurizers,0
ColumnTransformer doesn't propagate column names,0
|--> Test re-fit,0
Test SparseLinearDML,0
Test ForestDML,0
###################################,0
Mutiple treatments and outcomes #,0
###################################,0
Test LinearDML,0
Test SparseLinearDML,0
"Single outcome only, ORF does not support multiple outcomes",0
Test DMLOrthoForest,0
Test DROrthoForest,0
Test XLearner,0
Skipping population summary names test because bootstrap inference is too slow,0
Test SLearner,0
Test TLearner,0
Test LinearDRLearner,0
Test SparseLinearDRLearner,0
Test ForestDRLearner,0
Test LinearIntentToTreatDRIV,0
Test DeepIV,0
Test categorical treatments,0
Check refit,0
Check refit after setting categories,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
Treatment effect function,0
Outcome support,0
Treatment support,0
"Generate controls, covariates, treatments and outcomes",0
Heterogeneous treatment effects,0
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
through shap package.,0
test shap could generate the plot from the shap_values,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we need the controls' encoded column for training",0
"Thus, we append the controls column before the one-hot-encoded T",0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages",0
output is,0
"* a column of ones if X, W, and Z are all None",0
* just X or W or Z if both of the others are None,0
* hstack([arrs]) for whatever subset are not None otherwise,0
ensure Z is 2D,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
"instruments, and outcomes",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"TODO: check that Y, T, Z do not have multiple columns",0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res",0
TODO: allow the final model to actually use X? Then we'd need to rename the class,1
since we would actually be calculating a CATE rather than ATE.,0
TODO: allow the final model to actually use X?,1
TODO: allow the final model to actually use X?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring",0
TODO: would it be useful to extend to handle controls ala vanilla DML?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values",0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Call weighted lasso on reduced design matrix,0
Weighted tau,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to WeightedLassoCV by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
average the outcome dimension if it exists and ensure 2d y_pred,0
get index of best treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
d_t=None here since we measure the effect across all Ts,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile",0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)",0
def mlasso_model(): return MultiTaskLassoCV(,0
"cv=3, alphas=alpha_regs, max_iter=200)",0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
"alpha_regs = [5e-3, 1e-2, 5e-2]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)",0
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)",0
subset of features that are exogenous and create heterogeneity,0
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features),0
subset of features wrt we estimate heterogeneity,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
introspect the constructor arguments to find the model parameters,0
to represent,0
Extract and sort argument names excluding 'self',0
create dataframe,0
currently dowhy only support single outcome and single treatment,0
column names,0
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update",0
cate estimator but not the effect.,0
don't proxy special methods,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
For when checking input values is disabled,0
Type to column extraction function,0
"Get number of arguments, some sklearn featurizer don't accept feature_names",0
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names',0
Get feature names using featurizer,0
All attempts at retrieving transformed feature names have failed,0
Delegate handling to downstream logic,0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
assume that we should perform nested cross-validation if and only if,0
the model has a 'cv' attribute; this is a somewhat brittle assumption...,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a 'groups' argument,0
now we have to compute the folds explicitly because some classifiers (like LassoCV),0
don't use the groups when calling split internally,0
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Any access should throw,0
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
input feature name is already updated by cate_feature_names.,0
define the index of d_x to filter for each given T,0
filter X after broadcast with T for each given T,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
This flag is true when names are set in a child class instead,0
"If names are set in a child class, add an attribute reflecting that",0
This works only if X is passed as a kwarg,0
We plan to enforce X as kwarg only in future releases,0
This checks if names have been set in a child class,0
"If names were set in a child class, don't do it again",0
"Wraps-up fit by setting attributes, cleaning up, etc.",0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"Treatment names is None, default to BaseCateEstimator",0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
Get input names,0
Summary,0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
Get input names,0
Summary,0
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
remove None arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same,0
alteration even when we only want to fit_final.,0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)",0
"however, sklearn doesn't support both stratifying and grouping (see",0
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply",0
their own object that supports grouping if they want to use groups.,0
######################################################,0
These should be removed once `n_splits` is deprecated,0
######################################################,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"since we only allow single dimensional y, we could flatten the prediction",0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Handles the corner case when X=None but featurizer might be not None,0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
TODO: support sample_var,1
Replacing to remove docstring,0
###################################################################,0
Everything below should be removed once parameters are deprecated,0
###################################################################,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE: important to use the rlearner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the rlearner's copy to retain the information from fitting,0
Handles the corner case when X=None but featurizer might be not None,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support sample_var,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add blb to parent's options,0
override only so that we can update the docstring to indicate,0
support for `GenericSingleTreatmentModelFinalInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
NOTE: important to get parent's wrapped copy so that,0
"after training wrapped featurizer is also trained, etc.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
Fit a doubly robust average effect,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
"If custom param grid, check that only estimator parameters are being altered",0
override only so that we can update the docstring to indicate support for `blb`,0
Get input names,0
Summary,0
######################################################,0
These should be removed once `n_splits` is deprecated,0
######################################################,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=None here since we measure the effect across all Ts,0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
squeeze the first axis,0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector,0
d_t=None here since we measure the effect across all Ts,0
d_t=None here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet",0
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx,0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code is a fork from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
Set parameters,0
Don't instantiate estimators now! Parameters of base_estimator might,0
"still change. Eg., when grid-searching with the nested object syntax.",0
self.estimators_ needs to be filled by the derived classes in fit.,0
Compute the number of jobs,0
Partition estimators between jobs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base GRF tree,0
=============================================================================,0
Determine output settings,0
"Important: This must be the first invocation of the random state at fit time, so that",0
train/test splits are re-generatable from an external object simply by knowing the,0
random_state parameter of the tree. Can be useful in the future if one wants to create local,0
linear predictions. Currently is also useful for testing.,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
Set min_weight_leaf from min_weight_fraction_leaf,0
Build tree,0
We calculate the maximum number of samples from each half-split that any node in the tree can,0
hold. Used by criterion for memory space savings.,0
Initialize the criterion object and the criterion_val object if honest.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
=============================================================================,0
A MultOutputWrapper for GRF classes,0
=============================================================================,0
=============================================================================,0
Instantiations of Generalized Random Forest,0
=============================================================================,0
"Append a constant treatment if `fit_intercept=True`, the coefficient",0
in front of the constant treatment is the intercept in the moment equation.,0
"Append a constant treatment and constant instrument if `fit_intercept=True`,",0
the coefficient in front of the constant treatment is the intercept in the moment equation.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Base Generalized Random Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle,0
We calculate the min eigenvalue proxy that each criterion is considering,0
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`",0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Generating indices a priori before parallelism ended up being orders of magnitude,0
faster than how sklearn does it. The reason is that random samplers do not release the,0
gil it seems.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
####################,0
Variance correction,0
####################,0
Subtract the average within bag variance. This ends up being equal to the,0
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).,0
The negative part is just sq_between.,0
Objective bayes debiasing for the diagonals where we know a-prior they are positive,0
"The off diagonals we have no objective prior, so no correction is applied.",0
Finally correcting the pred_cov or pred_var,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
test api,0
test accuracy,0
test the projection functionality of forests,0
test that the estimator calcualtes var correctly,0
test api,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
test that the subsampling scheme past to the trees is correct,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
Multi-dimensional output y,0
1-d y,0
multi-d y,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
only is not None when T1 is a constant or a list of constant,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test at least one estimator from each category,0
test causal graph,0
test refutation estimate,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure that we've got at least two of every row,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
need to make sure we get all *joint* combinations,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat requires X,0
ensure we can serialize unfit estimator,0
these support only W but not X,0
"these support only binary, not general discrete T and Z",0
ensure we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
TODO: add tests for extra properties like coef_ where they exist,1
TODO: add tests for extra properties like coef_ where they exist,1
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
"however, with custom splits the checking happens in the first stage wrapper",0
where we don't have all of the required information to do this;,0
we'd probably need to add it to _crossfit instead,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet",0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
helper class,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Define data features,0
Added `_df`to names to be different from the default cate_estimator names,0
Generate data,0
################################,0
Single treatment and outcome #,0
################################,0
Test LinearDML,0
|--> Test featurizers,0
ColumnTransformer doesn't propagate column names,0
|--> Test re-fit,0
Test SparseLinearDML,0
Test ForestDML,0
###################################,0
Mutiple treatments and outcomes #,0
###################################,0
Test LinearDML,0
Test SparseLinearDML,0
"Single outcome only, ORF does not support multiple outcomes",0
Test DMLOrthoForest,0
Test DROrthoForest,0
Test XLearner,0
Skipping population summary names test because bootstrap inference is too slow,0
Test SLearner,0
Test TLearner,0
Test LinearDRLearner,0
Test SparseLinearDRLearner,0
Test ForestDRLearner,0
Test LinearIntentToTreatDRIV,0
Test DeepIV,0
Test categorical treatments,0
Check refit,0
Check refit after setting categories,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
Treatment effect function,0
Outcome support,0
Treatment support,0
"Generate controls, covariates, treatments and outcomes",0
Heterogeneous treatment effects,0
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
through shap package.,0
test shap could generate the plot from the shap_values,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we need the controls' encoded column for training",0
"Thus, we append the controls column before the one-hot-encoded T",0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages",0
output is,0
"* a column of ones if X, W, and Z are all None",0
* just X or W or Z if both of the others are None,0
* hstack([arrs]) for whatever subset are not None otherwise,0
ensure Z is 2D,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
"instruments, and outcomes",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"TODO: check that Y, T, Z do not have multiple columns",0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res",0
TODO: allow the final model to actually use X? Then we'd need to rename the class,1
since we would actually be calculating a CATE rather than ATE.,0
TODO: allow the final model to actually use X?,1
TODO: allow the final model to actually use X?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring",0
TODO: would it be useful to extend to handle controls ala vanilla DML?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values",0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Call weighted lasso on reduced design matrix,0
Weighted tau,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to WeightedLassoCV by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: generalize to multiple treatment case?,1
get index of best treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make any access to matplotlib or plt throw an exception,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use first target,0
Write node mean CATE,0
Write node std of CATE,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
Write node mean CATE,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
d_t=None here since we measure the effect across all Ts,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile",0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)",0
def mlasso_model(): return MultiTaskLassoCV(,0
"cv=3, alphas=alpha_regs, max_iter=200)",0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
"alpha_regs = [5e-3, 1e-2, 5e-2]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)",0
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)",0
subset of features that are exogenous and create heterogeneity,0
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features),0
subset of features wrt we estimate heterogeneity,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
introspect the constructor arguments to find the model parameters,0
to represent,0
Extract and sort argument names excluding 'self',0
create dataframe,0
currently dowhy only support single outcome and single treatment,0
column names,0
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update",0
cate estimator but not the effect.,0
don't proxy special methods,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
For when checking input values is disabled,0
Type to column extraction function,0
"Get number of arguments, some sklearn featurizer don't accept feature_names",0
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names',0
Get feature names using featurizer,0
All attempts at retrieving transformed feature names have failed,0
Delegate handling to downstream logic,0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
assume that we should perform nested cross-validation if and only if,0
the model has a 'cv' attribute; this is a somewhat brittle assumption...,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a 'groups' argument,0
now we have to compute the folds explicitly because some classifiers (like LassoCV),0
don't use the groups when calling split internally,0
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Any access should throw,0
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
input feature name is already updated by cate_feature_names.,0
define the index of d_x to filter for each given T,0
filter X after broadcast with T for each given T,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
This flag is true when names are set in a child class instead,0
"If names are set in a child class, add an attribute reflecting that",0
This works only if X is passed as a kwarg,0
We plan to enforce X as kwarg only in future releases,0
This checks if names have been set in a child class,0
"If names were set in a child class, don't do it again",0
"Wraps-up fit by setting attributes, cleaning up, etc.",0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"Treatment names is None, default to BaseCateEstimator",0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
Get input names,0
Summary,0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
Get input names,0
Summary,0
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
remove None arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same,0
alteration even when we only want to fit_final.,0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)",0
"however, sklearn doesn't support both stratifying and grouping (see",0
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply",0
their own object that supports grouping if they want to use groups.,0
######################################################,0
These should be removed once `n_splits` is deprecated,0
######################################################,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"since we only allow single dimensional y, we could flatten the prediction",0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Handles the corner case when X=None but featurizer might be not None,0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
TODO: support sample_var,1
Replacing to remove docstring,0
###################################################################,0
Everything below should be removed once parameters are deprecated,0
###################################################################,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE: important to use the rlearner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the rlearner's copy to retain the information from fitting,0
Handles the corner case when X=None but featurizer might be not None,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support sample_var,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add blb to parent's options,0
override only so that we can update the docstring to indicate,0
support for `GenericSingleTreatmentModelFinalInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
NOTE: important to get parent's wrapped copy so that,0
"after training wrapped featurizer is also trained, etc.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `blb`,0
######################################################,0
These should be removed once `n_splits` is deprecated,0
######################################################,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=None here since we measure the effect across all Ts,0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
squeeze the first axis,0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=None here since we measure the effect across all Ts,0
set the mean_pred_stderr,0
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector,0
d_t=None here since we measure the effect across all Ts,0
d_t=None here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet",0
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx,0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code is a fork from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
Set parameters,0
Don't instantiate estimators now! Parameters of base_estimator might,0
"still change. Eg., when grid-searching with the nested object syntax.",0
self.estimators_ needs to be filled by the derived classes in fit.,0
Compute the number of jobs,0
Partition estimators between jobs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base GRF tree,0
=============================================================================,0
Determine output settings,0
"Important: This must be the first invocation of the random state at fit time, so that",0
train/test splits are re-generatable from an external object simply by knowing the,0
random_state parameter of the tree. Can be useful in the future if one wants to create local,0
linear predictions. Currently is also useful for testing.,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
Set min_weight_leaf from min_weight_fraction_leaf,0
Build tree,0
We calculate the maximum number of samples from each half-split that any node in the tree can,0
hold. Used by criterion for memory space savings.,0
Initialize the criterion object and the criterion_val object if honest.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
=============================================================================,0
A MultOutputWrapper for GRF classes,0
=============================================================================,0
=============================================================================,0
Instantiations of Generalized Random Forest,0
=============================================================================,0
"Append a constant treatment if `fit_intercept=True`, the coefficient",0
in front of the constant treatment is the intercept in the moment equation.,0
"Append a constant treatment and constant instrument if `fit_intercept=True`,",0
the coefficient in front of the constant treatment is the intercept in the moment equation.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Base Generalized Random Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle,0
We calculate the min eigenvalue proxy that each criterion is considering,0
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`",0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Generating indices a priori before parallelism ended up being orders of magnitude,0
faster than how sklearn does it. The reason is that random samplers do not release the,0
gil it seems.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
####################,0
Variance correction,0
####################,0
Subtract the average within bag variance. This ends up being equal to the,0
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).,0
The negative part is just sq_between.,0
Objective bayes debiasing for the diagonals where we know a-prior they are positive,0
"The off diagonals we have no objective prior, so no correction is applied.",0
Finally correcting the pred_cov or pred_var,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
test api,0
test accuracy,0
test the projection functionality of forests,0
test that the estimator calcualtes var correctly,0
test api,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
test that the subsampling scheme past to the trees is correct,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
Multi-dimensional output y,0
1-d y,0
multi-d y,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
only is not None when T1 is a constant or a list of constant,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test at least one estimator from each category,0
test causal graph,0
test refutation estimate,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure that we've got at least two of every row,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
need to make sure we get all *joint* combinations,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat requires X,0
ensure we can serialize unfit estimator,0
these support only W but not X,0
"these support only binary, not general discrete T and Z",0
ensure we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
TODO: add tests for extra properties like coef_ where they exist,1
TODO: add tests for extra properties like coef_ where they exist,1
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
"however, with custom splits the checking happens in the first stage wrapper",0
where we don't have all of the required information to do this;,0
we'd probably need to add it to _crossfit instead,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet",0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
helper class,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Define data features,0
Added `_df`to names to be different from the default cate_estimator names,0
Generate data,0
################################,0
Single treatment and outcome #,0
################################,0
Test LinearDML,0
|--> Test featurizers,0
ColumnTransformer doesn't propagate column names,0
|--> Test re-fit,0
Test SparseLinearDML,0
Test ForestDML,0
###################################,0
Mutiple treatments and outcomes #,0
###################################,0
Test LinearDML,0
Test SparseLinearDML,0
"Single outcome only, ORF does not support multiple outcomes",0
Test DMLOrthoForest,0
Test DROrthoForest,0
Test XLearner,0
Skipping population summary names test because bootstrap inference is too slow,0
Test SLearner,0
Test TLearner,0
Test LinearDRLearner,0
Test SparseLinearDRLearner,0
Test ForestDRLearner,0
Test LinearIntentToTreatDRIV,0
Test DeepIV,0
Test categorical treatments,0
Check refit,0
Check refit after setting categories,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test length of feature names equals to shap values shape,0
Treatment effect function,0
Outcome support,0
Treatment support,0
"Generate controls, covariates, treatments and outcomes",0
Heterogeneous treatment effects,0
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
through shap package.,0
test shap could generate the plot from the shap_values,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we need the controls' encoded column for training",0
"Thus, we append the controls column before the one-hot-encoded T",0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages",0
output is,0
"* a column of ones if X, W, and Z are all None",0
* just X or W or Z if both of the others are None,0
* hstack([arrs]) for whatever subset are not None otherwise,0
ensure Z is 2D,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
"instruments, and outcomes",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"TODO: check that Y, T, Z do not have multiple columns",0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res",0
TODO: allow the final model to actually use X? Then we'd need to rename the class,1
since we would actually be calculating a CATE rather than ATE.,0
TODO: allow the final model to actually use X?,1
TODO: allow the final model to actually use X?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring",0
TODO: would it be useful to extend to handle controls ala vanilla DML?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values",0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Call weighted lasso on reduced design matrix,0
Weighted tau,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to WeightedLassoCV by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: generalize to multiple treatment case?,1
get index of best treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make any access to matplotlib or plt throw an exception,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use first target,0
Write node mean CATE,0
Write node std of CATE,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
Write node mean CATE,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
d_t=None here since we measure the effect across all Ts,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile",0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)",0
def mlasso_model(): return MultiTaskLassoCV(,0
"cv=3, alphas=alpha_regs, max_iter=200)",0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
The first s_x state variables are confounders. The final s_x variables are exogenous and can create,0
heterogeneity,0
"alpha_regs = [5e-3, 1e-2, 5e-2]",0
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)",0
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)",0
subset of features that are exogenous and create heterogeneity,0
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features),0
subset of features wrt we estimate heterogeneity,0
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma",0
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
introspect the constructor arguments to find the model parameters,0
to represent,0
Extract and sort argument names excluding 'self',0
create dataframe,0
currently dowhy only support single outcome and single treatment,0
column names,0
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update",0
cate estimator but not the effect.,0
don't proxy special methods,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
Type to column extraction function,0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
assume that we should perform nested cross-validation if and only if,0
the model has a 'cv' attribute; this is a somewhat brittle assumption...,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a 'groups' argument,0
now we have to compute the folds explicitly because some classifiers (like LassoCV),0
don't use the groups when calling split internally,0
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Any access should throw,0
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
define the index of d_x to filter for each given T,0
filter X after broadcast with T for each given T,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
This flag is true when names are set in a child class instead,0
"If names are set in a child class, add an attribute reflecting that",0
This works only if X is passed as a kwarg,0
We plan to enforce X as kwarg only in future releases,0
This checks if names have been set in a child class,0
"If names were set in a child class, don't do it again",0
"Wraps-up fit by setting attributes, cleaning up, etc.",0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"Treatment names is None, default to BaseCateEstimator",0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
Get input names,0
Summary,0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
Get input names,0
Summary,0
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
remove None arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same,0
alteration even when we only want to fit_final.,0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)",0
"however, sklearn doesn't support both stratifying and grouping (see",0
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply",0
their own object that supports grouping if they want to use groups.,0
######################################################,0
These should be removed once `n_splits` is deprecated,0
######################################################,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Handles the corner case when X=None but featurizer might be not None,0
"This fails if X=None and featurizer is not None, but that case is handled above",0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
TODO: support sample_var,1
Replacing to remove docstring,0
###################################################################,0
Everything below should be removed once parameters are deprecated,0
###################################################################,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE: important to use the rlearner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the rlearner's copy to retain the information from fitting,0
Handles the corner case when X=None but featurizer might be not None,0
"This fails if X=None and featurizer is not None, but that case is handled above",0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support sample_var,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add blb to parent's options,0
override only so that we can update the docstring to indicate,0
support for `GenericSingleTreatmentModelFinalInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
NOTE: important to get parent's wrapped copy so that,0
"after training wrapped featurizer is also trained, etc.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `blb`,0
######################################################,0
These should be removed once `n_splits` is deprecated,0
######################################################,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=None here since we measure the effect across all Ts,0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
d_t=None here since we measure the effect across all Ts,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=None here since we measure the effect across all Ts,0
d_t=None here since we measure the effect across all Ts,0
d_t=None here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet",0
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx,0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code is a fork from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
Set parameters,0
Don't instantiate estimators now! Parameters of base_estimator might,0
"still change. Eg., when grid-searching with the nested object syntax.",0
self.estimators_ needs to be filled by the derived classes in fit.,0
Compute the number of jobs,0
Partition estimators between jobs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base GRF tree,0
=============================================================================,0
Determine output settings,0
"Important: This must be the first invocation of the random state at fit time, so that",0
train/test splits are re-generatable from an external object simply by knowing the,0
random_state parameter of the tree. Can be useful in the future if one wants to create local,0
linear predictions. Currently is also useful for testing.,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
Set min_weight_leaf from min_weight_fraction_leaf,0
Build tree,0
We calculate the maximum number of samples from each half-split that any node in the tree can,0
hold. Used by criterion for memory space savings.,0
Initialize the criterion object and the criterion_val object if honest.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
=============================================================================,0
A MultOutputWrapper for GRF classes,0
=============================================================================,0
=============================================================================,0
Instantiations of Generalized Random Forest,0
=============================================================================,0
"Append a constant treatment if `fit_intercept=True`, the coefficient",0
in front of the constant treatment is the intercept in the moment equation.,0
"Append a constant treatment and constant instrument if `fit_intercept=True`,",0
the coefficient in front of the constant treatment is the intercept in the moment equation.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Base Generalized Random Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle,0
We calculate the min eigenvalue proxy that each criterion is considering,0
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`",0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Generating indices a priori before parallelism ended up being orders of magnitude,0
faster than how sklearn does it. The reason is that random samplers do not release the,0
gil it seems.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
####################,0
Variance correction,0
####################,0
Subtract the average within bag variance. This ends up being equal to the,0
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).,0
The negative part is just sq_between.,0
Objective bayes debiasing for the diagonals where we know a-prior they are positive,0
"The off diagonals we have no objective prior, so no correction is applied.",0
Finally correcting the pred_cov or pred_var,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
test api,0
test accuracy,0
test the projection functionality of forests,0
test that the estimator calcualtes var correctly,0
test api,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
test that the subsampling scheme past to the trees is correct,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
Multi-dimensional output y,0
1-d y,0
multi-d y,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test at least one estimator from each category,0
test causal graph,0
test refutation estimate,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure that we've got at least two of every row,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
need to make sure we get all *joint* combinations,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat requires X,0
ensure we can serialize unfit estimator,0
these support only W but not X,0
"these support only binary, not general discrete T and Z",0
ensure we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
TODO: add tests for extra properties like coef_ where they exist,1
TODO: add tests for extra properties like coef_ where they exist,1
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
"however, with custom splits the checking happens in the first stage wrapper",0
where we don't have all of the required information to do this;,0
we'd probably need to add it to _crossfit instead,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet",0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Define data features,0
Added `_df`to names to be different from the default cate_estimator names,0
Generate data,0
################################,0
Single treatment and outcome #,0
################################,0
Test LinearDML,0
Test re-fit,0
Test SparseLinearDML,0
ForestDML,0
###################################,0
Mutiple treatments and outcomes #,0
###################################,0
Test LinearDML,0
Test SparseLinearDML,0
"Single outcome only, ORF does not support multiple outcomes",0
Test DMLOrthoForest,0
Test DROrthoForest,0
Test XLearner,0
Skipping population summary names test because bootstrap inference is too slow,0
Test SLearner,0
Test TLearner,0
Test LinearDRLearner,0
Test SparseLinearDRLearner,0
Test ForestDRLearner,0
Test LinearIntentToTreatDRIV,0
Test DeepIV,0
Test categorical treatments,0
Check refit,0
Check refit after setting categories,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
Treatment effect function,0
Outcome support,0
Treatment support,0
"Generate controls, covariates, treatments and outcomes",0
Heterogeneous treatment effects,0
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
through shap package.,0
test shap could generate the plot from the shap_values,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we need the controls' encoded column for training",0
"Thus, we append the controls column before the one-hot-encoded T",0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages",0
output is,0
"* a column of ones if X, W, and Z are all None",0
* just X or W or Z if both of the others are None,0
* hstack([arrs]) for whatever subset are not None otherwise,0
ensure Z is 2D,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
"instruments, and outcomes",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"TODO: check that Y, T, Z do not have multiple columns",0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res",0
TODO: allow the final model to actually use X? Then we'd need to rename the class,1
since we would actually be calculating a CATE rather than ATE.,0
TODO: allow the final model to actually use X?,1
TODO: allow the final model to actually use X?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring",0
TODO: would it be useful to extend to handle controls ala vanilla DML?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values",0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Call weighted lasso on reduced design matrix,0
Weighted tau,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to WeightedLassoCV by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: generalize to multiple treatment case?,1
get index of best treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make any access to matplotlib or plt throw an exception,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use first target,0
Write node mean CATE,0
Write node std of CATE,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
Write node mean CATE,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
d_t=None here since we measure the effect across all Ts,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
Type to column extraction function,0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
assume that we should perform nested cross-validation if and only if,0
the model has a 'cv' attribute; this is a somewhat brittle assumption...,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a 'groups' argument,0
now we have to compute the folds explicitly because some classifiers (like LassoCV),0
don't use the groups when calling split internally,0
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
define the index of d_x to filter for each given T,0
filter X after broadcast with T for each given T,0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default.",0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
This flag is true when names are set in a child class instead,0
"If names are set in a child class, add an attribute reflecting that",0
This works only if X is passed as a kwarg,0
We plan to enforce X as kwarg only in new releases,0
This checks if names have been set in a child class,0
"If names were set in a child class, don't do it again",0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
Get input names,0
Summary,0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
Get input names,0
Summary,0
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
remove None arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
generate an instance of the final model,0
generate an instance of the nuisance model,0
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same,0
alteration even when we only want to fit_final.,0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)",0
"however, sklearn doesn't support both stratifying and grouping (see",0
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply",0
their own object that supports grouping if they want to use groups.,0
######################################################,0
These should be removed once `n_splits` is deprecated,0
######################################################,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Handles the corner case when X=None but featurizer might be not None,0
"This fails if X=None and featurizer is not None, but that case is handled above",0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
TODO: support sample_var,1
Replacing to remove docstring,0
###################################################################,0
Everything below should be removed once parameters are deprecated,0
###################################################################,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE: important to use the rlearner_model_final_ attribute instead of the,0
attribute so that the trained featurizer will be passed through,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
We need to use the rlearner's copy to retain the information from fitting,0
Handles the corner case when X=None but featurizer might be not None,0
"This fails if X=None and featurizer is not None, but that case is handled above",0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support sample_var,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add blb to parent's options,0
override only so that we can update the docstring to indicate,0
support for `GenericSingleTreatmentModelFinalInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
NOTE: important to get parent's wrapped copy so that,0
"after training wrapped featurizer is also trained, etc.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `blb`,0
######################################################,0
These should be removed once `n_splits` is deprecated,0
######################################################,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=1 here since we measure the effect across all Ts,0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
d_t=1 here since we measure the effect across all Ts,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=1 here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code is a fork from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
Set parameters,0
Don't instantiate estimators now! Parameters of base_estimator might,0
"still change. Eg., when grid-searching with the nested object syntax.",0
self.estimators_ needs to be filled by the derived classes in fit.,0
Compute the number of jobs,0
Partition estimators between jobs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from:,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Types and constants,0
=============================================================================,0
=============================================================================,0
Base GRF tree,0
=============================================================================,0
Determine output settings,0
"Important: This must be the first invocation of the random state at fit time, so that",0
train/test splits are re-generatable from an external object simply by knowing the,0
random_state parameter of the tree. Can be useful in the future if one wants to create local,0
linear predictions. Currently is also useful for testing.,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
Set min_weight_leaf from min_weight_fraction_leaf,0
Build tree,0
We calculate the maximum number of samples from each half-split that any node in the tree can,0
hold. Used by criterion for memory space savings.,0
Initialize the criterion object and the criterion_val object if honest.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
=============================================================================,0
A MultOutputWrapper for GRF classes,0
=============================================================================,0
=============================================================================,0
Instantiations of Generalized Random Forest,0
=============================================================================,0
"Append a constant treatment if `fit_intercept=True`, the coefficient",0
in front of the constant treatment is the intercept in the moment equation.,0
"Append a constant treatment and constant instrument if `fit_intercept=True`,",0
the coefficient in front of the constant treatment is the intercept in the moment equation.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
,0
This code contains snippets of code from,0
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py,0
published under the following license and copyright:,0
BSD 3-Clause License,0
,0
Copyright (c) 2007-2020 The scikit-learn developers.,0
All rights reserved.,0
=============================================================================,0
Base Generalized Random Forest,0
=============================================================================,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Get subsample sample size,0
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle,0
We calculate the min eigenvalue proxy that each criterion is considering,0
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`",0
Check parameters,0
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or,0
if this is the first `fit` call of the warm start mode.,0
"Free allocated memory, if any",0
the below are needed to replicate randomness of subsampling when warm_start=True,0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Generating indices a priori before parallelism ended up being orders of magnitude,0
faster than how sklearn does it. The reason is that random samplers do not release the,0
gil it seems.,0
Advancing subsample_random_state. Assumes each prior fit call has the same number of,0
"samples at fit time. If not then this would not exactly replicate a single batch execution,",0
but would still advance randomness enough so that tree subsamples will be different.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
Collect newly grown trees,0
Check data,0
Assign chunk of trees to jobs,0
avoid storing the output of every estimator by summing them here,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
Check data,0
Assign chunk of trees to jobs,0
Parallel loop,0
####################,0
Variance correction,0
####################,0
Subtract the average within bag variance. This ends up being equal to the,0
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).,0
The negative part is just sq_between.,0
Objective bayes debiasing for the diagonals where we know a-prior they are positive,0
"The off diagonals we have no objective prior, so no correction is applied.",0
Finally correcting the pred_cov or pred_var,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
test that the subsampling scheme past to the trees is correct,0
test that the estimator calcualtes var correctly,0
test api,0
test accuracy,0
test the projection functionality of forests,0
test that the estimator calcualtes var correctly,0
test api,0
test that the estimator calcualtes var correctly,0
"test that the estimator accepts lists, tuples and pandas data frames",0
test that we raise errors in mishandled situations.,0
test that the subsampling scheme past to the trees is correct,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
testing importances,0
testing heterogeneity importances,0
Testing that all parameters do what they are supposed to,0
"testing predict, apply and decision path",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
Multi-dimensional output y,0
1-d y,0
multi-d y,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment",0
The average variance should be lower when using monte carlo iterations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure that we've got at least two of every row,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
need to make sure we get all *joint* combinations,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat requires X,0
ensure we can serialize unfit estimator,0
these support only W but not X,0
"these support only binary, not general discrete T and Z",0
ensure we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
TODO: add tests for extra properties like coef_ where they exist,1
TODO: add tests for extra properties like coef_ where they exist,1
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
"however, with custom splits the checking happens in the first stage wrapper",0
where we don't have all of the required information to do this;,0
we'd probably need to add it to _crossfit instead,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet",0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
test base values equals to mean of constant marginal effect,0
test shape of shap values output is as expected,0
test shape of attribute of explanation object is as expected,0
Treatment effect function,0
Outcome support,0
Treatment support,0
"Generate controls, covariates, treatments and outcomes",0
Heterogeneous treatment effects,0
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
through shap package.,0
test shap could generate the plot from the shap_values,0
Re-raise with more informative error message instead:,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we don't drop the first column because",0
we concatenate all treatments to the other features;,0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages",0
output is,0
"* a column of ones if X, W, and Z are all None",0
* just X or W or Z if both of the others are None,0
* hstack([arrs]) for whatever subset are not None otherwise,0
ensure Z is 2D,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
"instruments, and outcomes",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"TODO: check that Y, T, Z do not have multiple columns",0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res",0
TODO: allow the final model to actually use X? Then we'd need to rename the class,1
since we would actually be calculating a CATE rather than ATE.,0
TODO: allow the final model to actually use X?,1
TODO: allow the final model to actually use X?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring",0
TODO: would it be useful to extend to handle controls ala vanilla DML?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
TODO. The API of the private scikit-learn `_fit_and_predict` has changed,1
"between 0.23.2 and 0.24. For this to work with <0.24, we need to add a",0
case analysis based on sklearn version.,0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Call weighted lasso on reduced design matrix,0
Weighted tau,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to WeightedLassoCV by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: generalize to multiple treatment case?,1
get index of best treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use first target,0
Write node mean CATE,0
Write node std of CATE,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
Write node mean CATE,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we don't drop the first column because",0
we concatenate all treatments to the other features;,0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: consider working around relying on sklearn implementation details,1
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support sample_var,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################,0
Core DML Tests,0
#######################################,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect interval as a function of const_marginal_effect_interval for a single treatment,0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=1 here since we measure the effect across all Ts,0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
d_t=1 here since we measure the effect across all Ts,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=1 here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages",0
output is,0
"* a column of ones if X, W, and Z are all None",0
* just X or W or Z if both of the others are None,0
* hstack([arrs]) for whatever subset are not None otherwise,0
ensure Z is 2D,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res",0
TODO: allow the final model to actually use X? Then we'd need to rename the class,1
since we would actually be calculating a CATE rather than ATE.,0
TODO: allow the final model to actually use X?,1
TODO: allow the final model to actually use X?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring",0
TODO: would it be useful to extend to handle controls ala vanilla DML?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
"instruments, and outcomes",0
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"TODO: check that Y, T, Z do not have multiple columns",0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
TODO: support sample_var,1
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
Replacing to remove docstring,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: generalize to multiple treatment case?,1
get index of best treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################################,0
Perfect Data DGPs for Testing Correctness of Code,0
#######################################################,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect + eta,0
Generate random control co-variates,0
Use the same treatment vector for each row,0
Construct outcomes as y = X*beta + T*effect,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
assume that we should perform nested cross-validation if and only if,0
the model has a 'cv' attribute; this is a somewhat brittle assumption...,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a 'groups' argument,0
now we have to compute the folds explicitly because some classifiers (like LassoCV),0
don't use the groups when calling split internally,0
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use first target,0
Write node mean CATE,0
Write node std of CATE,0
Write confidence interval information if at leaf node,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
Write node mean CATE,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
remove None arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)",0
"however, sklearn doesn't support both stratifying and grouping (see",0
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply",0
their own object that supports grouping if they want to use groups.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimators,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
Multi-dimensional output y,0
1-d y,0
multi-d y,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure that we've got at least two of every row,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
need to make sure we get all *joint* combinations,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat requires X,0
ensure we can serialize unfit estimator,0
these support only W but not X,0
"these support only binary, not general discrete T and Z",0
ensure we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
TODO: add tests for extra properties like coef_ where they exist,1
TODO: add tests for extra properties like coef_ where they exist,1
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
"however, with custom splits the checking happens in the first stage wrapper",0
where we don't have all of the required information to do this;,0
we'd probably need to add it to _crossfit instead,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Test Causal Forest API,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters.,0
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Test CausalForest API,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
Test CausalForest API,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Test causal foret API,0
Test Causal Forest API,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
make sure we warn when using old aliases,0
make sure we can use the old alias as a type,0
make sure that we can still pickle the old aliases,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet",0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check that the point estimates are the same,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: conisder working around relying on sklearn implementation details,1
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
Concatenate the predictions,0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
TODO: consider working around relying on sklearn implementation details,1
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Call weighted lasso on reduced design matrix,0
Inherit some parameters from the parent,0
Weighted tau,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to WeightedLassoCV by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Construct the subsample of data,0
Split into estimation and splitting sample set,0
Fit the tree on the splitting sample,0
Set the estimation values based on the estimation split,0
Apply the trained tree on the estimation sample to get the path for every estimation sample,0
Calculate the total weight of estimation samples on each tree node:,0
\sum_i sample_weight[i] * 1{i \\in node},0
Calculate the total number of estimation samples on each tree node:,0
|node| = \sum_{i} 1{i \\in node},0
Calculate the weighted sum of responses on the estimation sample on each node:,0
\sum_{i} sample_weight[i] 1{i \\in node} Y_i,0
Calculate the predicted value on each node based on the estimation sample:,0
weighted sum of responses / total weight,0
"Calculate the criterion on each node based on the estimation sample and for each output dimension,",0
summing the impurity across dimensions.,0
First we calculate the difference of observed label y of each node and predicted value for each,0
node that the sample falls in: y[i] - value_est[node],0
If criterion is mse then calculate weighted sum of squared differences for each node,0
If criterion is mae then calculate weighted sum of absolute differences for each node,0
Normalize each weighted sum of criterion for each node by the total weight of each node,0
Prune tree to remove leafs that don't satisfy the leaf requirements on the estimation sample,0
and for each un-pruned tree set the value and the weight appropriately.,0
If minimum weight requirement or minimum leaf size requirement is not satisfied on estimation,0
"sample, then prune the whole sub-tree",0
Set the numerator of the node to: \sum_{i} sample_weight[i] 1{i \\in node} Y_i / |node|,0
Set the value of the node to:,0
\sum_{i} sample_weight[i] 1{i \\in node} Y_i / \sum_{i} sample_weight[i] 1{i \\in node},0
Set the denominator of the node to: \sum_{i} sample_weight[i] 1{i \\in node} / |node|,0
Set the weight of the node to: \sum_{i} sample_weight[i] 1{i \\in node},0
Set the count to the estimation split count,0
Set the node impurity to the estimation split impurity,0
Validate or convert input data,0
Pre-sort indices to avoid that each individual tree of the,0
ensemble sorts the indices.,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
"Free allocated memory, if any",0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
TODO. This slicing should ultimately be done inside the parallel function,1
so that we don't need to create a matrix of size roughly n_samples * n_estimators,0
Collect newly grown trees,0
Helper class that accumulates an arbitrary function in parallel on the accumulator acc,0
and calls the function fn on each tree e and returns the mean output. The function fn,0
should take as input a tree e and associated numerator n and denominator d structures and,0
"return another function g_e, which takes as input X, check_input",0
"If slice is not None, but rather a tuple (start, end), then a subset of the trees from",0
index start to index end will be used. The returned result is essentially:,0
(mean over e in slice)(g_e(X)).,0
Check data,0
Assign chunk of trees to jobs,0
Check data,0
Check data,0
avoid storing the output of every estimator by summing them here,0
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) Y_i",0
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x)",0
"Calculate for each slice S: Q(S) = 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) (Y_i - \theta(X))",0
where \theta(X) is the point estimate using the whole forest,0
Calculate the variance of the latter as E[Q(S)^2],0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we don't drop the first column because",0
we concatenate all treatments to the other features;,0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Convert rows to columns,0
Require group assignment t to be one-hot-encoded,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Override the CATE inference options,0
Add blb inference to parent's options,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Need to redefine fit here for auto inference to work due to a quirk in how,1
wrap_fit is defined,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Check that all discrete treatments are represented,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
returns shape-conforming residuals,0
Copy and/or define models,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Call `fit` from parent class,0
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent,0
"fit, we need to set explicitly d_t_in here after super fit is called.",0
Override to flatten output if T is flat,0
Expand one-hot encoding to include the zero treatment,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Calculate confidence intervals for the parameter (marginal effect),0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support sample_var,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################,0
Core DML Tests,0
#######################################,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
note that groups are not passed to score because they are only used for fitting,0
note that groups are not passed to score because they are only used for fitting,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
make T 2D if if was a vector,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect interval as a function of const_marginal_effect_interval for a single treatment,0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=1 here since we measure the effect across all Ts,0
once the estimator has been fit,0
"replacing _predict of super to fend against misuse, when the user has used a final linear model with",0
an intercept even when bias is part of coef.,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
d_t=1 here since we measure the effect across all Ts,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=1 here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
"in the degenerate case where every point in the distribution is equal to the value tested, return nan",0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
"if stderr is zero, ppf will return nans and the loop below would never terminate",0
so bail out early; note that it might be possible to correct the algorithm for,0
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't",0
be clean,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages",0
output is,0
"* a column of ones if X, W, and Z are all None",0
* just X or W or Z if both of the others are None,0
* hstack([arrs]) for whatever subset are not None otherwise,0
ensure Z is 2D,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res",0
TODO: allow the final model to actually use X? Then we'd need to rename the class,1
since we would actually be calculating a CATE rather than ATE.,0
TODO: allow the final model to actually use X?,1
TODO: allow the final model to actually use X?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring",0
TODO: would it be useful to extend to handle controls ala vanilla DML?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
"instruments, and outcomes",0
TODO: how do we incorporate the sample_weight and sample_var passed into this method,0
as arguments?,0
TODO: is there a good way to incorporate the other nuisance terms in the score?,1
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"TODO: check that Y, T, Z do not have multiple columns",0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
TODO: support sample_var,1
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
Replacing to remove docstring,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: generalize to multiple treatment case?,1
get index of best treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################################,0
Perfect Data DGPs for Testing Correctness of Code,0
#######################################################,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect + eta,0
Generate random control co-variates,0
Use the same treatment vector for each row,0
Construct outcomes as y = X*beta + T*effect,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
add blb to parent's options,0
TODO Share some logic with non-discrete version,1
add statsmodels to parent's options,0
add statsmodels to parent's options,0
add blb to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
assume that we should perform nested cross-validation if and only if,0
the model has a 'cv' attribute; this is a somewhat brittle assumption...,0
logic copied from check_cv,0
otherwise we will assume the user already set the cv attribute to something,0
compatible with splitting with a 'groups' argument,0
now we have to compute the folds explicitly because some classifiers (like LassoCV),0
don't use the groups when calling split internally,0
Normalize weights,0
This class is mainly derived from statsmodels.iolib.summary.Summary,0
"if we're decorating a class, just update the __init__ method,",0
so that the result is still a class instead of a wrapper method,0
"want to enforce that each bad_arg was either in kwargs,",0
or else it was in neither and is just taking its default value,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use first target,0
Write node mean CATE,0
Write node std of CATE,0
Write confidence interval information if at leaf node,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
Write node mean CATE,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
remove None arguments,0
"scores entries should be lists of scores, so make each entry a singleton list",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)",0
"however, sklearn doesn't support both stratifying and grouping (see",0
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply",0
their own object that supports grouping if they want to use groups.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimators,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
don't proxy special methods,0
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
second level bootstrap which would be prohibitive computationally?,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
can't import from econml.inference at top level without creating cyclical dependencies,0
Note that inference results are always methods even if the inference is for a property,0
(e.g. coef__inference() is a method but coef_ is a property),0
Therefore we must insert a lambda if getting inference for a non-callable,0
"If inference is for a property, create a fresh lambda to avoid passing args through",0
"try to get interval/std first if appropriate,",0
since we don't prefer a wrapped method with this name,0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
Multi-dimensional output y,0
1-d y,0
multi-d y,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Test inference results when `cate_feature_names` doesn not exist,0
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
pvalue for second column should be greater than zero since some points are on either side,0
of the tested value,0
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf",0
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value",0
pvalue is also nan when variance is 0 and the point tested is equal to the point tested,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
creating an instance should warn,0
using the instance should not warn,0
using the deprecated method should warn,0
don't warn if b and c are passed by keyword,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure that we've got at least two of every row,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
need to make sure we get all *joint* combinations,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat requires X,0
ensure we can serialize unfit estimator,0
these support only W but not X,0
"these support only binary, not general discrete T and Z",0
ensure we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
TODO: add tests for extra properties like coef_ where they exist,1
TODO: add tests for extra properties like coef_ where they exist,1
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
"however, with custom splits the checking happens in the first stage wrapper",0
where we don't have all of the required information to do this;,0
we'd probably need to add it to _crossfit instead,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Test Causal Forest API,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters.,0
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Test CausalForest API,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
Test CausalForest API,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Test causal foret API,0
Test Causal Forest API,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
predetermined splits ensure that all features are seen in each split,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
test outer grouping,0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
make sure we warn when using old aliases,0
make sure we can use the old alias as a type,0
make sure that we can still pickle the old aliases,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef_ and intercept_ inference,0
verify we can generate the summary,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
test outer grouping,0
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet",0
test nested grouping,0
ensure that the grouping has worked correctly and we get all 10 copies of the items in,0
whichever groups we saw,0
test nested grouping,0
"by default, we use 5 split cross-validation for our T and Y models",0
but we don't have enough groups here to split both the outer and inner samples with grouping,0
TODO: does this imply we should change some defaults to make this more likely to succeed?,1
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
#########################,0
WeightedLassoCVWrapper #,0
#########################,0
perform 1D fit,0
perform 2D fit,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
_penalized_inds is only set during fitting,0
cv exists on penalized model,0
now we can access _penalized_inds,0
check that we can read the cv attribute back out from the underlying model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check that the point estimates are the same,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
"If classification methods produce multiple columns of output,",0
we need to manually encode classes to ensure consistent column ordering.,0
We clone the estimator to make sure that all the folds are,0
"independent, and that it is pickle-able.",0
Concatenate the predictions,0
`predictions` is a list of method outputs from each fold.,0
"If each of those is also a list, then treat this as a",0
multioutput-multiclass task. We need to separately concatenate,0
the method outputs for each label into an `n_labels` long list.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Call weighted lasso on reduced design matrix,0
Inherit some parameters from the parent,0
Weighted tau,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set model to WeightedLassoCV by default so there's always a model to get and set attributes on,0
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV,0
(e.g. former has 'positive' and 'precompute' while latter does not),0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
don't proxy special methods,0
"don't pass get_params through to model, because that will cause sklearn to clone this",0
regressor incorrectly,0
"Note: for known attributes that have been set this method will not be called,",0
so we should just throw here because this is an attribute belonging to this class,0
but which hasn't yet been set on this instance,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Construct the subsample of data,0
Split into estimation and splitting sample set,0
Fit the tree on the splitting sample,0
Set the estimation values based on the estimation split,0
Apply the trained tree on the estimation sample to get the path for every estimation sample,0
Calculate the total weight of estimation samples on each tree node:,0
\sum_i sample_weight[i] * 1{i \\in node},0
Calculate the total number of estimation samples on each tree node:,0
|node| = \sum_{i} 1{i \\in node},0
Calculate the weighted sum of responses on the estimation sample on each node:,0
\sum_{i} sample_weight[i] 1{i \\in node} Y_i,0
Calculate the predicted value on each node based on the estimation sample:,0
weighted sum of responses / total weight,0
"Calculate the criterion on each node based on the estimation sample and for each output dimension,",0
summing the impurity across dimensions.,0
First we calculate the difference of observed label y of each node and predicted value for each,0
node that the sample falls in: y[i] - value_est[node],0
If criterion is mse then calculate weighted sum of squared differences for each node,0
If criterion is mae then calculate weighted sum of absolute differences for each node,0
Normalize each weighted sum of criterion for each node by the total weight of each node,0
Prune tree to remove leafs that don't satisfy the leaf requirements on the estimation sample,0
and for each un-pruned tree set the value and the weight appropriately.,0
If minimum weight requirement or minimum leaf size requirement is not satisfied on estimation,0
"sample, then prune the whole sub-tree",0
Set the numerator of the node to: \sum_{i} sample_weight[i] 1{i \\in node} Y_i / |node|,0
Set the value of the node to:,0
\sum_{i} sample_weight[i] 1{i \\in node} Y_i / \sum_{i} sample_weight[i] 1{i \\in node},0
Set the denominator of the node to: \sum_{i} sample_weight[i] 1{i \\in node} / |node|,0
Set the weight of the node to: \sum_{i} sample_weight[i] 1{i \\in node},0
Set the count to the estimation split count,0
Set the node impurity to the estimation split impurity,0
Validate or convert input data,0
Pre-sort indices to avoid that each individual tree of the,0
ensemble sorts the indices.,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
"Free allocated memory, if any",0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
TODO. This slicing should ultimately be done inside the parallel function,1
so that we don't need to create a matrix of size roughly n_samples * n_estimators,0
Collect newly grown trees,0
Helper class that accumulates an arbitrary function in parallel on the accumulator acc,0
and calls the function fn on each tree e and returns the mean output. The function fn,0
should take as input a tree e and associated numerator n and denominator d structures and,0
"return another function g_e, which takes as input X, check_input",0
"If slice is not None, but rather a tuple (start, end), then a subset of the trees from",0
index start to index end will be used. The returned result is essentially:,0
(mean over e in slice)(g_e(X)).,0
Check data,0
Assign chunk of trees to jobs,0
Check data,0
Check data,0
avoid storing the output of every estimator by summing them here,0
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) Y_i",0
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x)",0
"Calculate for each slice S: Q(S) = 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) (Y_i - \theta(X))",0
where \theta(X) is the point estimate using the whole forest,0
Calculate the variance of the latter as E[Q(S)^2],0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
"Note: unlike other Metalearners, we don't drop the first column because",0
we concatenate all treatments to the other features;,0
"We might want to revisit, though, since it's linearly determined by the others",0
Check inputs,0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Require group assignment t to be one-hot-encoded,0
Define an inner function that iterates over group predictions,0
Convert rows to columns,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Override the CATE inference options,0
Add blb inference to parent's options,0
Must normalize weights,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Override to flatten output if T is flat,0
T is flat,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
Copy and/or define models,0
Nuisance estimators shall be defined during fitting because they need to know the number of distinct,0
treatments,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Define number of classes,0
Call `fit` from parent class,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Test expansion of treatment,0
"If expanded treatments are a vector, flatten const_marginal_effect_interval",0
Calculate confidence intervals for the parameter (marginal effect),0
"If T is a vector, preserve shape of the effect interval",0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
"Get a list of (parameter, covariance matrix) pairs",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support sample_var,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add statsmodels to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################,0
Core DML Tests,0
#######################################,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
store number of columns of T so that we can pass scalars to effect,0
TODO: support vector T and Y,1
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect interval as a function of const_marginal_effect_interval for a single treatment,0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=1 here since we measure the effect across all Ts,0
once the estimator has been fit,0
We can write effect interval as a function of predict_interval of the final method for linear models,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
d_t=1 here since we measure the effect across all Ts,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=1 here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages",0
output is,0
"* a column of ones if X, W, and Z are all None",0
* just X or W or Z if both of the others are None,0
* hstack([arrs]) for whatever subset are not None otherwise,0
ensure Z is 2D,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res",0
TODO: allow the final model to actually use X? Then we'd need to rename the class,1
since we would actually be calculating a CATE rather than ATE.,0
TODO: allow the final model to actually use X?,1
TODO: allow the final model to actually use X?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring",0
TODO: would it be useful to extend to handle controls ala vanilla DML?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
"instruments, and outcomes",0
TODO: how do we incorporate the sample_weight and sample_var passed into this method,0
as arguments?,0
TODO: is there a good way to incorporate the other nuisance terms in the score?,1
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring",0
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"TODO: check that Y, T, Z do not have multiple columns",0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
TODO: support sample_var,1
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
add statsmodels to parent's options,0
Replacing to remove docstring,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: generalize to multiple treatment case?,1
get index of best treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################################,0
Perfect Data DGPs for Testing Correctness of Code,0
#######################################################,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect + eta,0
Generate random control co-variates,0
Use the same treatment vector for each row,0
Construct outcomes as y = X*beta + T*effect,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
TODO Share some logic with non-discrete version,1
add statsmodels to parent's options,0
add statsmodels to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
Normalize weights,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
This class is mainly derived from statsmodels.iolib.summary.Summary,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use first target,0
Write node mean CATE,0
Write node std of CATE,0
Write confidence interval information if at leaf node,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
Write node mean CATE,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"scores entries should be lists of scores, so make each entry a singleton list",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
use a binary array to get stratified split in case of discrete treatment,0
"stratify on T if discrete, and fine to pass T as second arg to KFold.split even when not",0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimators,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
"try to get interval first if appropriate, since we don't prefer a wrapped method with this name",0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
Multi-dimensional output y,0
1-d y,0
multi-d y,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Nuisance model has no score method, so nuisance_scores_ should be none",0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"y scores should be positive, since W predicts Y somewhat",0
"t scores might not be, since W and T are uncorrelated",0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure that we've got at least two of every row,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
need to make sure we get all *joint* combinations,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat only supports binary treatments/instruments,0
TODO: add stratification to bootstrap so that we can use it,0
even with discrete treatments,0
IntentToTreat requires X,0
ensure we can serialize unfit estimator,0
these support only W but not X,0
"these support only binary, not general discrete T and Z",0
ensure we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
TODO: add tests for extra properties like coef_ where they exist,1
TODO: add tests for extra properties like coef_ where they exist,1
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
"however, with custom splits the checking happens in the first stage wrapper",0
where we don't have all of the required information to do this;,0
we'd probably need to add it to _crossfit instead,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Create a wrapper around Lasso that doesn't support weights,0
since Lasso does natively support them starting in sklearn 0.23,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least 6 of every element,0
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete",0
NOTE: this number may need to change if the default number of folds in,0
WeightedStratifiedKFold changes,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
TODO: add stratification to bootstrap so that we can use it,0
even with discrete treatments,0
ensure we can serialize the unfit estimator,0
ensure we can pickle the fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
"TODO Add bootstrap inference, once discrete treatment issue is fixed",1
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
create a simple artificial setup where effect of moving from treatment,0
"a -> b is 2,",0
"a -> c is 1, and",0
"b -> c is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
should rule out some basic issues.,0
Note that explicitly specifying the dtype as object is necessary until,0
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616,0
estimated effects should be identical when treatment is explicitly given,0
but const_marginal_effect should be reordered based on the explicit cagetories,0
1-> 2 in original ordering; combination of 3->1 and 3->2,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
ensure that we can serialize unfit estimator,0
TODO: add stratification to bootstrap so that we can use it even with discrete treatments,1
ensure that we can serialize fit estimator,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check that the point estimates are the same,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Call weighted lasso on reduced design matrix,0
Inherit some parameters from the parent,0
Weighted tau,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Construct the subsample of data,0
Split into estimation and splitting sample set,0
Fit the tree on the splitting sample,0
Set the estimation values based on the estimation split,0
Apply the trained tree on the estimation sample to get the path for every estimation sample,0
Calculate the total weight of estimation samples on each tree node:,0
\sum_i sample_weight[i] * 1{i \\in node},0
Calculate the total number of estimation samples on each tree node:,0
|node| = \sum_{i} 1{i \\in node},0
Calculate the weighted sum of responses on the estimation sample on each node:,0
\sum_{i} sample_weight[i] 1{i \\in node} Y_i,0
Prune tree to remove leafs that don't satisfy the leaf requirements on the estimation sample,0
and for each un-pruned tree set the value and the weight appropriately.,0
If minimum weight requirement or minimum leaf size requirement is not satisfied on estimation,0
"sample, then prune the whole sub-tree",0
Set the value of the node to: \sum_{i} sample_weight[i] 1{i \\in node} Y_i / |node|,0
Set the weight of the node to: \sum_{i} sample_weight[i] 1{i \\in node} / |node|,0
Set the count to the estimation split count,0
Validate or convert input data,0
Pre-sort indices to avoid that each individual tree of the,0
ensemble sorts the indices.,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
"Free allocated memory, if any",0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
TODO. This slicing should ultimately be done inside the parallel function,1
so that we don't need to create a matrix of size roughly n_samples * n_estimators,0
Collect newly grown trees,0
Helper class that accumulates an arbitrary function in parallel on the accumulator acc,0
and calls the function fn on each tree e and returns the mean output. The function fn,0
"should take as input a tree e, and return another function g_e, which takes as input X, check_input",0
"If slice is not None, but rather a tuple (start, end), then a subset of the trees from",0
index start to index end will be used. The returned result is essentially:,0
(mean over e in slice)(g_e(X)).,0
Check data,0
Assign chunk of trees to jobs,0
Check data,0
Check data,0
avoid storing the output of every estimator by summing them here,0
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) Y_i",0
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x)",0
"Calculate for each slice S: Q(S) = 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) (Y_i - \theta(X))",0
where \theta(X) is the point estimate using the whole forest,0
Calculate the variance of the latter as E[Q(S)^2],0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Require group assignment t to be one-hot-encoded,0
Define an inner function that iterates over group predictions,0
Convert rows to columns,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Override the CATE inference options,0
Add blb inference to parent's options,0
Must normalize weights,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Override to flatten output if T is flat,0
T is flat,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
Copy and/or define models,0
Nuisance estimators shall be defined during fitting because they need to know the number of distinct,0
treatments,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
Define autoencoder,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Define number of classes,0
Call `fit` from parent class,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Test expansion of treatment,0
"If expanded treatments are a vector, flatten const_marginal_effect_interval",0
Calculate confidence intervals for the parameter (marginal effect),0
"If T is a vector, preserve shape of the effect interval",0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
"Get a list of (parameter, covariance matrix) pairs",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support sample_var,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add statsmodels to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################,0
Core DML Tests,0
#######################################,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
store number of columns of T so that we can pass scalars to effect,0
TODO: support vector T and Y,1
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect interval as a function of const_marginal_effect_interval for a single treatment,0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=1 here since we measure the effect across all Ts,0
once the estimator has been fit,0
We can write effect interval as a function of predict_interval of the final method for linear models,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
d_t=1 here since we measure the effect across all Ts,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=1 here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A cut-down version of the DML first stage wrapper, since we don't need to support W or linear first stages",0
"if both X and Z are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res",0
TODO: allow the final model to actually use X? Then we'd need to rename the class,1
since we would actually be calculating a CATE rather than ATE.,0
TODO: allow the final model to actually use X?,1
TODO: allow the final model to actually use X?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring",0
TODO: would it be useful to extend to handle controls ala vanilla DML?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
"instruments, and outcomes",0
TODO: how do we incorporate the sample_weight and sample_var passed into this method,0
as arguments?,0
TODO: is there a good way to incorporate the other nuisance terms in the score?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"TODO: check that Y, T, Z do not have multiple columns",0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
TODO: support sample_var,1
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
add statsmodels to parent's options,0
Replacing to remove docstring,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: generalize to multiple treatment case?,1
get index of best treatment,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################################,0
Perfect Data DGPs for Testing Correctness of Code,0
#######################################################,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect + eta,0
Generate random control co-variates,0
Use the same treatment vector for each row,0
Construct outcomes as y = X*beta + T*effect,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
TODO Share some logic with non-discrete version,1
add statsmodels to parent's options,0
add statsmodels to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
Normalize weights,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
This class is mainly derived from statsmodels.iolib.summary.Summary,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use first target,0
Write node mean CATE,0
Write node std of CATE,0
Write confidence interval information if at leaf node,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
Write node mean CATE,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
drop first column since all columns sum to one,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimators,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
"try to get interval first if appropriate, since we don't prefer a wrapped method with this name",0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
can interpret without uncertainty,0
can't treat before interpreting,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
Multi-dimensional output y,0
1-d y,0
multi-d y,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test inference results when `cate_feature_names` doesn not exist,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Test non keyword based calls to fit,0
test non-array inputs,0
Test custom splitter,0
Test incomplete set of test folds,0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure that we've got at least two of every row,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
need to make sure we get all *joint* combinations,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat only supports binary treatments/instruments,0
TODO: add stratification to bootstrap so that we can use it,0
even with discrete treatments,0
IntentToTreat requires X,0
these support only W but not X,0
"these support only binary, not general discrete T and Z",0
make sure we can call the marginal_effect and effect methods,0
TODO: add tests for extra properties like coef_ where they exist,1
TODO: add tests for extra properties like coef_ where they exist,1
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
"however, with custom splits the checking happens in the first stage wrapper",0
where we don't have all of the required information to do this;,0
we'd probably need to add it to _crossfit instead,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
TODO: add stratification to bootstrap so that we can use it,0
even with discrete treatments,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
"TODO Add bootstrap inference, once discrete treatment issue is fixed",1
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts",0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
TODO: add stratification to bootstrap so that we can use it even with discrete treatments,1
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Call weighted lasso on reduced design matrix,0
Inherit some parameters from the parent,0
Weighted tau,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Construct the subsample of data,0
Split into estimation and splitting sample set,0
Fit the tree on the splitting sample,0
Set the estimation values based on the estimation split,0
Apply the trained tree on the estimation sample to get the path for every estimation sample,0
Calculate the total weight of estimation samples on each tree node:,0
\sum_i sample_weight[i] * 1{i \\in node},0
Calculate the total number of estimation samples on each tree node:,0
|node| = \sum_{i} 1{i \\in node},0
Calculate the weighted sum of responses on the estimation sample on each node:,0
\sum_{i} sample_weight[i] 1{i \\in node} Y_i,0
Prune tree to remove leafs that don't satisfy the leaf requirements on the estimation sample,0
and for each un-pruned tree set the value and the weight appropriately.,0
If minimum weight requirement or minimum leaf size requirement is not satisfied on estimation,0
"sample, then prune the whole sub-tree",0
Set the value of the node to: \sum_{i} sample_weight[i] 1{i \\in node} Y_i / |node|,0
Set the weight of the node to: \sum_{i} sample_weight[i] 1{i \\in node} / |node|,0
Set the count to the estimation split count,0
Validate or convert input data,0
Pre-sort indices to avoid that each individual tree of the,0
ensemble sorts the indices.,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
"Free allocated memory, if any",0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
TODO. This slicing should ultimately be done inside the parallel function,1
so that we don't need to create a matrix of size roughly n_samples * n_estimators,0
Collect newly grown trees,0
Helper class that accumulates an arbitrary function in parallel on the accumulator acc,0
and calls the function fn on each tree e and returns the mean output. The function fn,0
"should take as input a tree e, and return another function g_e, which takes as input X, check_input",0
"If slice is not None, but rather a tuple (start, end), then a subset of the trees from",0
index start to index end will be used. The returned result is essentially:,0
(mean over e in slice)(g_e(X)).,0
Check data,0
Assign chunk of trees to jobs,0
Check data,0
Check data,0
avoid storing the output of every estimator by summing them here,0
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) Y_i",0
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x)",0
"Calculate for each slice S: Q(S) = 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) (Y_i - \theta(X))",0
where \theta(X) is the point estimate using the whole forest,0
Calculate the variance of the latter as E[Q(S)^2],0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Require group assignment t to be one-hot-encoded,0
Define an inner function that iterates over group predictions,0
Convert rows to columns,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Auxiliary attributes,0
Fit check,0
TODO: Check performance,1
Override the CATE inference options,0
Add blb inference to parent's options,0
Must normalize weights,0
Crossfitting,0
Compute weighted nuisance estimates,0
-------------------------------------------------------------------------------,0
Calculate the covariance matrix corresponding to the BLB inference,0
,0
1. Calculate the moments and gradient of the training data w.r.t the test point,0
2. Calculate the weighted moments for each tree slice to create a matrix,0
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation",0
in that slice from the overall parameter estimate.,0
3. Calculate the covariance matrix (V.T x V) / n_slices,0
-------------------------------------------------------------------------------,0
Calclulate covariance matrix through BLB,0
Generate subsample indices,0
Build trees in parallel,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Define subsample size,0
Safety check,0
Draw points to create little bags,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Override to flatten output if T is flat,0
T is flat,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
Copy and/or define models,0
Nuisance estimators shall be defined during fitting because they need to know the number of distinct,0
treatments,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
Define autoencoder,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Define number of classes,0
Call `fit` from parent class,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned is of shape (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Test whether the input estimator is supported,0
Test expansion of treatment,0
"If expanded treatments are a vector, flatten const_marginal_effect_interval",0
Calculate confidence intervals for the parameter (marginal effect),0
"If T is a vector, preserve shape of the effect interval",0
Calculate confidence intervals for the effect,0
Calculate the effects,0
Calculate the standard deviations for the effects,0
"Get a list of (parameter, covariance matrix) pairs",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support sample_var,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add statsmodels to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################,0
Core DML Tests,0
#######################################,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
store number of columns of T so that we can pass scalars to effect,0
TODO: support vector T and Y,1
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
TODO: is it right that the effective number of intruments is the,1
"product of ft_X and ft_Z, not just ft_Z?",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect interval as a function of const_marginal_effect_interval for a single treatment,0
We can write effect inference as a function of const_marginal_effect_inference for a single treatment,0
d_t=1 here since we measure the effect across all Ts,0
once the estimator has been fit,0
We can write effect interval as a function of predict_interval of the final method for linear models,0
We can write effect inference as a function of prediction and prediction standard error of,0
the final method for linear models,0
d_t=1 here since we measure the effect across all Ts,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
"send treatment to the end, pull bounds to the front",0
d_t=1 here since we measure the effect across all Ts,0
need to set the fit args before the estimator is fit,0
1. Uncertainty of Mean Point Estimate,0
2. Distribution of Point Estimate,0
3. Total Variance of Point Estimate,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A cut-down version of the DML first stage wrapper, since we don't need to support W or linear first stages",0
"if both X and Z are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res",0
TODO: allow the final model to actually use X? Then we'd need to rename the class,1
since we would actually be calculating a CATE rather than ATE.,0
TODO: allow the final model to actually use X?,1
TODO: allow the final model to actually use X?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring",0
TODO: would it be useful to extend to handle controls ala vanilla DML?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
"instruments, and outcomes",0
TODO: how do we incorporate the sample_weight and sample_var passed into this method,0
as arguments?,0
TODO: is there a good way to incorporate the other nuisance terms in the score?,1
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring",0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"we need to undo the one-hot encoding for calling effect,",0
since it expects raw values,0
"TODO: check that Y, T, Z do not have multiple columns",0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
TODO: support sample_var,1
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
add statsmodels to parent's options,0
Replacing to remove docstring,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################################,0
Perfect Data DGPs for Testing Correctness of Code,0
#######################################################,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect + eta,0
Generate random control co-variates,0
Use the same treatment vector for each row,0
Construct outcomes as y = X*beta + T*effect,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
TODO Share some logic with non-discrete version,1
add statsmodels to parent's options,0
add statsmodels to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
Normalize weights,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
This class is mainly derived from statsmodels.iolib.summary.Summary,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use first target,0
Write node mean CATE,0
Write node std of CATE,0
Write confidence interval information if at leaf node,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
Write node mean CATE,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
drop first column since all columns sum to one,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimators,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
"try to get interval first if appropriate, since we don't prefer a wrapped method with this name",0
AzureML,0
helper imports,0
write the details of the workspace to a configuration file to the notebook library,0
if y is a multioutput model,0
Make sure second dimension has 1 or more item,0
switch _inner Model to a MultiOutputRegressor,0
flatten array as automl only takes vectors for y,0
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor,0
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel,0
as an sklearn estimator,0
fit implementation for a single output model.,0
Create experiment for specified workspace,0
Configure automl_config with training set information.,0
"Wait for remote run to complete, the set the model",0
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them",0
create model and pass model into final.,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and add it to new_Args,0
"If item is an automl config, get its corresponding",0
AutomatedML Model and set it for this key in,0
kwargs,0
takes in either automated_ml config and instantiates,0
an AutomatedMLModel,0
The prefix can only be 18 characters long,0
"because prefixes come from kwarg_names, we must ensure they are",0
short enough.,0
Get workspace from config file.,0
Take the intersect of the white for sample,0
weights and linear models,0
"show output is not stored in the config in AutomatedML, so we need to make it a field.",0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
Multi-dimensional output y,0
1-d y,0
multi-d y,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Test non keyword based calls to fit,0
Test custom splitter,0
Test incomplete set of test folds,0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: test something rather than just print...,1
"Note: no noise, just testing that we can exactly recover when we ought to be able to",0
pick some arbitrary X,0
pick some arbitrary T,0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
ensure that we've got at least two of every row,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
need to make sure we get all *joint* combinations,0
IntentToTreat only supports binary treatments/instruments,0
IntentToTreat only supports binary treatments/instruments,0
TODO: add stratification to bootstrap so that we can use it,0
even with discrete treatments,0
IntentToTreat requires X,0
these support only W but not X,0
"these support only binary, not general discrete T and Z",0
make sure we can call the marginal_effect and effect methods,0
TODO: add tests for extra properties like coef_ where they exist,1
TODO: add tests for extra properties like coef_ where they exist,1
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
"however, with custom splits the checking happens in the first stage wrapper",0
where we don't have all of the required information to do this;,0
we'd probably need to add it to _crossfit instead,0
TODO: make IV related,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 20% test points to be outside of the confidence interval,0
Check that the intervals are not too wide,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
TODO: add stratification to bootstrap so that we can use it,0
even with discrete treatments,0
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
test coef__inference and intercept__inference,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"ExitStack can be used as a ""do nothing"" ContextManager",0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
"TODO Add bootstrap inference, once discrete treatment issue is fixed",1
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
TODO: add stratification to bootstrap so that we can use it even with discrete treatments,1
make sure we can call the marginal_effect and effect methods,0
test const marginal inference,0
test effect inference,0
test marginal effect inference,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
"for at least some of the examples, the CI should have nonzero width",0
test coef__inference function works,0
test intercept__inference function works,0
test summary function works,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Linear models are required for parametric dml,0
sample weighting models are required for nonparametric dml,0
Test values,0
TLearner test,0
Instantiate TLearner,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate DomainAdaptationLearner,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Call weighted lasso on reduced design matrix,0
Inherit some parameters from the parent,0
Weighted tau,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_stderr_,0
intercept_stderr_,0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Construct the subsample of data,0
Split into estimation and splitting sample set,0
Fit the tree on the splitting sample,0
Set the estimation values based on the estimation split,0
Apply the trained tree on the estimation sample to get the path for every estimation sample,0
Calculate the total weight of estimation samples on each tree node:,0
\sum_i sample_weight[i] * 1{i \\in node},0
Calculate the total number of estimation samples on each tree node:,0
|node| = \sum_{i} 1{i \\in node},0
Calculate the weighted sum of responses on the estimation sample on each node:,0
\sum_{i} sample_weight[i] 1{i \\in node} Y_i,0
Prune tree to remove leafs that don't satisfy the leaf requirements on the estimation sample,0
and for each un-pruned tree set the value and the weight appropriately.,0
If minimum weight requirement or minimum leaf size requirement is not satisfied on estimation,0
"sample, then prune the whole sub-tree",0
Set the value of the node to: \sum_{i} sample_weight[i] 1{i \\in node} Y_i / |node|,0
Set the weight of the node to: \sum_{i} sample_weight[i] 1{i \\in node} / |node|,0
Set the count to the estimation split count,0
Validate or convert input data,0
Pre-sort indices to avoid that each individual tree of the,0
ensemble sorts the indices.,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
"Free allocated memory, if any",0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
TODO. This slicing should ultimately be done inside the parallel function,1
so that we don't need to create a matrix of size roughly n_samples * n_estimators,0
Collect newly grown trees,0
Helper class that accumulates an arbitrary function in parallel on the accumulator acc,0
and calls the function fn on each tree e and returns the mean output. The function fn,0
"should take as input a tree e, and return another function g_e, which takes as input X, check_input",0
"If slice is not None, but rather a tuple (start, end), then a subset of the trees from",0
index start to index end will be used. The returned result is essentially:,0
(mean over e in slice)(g_e(X)).,0
Check data,0
Assign chunk of trees to jobs,0
Check data,0
Check data,0
avoid storing the output of every estimator by summing them here,0
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) Y_i",0
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x)",0
"Calculate for each slice S: Q(S) = 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) (Y_i - \theta(X))",0
where \theta(X) is the point estimate using the whole forest,0
Calculate the variance of the latter as E[Q(S)^2],0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Require group assignment t to be one-hot-encoded,0
Define an inner function that iterates over group predictions,0
Convert rows to columns,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Crossfitting,0
Compute weighted nuisance estimates,0
Generate subsample indices,0
Safety check,0
Build trees in parallel,0
Calculates weights,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned by LinearRegression is (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
Copy and/or define models,0
Nuisance estimators shall be defined during fitting because they need to know the number of distinct,0
treatments,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
Define autoencoder,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Define number of classes,0
Call `fit` from parent class,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned by LinearRegression is (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support sample_var,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add statsmodels to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################,0
Core DML Tests,0
#######################################,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
store number of columns of T so that we can pass scalars to effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect interval as a function of const_marginal_effect_interval for a single treatment,0
once the estimator has been fit,0
We can write effect interval as a function of predict_interval of the final method for linear models,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
need to set the fit args before the estimator is fit,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
TODO: support sample_var,1
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
add statsmodels to parent's options,0
Replacing to remove docstring,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################################,0
Perfect Data DGPs for Testing Correctness of Code,0
#######################################################,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect + eta,0
Generate random control co-variates,0
Use the same treatment vector for each row,0
Construct outcomes as y = X*beta + T*effect,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
TODO Share some logic with non-discrete version,1
add statsmodels to parent's options,0
add statsmodels to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
Normalize weights,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use first target,0
Write node mean CATE,0
Write node std of CATE,0
Write confidence interval information if at leaf node,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
Write node mean CATE,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
drop first column since all columns sum to one,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimators,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
"try to get interval first if appropriate, since we don't prefer a wrapped method with this name",0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
Multi-dimensional output y,0
1-d y,0
multi-d y,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Test non keyword based calls to fit,0
Test custom splitter,0
Test incomplete set of test folds,0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
TODO: add stratification to bootstrap so that we can use it,0
even with discrete treatments,0
make sure we can call the marginal_effect and effect methods,0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
"TODO Add bootstrap inference, once discrete treatment issue is fixed",1
make sure we can call the marginal_effect and effect methods,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
TODO: add stratification to bootstrap so that we can use it even with discrete treatments,1
make sure we can call the marginal_effect and effect methods,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Call weighted lasso on reduced design matrix,0
Inherit some parameters from the parent,0
Weighted tau,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_std_err_,0
intercept_std_err_,0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Construct the subsample of data,0
Split into estimation and splitting sample set,0
Fit the tree on the splitting sample,0
Set the estimation values based on the estimation split,0
Apply the trained tree on the estimation sample to get the path for every estimation sample,0
Calculate the total weight of estimation samples on each tree node:,0
\sum_i sample_weight[i] * 1{i \\in node},0
Calculate the total number of estimation samples on each tree node:,0
|node| = \sum_{i} 1{i \\in node},0
Calculate the weighted sum of responses on the estimation sample on each node:,0
\sum_{i} sample_weight[i] 1{i \\in node} Y_i,0
Prune tree to remove leafs that don't satisfy the leaf requirements on the estimation sample,0
and for each un-pruned tree set the value and the weight appropriately.,0
If minimum weight requirement or minimum leaf size requirement is not satisfied on estimation,0
"sample, then prune the whole sub-tree",0
Set the value of the node to: \sum_{i} sample_weight[i] 1{i \\in node} Y_i / |node|,0
Set the weight of the node to: \sum_{i} sample_weight[i] 1{i \\in node} / |node|,0
Set the count to the estimation split count,0
Validate or convert input data,0
Pre-sort indices to avoid that each individual tree of the,0
ensemble sorts the indices.,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
"Free allocated memory, if any",0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
TODO. This slicing should ultimately be done inside the parallel function,1
so that we don't need to create a matrix of size roughly n_samples * n_estimators,0
Collect newly grown trees,0
Helper class that accumulates an arbitrary function in parallel on the accumulator acc,0
and calls the function fn on each tree e and returns the mean output. The function fn,0
"should take as input a tree e, and return another function g_e, which takes as input X, check_input",0
"If slice is not None, but rather a tuple (start, end), then a subset of the trees from",0
index start to index end will be used. The returned result is essentially:,0
(mean over e in slice)(g_e(X)).,0
Check data,0
Assign chunk of trees to jobs,0
Check data,0
Check data,0
avoid storing the output of every estimator by summing them here,0
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) Y_i",0
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x)",0
"Calculate for each slice S: Q(S) = 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) (Y_i - \theta(X))",0
where \theta(X) is the point estimate using the whole forest,0
Calculate the variance of the latter as E[Q(S)^2],0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Require group assignment t to be one-hot-encoded,0
Define an inner function that iterates over group predictions,0
Convert rows to columns,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Crossfitting,0
Compute weighted nuisance estimates,0
Generate subsample indices,0
Safety check,0
Build trees in parallel,0
Calculates weights,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned by LinearRegression is (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
Copy and/or define models,0
Nuisance estimators shall be defined during fitting because they need to know the number of distinct,0
treatments,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
Define autoencoder,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Define number of classes,0
Call `fit` from parent class,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned by LinearRegression is (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
This works both with our without the weighting trick as the treatments T are unit vector,0
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional,0
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by,0
both Parametric and Non Parametric DML.,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support sample_var,1
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
add statsmodels to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################,0
Core DML Tests,0
#######################################,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
store number of columns of T so that we can pass scalars to effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
We can write effect interval as a function of const_marginal_effect_interval for a single treatment,0
once the estimator has been fit,0
We can write effect interval as a function of predict_interval of the final method for linear models,0
"once the estimator has been fit, it's kosher to store d_t here",0
(which needs to have been expanded if there's a discrete treatment),0
need to set the fit args before the estimator is fit,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"TODO Allow for non-vector y, i.e. of shape (n, 1)",0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
TODO: support sample_var,1
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
add statsmodels to parent's options,0
Replacing to remove docstring,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################################,0
Perfect Data DGPs for Testing Correctness of Code,0
#######################################################,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect + eta,0
Generate random control co-variates,0
Use the same treatment vector for each row,0
Construct outcomes as y = X*beta + T*effect,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
TODO Share some logic with non-discrete version,1
add statsmodels to parent's options,0
add statsmodels to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
Normalize weights,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
"However, the alternative is reimplementing a bunch of intricate stuff by hand",0
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
clean way of achieving this,0
make sure we don't accidentally escape anything in the substitution,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
in multi-target use first target,0
Write node mean CATE,0
Write node std of CATE,0
Write confidence interval information if at leaf node,0
Fetch appropriate color for node,0
"red for negative, green for positive",0
Write node mean CATE,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
drop first column since all columns sum to one,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimators,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
"try to get interval first if appropriate, since we don't prefer a wrapped method with this name",0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot",0
"prior to calling interpret, can't plot, render, etc.",0
can interpret without uncertainty,0
can't interpret with uncertainty if inference wasn't used during fit,0
can interpret with uncertainty if we refit,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
Multi-dimensional output y,0
1-d y,0
multi-d y,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Test non keyword based calls to fit,0
Test custom splitter,0
Test incomplete set of test folds,0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged,0
The __warningregistry__'s need to be in a pristine state for tests,0
to work properly.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params. Using n_jobs=1 since code coverage,0
does not work well with parallelism.,1
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
TODO: add stratification to bootstrap so that we can use it,0
even with discrete treatments,0
make sure we can call the marginal_effect and effect methods,0
"make sure we can call effect with implied scalar treatments,",0
"no matter the dimensions of T, and also that we warn when there",0
are multiple treatments,0
"ExitStack can be used as a ""do nothing"" ContextManager",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
"TODO Add bootstrap inference, once discrete treatment issue is fixed",1
make sure we can call the marginal_effect and effect methods,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
We concatenate the two copies data,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
TODO: add stratification to bootstrap so that we can use it even with discrete treatments,1
make sure we can call the marginal_effect and effect methods,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
selective ridge has a simple implementation that we can test against,0
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546,0
"it should be the case that when we set fit_intercept to true,",0
it doesn't matter whether the penalized model also fits an intercept or not,0
create an extra copy of rows with weight 2,0
"instead of a slice, explicitly return an array of indices",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Call weighted lasso on reduced design matrix,0
Inherit some parameters from the parent,0
Weighted tau,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_std_err_,0
intercept_std_err_,0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
"The unpenalized model can't contain an intercept, because in the analysis above",0
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same",0
"as (M X) beta + c, so the learned coef and intercept will be wrong",0
now regress X1 on y - X2 * beta2 to learn beta1,0
set coef_ and intercept_ attributes,0
Note that the penalized model should *not* have an intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Construct the subsample of data,0
Split into estimation and splitting sample set,0
Fit the tree on the splitting sample,0
Set the estimation values based on the estimation split,0
Apply the trained tree on the estimation sample to get the path for every estimation sample,0
Calculate the total weight of estimation samples on each tree node:,0
\sum_i sample_weight[i] * 1{i \\in node},0
Calculate the total number of estimation samples on each tree node:,0
|node| = \sum_{i} 1{i \\in node},0
Calculate the weighted sum of responses on the estimation sample on each node:,0
\sum_{i} sample_weight[i] 1{i \\in node} Y_i,0
Prune tree to remove leafs that don't satisfy the leaf requirements on the estimation sample,0
and for each un-pruned tree set the value and the weight appropriately.,0
If minimum weight requirement or minimum leaf size requirement is not satisfied on estimation,0
"sample, then prune the whole sub-tree",0
Set the value of the node to: \sum_{i} sample_weight[i] 1{i \\in node} Y_i / |node|,0
Set the weight of the node to: \sum_{i} sample_weight[i] 1{i \\in node} / |node|,0
Set the count to the estimation split count,0
Validate or convert input data,0
Pre-sort indices to avoid that each individual tree of the,0
ensemble sorts the indices.,0
Remap output,0
reshape is necessary to preserve the data contiguity against vs,0
"[:, np.newaxis] that does not.",0
Check parameters,0
"Free allocated memory, if any",0
We draw from the random state to get the random state we,0
would have got if we hadn't used a warm_start.,0
Parallel loop: we prefer the threading backend as the Cython code,0
for fitting the trees is internally releasing the Python GIL,0
making threading more efficient than multiprocessing in,0
"that case. However, for joblib 0.12+ we respect any",0
"parallel_backend contexts set at a higher level,",0
since correctness does not rely on using threads.,0
TODO. This slicing should ultimately be done inside the parallel function,1
so that we don't need to create a matrix of size roughly n_samples * n_estimators,0
Collect newly grown trees,0
Helper class that accumulates an arbitrary function in parallel on the accumulator acc,0
and calls the function fn on each tree e and returns the mean output. The function fn,0
"should take as input a tree e, and return another function g_e, which takes as input X, check_input",0
"If slice is not None, but rather a tuple (start, end), then a subset of the trees from",0
index start to index end will be used. The returned result is essentially:,0
(mean over e in slice)(g_e(X)).,0
Check data,0
Assign chunk of trees to jobs,0
Check data,0
Check data,0
avoid storing the output of every estimator by summing them here,0
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) Y_i",0
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x)",0
"Calculate for each slice S: Q(S) = 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) (Y_i - \theta(X))",0
where \theta(X) is the point estimate using the whole forest,0
Calculate the variance of the latter as E[Q(S)^2],0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
-- Options for doctest extension -------------------------------------------,0
we can document otherwise excluded entities here by returning False,0
or skip otherwise included entities by returning True,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Estimate response function,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Create splits of causal tree,0
Make sure the correct exception is being rethrown,0
Must make sure indices are merged correctly,0
Require group assignment t to be one-hot-encoded,0
Define an inner function that iterates over group predictions,0
Convert rows to columns,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Crossfitting,0
Compute weighted nuisance estimates,0
Generate subsample indices,0
Safety check,0
Build trees in parallel,0
Calculates weights,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned by LinearRegression is (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
Copy and/or define models,0
Nuisance estimators shall be defined during fitting because they need to know the number of distinct,0
treatments,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
Define autoencoder,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Define number of classes,0
Call `fit` from parent class,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned by LinearRegression is (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"TODO: consider whether we need more care around stateful featurizers,",1
since we clone it and fit separate copies,0
"if both X and W are None, just return a column of ones",0
"In this case, the Target is the one-hot-encoding of the treatment variable",0
We need to go back to the label representation of the one-hot so as to call,0
the classifier.,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
NOTE This is used by the inference methods and has to be the overall featurizer. intended,0
for internal use by the library,0
NOTE This is used by the inference methods and is more for internal use to the library,0
override only so that we can update the docstring to indicate support for `StatsModelsInference`,0
TODO: support sample_var,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################,0
Core DML Tests,0
#######################################,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
store number of columns of T so that we can pass scalars to effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"once the estimator has been fit, it's kosher to access its effect_op and store it here",0
"(which needs to have seen the expanded d_t if there's a discrete treatment, etc.)",0
"once the estimator has been fit, it's kosher to access its effect_op and store it here",0
"(which needs to have seen the expanded d_t if there's a discrete treatment, etc.)",0
need to set the fit args before the estimator is fit,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"TODO Allow for non-vector y, i.e. of shape (n, 1)",0
Coding Remark: The reasoning around the multitask_model_final could have been simplified if,0
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want",0
"to allow even for model_final objects whose fit(X, y) can accept X=None",0
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor",0
checks that X is 2D array.,0
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring",0
"Replacing fit from DRLearner, to add statsmodels inference in docstring",0
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring",0
TODO: support sample_var,1
"Replacing this method which is invalid for this class, so that we make the",0
dosctring empty and not appear in the docs.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################################,0
Perfect Data DGPs for Testing Correctness of Code,0
#######################################################,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect + eta,0
Generate random control co-variates,0
Use the same treatment vector for each row,0
Construct outcomes as y = X*beta + T*effect,0
Licensed under the MIT License.,0
"since inference objects can be stateful, we must copy it before fitting;",0
otherwise this sequence wouldn't work:,0
"est1.fit(..., inference=inf)",0
"est2.fit(..., inference=inf)",0
est1.effect_interval(...),0
because inf now stores state from fitting est2,0
call the wrapped fit method,0
NOTE: we call inference fit *after* calling the main fit method,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
if X is None then the shape of const_marginal_effect will be wrong because the number,0
of rows of T was not taken into account,0
need to store the *original* dimensions of T so that we can expand scalar inputs to match;,0
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments,0
"override effect to set defaults, which works with the new definition of _expand_treatments",0
"NOTE: don't explicitly expand treatments here, because it's done in the super call",0
add statsmodels to parent's options,0
add debiasedlasso to parent's options,0
TODO Share some logic with non-discrete version,1
add statsmodels to parent's options,0
add statsmodels to parent's options,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Our classes that derive from sklearn ones sometimes include,0
inherited docstrings that have embedded doctests; we need the following imports,0
so that they don't break.,0
Check if model is sparse enough for this model,0
"note that by default OneHotEncoder returns float64s, so need to convert to int",0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive),0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
Normalize weights,0
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: allow different subsets for L1 and L2 regularization?,1
TODO: any better way to deal with sparsity?,1
TODO: any better way to deal with sparsity?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
use a binary array to get stratified split in case of discrete treatment,0
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state",0
drop first column since all columns sum to one,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimators,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
group by product; sum and subtract original; divide by (n_p-1),0
group by product; sum and subtract original; divide by (n_p-1),0
"for now, require one feature per store/product combination",0
TODO: would be nice to relax this somehow,1
"alphas vary by product, not by store",0
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc.",0
"one cross-price term per product, which is based on the average price",0
of all other goods sold at the same store in the same week,0
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc.",0
store-specific and product-specific gammas and betas (which are positively correlated),0
"features: product dummies, store dummies",0
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)",0
"observe n_products * n_stores prices, same number of quantities",0
"for direct regression comparisons, we need a pivoted version",0
"""treatments"" for direct regression include treatments, plus treatments interacted with product dummies,",0
"plus the same for ""group treatments"" (average treatment of other products in the same store/week)",0
"for direct regression, we also need to append the features",0
"(both the ""constant features"" as well as the normal ones)",0
NOTE: need to set cv because default generic algorithm is super slow for sparse matrices,0
"alphas vary by product, not by store",0
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc.",0
store-specific and product-specific gammas,0
store-specific and product-specific betas,0
"features: product dummies, store dummies",0
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)",0
we need only the prices for the compound model; all dummies are created internally,0
"observe n_products * n_stores prices, same number of quantities",0
"simple results include treatments, plus treatments interacted with product dummies,",0
for use with the direct methods,0
X should have 0 columns; we will instead pivot Y and fit against the features passed into the constructor,0
underspecified model,0
Y = alpha T + \sum_i alpha_i T_i + X beta + eta,0
T = X gamma + eps,0
how to score? distance from line of all solutions?,0
"given that 0, a, b, c, d is equivalent to x, a-x, b-x, c-x, d-x, compute the error",0
"baselines: ridge, ridge-like (penalize alpha_i but not alpha_baseline or beta)",0
"comparison: 2ml (OLS for T on X, OLS for Y on XxX^e, ridge or ridge-like for alphas)",0
"features: one product dummy, one store dummy (each missing one level), constant",0
"alphas vary by product, not by store",0
store-specific and product-specific gammas,0
store-specific and product-specific betas,0
"features: product dummies, store dummies",0
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)",0
"columns: prices interacted with products (and constant), features",0
"observe n_products * n_stores prices, same number of quantities",0
use features starting at index 1+n_products to skip all prices,0
"pickleFile = open('pickledSparse_{0}_{1}_{2}_{3}.pickle'.format(n_exp, n_products, n_stores, n_weeks), 'rb')",0
"alphass, ridges, lassos, doubleMls = pickle.load(pickleFile)",0
pickleFile.close(),0
#############################################,0
Defining the parameters of Monte Carlo,0
#############################################,0
Estimation parameters,0
###################################################################,0
Estimating the parameters of the DGP with DML. Running multiple,0
Monte Carlo experiments.,0
###################################################################,0
Sparse coefficients of treatment as a function of co-variates,0
Coefficients of outcomes as a function of co-variates,0
"DGP. Create samples of data (y, T, X) from known truth",0
DML Estimation.,0
Estimation with other methods for comparison,0
#########################################################,0
Plotting the results and saving,0
#########################################################,0
"plt.figure(figsize=(20, 10))",0
"plt.subplot(1, 4, 1)",0
"plt.title(""DML R^2: median {:.3f}, mean {:.3f}"".format(np.median(dml_r2score), np.mean(dml_r2score)))",0
plt.hist(dml_r2score),0
"plt.subplot(1, 4, 2)",0
"plt.title(""Direct Lasso R^2: median {:.3f}, mean {:.3f}"".format(np.median(direct_r2score),",0
np.mean(direct_r2score))),0
plt.hist(direct_r2score),0
"plt.subplot(1, 4, 3)",0
"plt.title(""DML Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(dml_te),",0
np.std(dml_te))),0
plt.hist(np.array(dml_te).flatten()),0
"plt.subplot(1, 4, 4)",0
"plt.title(""Direct Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(direct_te),",0
np.std(direct_te))),0
plt.hist(np.array(direct_te).flatten()),0
plt.tight_layout(),0
"plt.savefig(""r2_comparison.png"")",0
Plotting the results and saving,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
"try to get interval first if appropriate, since we don't prefer a wrapped method with this name",0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make this test actually test something instead of generating images,1
Sparse coefficients of treatment as a function of co-variates,0
Coefficients of outcomes as a function of co-variates,0
"DGP. Create samples of data (y, T, X) from known truth",0
"DGP. Create samples of data (y, T, X) from known truth",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval",0
"statsmodels uses the last dimension instead of the first to store the confidence intervals,",0
so we need to transpose the result,0
1-d output,0
2-d output,0
Single dimensional output y,0
Multi-dimensional output y,0
1-d y,0
multi-d y,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
test that we can do the same thing once we provide alpha explicitly,0
test that the lower and upper bounds differ,0
test that the estimated effect is usually within the bounds,0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that the lower and upper bounds differ,0
TODO: test that the estimated effect is usually within the bounds,1
and that the true effect is also usually within the bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Test non keyword based calls to fit,0
Test custom splitter,0
Test incomplete set of test folds,0
"theta needs to be of dimension (1, d_t) if T is (n, d_t)",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
make sure cross product varies more slowly with first array,0
and that vectors are okay as inputs,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect,0
Constant treatment with multi output Y,0
Heterogeneous treatment,0
Heterogeneous treatment with multi output Y,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test constant treatment effect with multi output Y,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Test heterogeneous treatment effect with multi output Y,0
Instantiate XLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Instantiate DomainAdaptationLearner,0
Test inputs,0
"Test constant and heterogeneous treatment effect, single and multi output y",0
Get the true treatment effect,0
Get the true treatment effect,0
Fit learner and get the effect and marginal effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check whether the output shape is right,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
DGP coefficients,0
Generated outcomes,0
################,0
WeightedLasso #,0
################,0
Define weights,0
Define extended datasets,0
Range of alphas,0
Compare with Lasso,0
--> No intercept,0
--> With intercept,0
When DGP has no intercept,0
When DGP has intercept,0
--> Coerce coefficients to be positive,0
--> Toggle max_iter & tol,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
Mixed DGP scenario.,0
Define extended datasets,0
Define weights,0
Define multioutput,0
##################,0
WeightedLassoCV #,0
##################,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
Choose a smaller n to speed-up process,0
Compare fold weights,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
--> Force parameters to be positive,0
###########################,0
MultiTaskWeightedLassoCV #,0
###########################,0
Define alphas to test,0
Define splitter,0
Compare with MultiTaskLassoCV,0
--> No intercept,0
--> With intercept,0
Define weights,0
Define extended datasets,0
Define splitters,0
WeightedKFold splitter,0
Map weighted splitter to an extended splitter,0
Define alphas to test,0
Compare with LassoCV,0
--> No intercept,0
--> With intercept,0
################,0
DebiasedLasso #,0
################,0
Test DebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check 5-95 CI coverage for unit vectors,0
Test DebiasedLasso with weights for one DGP,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Define weights,0
Data from one DGP has weight 0. Check that we recover correct coefficients,0
--> Check debiased coeffcients,0
Test that attributes propagate correctly,0
Test MultiOutputDebiasedLasso without weights,0
--> Check debiased coeffcients without intercept,0
--> Check debiased coeffcients with intercept,0
--> Check CI coverage,0
Test MultiOutputDebiasedLasso with weights,0
Define weights,0
Define extended datasets,0
--> Check debiased coefficients,0
Unit vectors,0
Unit vectors,0
Check coeffcients and intercept are the same within tolerance,0
Check results are similar with tolerance 1e-6,0
Check if multitask,0
Check that same alpha is chosen,0
Check that the coefficients are similar,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params,0
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity.",0
The rest for controls. Just as an example.,0
Generating A/B test data,0
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity,0
We also have confounding on the first variable. We also have heteroskedastic errors.,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
TODO: add stratification to bootstrap so that we can use it even with discrete treatments,1
make sure we can call the marginal_effect and effect methods,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
(incorrectly) use a final model with an intercept,0
"Because final model is fixed, actual values of T and Y don't matter",0
Ensure reproducibility,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
ensure that we've got at least two of every element,0
"since T isn't passed to const_marginal_effect, defaults to one row if X is None",0
TODO: add stratification to bootstrap so that we can use it even with discrete treatments,1
make sure we can call the marginal_effect and effect methods,0
"make sure we can call effect with implied scalar treatments, no matter the",0
"dimensions of T, and also that we warn when there are multiple treatments",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
test that we can fit with a KFold instance,0
test that we can fit with a train/test iterable,0
Test inputs,0
self._test_inputs(DR_learner),0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Sparse DGP,0
Treatment effect coef,0
Other coefs,0
Features and controls,0
Test sparse estimator,0
"--> test coef_, intercept_",0
--> test treatment effects,0
Restrict x_test to vectors of norm < 1,0
--> check inference,0
Check that a majority of true effects lie in the 5-95% CI,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Found a good split, return.",0
Record all splits in case the stratification by weight yeilds a worse partition,0
Reseed random generator and try again,0
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups",0
"Found a good split, return.",0
Did not find a good split,0
Record the devaiation for the weight-stratified split to compare with KFold splits,0
Return most weight-balanced partition,0
Weight stratification algorithm,0
Sort weights for weight strata search,0
There are some leftover indices that have yet to be assigned,0
Append stratum splits to overall splits,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"Convert X, y into numpy arrays",0
Define fit parameters,0
Some algorithms don't have a check_input option,0
Check weights array,0
Check that weights are size-compatible,0
Normalize inputs,0
Weight inputs,0
Fit base class without intercept,0
Fit Lasso,0
Reset intercept,0
The intercept is not calculated properly due the sqrt(weights) factor,0
so it must be recomputed,0
Fit lasso without weights,0
Make weighted splitter,0
Fit weighted model,0
Make weighted splitter,0
Fit weighted model,0
Select optimal penalty,0
Warn about consistency,0
"Convert X, y into numpy arrays",0
Fit weighted lasso with user input,0
"Center X, y",0
Calculate quantities that will be used later on. Account for centered data,0
Calculate coefficient and error variance,0
Add coefficient correction,0
Set coefficients and intercept standard errors,0
Set intercept,0
Return alpha to 'auto' state,0
"Note that in the case of no intercept, X_offset is 0",0
Calculate the variance of the predictions,0
Calculate prediction confidence intervals,0
Assumes flattened y,0
Compute weighted residuals,0
To be done once per target. Assumes y can be flattened.,0
Assumes that X has already been offset,0
Special case: n_features=1,0
Compute Lasso coefficients for the columns of the design matrix,0
Call weighted lasso on reduced design matrix,0
Inherit some parameters from the parent,0
Weighted tau,0
Compute C_hat,0
Compute theta_hat,0
Allow for single output as well,0
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso",0
Set coef_ attribute,0
Set intercept_ attribute,0
Set selected_alpha_ attribute,0
Set coef_std_err_,0
intercept_std_err_,0
set intercept_ attribute,0
set coef_ attribute,0
set alpha_ attribute,0
set alphas_ attribute,0
set n_iter_ attribute,0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Calculate residuals,0
Estimate E[T_res | Z_res],0
TODO. Deal with multi-class instrument,1
Calculate nuisances,0
Estimate E[T_res | Z_res],0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"We do a three way split, as typically a preliminary theta estimator would require",0
many samples. So having 2/3 of the sample to train model_theta seems appropriate.,0
TODO. Deal with multi-class instrument,1
Estimate final model of theta(X) by minimizing the square loss:,0
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2",0
"We clip the covariance so that it is bounded away from zero, so as to reduce variance",0
at the expense of some small bias. For points with very small covariance we revert,0
to the model-based preliminary estimate and do not add the correction term.,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate r(Z) = E[Z | X] in cross fitting manner,0
Calculate residual T_res = T - p(X) and Z_res = Z - r(X),0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]",0
TODO. The solution below is not really a valid cross-fitting,1
as the test data are used to create the proj_t on the train,0
which in the second train-test loop is used to create the nuisance,0
cov on the test data. Hence the T variable of some sample,0
"is implicitly correlated with its cov nuisance, through this flow",0
"of information. However, this seems a rather weak correlation.",0
The more kosher would be to do an internal nested cv loop for the T_XZ,0
model.,0
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner",0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2",0
#############################################################################,0
Classes for the DRIV implementation for the special case of intent-to-treat,0
A/B test,0
#############################################################################,0
Estimate preliminary theta in cross fitting manner,0
Estimate p(X) = E[T | X] in cross fitting manner,0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary",0
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split",0
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.,0
model_T_XZ = lambda: model_clf(),0
#'days_visited': lambda:,0
"#X = np.random.uniform(-1, 1, size=(n, d))",0
Turn strings into categories for numeric mapping,0
### Defining some generic regressors and classifiers,0
This a generic non-parametric regressor,0
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)",0
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='rmse', binary=False)",0
model = lambda: RandomForestRegressor(n_estimators=100),0
model = lambda: Lasso(alpha=0.0001) #CV(cv=5),0
model = lambda: GradientBoostingRegressor(n_estimators=60),0
model = lambda: LinearRegression(n_jobs=-1),0
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because",0
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the,0
underlying model whenever predict is called.,0
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,",0
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))",0
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),",0
"early_stopping_rounds=5, eval_metric='logloss', binary=True))",0
model_clf = lambda: RandomForestClassifier(n_estimators=100),0
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60)),0
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))",0
We need to specify models to be used for each of these residualizations,0
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary,0
"E[T | X, Z]",0
E[TZ | X],0
We fit DMLATEIV with these models and then we call effect() to get the ATE.,0
n_splits determines the number of splits to be used for cross-fitting.,0
# Algorithm 2 - Current Method,0
In[121]:,0
# Algorithm 3 - DRIV ATE,0
dmliv_model_effect = lambda: model(),0
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),",0
"dmliv_model_effect(),",0
n_splits=1),0
reshape in case we get fewer dimensions than expected from h (e.g. a scalar),0
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
"Once multiple treatments are supported, we'll need to fix this",1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface.",0
We can use statsmodel for all hypothesis testing capabilities,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO. Deal with multi-class instrument/treatment,1
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner",0
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner,0
Estimate p(X) = E[T | X] in cross-fitting manner,0
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2",0
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Check inputs,0
Check inputs,0
Fit outcome model on X||W||T (concatenated),0
Check inputs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Create splits of causal tree,0
Must make sure indices are merged correctly,0
Require group assignment t to be one-hot-encoded,0
Define an inner function that iterates over group predictions,0
Convert rows to columns,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Crossfitting,0
Compute weighted nuisance estimates,0
Generate subsample indices,0
Safety check,0
Build trees in parallel,0
Calculates weights,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned by LinearRegression is (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
Copy and/or define models,0
Nuisance estimators shall be defined during fitting because they need to know the number of distinct,0
treatments,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
Define autoencoder,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Define number of classes,0
Call `fit` from parent class,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned by LinearRegression is (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"TODO: If T is a vector rather than a 2-D array, then the model's fit must accept a vector...",1
"Do we want to reshape to an nx1, or just trust the user's choice of input?",0
(Likewise for Y below),0
need to override effect in case of discrete treatments,0
TODO: should this logic be moved up to the LinearCateEstimator class and,1
removed from here and from the OrthoForest implementation?,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
create an identity matrix of size d_t (or just a 1-element array if T was a vector),0
the nth row will allow us to compute the marginal effect of the nth component of treatment,0
TODO: Doing this kronecker/reshaping/transposing stuff so that predict can be called,1
"rather than just using coef_ seems silly, but one benefit is that we can use linear models",0
that don't expose a coef_ (e.g. a GridSearchCV over underlying linear models),0
TODO: handle case where final model doesn't directly expose coef_?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################,0
Core DML Tests,0
#######################################,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect,0
store number of columns of T so that we can pass scalars to effect,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
"regress T expansion on X,Z expansions concatenated with W",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output",0
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################################,0
Perfect Data DGPs for Testing Correctness of Code,0
#######################################################,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect + eta,0
Generate random control co-variates,0
Use the same treatment vector for each row,0
Construct outcomes as y = X*beta + T*effect,0
Licensed under the MIT License.,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
"TODO: if T0 or T1 are scalars, we'll promote them to vectors;",1
should it be possible to promote them to 2D arrays if that's what we saw during training?,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
TODO: wouldn't making X1 vary more slowly than X2 be more intuitive?,1
(but note that changing this would necessitate changes to callers,0
to switch the order to preserve behavior where order is important),0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
Normalize weights,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: allow different subsets for L1 and L2 regularization?,1
TODO: any better way to deal with sparsity?,1
TODO: any better way to deal with sparsity?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimators,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
group by product; sum and subtract original; divide by (n_p-1),0
group by product; sum and subtract original; divide by (n_p-1),0
"for now, require one feature per store/product combination",0
TODO: would be nice to relax this somehow,1
"alphas vary by product, not by store",0
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc.",0
"one cross-price term per product, which is based on the average price",0
of all other goods sold at the same store in the same week,0
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc.",0
store-specific and product-specific gammas and betas (which are positively correlated),0
"features: product dummies, store dummies",0
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)",0
"observe n_products * n_stores prices, same number of quantities",0
"for direct regression comparisons, we need a pivoted version",0
"""treatments"" for direct regression include treatments, plus treatments interacted with product dummies,",0
"plus the same for ""group treatments"" (average treatment of other products in the same store/week)",0
"for direct regression, we also need to append the features",0
"(both the ""constant features"" as well as the normal ones)",0
NOTE: need to set cv because default generic algorithm is super slow for sparse matrices,0
"alphas vary by product, not by store",0
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc.",0
store-specific and product-specific gammas,0
store-specific and product-specific betas,0
"features: product dummies, store dummies",0
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)",0
we need only the prices for the compound model; all dummies are created internally,0
"observe n_products * n_stores prices, same number of quantities",0
"simple results include treatments, plus treatments interacted with product dummies,",0
for use with the direct methods,0
X should have 0 columns; we will instead pivot Y and fit against the features passed into the constructor,0
underspecified model,0
Y = alpha T + \sum_i alpha_i T_i + X beta + eta,0
T = X gamma + eps,0
how to score? distance from line of all solutions?,0
"given that 0, a, b, c, d is equivalent to x, a-x, b-x, c-x, d-x, compute the error",0
"baselines: ridge, ridge-like (penalize alpha_i but not alpha_baseline or beta)",0
"comparison: 2ml (OLS for T on X, OLS for Y on XxX^e, ridge or ridge-like for alphas)",0
"features: one product dummy, one store dummy (each missing one level), constant",0
"alphas vary by product, not by store",0
store-specific and product-specific gammas,0
store-specific and product-specific betas,0
"features: product dummies, store dummies",0
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)",0
"columns: prices interacted with products (and constant), features",0
"observe n_products * n_stores prices, same number of quantities",0
use features starting at index 1+n_products to skip all prices,0
"pickleFile = open('pickledSparse_{0}_{1}_{2}_{3}.pickle'.format(n_exp, n_products, n_stores, n_weeks), 'rb')",0
"alphass, ridges, lassos, doubleMls = pickle.load(pickleFile)",0
pickleFile.close(),0
#############################################,0
Defining the parameters of Monte Carlo,0
#############################################,0
Estimation parameters,0
###################################################################,0
Estimating the parameters of the DGP with DML. Running multiple,0
Monte Carlo experiments.,0
###################################################################,0
Sparse coefficients of treatment as a function of co-variates,0
Coefficients of outcomes as a function of co-variates,0
"DGP. Create samples of data (y, T, X) from known truth",0
DML Estimation.,0
Estimation with other methods for comparison,0
#########################################################,0
Plotting the results and saving,0
#########################################################,0
"plt.figure(figsize=(20, 10))",0
"plt.subplot(1, 4, 1)",0
"plt.title(""DML R^2: median {:.3f}, mean {:.3f}"".format(np.median(dml_r2score), np.mean(dml_r2score)))",0
plt.hist(dml_r2score),0
"plt.subplot(1, 4, 2)",0
"plt.title(""Direct Lasso R^2: median {:.3f}, mean {:.3f}"".format(np.median(direct_r2score),",0
np.mean(direct_r2score))),0
plt.hist(direct_r2score),0
"plt.subplot(1, 4, 3)",0
"plt.title(""DML Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(dml_te),",0
np.std(dml_te))),0
plt.hist(np.array(dml_te).flatten()),0
"plt.subplot(1, 4, 4)",0
"plt.title(""Direct Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(direct_te),",0
np.std(direct_te))),0
plt.hist(np.array(direct_te).flatten()),0
plt.tight_layout(),0
"plt.savefig(""r2_comparison.png"")",0
Plotting the results and saving,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
TODO: what if some args can be None?,1
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make this test actually test something instead of generating images,1
Sparse coefficients of treatment as a function of co-variates,0
Coefficients of outcomes as a function of co-variates,0
"DGP. Create samples of data (y, T, X) from known truth",0
"DGP. Create samples of data (y, T, X) from known truth",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
also test vector t and y,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that the lower and upper bounds differ,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that we can do the same thing once we provide percentile bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Instantiate XLearner,0
Test inputs,0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Instantiate DomainAdaptationLearner,0
Test inputs,0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Instantiate DomainAdaptationLearner,0
Test inputs,0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params,0
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
just make sure we can call the marginal_effect and effect methods,0
"for vector-valued T, verify that default scalar T0 and T1 work",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
TODO: it seems like roughly 20% of the calls to _test_sparse are failing - find out what's going wrong,1
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
note that this would fail for the non-sparse DMLCateEstimator,0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Check inputs,0
Check inputs,0
Fit outcome model on X||W||T (concatenated),0
Check inputs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Create splits of causal tree,0
Must make sure indices are merged correctly,0
Require group assignment t to be one-hot-encoded,0
Define an inner function that iterates over group predictions,0
Convert rows to columns,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Crossfitting,0
Compute weighted nuisance estimates,0
Generate subsample indices,0
Safety check,0
Build trees in parallel,0
Calculates weights,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned by LinearRegression is (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
Copy and/or define models,0
Nuisance estimators shall be defined during fitting because they need to know the number of distinct,0
treatments,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
Define autoencoder,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Define number of classes,0
Call `fit` from parent class,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned by LinearRegression is (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"TODO: If T is a vector rather than a 2-D array, then the model's fit must accept a vector...",1
"Do we want to reshape to an nx1, or just trust the user's choice of input?",0
(Likewise for Y below),0
need to override effect in case of discrete treatments,0
TODO: should this logic be moved up to the LinearCateEstimator class and,1
removed from here and from the OrthoForest implementation?,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
create an identity matrix of size d_t (or just a 1-element array if T was a vector),0
the nth row will allow us to compute the marginal effect of the nth component of treatment,0
TODO: Doing this kronecker/reshaping/transposing stuff so that predict can be called,1
"rather than just using coef_ seems silly, but one benefit is that we can use linear models",0
that don't expose a coef_ (e.g. a GridSearchCV over underlying linear models),0
TODO: handle case where final model doesn't directly expose coef_?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################,0
Core DML Tests,0
#######################################,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
"regress T expansion on X,Z expansions",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
TODO: allow 1D arguments for Y and T,1
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################################,0
Perfect Data DGPs for Testing Correctness of Code,0
#######################################################,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect + eta,0
Generate random control co-variates,0
Use the same treatment vector for each row,0
Construct outcomes as y = X*beta + T*effect,0
Licensed under the MIT License.,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
"TODO: if T0 or T1 are scalars, we'll promote them to vectors;",1
should it be possible to promote them to 2D arrays if that's what we saw during training?,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
TODO: wouldn't making X1 vary more slowly than X2 be more intuitive?,1
(but note that changing this would necessitate changes to callers,0
to switch the order to preserve behavior where order is important),0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
Normalize weights,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: allow different subsets for L1 and L2 regularization?,1
TODO: any better way to deal with sparsity?,1
TODO: any better way to deal with sparsity?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimators,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
group by product; sum and subtract original; divide by (n_p-1),0
group by product; sum and subtract original; divide by (n_p-1),0
"for now, require one feature per store/product combination",0
TODO: would be nice to relax this somehow,1
"alphas vary by product, not by store",0
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc.",0
"one cross-price term per product, which is based on the average price",0
of all other goods sold at the same store in the same week,0
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc.",0
store-specific and product-specific gammas and betas (which are positively correlated),0
"features: product dummies, store dummies",0
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)",0
"observe n_products * n_stores prices, same number of quantities",0
"for direct regression comparisons, we need a pivoted version",0
"""treatments"" for direct regression include treatments, plus treatments interacted with product dummies,",0
"plus the same for ""group treatments"" (average treatment of other products in the same store/week)",0
"for direct regression, we also need to append the features",0
"(both the ""constant features"" as well as the normal ones)",0
NOTE: need to set cv because default generic algorithm is super slow for sparse matrices,0
"alphas vary by product, not by store",0
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc.",0
store-specific and product-specific gammas,0
store-specific and product-specific betas,0
"features: product dummies, store dummies",0
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)",0
we need only the prices for the compound model; all dummies are created internally,0
"observe n_products * n_stores prices, same number of quantities",0
"simple results include treatments, plus treatments interacted with product dummies,",0
for use with the direct methods,0
X should have 0 columns; we will instead pivot Y and fit against the features passed into the constructor,0
underspecified model,0
Y = alpha T + \sum_i alpha_i T_i + X beta + eta,0
T = X gamma + eps,0
how to score? distance from line of all solutions?,0
"given that 0, a, b, c, d is equivalent to x, a-x, b-x, c-x, d-x, compute the error",0
"baselines: ridge, ridge-like (penalize alpha_i but not alpha_baseline or beta)",0
"comparison: 2ml (OLS for T on X, OLS for Y on XxX^e, ridge or ridge-like for alphas)",0
"features: one product dummy, one store dummy (each missing one level), constant",0
"alphas vary by product, not by store",0
store-specific and product-specific gammas,0
store-specific and product-specific betas,0
"features: product dummies, store dummies",0
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)",0
"columns: prices interacted with products (and constant), features",0
"observe n_products * n_stores prices, same number of quantities",0
use features starting at index 1+n_products to skip all prices,0
"pickleFile = open('pickledSparse_{0}_{1}_{2}_{3}.pickle'.format(n_exp, n_products, n_stores, n_weeks), 'rb')",0
"alphass, ridges, lassos, doubleMls = pickle.load(pickleFile)",0
pickleFile.close(),0
#############################################,0
Defining the parameters of Monte Carlo,0
#############################################,0
Estimation parameters,0
###################################################################,0
Estimating the parameters of the DGP with DML. Running multiple,0
Monte Carlo experiments.,0
###################################################################,0
Sparse coefficients of treatment as a function of co-variates,0
Coefficients of outcomes as a function of co-variates,0
"DGP. Create samples of data (y, T, X) from known truth",0
DML Estimation.,0
Estimation with other methods for comparison,0
#########################################################,0
Plotting the results and saving,0
#########################################################,0
"plt.figure(figsize=(20, 10))",0
"plt.subplot(1, 4, 1)",0
"plt.title(""DML R^2: median {:.3f}, mean {:.3f}"".format(np.median(dml_r2score), np.mean(dml_r2score)))",0
plt.hist(dml_r2score),0
"plt.subplot(1, 4, 2)",0
"plt.title(""Direct Lasso R^2: median {:.3f}, mean {:.3f}"".format(np.median(direct_r2score),",0
np.mean(direct_r2score))),0
plt.hist(direct_r2score),0
"plt.subplot(1, 4, 3)",0
"plt.title(""DML Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(dml_te),",0
np.std(dml_te))),0
plt.hist(np.array(dml_te).flatten()),0
"plt.subplot(1, 4, 4)",0
"plt.title(""Direct Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(direct_te),",0
np.std(direct_te))),0
plt.hist(np.array(direct_te).flatten()),0
plt.tight_layout(),0
"plt.savefig(""r2_comparison.png"")",0
Plotting the results and saving,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
TODO: what if some args can be None?,1
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make this test actually test something instead of generating images,1
Sparse coefficients of treatment as a function of co-variates,0
Coefficients of outcomes as a function of co-variates,0
"DGP. Create samples of data (y, T, X) from known truth",0
"DGP. Create samples of data (y, T, X) from known truth",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that we can do the same thing once we provide percentile bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivitaves are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Instantiate XLearner,0
Test inputs,0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Instantiate DomainAdaptationLearner,0
Test inputs,0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Instantiate DomainAdaptationLearner,0
Test inputs,0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params,0
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
just make sure we can call the marginal_effect and effect methods,0
"for vector-valued T, verify that default scalar T0 and T1 work",0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
TODO: it seems like roughly 20% of the calls to _test_sparse are failing - find out what's going wrong,1
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
note that this would fail for the non-sparse DMLCateEstimator,0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Check inputs,0
Check inputs,0
Fit outcome model on X||W||T (concatenated),0
Check inputs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Create splits of causal tree,0
Must make sure indices are merged correctly,0
Require group assignment t to be one-hot-encoded,0
Define an inner function that iterates over group predictions,0
Convert rows to columns,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Crossfitting,0
Compute weighted nuisance estimates,0
Generate subsample indices,0
Safety check,0
Build trees in parallel,0
Calculates weights,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
need safe=False when cloning for WeightedModelWrapper,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Compute residuals,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned by LinearRegression is (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
Copy and/or define models,0
Nuisance estimators shall be defined during fitting because they need to know the number of distinct,0
treatments,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
Define autoencoder,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Define number of classes,0
Call `fit` from parent class,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Compute partial moments,0
Compute coefficient by OLS on residuals,0
ell_2 regularization,0
Ridge regression estimate,0
"Parameter returned by LinearRegression is (d_T, )",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"TODO: If T is a vector rather than a 2-D array, then the model's fit must accept a vector...",1
"Do we want to reshape to an nx1, or just trust the user's choice of input?",0
(Likewise for Y below),0
need to override effect in case of discrete treatments,0
TODO: should this logic be moved up to the LinearCateEstimator class and,1
removed from here and from the OrthoForest implementation?,0
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array,0
create an identity matrix of size d_t (or just a 1-element array if T was a vector),0
the nth row will allow us to compute the marginal effect of the nth component of treatment,0
TODO: Doing this kronecker/reshaping/transposing stuff so that predict can be called,1
"rather than just using coef_ seems silly, but one benefit is that we can use linear models",0
that don't expose a coef_ (e.g. a GridSearchCV over underlying linear models),0
TODO: handle case where final model doesn't directly expose coef_?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################,0
Core DML Tests,0
#######################################,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
"regress T expansion on X,Z expansions",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
TODO: allow 1D arguments for Y and T,1
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################################,0
Perfect Data DGPs for Testing Correctness of Code,0
#######################################################,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect + eta,0
Generate random control co-variates,0
Use the same treatment vector for each row,0
Construct outcomes as y = X*beta + T*effect,0
Licensed under the MIT License.,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
TODO: wouldn't making X1 vary more slowly than X2 be more intuitive?,1
(but note that changing this would necessitate changes to callers,0
to switch the order to preserve behavior where order is important),0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
Normalize weights,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: allow different subsets for L1 and L2 regularization?,1
TODO: any better way to deal with sparsity?,1
TODO: any better way to deal with sparsity?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Estimators,0
Causal tree parameters,0
Tree structure,0
No need for a random split since the data is already,0
a random subsample from the original input,0
node list stores the nodes that are yet to be splitted,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
a split is determined by a feature and a sample pair,0
the number of possible splits is at most (number of features) * (number of node samples),0
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}",0
parse row and column of random pair,0
the sample of the pair is the integer division of the random number with n_feats,0
calculate the binary indicator of whether sample i is on the left or the right,0
side of proposed split j. So this is an n_samples x n_proposals matrix,0
calculate the number of samples on the left child for each proposed split,0
calculate the analogous binary indicator for the samples in the estimation set,0
calculate the number of estimation samples on the left child of each proposed split,0
find the upper and lower bound on the size of the left split for the split,0
to be valid so as for the split to be balanced and leave at least min_leaf_size,0
on each side.,0
similarly for the estimation sample set,0
if there is no valid split then don't create any children,0
filter only the valid splits,0
calculate the average influence vector of the samples in the left child,0
calculate the average influence vector of the samples in the right child,0
take the square of each of the entries of the influence vectors and normalize,0
by size of each child,0
calculate the vector score of each candidate split as the average of left and right,0
influence vectors,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
calculate the scalar score of each split by aggregating across the vector of scores,0
Find split that minimizes criterion,0
Create child nodes with corresponding subsamples,0
add the created children to the list of not yet split nodes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
group by product; sum and subtract original; divide by (n_p-1),0
group by product; sum and subtract original; divide by (n_p-1),0
"for now, require one feature per store/product combination",0
TODO: would be nice to relax this somehow,1
"alphas vary by product, not by store",0
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc.",0
"one cross-price term per product, which is based on the average price",0
of all other goods sold at the same store in the same week,0
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc.",0
store-specific and product-specific gammas and betas (which are positively correlated),0
"features: product dummies, store dummies",0
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)",0
"observe n_products * n_stores prices, same number of quantities",0
"for direct regression comparisons, we need a pivoted version",0
"""treatments"" for direct regression include treatments, plus treatments interacted with product dummies,",0
"plus the same for ""group treatments"" (average treatment of other products in the same store/week)",0
"for direct regression, we also need to append the features",0
"(both the ""constant features"" as well as the normal ones)",0
NOTE: need to set cv because default generic algorithm is super slow for sparse matrices,0
"alphas vary by product, not by store",0
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc.",0
store-specific and product-specific gammas,0
store-specific and product-specific betas,0
"features: product dummies, store dummies",0
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)",0
we need only the prices for the compound model; all dummies are created internally,0
"observe n_products * n_stores prices, same number of quantities",0
"simple results include treatments, plus treatments interacted with product dummies,",0
for use with the direct methods,0
X should have 0 columns; we will instead pivot Y and fit against the features passed into the constructor,0
underspecified model,0
Y = alpha T + \sum_i alpha_i T_i + X beta + eta,0
T = X gamma + eps,0
how to score? distance from line of all solutions?,0
"given that 0, a, b, c, d is equivalent to x, a-x, b-x, c-x, d-x, compute the error",0
"baselines: ridge, ridge-like (penalize alpha_i but not alpha_baseline or beta)",0
"comparison: 2ml (OLS for T on X, OLS for Y on XxX^e, ridge or ridge-like for alphas)",0
"features: one product dummy, one store dummy (each missing one level), constant",0
"alphas vary by product, not by store",0
store-specific and product-specific gammas,0
store-specific and product-specific betas,0
"features: product dummies, store dummies",0
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)",0
"columns: prices interacted with products (and constant), features",0
"observe n_products * n_stores prices, same number of quantities",0
use features starting at index 1+n_products to skip all prices,0
"pickleFile = open('pickledSparse_{0}_{1}_{2}_{3}.pickle'.format(n_exp, n_products, n_stores, n_weeks), 'rb')",0
"alphass, ridges, lassos, doubleMls = pickle.load(pickleFile)",0
pickleFile.close(),0
#############################################,0
Defining the parameters of Monte Carlo,0
#############################################,0
Estimation parameters,0
###################################################################,0
Estimating the parameters of the DGP with DML. Running multiple,0
Monte Carlo experiments.,0
###################################################################,0
Sparse coefficients of treatment as a function of co-variates,0
Coefficients of outcomes as a function of co-variates,0
"DGP. Create samples of data (y, T, X) from known truth",0
DML Estimation.,0
Estimation with other methods for comparison,0
#########################################################,0
Plotting the results and saving,0
#########################################################,0
"plt.figure(figsize=(20, 10))",0
"plt.subplot(1, 4, 1)",0
"plt.title(""DML R^2: median {:.3f}, mean {:.3f}"".format(np.median(dml_r2score), np.mean(dml_r2score)))",0
plt.hist(dml_r2score),0
"plt.subplot(1, 4, 2)",0
"plt.title(""Direct Lasso R^2: median {:.3f}, mean {:.3f}"".format(np.median(direct_r2score),",0
np.mean(direct_r2score))),0
plt.hist(direct_r2score),0
"plt.subplot(1, 4, 3)",0
"plt.title(""DML Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(dml_te),",0
np.std(dml_te))),0
plt.hist(np.array(dml_te).flatten()),0
"plt.subplot(1, 4, 4)",0
"plt.title(""Direct Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(direct_te),",0
np.std(direct_te))),0
plt.hist(np.array(direct_te).flatten()),0
plt.tight_layout(),0
"plt.savefig(""r2_comparison.png"")",0
Plotting the results and saving,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
TODO: what if some args can be None?,1
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"require all cells to complete within 15 minutes, which will help prevent us from",0
creating notebooks that are annoying for our users to actually run themselves,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make this test actually test something instead of generating images,1
Sparse coefficients of treatment as a function of co-variates,0
Coefficients of outcomes as a function of co-variates,0
"DGP. Create samples of data (y, T, X) from known truth",0
"DGP. Create samples of data (y, T, X) from known truth",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that we can do the same thing once we provide percentile bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivitaves are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Set random seed,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Instantiate XLearner,0
Test inputs,0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Instantiate DomainAdaptationLearner,0
Test inputs,0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Instantiate DomainAdaptationLearner,0
Test inputs,0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Test heterogenous treatment effect for W =/= None,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Only for heterogeneous TE,0
Fit learner on X and W and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params,0
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
just make sure we can call the marginal_effect and effect methods,0
create a simple artificial setup where effect of moving from treatment,0
"1 -> 2 is 2,",0
"1 -> 3 is 1, and",0
"2 -> 3 is -1 (necessarily, by composing the previous two effects)",0
"Using an uneven number of examples from different classes,",0
"and having the treatments in non-lexicographic order,",0
Should rule out some basic issues.,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
TODO: it seems like roughly 20% of the calls to _test_sparse are failing - find out what's going wrong,1
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
note that this would fail for the non-sparse DMLCateEstimator,0
configuration is all pulled from setup.cfg,0
-*- coding: utf-8 -*-,0
,0
Configuration file for the Sphinx documentation builder.,0
,0
This file does only contain a selection of the most common options. For a,0
full list see the documentation:,0
http://www.sphinx-doc.org/en/master/config,0
-- Path setup --------------------------------------------------------------,0
"If extensions (or modules to document with autodoc) are in another directory,",0
add these directories to sys.path here. If the directory is relative to the,0
"documentation root, use os.path.abspath to make it absolute, like shown here.",0
,0
-- Project information -----------------------------------------------------,0
-- General configuration ---------------------------------------------------,0
"If your documentation needs a minimal Sphinx version, state it here.",0
,0
needs_sphinx = '1.0',0
"Add any Sphinx extension module names here, as strings. They can be",0
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,0
ones.,0
"Add any paths that contain templates here, relative to this directory.",0
The suffix(es) of source filenames.,0
You can specify multiple suffix as a list of string:,0
,0
"source_suffix = ['.rst', '.md']",0
The master toctree document.,0
The language for content autogenerated by Sphinx. Refer to documentation,0
for a list of supported languages.,0
,0
This is also used if you do content translation via gettext catalogs.,0
"Usually you set ""language"" from the command line for these cases.",0
"List of patterns, relative to source directory, that match files and",0
directories to ignore when looking for source files.,0
This pattern also affects html_static_path and html_extra_path.,0
The name of the Pygments (syntax highlighting) style to use.,0
-- Options for HTML output -------------------------------------------------,0
The theme to use for HTML and HTML Help pages.  See the documentation for,0
a list of builtin themes.,0
,0
Theme options are theme-specific and customize the look and feel of a theme,0
"further.  For a list of options available for each theme, see the",0
documentation.,0
,0
"Add any paths that contain custom static files (such as style sheets) here,",0
"relative to this directory. They are copied after the builtin static files,",0
"so a file named ""default.css"" will overwrite the builtin ""default.css"".",0
html_static_path = ['_static'],0
"Custom sidebar templates, must be a dictionary that maps document names",0
to template names.,0
,0
The default sidebars (for documents that don't match any pattern) are,0
defined by theme itself.  Builtin themes are using these templates by,0
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",0
'searchbox.html']``.,0
,0
html_sidebars = {},0
-- Options for HTMLHelp output ---------------------------------------------,0
Output file base name for HTML help builder.,0
-- Options for LaTeX output ------------------------------------------------,0
The paper size ('letterpaper' or 'a4paper').,0
,0
"'papersize': 'letterpaper',",0
"The font size ('10pt', '11pt' or '12pt').",0
,0
"'pointsize': '10pt',",0
Additional stuff for the LaTeX preamble.,0
,0
"'preamble': '',",0
Latex figure (float) alignment,0
,0
"'figure_align': 'htbp',",0
Grouping the document tree into LaTeX files. List of tuples,0
"(source start file, target name, title,",0
"author, documentclass [howto, manual, or own class]).",0
-- Options for manual page output ------------------------------------------,0
One entry per manual page. List of tuples,0
"(source start file, name, description, authors, manual section).",0
-- Options for Texinfo output ----------------------------------------------,0
Grouping the document tree into Texinfo files. List of tuples,0
"(source start file, target name, title, author,",0
"dir menu entry, description, category)",0
-- Options for Epub output -------------------------------------------------,0
Bibliographic Dublin Core info.,0
The unique identifier of the text. This can be a ISBN number,0
or the project homepage.,0
,0
epub_identifier = '',0
A unique identification for the text.,0
,0
epub_uid = '',0
A list of files that should not be packed into the epub file.,0
-- Extension configuration -------------------------------------------------,0
-- Options for intersphinx extension ---------------------------------------,0
Example configuration for intersphinx: refer to the Python standard library.,0
-- Options for todo extension ----------------------------------------------,1
"If true, `todo` and `todoList` produce output, else they produce nothing.",1
##################,0
Global settings #,0
##################,0
Global plotting controls,0
"Control for support size, can control for more",0
#################,0
File utilities #,0
#################,0
#################,0
Plotting utils #,0
#################,0
bias,0
var,0
rmse,0
r2,0
Infer feature dimension,0
Metrics by support plots,0
Authors: Miruna Oprescu <moprescu@microsoft.com>,0
Vasilis Syrgkanis <vasy@microsoft.com>,0
Steven Wu <zhiww@microsoft.com>,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Find the leaf node that this x belongs too and parse the corresponding estimate,0
Safety check,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
Doesn't have sample weights,0
Is a linear model,0
Weighted linear regression,0
Calculates weights,0
Bootstraping has repetitions in tree sample so we need to iterate,0
over all indices,0
Similar for `a` weights,0
normalize weights,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of W and the outcome as,0
"a function of W, using only the train fold",0
Then compute residuals T-g(W) and Y-f(W) on test fold,0
We create fake treatment points from the same distribution as the residuals created during the fit process,0
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average",0
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]",0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Compute coefficient by OLS on residuals,0
"Split the data in half, train and test",0
Fit with LassoCV the treatment as a function of x and the outcome as,0
"a function of x, using only the train fold",0
Then compute residuals p-g(x) and q-q(x) on test fold,0
Estimate multipliers for second order orthogonal method,0
"split the data into two parts: one for splitting, the other for estimation at the leafs",0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
compute the base estimate for the current node using double ml or second order double ml,0
compute the influence functions here that are used for the criterion,0
generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
compute criterion for each proposal,0
if splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
estimate the local parameter at the leaf using the estimate data,0
###################,0
Argument parsing #,0
###################,0
#########################################,0
Parameters constant across experiments #,0
#########################################,0
Outcome support,0
Treatment support,0
Evaluation grid,0
Treatment effects array,0
Other variables,0
##########################,0
Data Generating Process #,0
##########################,0
Log iteration,0
"Generate controls, features, treatment and outcome",0
T and Y residuals to be used in later scripts,0
Save generated dataset,0
#################,0
ORF parameters #,0
#################,0
######################################,0
Train and evaluate treatment effect #,0
######################################,0
########,0
Plots #,0
########,0
###############,0
Save results #,0
###############,0
##############,0
Run Rscript #,0
##############,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Check inputs,0
Train model on controls. Assign higher weight to units resembling,0
treated units.,0
Train model on the treated. Assign higher weight to units resembling,0
control units.,0
Check inputs,0
Check inputs,0
Check inputs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Must make sure indices are merged correctly,0
Require group assignment t to be one-hot-encoded,0
Define an inner function that iterates over group predictions,0
Convert rows to columns,0
Get predictions for the 2 splits,0
Must make sure indices are merged correctly,0
Estimators,0
OrthoTree parameters,0
Tree structure,0
Initialize causal tree parameters,0
Create splits of causal tree,0
Estimate treatment effects at the leafs,0
Compute heterogeneous treatement effect for x's in x_list by finding,0
the corresponding split and associating the effect computed on that leaf,0
Estimators,0
OrthoForest parameters,0
Sub-forests,0
Fit check,0
TODO: Check performance,1
Must normalize weights,0
Crossfitting,0
Compute weighted nuisance estimates,0
Generate subsample indices,0
Safety check,0
Build trees in parallel,0
Calculates weights,0
Bootstraping has repetitions in tree sample,0
Similar for `a` weights,0
Bootstraping has repetitions in tree sample,0
Copy and/or define models,0
Define nuisance estimators,0
Define parameter estimators,0
Define,0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Compute residuals,0
Compute coefficient by OLS on residuals,0
"Parameter returned by LinearRegression is (d_T, )",0
Return moments and gradients,0
Compute residuals,0
Compute moments,0
"Moments shape is (n, d_T)",0
Compute moment gradients,0
Copy and/or define models,0
Nuisance estimators shall be defined during fitting because they need to know the number of distinct,0
treatments,0
Define parameter estimators,0
Define moment and mean gradient estimator,0
Define autoencoder,0
"Check that T is shape (n, )",0
Check T is numeric,0
Train label encoder,0
Define number of classes,0
Call `fit` from parent class,0
"Test that T contains all treatments. If not, return None",0
Nuissance estimates evaluated with cross-fitting,0
Define 2-fold iterator,0
Check if there is only one example of some class,0
No need to crossfit for internal nodes,0
Compute partial moments,0
"If any of the values in the parameter estimate is nan, return None",0
Return moments and gradients,0
Compute partial moments,0
Compute moments,0
"Moments shape is (n, d_T-1)",0
Compute moment gradients,0
Need to calculate this in an elegant way for when propensity is 0,0
This will flatten T,0
Check that T is numeric,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Handle case where Y or T is a vector instead of a 2-dimensional array,0
"TODO: If T is a vector rather than a 2-D array, then the model's fit must accept a vector...",1
"Do we want to reshape to an nx1, or just trust the user's choice of input?",0
(Likewise for Y below),0
NOTE: the fact that we stack X first then W is relied upon,0
by the SparseLinearDMLCateEstimator implementation;,0
"if it's changed here then it needs to be changed there, too",0
TODO: Doing this kronecker/reshaping/transposing stuff so that predict can be called,1
"rather than just using coef_ seems silly, but one benefit is that we can use linear models",0
that don't expose a coef_ (e.g. a GridSearchCV over underlying linear models),0
TODO: handle case where final model doesn't directly expose coef_?,1
"TODO: yuck, is there any way to avoid having to know the structure of XW",1
and the size of X to apply the features here?,0
flatten the matrix features so that we can properly perform the cross product,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################,0
Core DML Tests,0
#######################################,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
How many samples,0
How many control features,0
How many treatment variables,0
Coefficients of how controls affect treatments,0
Coefficients of how controls affect outcome,0
Treatment effects that we want to estimate,0
Run dml estimation,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"this will have dimension (d,) + shape(X)",0
send the first dimension to the end,0
columns are featurized independently; partial derivatives are only non-zero,0
when taken with respect to the same column each time,0
don't fit intercept; manually add column of ones to the data instead;,0
this allows us to ignore the intercept when computing marginal effects,0
two stage approximation,0
"first, get basis expansions of T, X, and Z",0
"regress T expansion on X,Z expansions",0
"predict ft_T from interacted ft_X, ft_Z",0
"dT should be an ndf array (but if T was a vector, or if there is only one feature,",0
dT may be only 2-dimensional),0
promote dT to 3D if necessary (e.g. if T was a vector),0
reshape ft_X and dT to allow cross product (result has shape ndff_x),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make sure to use random seeds wherever necessary,0
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
"unfortunately with the Theano and Tensorflow backends,",0
the straightforward use of K.stop_gradient can cause an error,0
because the parameters of the intermediate layers are now disconnected from the loss;,0
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs,0
so that those layers remain connected but with 0 gradient,0
|| t - mu_i || ^2,0
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2)))),0
Use logsumexp for numeric stability:,0
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d)))),0
TODO: does the numeric stability actually make any difference?,1
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras),0
generate cumulative sum via matrix multiplication,0
"Generate standard uniform values in shape (batch_size,1)",0
"(since we can't use the dynamic batch_size with random.uniform in CNTK,",0
we use uniform_like instead with an input of an appropriate shape),0
convert to floats and multiply to perform equivalent of logical AND,0
"Generate standard normal values in shape (batch_size,1,d_t)",0
"(since we can't use the dynamic batch_size with random.normal in CNTK,",0
we use normal_like instead with an input of an appropriate shape),0
"exactly one entry should be nonzero for each b,d combination; use sum to select it",0
prevent gradient from passing through sampling,0
three options: biased or upper-bound loss require a single number of samples;,0
unbiased can take different numbers for the network and its gradient,0
"sample: (() -> Layer, int) -> Layer",0
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant,0
"the overall computation ensures that we have an interpretable loss (y-h(p,x)),",0
"but also that the gradient is -2(y-h(p,x))h(p,x) with *different* samples used for each average",0
TODO: allow 1D arguments for Y and T,1
the dimensionality of the output of the network,0
TODO: is there a more robust way to do this?,1
TODO: do we need to give the user more control over other arguments to fit?,1
"subtle point: we need to build a new model each time,",0
because each model encapsulates its randomness,0
TODO: do we need to give the user more control over other arguments to fit?,1
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
not a general tensor (because of how backprop works in every framework),0
"(alternatively, we could iterate through the batch in addition to iterating through the output,",0
but this seems annoying...),0
"Therefore, it's important that we use a batch size of 1 when we call predict with this model",0
TODO: any way to get this to work on batches of arbitrary size?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
#######################################################,0
Perfect Data DGPs for Testing Correctness of Code,0
#######################################################,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments that deterministically sum up to,0
zero,0
Re-calibrate epsilon to make sure that empirical distribution of epsilon,0
conditional on each co-variate vector is equal to zero,0
We simply subtract the conditional mean from the epsilons,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect,0
Generate random control co-variates,0
Create epsilon residual treatments,0
Construct treatments as T = X*A + epsilon,0
Construct outcomes as y = X*beta + T*effect + eta,0
Generate random control co-variates,0
Use the same treatment vector for each row,0
Construct outcomes as y = X*beta + T*effect,0
Licensed under the MIT License.,0
"TODO: what if input is sparse? - there's no equivalent to einsum,",1
but tensordot can't be applied to this problem because we don't sum over m,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: any way to avoid creating a copy if the array was already dense?,1
"the call is necessary if the input was something like a list, though",0
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),",0
so convert to pydata sparse first,0
"in the 2D case, we can convert back to scipy sparse; in other cases we can't",0
both inputs were scipy and we can safely convert back to scipy because it's 2D,0
TODO: wouldn't making X1 vary more slowly than X2 be more intuitive?,1
(but note that changing this would necessitate changes to callers,0
to switch the order to preserve behavior where order is important),0
note: in contrast to np.hstack this only works with arrays of dimension at least 2,0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)",0
same number of input definitions as arrays,0
input definitions have same number of dimensions as each array,0
all result indices are unique,0
all result indices must match at least one input index,0
"map indices to all array, axis pairs for that index",0
each index has the same cardinality wherever it appears,0
"State: list of (set of letters, list of (corresponding indices, value))",0
Algo: while list contains more than one entry,0
take two entries,0
sort both lists by intersection of their indices,0
"merge compatible entries (where intersection of indices is equal - in the resulting list,",0
"take the union of indices and the product of values), stepping through each list linearly",0
TODO: might be faster to break into connected components first,1
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,",0
"so compute their content separately, then take cartesian product",0
this would save a few pointless sorts by empty tuples,0
TODO: Consider investigating other performance ideas for these cases,1
where the dense method beat the sparse method (usually sparse is faster),0
"e,facd,c->cfed",0
sparse: 0.0335489,0
dense:  0.011465999999999997,0
"gbd,da,egb->da",0
sparse: 0.0791625,0
dense:  0.007319099999999995,0
"dcc,d,faedb,c->abe",0
sparse: 1.2868097,0
dense:  0.44605229999999985,0
"when indices are repeated within an array, pre-filter the coordinates and data",0
TODO: would using einsum's paths to optimize the order of merging help?,1
Normalize weights,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: allow different subsets for L1 and L2 regularization?,1
TODO: any better way to deal with sparsity?,1
TODO: any better way to deal with sparsity?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Input datasets,0
Estimators,0
Causal tree parameters,0
Tree structure,0
If by splitting we have too small leaves or if we reached the maximum number of splits we stop,0
Create local sample set,0
Compute nuisance estimates for the current node,0
Nuisance estimate cannot be calculated,0
Estimate parameter for current node,0
Node estimate cannot be calculated,0
Calculate moments and gradient of moments for current data,0
Calculate inverse gradient,0
The gradient matrix is not invertible.,0
No good split can be found,0
Calculate point-wise pseudo-outcomes rho,0
Generate random proposals of dimensions to split,0
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen",0
Compute criterion for each proposal,0
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity,0
across parameters. we give some benefit to individual heterogeneity factors for cases,0
where there might be large discontinuities in some parameter as the conditioning set varies,0
If splitting creates valid leafs in terms of mean leaf size,0
Calculate criterion for split,0
Else set criterion to infinity so that this split is not chosen,0
If no good split was found,0
Find split that minimizes criterion,0
Set the split attributes at the node,0
Create child nodes with corresponding subsamples,0
Recursively split children,0
Return parent node,0
No need for a random split since the data is already,0
a random subsample from the original input,0
Estimate the local parameter at the leaf using the estimate data,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
group by product; sum and subtract original; divide by (n_p-1),0
group by product; sum and subtract original; divide by (n_p-1),0
"for now, require one feature per store/product combination",0
TODO: would be nice to relax this somehow,1
"alphas vary by product, not by store",0
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc.",0
"one cross-price term per product, which is based on the average price",0
of all other goods sold at the same store in the same week,0
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc.",0
store-specific and product-specific gammas and betas (which are positively correlated),0
"features: product dummies, store dummies",0
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)",0
"observe n_products * n_stores prices, same number of quantities",0
"for direct regression comparisons, we need a pivoted version",0
"""treatments"" for direct regression include treatments, plus treatments interacted with product dummies,",0
"plus the same for ""group treatments"" (average treatment of other products in the same store/week)",0
"for direct regression, we also need to append the features",0
"(both the ""constant features"" as well as the normal ones)",0
NOTE: need to set cv because default generic algorithm is super slow for sparse matrices,0
"alphas vary by product, not by store",0
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc.",0
store-specific and product-specific gammas,0
store-specific and product-specific betas,0
"features: product dummies, store dummies",0
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)",0
we need only the prices for the compound model; all dummies are created internally,0
"observe n_products * n_stores prices, same number of quantities",0
"simple results include treatments, plus treatments interacted with product dummies,",0
for use with the direct methods,0
X should have 0 columns; we will instead pivot Y and fit against the features passed into the constructor,0
underspecified model,0
Y = alpha T + \sum_i alpha_i T_i + X beta + eta,0
T = X gamma + eps,0
how to score? distance from line of all solutions?,0
"given that 0, a, b, c, d is equivalent to x, a-x, b-x, c-x, d-x, compute the error",0
"baselines: ridge, ridge-like (penalize alpha_i but not alpha_baseline or beta)",0
"comparison: 2ml (OLS for T on X, OLS for Y on XxX^e, ridge or ridge-like for alphas)",0
"features: one product dummy, one store dummy (each missing one level), constant",0
"alphas vary by product, not by store",0
store-specific and product-specific gammas,0
store-specific and product-specific betas,0
"features: product dummies, store dummies",0
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)",0
"columns: prices interacted with products (and constant), features",0
"observe n_products * n_stores prices, same number of quantities",0
use features starting at index 1+n_products to skip all prices,0
"pickleFile = open('pickledSparse_{0}_{1}_{2}_{3}.pickle'.format(n_exp, n_products, n_stores, n_weeks), 'rb')",0
"alphass, ridges, lassos, doubleMls = pickle.load(pickleFile)",0
pickleFile.close(),0
#############################################,0
Defining the parameters of Monte Carlo,0
#############################################,0
Estimation parameters,0
###################################################################,0
Estimating the parameters of the DGP with DML. Running multiple,0
Monte Carlo experiments.,0
###################################################################,0
Sparse coefficients of treatment as a function of co-variates,0
Coefficients of outcomes as a function of co-variates,0
"DGP. Create samples of data (y, T, X) from known truth",0
DML Estimation.,0
Estimation with other methods for comparison,0
#########################################################,0
Plotting the results and saving,0
#########################################################,0
"plt.figure(figsize=(20, 10))",0
"plt.subplot(1, 4, 1)",0
"plt.title(""DML R^2: median {:.3f}, mean {:.3f}"".format(np.median(dml_r2score), np.mean(dml_r2score)))",0
plt.hist(dml_r2score),0
"plt.subplot(1, 4, 2)",0
"plt.title(""Direct Lasso R^2: median {:.3f}, mean {:.3f}"".format(np.median(direct_r2score),",0
np.mean(direct_r2score))),0
plt.hist(direct_r2score),0
"plt.subplot(1, 4, 3)",0
"plt.title(""DML Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(dml_te),",0
np.std(dml_te))),0
plt.hist(np.array(dml_te).flatten()),0
"plt.subplot(1, 4, 4)",0
"plt.title(""Direct Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(direct_te),",0
np.std(direct_te))),0
plt.hist(np.array(direct_te).flatten()),0
plt.tight_layout(),0
"plt.savefig(""r2_comparison.png"")",0
Plotting the results and saving,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: Add a __dir__ implementation?,1
TODO: what if some args can be None?,1
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls",0
"if the attribute exists on the wrapped object once we remove the suffix,",0
then we should be computing a confidence interval for the wrapped calls,0
"collect extra arguments and pass them through, if the wrapped attribute was callable",0
don't pass extra arguments if the wrapped attribute wasn't callable to begin with,0
Remove children with nonwhite mothers from the treatment group,0
Remove children with nonwhite mothers from the treatment group,0
Select columns,0
Scale the numeric variables,0
"Change the binary variable 'first' takes values in {1,2}",0
Append a column of ones as intercept,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
TODO: make this test actually test something instead of generating images,1
Sparse coefficients of treatment as a function of co-variates,0
Coefficients of outcomes as a function of co-variates,0
"DGP. Create samples of data (y, T, X) from known truth",0
"DGP. Create samples of data (y, T, X) from known truth",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
simple DGP only for illustration,0
Define the treatment model neural network architecture,0
"This will take the concatenation of one-dimensional values z and x as input,",0
"so the input shape is (d_z + d_x,)",0
The exact shape of the final layer is not critical because the Deep IV framework will,0
add extra layers on top for the mixture density network,0
Define the response model neural network architecture,0
"This will take the concatenation of one-dimensional values t and x as input,",0
"so the input shape is (d_t + d_x,)",0
"The output should match the shape of y, so it must have shape (d_y,) in this case",0
"NOTE: For the response model, it is important to define the model *outside*",0
"of the lambda passed to the DeepIvEstimator, as we do here,",0
so that the same weights will be reused in each instantiation,0
number of samples to use in second estimate of the response,0
(to make loss estimate unbiased),0
Keras optimizer to use for training - see https://keras.io/optimizers/,0
do something with predictions...,0
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
test = True ensures we draw test set images,0
test = True ensures we draw test set images,0
re-draw to get new independent treatment and implied response,0
we need to make sure z _never_ does anything in these g functions (fitted and true),0
above is necesary so that reduced form doesn't win,0
covariates: time and emotion,0
random instrument,0
z -> price,0
true observable demand function,0
errors,0
response,0
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)",0
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])",0
For some reason this doesn't work at all when run against the CNTK backend...,1
"model.compile('nadam', loss=lambda _,l:l)",0
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)",0
generate a valiation set,0
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T",0
convex combinations of semidefinite covariance matrices are themselves semidefinite,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that we can do the same thing once we provide percentile bounds,0
test that we can fit with the same arguments as the base estimator,0
"test that we can get the same attribute for the bootstrap as the original, with the same shape",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that we can do the same thing once we provide percentile bounds,0
"test that we can do the same thing with the results of a method, rather than an attribute",0
"test that we can get an interval for the same attribute for the bootstrap as the original,",0
with the same shape for the lower and upper bounds,0
test that we can do the same thing once we provide percentile bounds,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
number of inputs in specification must match number of inputs,0
must have an output,0
output indices must be unique,0
output indices must be present in an input,0
number of indices must match number of dimensions for each input,0
repeated indices must always have consistent sizes,0
transpose,0
tensordot,0
trace,0
TODO: set up proper flag for this,1
pick indices at random with replacement from the first 7 letters of the alphabet,0
"of all of the distinct indices that appear in any input,",0
pick a random subset of them (of size at most 5) to appear in the output,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Preprocess data,0
Convert 'week' to a date,0
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")",0
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero",0
Take log of price,0
Make brand numeric,0
"remove meaningless features (e.g. cross-price effects of products on themselves),",0
which have all zero coeffs,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"first polynomials are 1, x, x*x-1, x*x*x-3*x",0
"first derivitaves are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)",0
TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Generate data,0
DGP constants,0
Test data,0
Constant treatment effect and propensity,0
Heterogeneous treatment and propensity,0
TLearner test,0
Instantiate TLearner,0
Test inputs,0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Instantiate SLearner,0
Test inputs,0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Need interactions between T and features,0
Instantiate XLearner,0
Test inputs,0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Instantiate DomainAdaptationLearner,0
Test inputs,0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Instantiate DomainAdaptationLearner,0
Test inputs,0
Test constant treatment effect,0
Test heterogeneous treatment effect,0
Fit learner and get the effect,0
Get the true treatment effect,0
Compute treatment effect residuals (absolute),0
Check that at least 90% of predictions are within tolerance interval,0
Check that one can pass in regular lists,0
Check that it fails correctly if lists of different shape are passed in,0
Check that it fails when T contains values other than 0 and 1,0
"Check that it works when T, Y have shape (n, 1)",0
learner_instance.effect(TestMetalearners.X_test),0
Generate covariates,0
Generate treatment,0
Calculate outcome,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
DGP constants,0
Generate data,0
Test data,0
Remove warnings that might be raised by the models passed into the ORF,0
Generate data with continuous treatments,0
Instantiate model with most of the default parameters,0
Test inputs for continuous treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
Check that outputs have the correct shape,0
Test continuous treatments with controls,0
Test continuous treatments without controls,0
Generate data with binary treatments,0
Instantiate model with default params,0
Test inputs for binary treatments,0
--> Check that one can pass in regular lists,0
--> Check that it fails correctly if lists of different shape are passed in,0
"--> Check that it works when T, Y have shape (n, 1)",0
"--> Check that it fails correctly when T has shape (n, 2)",0
--> Check that it fails correctly when the treatments are not numeric,0
Check that outputs have the correct shape,0
Test binary treatments with controls,0
Test binary treatments without controls,0
Only applicable to continuous treatments,0
Generate data for 2 treatments,0
Test multiple treatments with controls,0
Compute the treatment effect on test points,0
Compute treatment effect residuals,0
Multiple treatments,0
Allow at most 10% test points to be outside of the tolerance interval,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
add column of ones to X,0
"for each row, create the d_y*d_t*(d_x+1) features (which are matrices of size d_y by d_t)",0
all solutions to underdetermined (or exactly determined) Ax=b are given by Ab+(I-AA)w for some arbitrary w,0
"note that if Ax=b is overdetermined, this will raise an assertion error",0
just make sure we can call the marginal_effect and effect methods,0
just make sure we can call the marginal_effect and effect methods,0
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]",0
"to correctly recover coefficients for Y via OLS, we need ([X; W][1; (X); W]) e_y =",0
-([X; W][1; (X); W]) (((X)e_t)a_X+(We_t)a_W),0
"then, to correctly recover a in the third stage, we additionally need ((X)e_t) e_y = 0",0
TODO: it seems like roughly 20% of the calls to _test_sparse are failing - find out what's going wrong,1
sparse test case: heterogeneous effect by product,0
need at least as many rows in e_y as there are distinct columns,0
in [X;XW;WW;Xe_t] to find a solution for e_t,0
note that this would fail for the non-sparse DMLCateEstimator,0
recover simple features by initializing complex features appropriately,0
using full set of matrix features should be equivalent to using non-matrix featurizer,0
